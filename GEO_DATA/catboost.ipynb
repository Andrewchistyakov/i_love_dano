{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import h3\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostRegressor, Pool, cv\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import rasterio\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import rasterio\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b707c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# def add_multiple_kmeans_clusters(train_path: str, test_path: str, random_state: int = 42):\n",
    "#     train_df = pd.read_csv(train_path)\n",
    "#     test_df = pd.read_csv(test_path)\n",
    "\n",
    "#     feature_cols = [c for c in train_df.columns if c.startswith('feat_')]\n",
    "#     X_train = train_df[feature_cols].values\n",
    "#     X_test = test_df[feature_cols].values\n",
    "\n",
    "#     cluster_counts = [2, 10, 20]\n",
    "\n",
    "#     for n_clusters in cluster_counts:\n",
    "#         print(f\"Обучаем KMeans для k={n_clusters}...\")\n",
    "#         kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init='auto')\n",
    "#         kmeans.fit(X_train)\n",
    "\n",
    "#         train_df[f'cluster_{n_clusters}'] = pd.Categorical(kmeans.predict(X_train))\n",
    "#         test_df[f'cluster_{n_clusters}'] = pd.Categorical(kmeans.predict(X_test))\n",
    "\n",
    "#     train_df.to_csv('train_with_multi_clusters.csv', index=False)\n",
    "#     test_df.to_csv('test_with_multi_clusters.csv', index=False)\n",
    "\n",
    "#     print(\"Файлы сохранены: train_with_multi_clusters.csv и test_with_multi_clusters.csv\")\n",
    "\n",
    "# add_multiple_kmeans_clusters(\"train_bert_features.csv\", \"test_bert_features.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7751f146",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "test = pd.read_csv(\"test.tsv\", sep='\\t')\n",
    "reviews = pd.read_csv(\"reviews.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c4f3e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['lon', 'lat']] = test['coordinates'].apply(lambda x: pd.Series(eval(x)))\n",
    "train[['lon', 'lat']] = train['coordinates'].apply(lambda x: pd.Series(eval(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9f99732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['coordinates'], inplace=True)\n",
    "test.drop(columns=['coordinates'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8085d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_custom_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    def ratio(a, b): \n",
    "        return np.where(b != 0, a / b, 0)\n",
    "\n",
    "    df['traffic_density_ratio'] = ratio(df['traffic_300m'], df['traffic_1000m'])\n",
    "    df['homes_density_ratio'] = ratio(df['homes_300m'], df['homes_1000m'])\n",
    "    df['works_density_ratio'] = ratio(df['works_300m'], df['works_1000m'])\n",
    "    df['female_ratio'] = ratio(df['female_300m'], df['female_1000m'])\n",
    "    df['employed_ratio'] = ratio(df['employed_300m'], df['unemployed_300m'] + 1)\n",
    "    df['children_ratio'] = ratio(df['has_children_300m'], df['no_children_300m'] + 1)\n",
    "\n",
    "    df['mean_age_index'] = (\n",
    "        17 * df['age_<17_300m'] + 21 * df['age_18-24_300m'] +\n",
    "        30 * df['age_25-34_300m'] + 40 * df['age_35-44_300m'] +\n",
    "        50 * df['age_45-54_300m'] + 60 * df['age_>55_300m']\n",
    "    ) / (\n",
    "        df[['age_<17_300m','age_18-24_300m','age_25-34_300m',\n",
    "            'age_35-44_300m','age_45-54_300m','age_>55_300m']].sum(axis=1) + 1\n",
    "    )\n",
    "\n",
    "    df['income_balance_index'] = (\n",
    "        df['premium_income_300m'] * 3 + df['high_income_300m'] * 2 +\n",
    "        df['above_average_income_300m'] - df['below_average_income_300m']\n",
    "    )\n",
    "    df['education_index'] = (\n",
    "        df['higher_education_300m'] * 3 +\n",
    "        df['secondary_education_300m'] * 2 -\n",
    "        df['no_higher_education_300m']\n",
    "    )\n",
    "\n",
    "    df['food_index'] = (\n",
    "        df['restaurants_cafes_bars_300m'] + df['food_delivery_300m'] +\n",
    "        df['pizza_delivery_300m'] + df['sushi_delivery_300m'] +\n",
    "        df['coffee_shops_300m'] + df['grocery_stores_300m']\n",
    "    )\n",
    "    df['shopping_index'] = (\n",
    "        df['online_shops_300m'] + df['clothing_shoes_accessories_300m'] +\n",
    "        df['home_goods_300m'] + df['beauty_salons_300m'] +\n",
    "        df['cosmetics_and_perfumes_300m'] + df['mens_clothing_300m'] +\n",
    "        df['womens_clothing_300m']\n",
    "    )\n",
    "    df['health_index'] = (\n",
    "        df['clinics_300m'] + df['pharmacy_300m'] +\n",
    "        df['beauty_and_health_devices_300m'] + df['childrens_medicine_300m']\n",
    "    )\n",
    "    df['education_activity_index'] = (\n",
    "        df['education_300m'] + df['courses_300m'] + df['language_courses_300m']\n",
    "    )\n",
    "\n",
    "    df['food_to_shops_ratio'] = ratio(df['food_index'], df['shopping_index'] + 1)\n",
    "    df['health_to_education_ratio'] = ratio(df['health_index'], df['education_activity_index'] + 1)\n",
    "    df['cafe_to_population_ratio'] = ratio(df['restaurants_cafes_bars_300m'], df['homes_300m'] + 1)\n",
    "    df['shops_to_population_ratio'] = ratio(df['shopping_index'], df['homes_300m'] + 1)\n",
    "\n",
    "    df['car_enthusiasm_index'] = (\n",
    "        df['car_owners_300m'] + df['car_services_300m'] +\n",
    "        df['car_parts_300m'] + df['car_market_300m'] +\n",
    "        df['interest_in_buying_new_car_300m']\n",
    "    )\n",
    "    df['public_transport_ratio'] = ratio(df['train_ticket_order_300m'], df['car_owners_300m'] + 1)\n",
    "\n",
    "    df['leisure_index'] = (\n",
    "        df['leisure_and_entertainment_300m'] + df['sports_300m'] +\n",
    "        df['cultural_leisure_events_300m'] + df['movies_and_series_300m'] +\n",
    "        df['restaurants_cafes_300m'] + df['bars_300m']\n",
    "    )\n",
    "    df['culture_to_leisure_ratio'] = ratio(df['culture_300m'], df['leisure_index'] + 1)\n",
    "\n",
    "    df['female_to_male_ratio'] = ratio(df['female_300m'], df['male_300m'] + 1)\n",
    "    df['married_to_unmarried_ratio'] = ratio(df['married_300m'], df['not_married_300m'] + 1)\n",
    "    df['employed_to_population_ratio'] = ratio(df['employed_300m'], df['homes_300m'] + 1)\n",
    "    df['income_per_home'] = ratio(df['mean_income_300m'], df['homes_300m'] + 1)\n",
    "\n",
    "    df['traffic_per_shop'] = ratio(df['traffic_300m'], df['shopping_index'] + 1)\n",
    "    df['traffic_per_food'] = ratio(df['traffic_300m'], df['food_index'] + 1)\n",
    "    df['traffic_per_leisure'] = ratio(df['traffic_300m'], df['leisure_index'] + 1)\n",
    "    df['shops_density_1000_to_300'] = ratio(df['shopping_index'], df['online_shops_1000m'] + 1)\n",
    "\n",
    "    df['total_commercial_index'] = (\n",
    "        df['food_index'] + df['shopping_index'] +\n",
    "        df['health_index'] + df['education_activity_index']\n",
    "    )\n",
    "    df['social_activity_index'] = (\n",
    "        df['culture_300m'] + df['society_300m'] + df['politics_300m'] + df['events_300m']\n",
    "    )\n",
    "    df['digital_activity_index'] = (\n",
    "        df['online_video_300m'] + df['computer_games_300m'] + df['manga_300m'] +\n",
    "        df['anime_300m'] + df['doramas_300m'] + df['k-pop_300m']\n",
    "    )\n",
    "\n",
    "    if 'lat' in df.columns and 'lon' in df.columns:\n",
    "        df['lat_bin'] = pd.cut(df['lat'], bins=50, labels=False)\n",
    "        df['lon_bin'] = pd.cut(df['lon'], bins=50, labels=False)\n",
    "        df['location_cluster'] = (df['lat_bin'] * 100 + df['lon_bin']).astype(int)\n",
    "        df['lat_lon_sum'] = df['lat'] + df['lon']\n",
    "        df['lat_lon_diff'] = df['lat'] - df['lon']\n",
    "\n",
    "    for col in ['traffic_300m', 'homes_300m', 'works_300m', 'food_index', 'shopping_index']:\n",
    "        df[f'log_{col}'] = np.log1p(df[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "960b40d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = apply_custom_features(train)\n",
    "test = apply_custom_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72801030",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert_features = pd.read_csv(\"train_bert_features.csv\")\n",
    "train = train.merge(train_bert_features, on=\"id\", how=\"left\")\n",
    "test_bert_features = pd.read_csv(\"test_bert_features.csv\")\n",
    "test = test.merge(test_bert_features, on=\"id\", how=\"left\")\n",
    "# intfloat_features = pd.read_csv(\"intfloat_features.csv\")\n",
    "# train = train.merge(intfloat_features, on=\"id\", how=\"left\")\n",
    "# test = test.merge(intfloat_features, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5b72ffb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knn_features(train_df, test_df, feature_prefix=\"feat_\", target_col=\"target\", n_neighbors=20):\n",
    "    feat_cols = [col for col in train_df.columns if col.startswith(feature_prefix)]\n",
    "    \n",
    "    valid_train = ~train_df[feat_cols].isna().any(axis=1)\n",
    "    valid_test = ~test_df[feat_cols].isna().any(axis=1)\n",
    "    \n",
    "    model = NearestNeighbors(n_neighbors=n_neighbors, metric=\"cosine\")\n",
    "    model.fit(train_df.loc[valid_train, feat_cols])\n",
    "    \n",
    "    for stat_name in [\"mean\", \"std\", \"min\", \"max\"]:\n",
    "        train_df[f\"knn_target_{stat_name}\"] = np.nan\n",
    "        test_df[f\"knn_target_{stat_name}\"] = np.nan\n",
    "    \n",
    "    dists, idxs = model.kneighbors(train_df.loc[valid_train, feat_cols])\n",
    "    neigh_targets = train_df.loc[valid_train, target_col].values[idxs]\n",
    "    train_df.loc[valid_train, \"knn_target_mean\"] = neigh_targets.mean(axis=1)\n",
    "    train_df.loc[valid_train, \"knn_target_std\"] = neigh_targets.std(axis=1)\n",
    "    train_df.loc[valid_train, \"knn_target_min\"] = neigh_targets.min(axis=1)\n",
    "    train_df.loc[valid_train, \"knn_target_max\"] = neigh_targets.max(axis=1)\n",
    "    \n",
    "    dists, idxs = model.kneighbors(test_df.loc[valid_test, feat_cols])\n",
    "    neigh_targets = train_df.loc[valid_train, target_col].values[idxs]\n",
    "    test_df.loc[valid_test, \"knn_target_mean\"] = neigh_targets.mean(axis=1)\n",
    "    test_df.loc[valid_test, \"knn_target_std\"] = neigh_targets.std(axis=1)\n",
    "    test_df.loc[valid_test, \"knn_target_min\"] = neigh_targets.min(axis=1)\n",
    "    test_df.loc[valid_test, \"knn_target_max\"] = neigh_targets.max(axis=1)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "train, test = add_knn_features(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2bbc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features = pd.read_csv(\"id_features.csv\")\n",
    "train = train.merge(tfidf_features, on=\"id\", how=\"left\")\n",
    "test = test.merge(tfidf_features, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "526e887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R_EARTH = 6_371_000.0\n",
    "\n",
    "def _safe_div(a, b):\n",
    "    b = np.asarray(b)\n",
    "    return np.where(b != 0, np.asarray(a) / b, np.nan)\n",
    "\n",
    "METRO_STATIONS = [\n",
    "    (\"Боровицкая\",55.75034,37.60857), (\"Тверская\",55.7652,37.60352),\n",
    "    (\"Каширская\",55.65412,37.64738), (\"Сокол\",55.80518,37.51495),\n",
    "    (\"Южная\",55.62122,37.60752), (\"Митино (стр.)\",55.84589,37.35909),\n",
    "    (\"Новокузнецкая\",55.74212,37.62901), (\"Проспект Вернадского\",55.67613,37.5045),\n",
    "    (\"ВДНХ\",55.82177,37.64107), (\"Динамо\",55.78867,37.55936),\n",
    "    (\"Щелковская\",55.80955,37.79884), (\"Театральная\",55.75857,37.6177),\n",
    "    (\"Савеловская\",55.79421,37.58666), (\"Царицино\",55.62011,37.66939),\n",
    "    (\"Волгоградский проспект\",55.7243,37.68795), (\"Пушкинская\",55.76565,37.60417),\n",
    "    (\"Китай город\",55.75634,37.63002), (\"Беговая\",55.77378,37.54412),\n",
    "    (\"Рижская\",55.79222,37.63557), (\"Беляево\",55.64371,37.52762),\n",
    "    (\"Крылатское\",55.75879,37.40633), (\"Электрозаводская\",55.78177,37.70471),\n",
    "    (\"Полянка\",55.73654,37.61856), (\"Аннино (стр.)\",55.581818,37.594978),\n",
    "    (\"Домодедовская\",55.61009,37.71612), (\"Измайловская\",55.78768,37.78329),\n",
    "    (\"Тургеневская\",55.7646,37.63623), (\"Перово\",55.75109,37.78854),\n",
    "    (\"Новослободская\",55.77921,37.6009), (\"Маяковская\",55.76909,37.59635),\n",
    "    (\"Комсомольская\",55.77717,37.655689), (\"Авиамоторная\",55.75208,37.71677),\n",
    "    (\"Улица 1905 года\",55.76355,37.56375), (\"Отрадное\",55.86417,37.60488),\n",
    "    (\"Калужская\",55.65566,37.53923), (\"Аэропорт\",55.79981,37.53412),\n",
    "    (\"Парк Победы (стр.)\",55.736559,37.512591), (\"Шаболовская\",55.71886,37.60797),\n",
    "    (\"Октябрьское поле\",55.793615,37.493496), (\"Войковская\",55.81811,37.49905),\n",
    "    (\"Киевская\",55.74388,37.56673), (\"Фили\",55.74673,37.51384),\n",
    "    (\"Александровский сад\",55.75219,37.60836), (\"Марьина роща (стр.)\",55.793602,37.615762),\n",
    "    (\"Парк культуры\",55.73512,37.59328), (\"Таганская\",55.74255,37.65389),\n",
    "    (\"Лубянка\",55.75876,37.62573), (\"Октябрьская\",55.729,37.61139),\n",
    "    (\"Печатники\",55.69252,37.7295), (\"Тимирязевская\",55.81842,37.57571),\n",
    "    (\"Юго-западная\",55.66464,37.48421), (\"Владыкино\",55.84669,37.59251),\n",
    "    (\"Орехово\",55.61214,37.69584), (\"Цветной бульвар\",55.7716,37.62058),\n",
    "    (\"Баррикадная\",55.76027,37.58111), (\"Павелецкая\",55.7313,37.63612),\n",
    "    (\"Теплый стан\",55.61814,37.50814), (\"Черкизовская\",55.802,37.74438),\n",
    "    (\"Каширская\",55.65412,37.64738), (\"Академическая\",55.68808,37.57501),\n",
    "    (\"Бибирево\",55.88294,37.60523), (\"Кантемировская\",55.6343,37.65632),\n",
    "    (\"Волоколамская (стр.)\",55.83459,37.38367), (\"Новогиреево\",55.75111,37.81564),\n",
    "    (\"Южнопортовая (нов.)\",55.70622,37.68899),\n",
    "    (\"Рассказовка\",55.63282,37.33274),\n",
    "    (\"Косино\",55.7029,37.8513),\n",
    "    (\"Нижегородская\",55.7325,37.7287),\n",
    "    (\"Беломорская\",55.8647,37.4743),\n",
    "    (\"Мичуринский проспект\",55.6878,37.4906),\n",
    "    (\"Ломоносовский проспект\",55.6982,37.5191),\n",
    "    (\"Хорошевская\",55.7767,37.5251),\n",
    "    (\"Окружная\",55.8489,37.5732),\n",
    "    (\"Зюзино\",55.6568,37.5821)\n",
    "]\n",
    "\n",
    "\n",
    "metro_df = pd.DataFrame(METRO_STATIONS, columns=[\"name\",\"lat\",\"lon\"])\n",
    "\n",
    "def enrich_with_metro_features(train_df, test_df, metro_df,\n",
    "                               lat_col=\"lat\", lon_col=\"lon\",\n",
    "                               ks=(1, 3, 5), radii=(300, 500, 1000),\n",
    "                               prefix=\"metro\"):\n",
    "\n",
    "    if metro_df is None or metro_df.empty:\n",
    "        print(f\"[{prefix}] dataset is empty — skipped\")\n",
    "        return\n",
    "\n",
    "    lat_vals = pd.to_numeric(metro_df[\"lat\"], errors=\"coerce\").to_numpy()\n",
    "    lon_vals = pd.to_numeric(metro_df[\"lon\"], errors=\"coerce\").to_numpy()\n",
    "    mask = np.isfinite(lat_vals) & np.isfinite(lon_vals)\n",
    "    coords_rad = np.vstack((np.deg2rad(lat_vals[mask]), np.deg2rad(lon_vals[mask]))).T\n",
    "\n",
    "    if coords_rad.size == 0:\n",
    "        print(f\"[{prefix}] no valid coordinates — skipped\")\n",
    "        return\n",
    "\n",
    "    tree = BallTree(coords_rad, metric=\"haversine\")\n",
    "\n",
    "    def process_one(df):\n",
    "        df_lat = pd.to_numeric(df[lat_col], errors=\"coerce\").to_numpy()\n",
    "        df_lon = pd.to_numeric(df[lon_col], errors=\"coerce\").to_numpy()\n",
    "        ok = np.isfinite(df_lat) & np.isfinite(df_lon)\n",
    "\n",
    "        sample_coords = np.zeros((len(df), 2))\n",
    "        sample_coords[:, 0] = np.deg2rad(np.where(ok, df_lat, 0.0))\n",
    "        sample_coords[:, 1] = np.deg2rad(np.where(ok, df_lon, 0.0))\n",
    "\n",
    "        for k in ks:\n",
    "            k_use = min(k, coords_rad.shape[0])\n",
    "            dist, _ = tree.query(sample_coords[ok], k=k_use)\n",
    "            dist_m = dist * R_EARTH\n",
    "\n",
    "            if k_use == 1:\n",
    "                dmin = np.full(len(df), np.nan, dtype=np.float32)\n",
    "                dmin[ok] = dist_m[:, 0]\n",
    "                df[f\"{prefix}_dist_min_m\"] = dmin\n",
    "\n",
    "            dmean = np.full(len(df), np.nan, dtype=np.float32)\n",
    "            dmean[ok] = dist_m.mean(axis=1)\n",
    "            df[f\"{prefix}_mean_k{k_use}_m\"] = dmean\n",
    "\n",
    "        for r in radii:\n",
    "            nearby = tree.query_radius(sample_coords[ok], r / R_EARTH)\n",
    "            counts = np.array([len(v) for v in nearby], dtype=np.int32)\n",
    "            out = np.full(len(df), np.nan, dtype=np.float32)\n",
    "            out[ok] = counts\n",
    "            df[f\"{prefix}_cnt_r{r}\"] = out\n",
    "\n",
    "    for d in (train_df, test_df):\n",
    "        process_one(d)\n",
    "\n",
    "enrich_with_metro_features(train, test, metro_df, ks=(1, 3, 5), radii=(300, 500, 1000), prefix=\"metro\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "25d8b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "KREMLIN_CENTER_LAT, KREMLIN_CENTER_LON = 55.752023, 37.617499\n",
    "\n",
    "\n",
    "def add_center_angle_features(train_df, test_df, n_sectors=16):\n",
    "    for df in (train_df, test_df):\n",
    "        lat = pd.to_numeric(df[\"lat\"], errors=\"coerce\").values\n",
    "        lon = pd.to_numeric(df[\"lon\"], errors=\"coerce\").values\n",
    "        ang = np.arctan2(lat - KREMLIN_CENTER_LAT, lon - KREMLIN_CENTER_LON)\n",
    "        ang[~np.isfinite(ang)] = np.nan\n",
    "        df[\"center_angle_sin\"] = np.sin(ang).astype(\"float32\")\n",
    "        df[\"center_angle_cos\"] = np.cos(ang).astype(\"float32\")\n",
    "        sec = np.floor(((ang + np.pi) / (2*np.pi)) * n_sectors)\n",
    "        df[f\"center_sector_{n_sectors}\"] = sec.astype(\"float32\")\n",
    "\n",
    "add_center_angle_features(train, test, n_sectors=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fd649f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_category_diversity(train_df, test_df, radii=(200,300)):\n",
    "    all_df = pd.concat([train_df[[\"lat\",\"lon\",\"category\"]],\n",
    "                        test_df[[\"lat\",\"lon\",\"category\"]]], ignore_index=True)\n",
    "    lat = pd.to_numeric(all_df[\"lat\"], errors=\"coerce\").values\n",
    "    lon = pd.to_numeric(all_df[\"lon\"], errors=\"coerce\").values\n",
    "    cats = all_df[\"category\"].astype(str).fillna(\"Unknown\").values\n",
    "    ok = np.isfinite(lat) & np.isfinite(lon)\n",
    "    X = np.c_[np.deg2rad(np.where(ok, lat, 0.0)), np.deg2rad(np.where(ok, lon, 0.0))]\n",
    "    tree = BallTree(X[ok], metric=\"haversine\")\n",
    "\n",
    "    for r in radii:\n",
    "        ent = np.full(len(all_df), np.nan, dtype=np.float32)\n",
    "        hhi = np.full(len(all_df), np.nan, dtype=np.float32)\n",
    "        ind = tree.query_radius(X[ok], r=r/R_EARTH, return_distance=False)\n",
    "        ok_idx = np.where(ok)[0]\n",
    "        for i, nb_local in enumerate(ind):\n",
    "            gidx = ok_idx[i]\n",
    "            nb = ok_idx[nb_local]\n",
    "            nb = nb[nb != gidx]\n",
    "            if nb.size == 0: \n",
    "                ent[i] = np.nan; hhi[i] = np.nan; continue\n",
    "            vals, cnts = np.unique(cats[nb], return_counts=True)\n",
    "            p = cnts / cnts.sum()\n",
    "            ent[i] = float(-(p * np.log(p + 1e-12)).sum())       # Shannon\n",
    "            hhi[i] = float((p**2).sum())                         # Herfindahl\n",
    "        # раскладываем по train/test\n",
    "        ntr = len(train_df)\n",
    "        train_df[f\"cat_entropy_r{r}\"] = ent[:ntr]\n",
    "        test_df[f\"cat_entropy_r{r}\"]  = ent[ntr:]\n",
    "        train_df[f\"cat_hhi_r{r}\"]     = hhi[:ntr]\n",
    "        test_df[f\"cat_hhi_r{r}\"]      = hhi[ntr:]\n",
    "\n",
    "add_category_diversity(train, test, radii=(200,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dfae4786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import folium\n",
    "# from folium.plugins import FastMarkerCluster\n",
    "\n",
    "# def df_to_map(df, lon_col=\"lon\", lat_col=\"lat\", output_html=\"map.html\"):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     if lon_col not in df.columns or lat_col not in df.columns:\n",
    "#         raise KeyError(f\"DataFrame должен содержать колонки '{lon_col}' и '{lat_col}'\")\n",
    "\n",
    "#     df_clean = df.copy()\n",
    "#     df_clean = df_clean.dropna(subset=[lon_col, lat_col])\n",
    "#     df_clean = df_clean[\n",
    "#         (df_clean[lon_col].between(-180, 180)) & \n",
    "#         (df_clean[lat_col].between(-90, 90))\n",
    "#     ]\n",
    "\n",
    "#     points = df_clean[[lat_col, lon_col]].values.tolist()\n",
    "\n",
    "#     m = folium.Map(location=[0, 0], zoom_start=2, tiles=\"CartoDB positron\", control_scale=True)\n",
    "\n",
    "#     FastMarkerCluster(points).add_to(m)\n",
    "\n",
    "#     if points:\n",
    "#         lats = [p[0] for p in points]\n",
    "#         lons = [p[1] for p in points]\n",
    "#         m.fit_bounds([[min(lats), min(lons)], [max(lats), max(lons)]])\n",
    "\n",
    "#     m.save(output_html)\n",
    "#     return output_html\n",
    "\n",
    "\n",
    "# df_to_map(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1264d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "moscow_center = (55.751244, 37.618423)\n",
    "def calculate_distance(lat, lon):\n",
    "    return geodesic(moscow_center, (lat, lon)).meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "149b8105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_h3_indx(lat, long, size):\n",
    "    h3_address = h3.latlng_to_cell(lat, long,  size)      \n",
    "    return h3_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a65726ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Расстояние по сфере (км) между точками 1 и 2.\"\"\"\n",
    "    R = 6371.0088\n",
    "    dlat = np.radians(lat2 - lat1)\n",
    "    dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2229e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_geo_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    result_df = df.copy()\n",
    "\n",
    "    for size, name in [\n",
    "        (11, 'small'),\n",
    "        (9, 'mid'),\n",
    "        (8, 'norm'),\n",
    "        (7, 'large'),\n",
    "        (6, 'super_large')\n",
    "    ]:\n",
    "        col_name = f\"h3_first_indx_{name}\"\n",
    "        result_df[col_name] = result_df[['lon', 'lat']].apply(\n",
    "            lambda row: take_h3_indx(*row, size), axis=1\n",
    "        )\n",
    "\n",
    "    result_df['distance_to_moscow'] = result_df[['lat', 'lon']].apply(\n",
    "        lambda row: calculate_distance(*row), axis=1\n",
    "    )\n",
    "\n",
    "    result_df['is_moscow'] = (\n",
    "        result_df['address'].str.contains('Москва', na=False)\n",
    "    )\n",
    "\n",
    "    coords = list(zip(result_df[\"lon\"], result_df[\"lat\"]))\n",
    "    with rasterio.open(\"rus_pd_2020_1km.tif\") as dataset:\n",
    "        densities = [\n",
    "            (val[0] if val[0] != dataset.nodata else np.nan)\n",
    "            for val in tqdm(dataset.sample(coords), total=len(coords), desc=\"Processing density\")\n",
    "        ]\n",
    "    result_df[\"rasterio_density\"] = densities\n",
    "    result_df.loc[result_df[\"rasterio_density\"] < 0, \"rasterio_density\"] = np.nan\n",
    "\n",
    "    with rasterio.open(\"rus_ppp_2020_constrained.tif\") as dataset:\n",
    "        population = [\n",
    "            (val[0] if val[0] != dataset.nodata else np.nan)\n",
    "            for val in tqdm(dataset.sample(coords), total=len(coords), desc=\"Processing population\")\n",
    "        ]\n",
    "    result_df[\"rasterio_population\"] = population\n",
    "    result_df.loc[result_df[\"rasterio_population\"] < 0, \"rasterio_population\"] = np.nan\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "10340c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing density: 100%|██████████| 41105/41105 [00:02<00:00, 17768.87it/s]\n",
      "Processing population: 100%|██████████| 41105/41105 [00:04<00:00, 9748.85it/s] \n"
     ]
    }
   ],
   "source": [
    "processed_train = preprocess_geo_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9bea3bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing density: 100%|██████████| 9276/9276 [00:00<00:00, 17484.51it/s]\n",
      "Processing population: 100%|██████████| 9276/9276 [00:02<00:00, 4363.85it/s] \n"
     ]
    }
   ],
   "source": [
    "processed_test = preprocess_geo_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6bb091a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_text_features(df: pd.DataFrame, reviews_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    def count_occurrences(texts: pd.Series, pattern: str) -> int:\n",
    "        return texts.str.count(pattern, flags=re.IGNORECASE).sum()\n",
    "    agg_df = (\n",
    "        reviews_df\n",
    "        .groupby(\"id\")[\"text\"]\n",
    "        .agg(\n",
    "            text_count=\"count\",\n",
    "            text_mean_len=lambda x: x.str.len().mean(),\n",
    "            text_max_len=lambda x: x.str.len().max(),\n",
    "            count_no=lambda x: count_occurrences(x, r\"\\bно\\b\"),\n",
    "            count_ne_nrav=lambda x: count_occurrences(x, r\"\\bне\\s+нравится\\b\"),\n",
    "            count_nrav=lambda x: count_occurrences(x, r\"(?<!не\\s)\\bнравится\\b\"),\n",
    "            count_horosh=lambda x: count_occurrences(x, r\"(?<!не\\s)\\bхорош\\w*\\b\"),\n",
    "            count_ne_horosh=lambda x: count_occurrences(x, r\"\\bне\\s+хорош\\w*\\b\"),\n",
    "            count_otlich=lambda x: count_occurrences(x, r\"(?<!не\\s)\\bотличн\\w*\\b\"),\n",
    "            count_hotya=lambda x: count_occurrences(x, r\"\\bхотя\\b\"),\n",
    "            count_ponrav=lambda x: count_occurrences(x, r\"(?<!не\\s)\\bпонрав\\w*\\b\"),\n",
    "            count_ne_ponrav=lambda x: count_occurrences(x, r\"\\bне\\s+понрав\\w*\\b\"),\n",
    "            count_priyatn=lambda x: count_occurrences(x, r\"(?<!не\\s)\\bприятн\\w*\\b\"),\n",
    "            count_ne_priyatn=lambda x: count_occurrences(x, r\"\\bне\\s+приятн\\w*\\b\"),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    result = df.merge(agg_df, on=\"id\", how=\"left\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6abcadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = add_text_features(processed_train, reviews)\n",
    "processed_test = add_text_features(processed_test, reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e7a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_features = ['text_count', 'text_mean_len', 'text_max_len', 'count_no', 'count_ne_nrav', 'count_nrav', 'count_horosh', 'count_ne_horosh', 'count_otlich', 'count_hotya', 'count_ponrav', 'count_ne_ponrav', 'count_priyatn', 'count_ne_priyatn']\n",
    "\n",
    "# text_train = pd.read_csv(\"text_features_train.csv\")\n",
    "# text_test = pd.read_csv(\"text_features_test.csv\")\n",
    "\n",
    "# processed_train = pd.concat([processed_train, text_train], axis=1)\n",
    "# processed_test = pd.concat([processed_test, text_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "616a986a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_train = processed_train[text_features]\n",
    "# text_test = processed_test[text_features]\n",
    "# text_train.to_csv(\"text_features_train.csv\", index=False)\n",
    "# text_test.to_csv(\"text_features_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8d8a67c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geo_features(processed_train: pd.DataFrame, processed_test: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Добавляет гео-фичи с tqdm.\n",
    "    Для train: исключает саму точку из усреднения, чтобы избежать target leakage.\n",
    "    \"\"\"\n",
    "    def compute_features(df_base, df_ref, radius_m_list, use_target=True, desc_prefix=\"\", exclude_self=False):\n",
    "        coords_ref = np.radians(df_ref[['lat', 'lon']].values)\n",
    "        tree = BallTree(coords_ref, metric='haversine')\n",
    "        coords_base = np.radians(df_base[['lat', 'lon']].values)\n",
    "\n",
    "        features = pd.DataFrame(index=df_base.index)\n",
    "\n",
    "        for r_m in radius_m_list:\n",
    "            tqdm_desc = f\"{desc_prefix}{r_m}m\"\n",
    "            r = r_m / 6371000\n",
    "\n",
    "            ind_in_radius = tree.query_radius(coords_base, r)\n",
    "\n",
    "            samecat_mean, allcat_mean = [], []\n",
    "            samecat_cnt, allcat_cnt = [], []\n",
    "            samecat_wmean, allcat_wmean = [], []\n",
    "            samecat_sim = []\n",
    "\n",
    "            for i, inds in tqdm(enumerate(ind_in_radius), total=len(ind_in_radius), desc=tqdm_desc, leave=False):\n",
    "                if len(inds) == 0:\n",
    "                    samecat_mean.append(np.nan)\n",
    "                    allcat_mean.append(np.nan)\n",
    "                    samecat_cnt.append(0)\n",
    "                    allcat_cnt.append(0)\n",
    "                    samecat_wmean.append(np.nan)\n",
    "                    allcat_wmean.append(np.nan)\n",
    "                    samecat_sim.append(0)\n",
    "                    continue\n",
    "\n",
    "                if exclude_self and i < len(df_ref):\n",
    "                    inds = inds[inds != i]\n",
    "                    if len(inds) == 0:\n",
    "                        samecat_mean.append(np.nan)\n",
    "                        allcat_mean.append(np.nan)\n",
    "                        samecat_cnt.append(0)\n",
    "                        allcat_cnt.append(0)\n",
    "                        samecat_wmean.append(np.nan)\n",
    "                        allcat_wmean.append(np.nan)\n",
    "                        samecat_sim.append(0)\n",
    "                        continue\n",
    "\n",
    "                base_cat = df_base.loc[i, 'category']\n",
    "                cats = df_ref.iloc[inds]['category'].values\n",
    "                mask_same = cats == base_cat\n",
    "\n",
    "                samecat_cnt.append(mask_same.sum())\n",
    "                allcat_cnt.append(len(inds))\n",
    "                samecat_sim.append(mask_same.mean())\n",
    "\n",
    "                if not use_target:\n",
    "                    samecat_mean.append(np.nan)\n",
    "                    allcat_mean.append(np.nan)\n",
    "                    samecat_wmean.append(np.nan)\n",
    "                    allcat_wmean.append(np.nan)\n",
    "                    continue\n",
    "\n",
    "                # tgts = df_ref.iloc[inds]['target'].values\n",
    "                # dists = np.maximum(\n",
    "                #     np.linalg.norm(coords_base[i] - coords_ref[inds], axis=1), 1e-6\n",
    "                # )\n",
    "\n",
    "                # same_tgts = tgts[mask_same]\n",
    "                # same_d = dists[mask_same]\n",
    "\n",
    "                # samecat_mean.append(np.mean(same_tgts) if len(same_tgts) else np.nan)\n",
    "                # allcat_mean.append(np.mean(tgts) if len(tgts) else np.nan)\n",
    "                # samecat_wmean.append(np.average(same_tgts, weights=1/same_d) if len(same_tgts) else np.nan)\n",
    "                # allcat_wmean.append(np.average(tgts, weights=1/dists) if len(tgts) else np.nan)\n",
    "\n",
    "            # features[f'geo_samecat_mean_target_{r_m}m'] = samecat_mean\n",
    "            # features[f'geo_allcat_mean_target_{r_m}m'] = allcat_mean\n",
    "            features[f'geo_samecat_sim_{r_m}m'] = samecat_sim\n",
    "\n",
    "            if r_m in [300, 1000]:\n",
    "                features[f'geo_samecat_cnt_{r_m}m'] = samecat_cnt\n",
    "                features[f'geo_allcat_cnt_{r_m}m'] = allcat_cnt\n",
    "                # features[f'geo_samecat_wmean_target_{r_m}m'] = samecat_wmean\n",
    "                # features[f'geo_allcat_wmean_target_{r_m}m'] = allcat_wmean\n",
    "\n",
    "        return features\n",
    "\n",
    "    radii = [300, 600, 1000]\n",
    "\n",
    "    print(\"▶️ Calculating features for TRAIN (self excluded)...\")\n",
    "    train_features = compute_features(processed_train, processed_train, radii,\n",
    "                                      use_target=True, desc_prefix=\"train \", exclude_self=True)\n",
    "\n",
    "    print(\"▶️ Calculating features for TEST (using train targets)...\")\n",
    "    test_features = compute_features(processed_test, processed_train, radii,\n",
    "                                     use_target=True, desc_prefix=\"test \", exclude_self=False)\n",
    "\n",
    "    processed_train = pd.concat([processed_train, train_features], axis=1)\n",
    "    processed_test = pd.concat([processed_test, test_features], axis=1)\n",
    "    return processed_train, processed_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eae7cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_train, processed_test = add_geo_features(processed_train, processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f2be3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_cols = [c for c in processed_train_geo.columns if c not in processed_train.columns]\n",
    "# train_geo_features = processed_train_geo[new_cols]\n",
    "# test_geo_features = processed_test_geo[new_cols]\n",
    "\n",
    "# train_geo_features.to_csv(\"geo_features_train.csv\", index=False)\n",
    "# test_geo_features.to_csv(\"geo_features_test.csv\", index=False)\n",
    "\n",
    "# print(f\"✅ Saved geo_features_train.csv ({train_geo_features.shape})\")\n",
    "# print(f\"✅ Saved geo_features_test.csv ({test_geo_features.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a321988",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_train = pd.read_csv(\"geo_features_train.csv\")\n",
    "geo_test = pd.read_csv(\"geo_features_test.csv\")\n",
    "\n",
    "processed_train = pd.concat([processed_train, geo_train], axis=1)\n",
    "processed_test = pd.concat([processed_test, geo_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95c5c48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_needed_geo_features = ['geo_samecat_mean_target_300m', 'geo_allcat_mean_target_300m', 'geo_samecat_wmean_target_300m', 'geo_allcat_wmean_target_300m', 'geo_samecat_mean_target_600m', 'geo_allcat_mean_target_600m', 'geo_samecat_mean_target_1000m', 'geo_allcat_mean_target_1000m', 'geo_samecat_wmean_target_1000m', 'geo_allcat_wmean_target_1000m']\n",
    "processed_train = processed_train.drop(columns=not_needed_geo_features)\n",
    "processed_test = processed_test.drop(columns=not_needed_geo_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d9cbde0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features_loaded = pd.read_csv('selected_features.csv')['selected_features'].tolist()\n",
    "# selected_features_loaded.append('target')\n",
    "\n",
    "# processed_train = processed_train[selected_features_loaded]\n",
    "\n",
    "# selected_features_loaded.remove('target')\n",
    "# processed_test = processed_test[selected_features_loaded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8c39857",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = processed_train[processed_train['target'] >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7fa42f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    result_df = df.copy()\n",
    "    result_df.drop(columns=['id', 'name', 'address'], inplace=True)\n",
    "    \n",
    "    for col in result_df.select_dtypes(include=['category']).columns:\n",
    "        result_df[col] = result_df[col].cat.add_categories(['missing']).fillna('missing')\n",
    "\n",
    "    for col in result_df.select_dtypes(include=['object']).columns:\n",
    "        result_df[col] = result_df[col].fillna(\"missing\").astype(\"category\")\n",
    "        \n",
    "    cat_features = result_df.select_dtypes(include=['category']).columns.tolist()\n",
    "    return result_df, cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "da206d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train, cat_features = preprocess_data(processed_train)\n",
    "processed_test, _ = preprocess_data(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f1fba9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = processed_train.drop(columns=['target']), processed_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ced51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"loss_function\": \"MAE\",\n",
    "        \"eval_metric\": \"MAE\",\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.02, 0.3, step=0.01),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1.0, 10.0, step=0.5),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 64),\n",
    "        \"iterations\": 2000,\n",
    "        \"task_type\": \"GPU\",\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": False,\n",
    "        \"early_stopping_rounds\": 100,\n",
    "    }\n",
    "\n",
    "    mae_scores = []\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        train_pool = Pool(X_tr, y_tr, cat_features=cat_features)\n",
    "        val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "\n",
    "        model = CatBoostRegressor(**params)\n",
    "        model.fit(train_pool, eval_set=val_pool)\n",
    "\n",
    "        preds = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        mae_scores.append(mae)\n",
    "\n",
    "    mean_mae = np.mean(mae_scores)\n",
    "    return mean_mae\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=2, n_jobs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b55b0836",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8d8bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "models = []\n",
    "scores = []\n",
    "mae_scores = []\n",
    "\n",
    "for train_idx, val_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_features)\n",
    "    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "        loss_function=\"MAE\",\n",
    "        eval_metric=\"MAE\",\n",
    "        iterations=2000,\n",
    "        task_type=\"GPU\",\n",
    "        random_seed=42,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=100,\n",
    "        **best_params\n",
    "        )\n",
    "    model.fit(train_pool, eval_set=val_pool, verbose=False)\n",
    "\n",
    "    preds = model.predict(X_val)\n",
    "\n",
    "    score = model.score(X_val, y_val)\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    print(mae)\n",
    "    scores.append(score)\n",
    "    mae_scores.append(mae)\n",
    "    models.append(model)\n",
    "\n",
    "print(\"Средний R² скор:\", np.mean(scores))\n",
    "print(\"Средний MAE:\", np.mean(mae_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = []\n",
    "for model in models:\n",
    "    fi = model.get_feature_importance(prettified=True)\n",
    "    fi = fi.rename(columns={'Feature Id': 'feature', 'Importances': 'importance'})\n",
    "    feature_importances.append(fi[['feature', 'importance']])\n",
    "\n",
    "df_fi = pd.concat(feature_importances)\n",
    "mean_fi = df_fi.groupby('feature', as_index=False)['importance'].mean().sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a35ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_predictions = np.mean([m.predict(processed_test) for m in models], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73b4cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'id': test['id'], 'target': catboost_predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1d3f90e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('catboost_submission49.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723ef17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# params = {\n",
    "#     \"objective\": \"regression_l1\",\n",
    "#     \"metric\": \"mae\",\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"max_depth\": 7,\n",
    "#     \"num_leaves\": 2**7 - 1,\n",
    "#     \"n_estimators\": 800,\n",
    "#     \"random_state\": 42,\n",
    "#     \"verbosity\": -1,\n",
    "# }\n",
    "\n",
    "# train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cat_features)\n",
    "# val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cat_features)\n",
    "\n",
    "# model_lgb = lgb.train(\n",
    "#     params,\n",
    "#     train_data,\n",
    "#     valid_sets=[val_data],\n",
    "#     callbacks=[\n",
    "#         lgb.early_stopping(500),\n",
    "#         lgb.log_evaluation(100),\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# y_pred = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n",
    "# mae = mean_absolute_error(y_val, y_pred)\n",
    "# print(f\"Validation MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c092b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_preds = model_lgb.predict(processed_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df = pd.DataFrame({'id': test['id'], 'target': lgb_preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fe0af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission_df.to_csv('lgb3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e8c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mixed_submission = pd.DataFrame(\n",
    "#     {\n",
    "#     'id': test['id'], \n",
    "#     'target': (lgb_preds * 0.2 + catboost_predictions * 0.8)\n",
    "#     }\n",
    "# )\n",
    "# mixed_submission.to_csv('combined_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
