{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 1: –ò–º–ø–æ—Ä—Ç—ã, –∫–æ–Ω—Ñ–∏–≥ –∏ –±–∞–∑–æ–≤—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã  (–î–û–ë–ê–í–õ–ï–ù tqdm)\n",
    "import os, gc, math, json, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD, PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from tqdm import tqdm  # ‚Üê –≤–æ—Ç –æ–Ω\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# –§–ª–∞–≥–∏ —Ç—è–∂—ë–ª—ã—Ö –±–ª–æ–∫–æ–≤\n",
    "RUN_RUBERT             = True         # ruBERT-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç–∑—ã–≤–æ–≤ (—Å –∫—ç—à–µ–º)\n",
    "RUN_RUBERT_SVD         = True         # SVD-—Å–∂–∞—Ç–∏–µ ruBERT\n",
    "RUBERT_SVD_COMPONENTS  = 64\n",
    "RUN_SENTIMENT          = True         # –±—ã—Å—Ç—Ä—ã–π sentiment (POS/NEG/NEU) —Å –∫—ç—à–µ–º\n",
    "MAX_REVIEWS_PER_PLACE  = 3\n",
    "\n",
    "# –ì–µ–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∏\n",
    "RADII_M = [100, 300, 600, 1000, 1500]\n",
    "EARTH_RADIUS_M  = 6371000.0\n",
    "EARTH_RADIUS_KM = 6371.0088\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (41105, 286) (9276, 285) (440082, 2)\n",
      "     id                                     name             coordinates  \\\n",
      "0  1365  –ì–æ—Ä–æ–¥—Å–∫–∞—è –ø–æ–ª–∏–∫–ª–∏–Ω–∏–∫–∞ ‚Ññ 109, —Ñ–∏–ª–∏–∞–ª ‚Ññ 2  [37.735049, 55.719667]   \n",
      "1  8230                       Wellness Club Nebo  [37.537083, 55.749511]   \n",
      "\n",
      "        category                                   address  target  \\\n",
      "0         health  –ì—Ä–∞–π–≤–æ—Ä–æ–Ω–æ–≤—Å–∫–∞—è —É–ª., 18, –∫–æ—Ä–ø. 1, –ú–æ—Å–∫–≤–∞     4.1   \n",
      "1  swimming_pool              –ü—Ä–µ—Å–Ω–µ–Ω—Å–∫–∞—è –Ω–∞–±., 12, –ú–æ—Å–∫–≤–∞     3.6   \n",
      "\n",
      "   traffic_300m    homes_300m    works_300m  female_300m  ...  doramas_1000m  \\\n",
      "0         75429  16113.582471  15756.246444      51316.0  ...         4668.0   \n",
      "1        246535   8578.458740  31315.672794     192547.0  ...         3431.0   \n",
      "\n",
      "   computer_components_1000m  humor_1000m  car_market_1000m  \\\n",
      "0                     7718.0      33389.0           18306.0   \n",
      "1                    11463.0      61107.0           23662.0   \n",
      "\n",
      "   no_higher_education_1000m  goods_for_moms_and_babies_1000m  \\\n",
      "0                   426241.0                            415.0   \n",
      "1                   488685.0                            356.0   \n",
      "\n",
      "   age_25-34_1000m  male_1000m  phone_repair_1000m  mean_income_1000m  \n",
      "0         380148.0    619550.0              2781.0      113767.387249  \n",
      "1         436721.0    764733.0              4264.0      122931.921255  \n",
      "\n",
      "[2 rows x 286 columns]\n"
     ]
    }
   ],
   "source": [
    "# –®–ê–ì 2: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "train = pd.read_csv('train.tsv', sep='\\t')\n",
    "test  = pd.read_csv('test.tsv',  sep='\\t')\n",
    "reviews = pd.read_csv('reviews.tsv', sep='\\t')\n",
    "\n",
    "for df in (train, test, reviews):\n",
    "    df['id'] = df['id'].astype(str)\n",
    "\n",
    "print(\"Shapes:\", train.shape, test.shape, reviews.shape)\n",
    "print(train.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 3: –£—Ç–∏–ª–∏—Ç—ã (–≥–µ–æ/–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è/—Å–ø–ª–∏—Ç—ã)\n",
    "EARTH_RADIUS_M  = 6371000.0\n",
    "EARTH_RADIUS_KM = 6371.0088\n",
    "\n",
    "def meters_to_radians(m): \n",
    "    return m / EARTH_RADIUS_M\n",
    "\n",
    "def safe_ratio(a, b): \n",
    "    return a / (b + 1e-6)\n",
    "\n",
    "def l2norm(X):\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True) + 1e-12\n",
    "    return X / n\n",
    "\n",
    "def parse_lonlat(df):\n",
    "    out = df.copy()\n",
    "    if 'coordinates' in out.columns:\n",
    "        def _parse(val):\n",
    "            if isinstance(val, str):\n",
    "                s = val.strip().replace('[','').replace(']','')\n",
    "                parts = [p.strip() for p in s.split(',')]\n",
    "                if len(parts) >= 2:\n",
    "                    try:   return float(parts[0]), float(parts[1])\n",
    "                    except: return np.nan, np.nan\n",
    "            if isinstance(val, (list, tuple)) and len(val)>=2:\n",
    "                return float(val[0]), float(val[1])\n",
    "            return np.nan, np.nan\n",
    "        lonlat = out['coordinates'].map(_parse)\n",
    "        out['lon'] = [t[0] for t in lonlat]\n",
    "        out['lat'] = [t[1] for t in lonlat]\n",
    "    out['lon'] = out.get('lon', np.nan)\n",
    "    out['lat'] = out.get('lat', np.nan)\n",
    "    return out\n",
    "\n",
    "# Fourier —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, —Å–Ω—è—Ç—ã–º–∏ –ø–æ train\n",
    "def add_fourier_train_test(train_df, test_df, fourier_k=8):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    for col in ['lon','lat']:\n",
    "        v_tr = tr[col].astype(float)\n",
    "        mn, mx = np.nanmin(v_tr), np.nanmax(v_tr)\n",
    "        if not np.isfinite(mn) or not np.isfinite(mx) or mn==mx:\n",
    "            norm_tr = np.zeros_like(v_tr)\n",
    "            norm_te = np.zeros_like(te[col].astype(float))\n",
    "        else:\n",
    "            norm_tr = (v_tr - mn) / (mx - mn)\n",
    "            norm_te = (te[col].astype(float) - mn) / (mx - mn)\n",
    "        for k in range(1, fourier_k+1):\n",
    "            tr[f'{col}_sin_{k}'] = np.sin(2*np.pi*k*norm_tr)\n",
    "            tr[f'{col}_cos_{k}'] = np.cos(2*np.pi*k*norm_tr)\n",
    "            te[f'{col}_sin_{k}'] = np.sin(2*np.pi*k*norm_te)\n",
    "            te[f'{col}_cos_{k}'] = np.cos(2*np.pi*k*norm_te)\n",
    "    return tr, te\n",
    "\n",
    "# KMeans –ø–æ –≥–µ–æ (–≤ —Ä–∞–¥–∏–∞–Ω–∞—Ö) + —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞\n",
    "def add_geo_kmeans(train_df, test_df, n_clusters=64):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    lat_tr = np.deg2rad(tr['lat'].astype(float).values)\n",
    "    lon_tr = np.deg2rad(tr['lon'].astype(float).values)\n",
    "    lat_te = np.deg2rad(te['lat'].astype(float).values)\n",
    "    lon_te = np.deg2rad(te['lon'].astype(float).values)\n",
    "    XY_tr = np.c_[lat_tr, lon_tr]\n",
    "    XY_te = np.c_[lat_te, lon_te]\n",
    "    km = MiniBatchKMeans(n_clusters=n_clusters, random_state=RANDOM_SEED, batch_size=4096)\n",
    "    km.fit(XY_tr)\n",
    "    tr['geo_cluster'] = km.labels_.astype(int)\n",
    "    te['geo_cluster'] = km.predict(XY_te).astype(int)\n",
    "    centers = km.cluster_centers_\n",
    "    ctr_tr = centers[tr['geo_cluster'].values]\n",
    "    ctr_te = centers[te['geo_cluster'].values]\n",
    "    tr['geo_dist2centroid_km'] = np.sqrt(((XY_tr-ctr_tr)**2).sum(axis=1)) * EARTH_RADIUS_KM\n",
    "    te['geo_dist2centroid_km'] = np.sqrt(((XY_te-ctr_te)**2).sum(axis=1)) * EARTH_RADIUS_KM\n",
    "    return tr, te\n",
    "\n",
    "# –ë–∞–∑–æ–≤—ã–µ 5-—Ñ–æ–ª–¥–æ–≤—ã–µ —Å–ø–ª–∏—Ç—ã –ø–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º —Å—Ç—Ä–æ–∫–∞–º (–æ–±—â–∏–µ –¥–ª—è –≤—Å–µ—Ö OOF)\n",
    "def build_base_splits(train_df, n_splits=5, seed=42):\n",
    "    lab_idx = train_df.index[(train_df['target']>0) & train_df['target'].notna()].to_numpy()\n",
    "    y = train_df.loc[lab_idx, 'target'].astype(float)\n",
    "    bins = pd.qcut(y, q=min(10, max(2, y.nunique())), duplicates='drop').cat.codes\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    base_splits = [(lab_idx[tr], lab_idx[va]) for tr, va in skf.split(lab_idx, bins)]\n",
    "    return base_splits, lab_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 4: –ì–µ–æ-–±–∞–∑–∞ (lon/lat, Fourier, KMeans)\n",
    "train_geo = parse_lonlat(train)\n",
    "test_geo  = parse_lonlat(test)\n",
    "train_geo, test_geo = add_fourier_train_test(train_geo, test_geo, fourier_k=8)\n",
    "train_geo, test_geo = add_geo_kmeans(train_geo, test_geo, n_clusters=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 5: –¢–µ–∫—Å—Ç–æ–≤—ã–µ –ª–∞—Ç–µ–Ω—Ç—ã TF-IDF+SVD (–∞–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ id) + –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç-—Å—Ç–∞—Ç—ã\n",
    "def build_text_latents(reviews: pd.DataFrame, n_components=300, max_features=120_000):\n",
    "    if reviews.empty:\n",
    "        return pd.DataFrame(columns=['id'])\n",
    "    rv = reviews.copy()\n",
    "    rv['text'] = rv['text'].fillna('')\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        lowercase=True,\n",
    "        token_pattern=r\"[A-Za-z–ê-–Ø–∞-—è–Å—ë0-9_]+\",\n",
    "        ngram_range=(1,2),\n",
    "        max_features=max_features,\n",
    "        min_df=2\n",
    "    )\n",
    "    X = vectorizer.fit_transform(rv['text'])  # —Ç—Ä–∞–Ω—Å–¥—É–∫—Ç–∏–≤–Ω–æ –æ–∫\n",
    "    n_components = min(n_components, max(1, X.shape[1]-1))\n",
    "    svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_SEED)\n",
    "    X_svd = svd.fit_transform(X)\n",
    "    df = pd.DataFrame(X_svd, columns=[f'txt_svd_{i}' for i in range(n_components)])\n",
    "    df.insert(0, 'id', rv['id'].values)\n",
    "    agg = df.groupby('id').mean().reset_index()\n",
    "    stats = (rv.assign(len_char=rv['text'].str.len(),\n",
    "                       len_tok=rv['text'].str.split().map(lambda x: len(x) if isinstance(x, list) else 0))\n",
    "             .groupby('id')\n",
    "             .agg(n_reviews=('text','count'),\n",
    "                  mean_len_char=('len_char','mean'),\n",
    "                  mean_len_tok=('len_tok','mean'))\n",
    "             .reset_index())\n",
    "    return agg.merge(stats, on='id', how='left')\n",
    "\n",
    "txt_lat = build_text_latents(reviews)\n",
    "train_geo = train_geo.merge(txt_lat, on='id', how='left')\n",
    "test_geo  = test_geo.merge(txt_lat, on='id',  how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 6: –ë–ª–æ–∫ —Ñ–∏—á –ø–æ —Ä–∞–¥–∏—É—Å–∞–º (–¥–æ–ª–∏/ratio/–∏–Ω–¥–µ–∫—Å—ã + –ª–æ–≥–∞—Ä–∏—Ñ–º—ã/–Ω–∞–≥—Ä—É–∑–∫–∏)\n",
    "def make_block_features(df):\n",
    "    out = df.copy()\n",
    "    # –±–∞–∑–æ–≤—ã–µ ratio/diff\n",
    "    for base in ['traffic','homes','works','mean_income']:\n",
    "        for r in ['300m','1000m']:\n",
    "            col = f'{base}_{r}'\n",
    "            if col not in out.columns:\n",
    "                out[col] = np.nan\n",
    "        a = out[f'{base}_300m']; b = out[f'{base}_1000m']\n",
    "        out[f'{base}_ratio'] = safe_ratio(a, b)\n",
    "        out[f'{base}_diff']  = (a - b)\n",
    "\n",
    "    # –¥–µ–º–æ–≥—Ä–∞—Ñ–∏—è/—Å–æ—Ü-—ç–∫–æ –ø–æ —Ä–∞–¥–∏—É—Å–∞–º\n",
    "    for r in ['300m','1000m']:\n",
    "        female = out.get(f'female_{r}',0).fillna(0)\n",
    "        male   = out.get(f'male_{r}',0).fillna(0)\n",
    "        gtot = female + male\n",
    "        out[f'female_share_{r}'] = safe_ratio(female, gtot)\n",
    "\n",
    "        age_cols = [c for c in out.columns if c.startswith('age_') and c.endswith(f'_{r}')]\n",
    "        if age_cols:\n",
    "            atot = out[age_cols].sum(axis=1)\n",
    "            youth = out[[c for c in age_cols if ('18-24' in c) or ('25-34' in c)]].sum(axis=1)\n",
    "            elderly = out[[c for c in age_cols if ('>55' in c)]].sum(axis=1)\n",
    "            out[f'youth_share_{r}'] = safe_ratio(youth, atot)\n",
    "            out[f'elderly_share_{r}'] = safe_ratio(elderly, atot)\n",
    "\n",
    "        he = out.get(f'higher_education_{r}',0).fillna(0)\n",
    "        nhe= out.get(f'no_higher_education_{r}',0).fillna(0)\n",
    "        out[f'higher_ed_share_{r}'] = safe_ratio(he, he+nhe)\n",
    "\n",
    "        emp = out.get(f'employed_{r}',0).fillna(0)\n",
    "        une = out.get(f'unemployed_{r}',0).fillna(0)\n",
    "        out[f'employment_rate_{r}'] = safe_ratio(emp, emp+une)\n",
    "\n",
    "        hc  = out.get(f'has_children_{r}',0).fillna(0)\n",
    "        nc  = out.get(f'no_children_{r}',0).fillna(0)\n",
    "        out[f'children_share_{r}'] = safe_ratio(hc, hc+nc)\n",
    "\n",
    "        mar = out.get(f'married_{r}',0).fillna(0)\n",
    "        nmar= out.get(f'not_married_{r}',0).fillna(0)\n",
    "        out[f'married_share_{r}'] = safe_ratio(mar, mar+nmar)\n",
    "\n",
    "        # –¥–æ—Ö–æ–¥–Ω—ã–µ –¥–æ–ª–∏\n",
    "        buckets = [f'below_average_income_{r}', f'average_income_{r}', f'above_average_income_{r}', f'high_income_{r}', f'premium_income_{r}']\n",
    "        pres = [c for c in buckets if c in out.columns]\n",
    "        if pres:\n",
    "            tot = out[pres].sum(axis=1)\n",
    "            hi = out[[c for c in pres if ('above_average' in c) or ('high_income' in c) or ('premium' in c)]].sum(axis=1)\n",
    "            out[f'high_income_share_{r}'] = safe_ratio(hi, tot)\n",
    "\n",
    "    # cross-radius\n",
    "    for name in ['female_share','youth_share','elderly_share','higher_ed_share','employment_rate','children_share','married_share','high_income_share']:\n",
    "        a = out.get(f'{name}_300m'); b = out.get(f'{name}_1000m')\n",
    "        if a is not None and b is not None:\n",
    "            out[f'{name}_ratio'] = safe_ratio(a, b)\n",
    "            out[f'{name}_diff']  = (a - b)\n",
    "\n",
    "    # –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã\n",
    "    out['socio_index']  = 0.5*out.get('high_income_share_300m',0).fillna(0) + 0.3*out.get('higher_ed_share_300m',0).fillna(0) + 0.2*out.get('employment_rate_300m',0).fillna(0)\n",
    "    out['family_index'] = 0.6*out.get('children_share_300m',0).fillna(0)     + 0.4*out.get('married_share_300m',0).fillna(0)\n",
    "\n",
    "    # –ª–æ–≥–∞—Ä–∏—Ñ–º—ã –∏ –Ω–∞–≥—Ä—É–∑–∫–∏\n",
    "    base_cols = [\n",
    "        'traffic_300m','homes_300m','works_300m','mean_income_300m',\n",
    "        'traffic_1000m','homes_1000m','works_1000m','mean_income_1000m'\n",
    "    ]\n",
    "    for c in base_cols:\n",
    "        if c in out.columns:\n",
    "            out[c+'_log1p'] = np.log1p(out[c])\n",
    "\n",
    "    if {'female_300m','male_300m'}.issubset(out.columns):\n",
    "        out['pop_300m'] = out['female_300m'] + out['male_300m']\n",
    "        out['sex_ratio_300m'] = safe_ratio(out['female_300m'], out['male_300m'])\n",
    "    if {'female_1000m','male_1000m'}.issubset(out.columns):\n",
    "        out['pop_1000m'] = out['female_1000m'] + out['male_1000m']\n",
    "        out['sex_ratio_1000m'] = safe_ratio(out['female_1000m'], out['male_1000m'])\n",
    "\n",
    "    if {'traffic_300m','homes_300m'}.issubset(out.columns):\n",
    "        out['traffic_per_home_300m'] = safe_ratio(out['traffic_300m'], out['homes_300m'])\n",
    "    if {'works_300m','homes_300m'}.issubset(out.columns):\n",
    "        out['works_per_home_300m'] = safe_ratio(out['works_300m'], out['homes_300m'])\n",
    "    if {'traffic_1000m','homes_1000m'}.issubset(out.columns):\n",
    "        out['traffic_per_home_1000m'] = safe_ratio(out['traffic_1000m'], out['homes_1000m'])\n",
    "    if {'works_1000m','homes_1000m'}.issubset(out.columns):\n",
    "        out['works_per_home_1000m'] = safe_ratio(out['works_1000m'], out['homes_1000m'])\n",
    "\n",
    "    if {'mean_income_300m','pop_300m'}.issubset(out.columns):\n",
    "        out['income_per_capita_300m'] = safe_ratio(out['mean_income_300m'], out['pop_300m'])\n",
    "    if {'mean_income_1000m','pop_1000m'}.issubset(out.columns):\n",
    "        out['income_per_capita_1000m'] = safe_ratio(out['mean_income_1000m'], out['pop_1000m'])\n",
    "\n",
    "    return out\n",
    "\n",
    "train_fe = make_block_features(train_geo)\n",
    "test_fe  = make_block_features(test_geo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 7: –ò–Ω—Ç–µ—Ä–µ—Å—ã ‚Äî —ç–Ω—Ç—Ä–æ–ø–∏—è + —Ä–∞–∑–¥–µ–ª—å–Ω—ã–π PCA –¥–ª—è 300–º –∏ 1000–º\n",
    "def get_interest_cols(df, radius_tag):\n",
    "    excl = ['traffic','homes','works','mean_income','female','male','age_','married','not_married',\n",
    "            'has_children','no_children','employed','unemployed','higher_education','no_higher_education',\n",
    "            'below_average_income','average_income','above_average_income','high_income','premium_income']\n",
    "    cols = [c for c in df.columns if c.endswith(f'_{radius_tag}') and not any(c.startswith(p) for p in excl)]\n",
    "    return cols\n",
    "\n",
    "def add_interest_entropy_and_pca(train_df, test_df, n_components=12):\n",
    "    tr = train_df.copy(); te = test_df.copy()\n",
    "    for radius in ['300m','1000m']:\n",
    "        use_cols = sorted(set(get_interest_cols(tr, radius) + get_interest_cols(te, radius)))\n",
    "        if not use_cols:\n",
    "            continue\n",
    "        def normalize_block(df):\n",
    "            X = df[use_cols].fillna(0.0).astype(float)\n",
    "            row_sum = X.sum(axis=1).replace(0, np.nan)\n",
    "            Xn = X.div(row_sum, axis=0).fillna(0.0)\n",
    "            # —ç–Ω—Ç—Ä–æ–ø–∏—è\n",
    "            p = Xn.values\n",
    "            ent = -(p * np.log(p + 1e-12)).sum(axis=1)\n",
    "            return Xn, ent\n",
    "        Xtr, ent_tr = normalize_block(tr)\n",
    "        Xte, ent_te = normalize_block(te)\n",
    "        tr[f'int_entropy_{radius}'] = ent_tr\n",
    "        te[f'int_entropy_{radius}'] = ent_te\n",
    "        pca = PCA(n_components=min(n_components, max(1, min(Xtr.shape[1], 64))), random_state=RANDOM_SEED)\n",
    "        pca.fit(pd.concat([Xtr, Xte], axis=0))\n",
    "        tr_lat = pca.transform(Xtr); te_lat = pca.transform(Xte)\n",
    "        for i in range(tr_lat.shape[1]):\n",
    "            tr[f'int_{radius}_lat_{i}'] = tr_lat[:, i]\n",
    "            te[f'int_{radius}_lat_{i}'] = te_lat[:, i]\n",
    "    return tr, te\n",
    "\n",
    "train_fe, test_fe = add_interest_entropy_and_pca(train_fe, test_fe, n_components=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ –ó–∞–≥—Ä—É–∂–∞—é ruBERT –∏–∑ –∫—ç—à–∞‚Ä¶\n",
      "üìÇ –ó–∞–≥—Ä—É–∂–∞—é SVD-–ø—Ä–æ–µ–∫—Ü–∏–∏ –∏–∑ –∫—ç—à–∞‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "# –®–ê–ì 8: ruBERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –æ—Ç–∑—ã–≤–æ–≤ (–∫—ç—à) + SVD + \"–≤–µ—Å\" –ø–æ —á–∏—Å–ª—É –æ—Ç–∑—ã–≤–æ–≤\n",
    "rubert_cols = []\n",
    "if RUN_RUBERT:\n",
    "    try:\n",
    "        import torch\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        RUBERT_PKL = 'rubert_latents.pkl'\n",
    "        if os.path.exists(RUBERT_PKL):\n",
    "            print(\"üìÇ –ó–∞–≥—Ä—É–∂–∞—é ruBERT –∏–∑ –∫—ç—à–∞‚Ä¶\")\n",
    "            rubert_latents = pd.read_pickle(RUBERT_PKL)\n",
    "        else:\n",
    "            print(\"‚öôÔ∏è –°—Ç—Ä–æ—é ruBERT —ç–º–±–µ–¥–¥–∏–Ω–≥–∏‚Ä¶\")\n",
    "            device = 'cuda' if torch.cuda.is_available() else ('mps' if getattr(torch.backends,'mps',None) and torch.backends.mps.is_available() else 'cpu')\n",
    "            model = SentenceTransformer('DeepPavlov/rubert-base-cased-sentence', device=device)\n",
    "            rv = reviews[['id','text']].copy()\n",
    "            rv['text'] = rv['text'].fillna('')\n",
    "            embs = model.encode(rv['text'].tolist(),\n",
    "                                show_progress_bar=True, batch_size=64,\n",
    "                                convert_to_numpy=True, normalize_embeddings=True)\n",
    "            df_emb = pd.DataFrame(embs)\n",
    "            df_emb['id'] = rv['id'].values\n",
    "            rubert_latents = df_emb.groupby('id').mean().reset_index()\n",
    "            rubert_latents.columns = ['id'] + [f'rubert_{i}' for i in range(embs.shape[1])]\n",
    "            rubert_latents.to_pickle(RUBERT_PKL)\n",
    "            print(\"üíæ ruBERT –∫—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω:\", RUBERT_PKL)\n",
    "\n",
    "        train_fe = train_fe.merge(rubert_latents, on='id', how='left')\n",
    "        test_fe  = test_fe.merge(rubert_latents,  on='id', how='left')\n",
    "        rubert_cols = [c for c in train_fe.columns if c.startswith('rubert_')]\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è –û—à–∏–±–∫–∞ ruBERT:\", e)\n",
    "        RUN_RUBERT = False\n",
    "\n",
    "# SVD-—Å–∂–∞—Ç–∏–µ ruBERT (–∏ —Ç–æ–ª—å–∫–æ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞–∫ –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
    "if RUN_RUBERT and RUN_RUBERT_SVD and rubert_cols:\n",
    "    SVD_PKL = \"rubert_svd_latents.pkl\"\n",
    "    if os.path.exists(SVD_PKL):\n",
    "        print(\"üìÇ –ó–∞–≥—Ä—É–∂–∞—é SVD-–ø—Ä–æ–µ–∫—Ü–∏–∏ –∏–∑ –∫—ç—à–∞‚Ä¶\")\n",
    "        svd_lat = pd.read_pickle(SVD_PKL)\n",
    "        svd_train = svd_lat.query(\"split=='train'\").drop(columns=[\"split\"])\n",
    "        svd_test  = svd_lat.query(\"split=='test'\").drop(columns=[\"split\"])\n",
    "    else:\n",
    "        tr_mat = train_fe[rubert_cols].astype(\"float32\").fillna(0.0).values\n",
    "        te_mat = test_fe[rubert_cols].astype(\"float32\").fillna(0.0).values\n",
    "        max_comp = min(RUBERT_SVD_COMPONENTS, tr_mat.shape[1], max(2, tr_mat.shape[0]-1))\n",
    "        svd = TruncatedSVD(n_components=max_comp, random_state=RANDOM_SEED)\n",
    "        tr_proj = svd.fit_transform(tr_mat)\n",
    "        te_proj = svd.transform(te_mat)\n",
    "        svd_train = pd.DataFrame(tr_proj, columns=[f\"rubert_svd_{i}\" for i in range(tr_proj.shape[1])])\n",
    "        svd_test  = pd.DataFrame(te_proj,  columns=[f\"rubert_svd_{i}\" for i in range(te_proj.shape[1])])\n",
    "        svd_train[\"id\"] = train_fe[\"id\"].values\n",
    "        svd_test[\"id\"]  = test_fe[\"id\"].values\n",
    "        svd_save = pd.concat([svd_train.assign(split=\"train\"), svd_test.assign(split=\"test\")], ignore_index=True)\n",
    "        svd_save.to_pickle(SVD_PKL)\n",
    "        print(\"üíæ SVD –∫—ç—à —Å–æ—Ö—Ä–∞–Ω—ë–Ω:\", SVD_PKL)\n",
    "\n",
    "    train_fe = train_fe.merge(svd_train, on=\"id\", how=\"left\")\n",
    "    test_fe  = test_fe.merge(svd_test,  on=\"id\", how=\"left\")\n",
    "    # –≤–µ—Å –ø–æ —á–∏—Å–ª—É –æ—Ç–∑—ã–≤–æ–≤ (–µ—Å–ª–∏ n_reviews –µ—Å—Ç—å)\n",
    "    if 'n_reviews' in train_fe.columns:\n",
    "        for c in [col for col in train_fe.columns if col.startswith('rubert_svd_')]:\n",
    "            train_fe[c] = train_fe[c] * np.log1p(train_fe['n_reviews'].fillna(0).values)\n",
    "            test_fe[c]  = test_fe[c]  * np.log1p(test_fe['n_reviews'].fillna(0).values)\n",
    "    # —É–¥–∞–ª–∏–º —Å—ã—Ä—ã–µ rubert_*\n",
    "    train_fe.drop(columns=[c for c in rubert_cols if c in train_fe.columns], inplace=True, errors='ignore')\n",
    "    test_fe.drop(columns=[c for c in rubert_cols if c in test_fe.columns], inplace=True, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–æ–±—Ä–∞–Ω–æ 115,657 –æ—Ç–∑—ã–≤–æ–≤ (‚â§3 –Ω–∞ id).\n",
      "üñ•Ô∏è  –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞: Apple MPS\n",
      "üì¶ batch_size=256, max_len=256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üé≠ Sentiment v2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 452/452 [30:01<00:00,  3.99s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ –°–æ—Ö—Ä–∞–Ω–∏–ª sentiment –∫—ç—à: sent_latents_fast_v3.pkl\n"
     ]
    }
   ],
   "source": [
    "# –®–ê–ì 9: –ë—ã—Å—Ç—Ä—ã–π SENTIMENT v2 (–±–µ–∑ pipeline, –±—ã—Å—Ç—Ä–µ–µ –Ω–∞ MPS/CPU, —Å tqdm)\n",
    "import os, torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "if RUN_SENTIMENT:\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"  # —Ä–∞–∑—Ä–µ—à–∏–º —Ä–∞—Å–ø–∞—Ä–∞–ª–ª–µ–ª–∏–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–∞\n",
    "    FAST_PKL = \"sent_latents_fast_v3.pkl\"\n",
    "    sent_latents = None\n",
    "\n",
    "    if os.path.exists(FAST_PKL):\n",
    "        print(\"üìÇ –ó–∞–≥—Ä—É–∂–∞—é sentiment –∏–∑ –∫—ç—à–∞‚Ä¶\")\n",
    "        sent_latents = pd.read_pickle(FAST_PKL)\n",
    "    else:\n",
    "        # 1) –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ—Ä–ø—É—Å–∞\n",
    "        rv = reviews[['id','text']].copy()\n",
    "        rv['text'] = rv['text'].fillna('')\n",
    "        rv['len_char'] = rv['text'].str.len()\n",
    "        rv_sampled = (rv.sort_values(['id','len_char'], ascending=[True, False])\n",
    "                        .groupby('id', as_index=False)\n",
    "                        .head(MAX_REVIEWS_PER_PLACE)\n",
    "                        .reset_index(drop=True))\n",
    "        print(f\"üîé –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–æ–±—Ä–∞–Ω–æ {len(rv_sampled):,} –æ—Ç–∑—ã–≤–æ–≤ (‚â§{MAX_REVIEWS_PER_PLACE} –Ω–∞ id).\")\n",
    "\n",
    "        # 2) –î–µ–≤–∞–π—Å\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\"); dev_name = \"CUDA\"\n",
    "        elif getattr(torch.backends, 'mps', None) and torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\"); dev_name = \"Apple MPS\"\n",
    "        else:\n",
    "            device = torch.device(\"cpu\"); dev_name = \"CPU\"\n",
    "        print(f\"üñ•Ô∏è  –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞: {dev_name}\")\n",
    "\n",
    "        # 3) –ú–æ–¥–µ–ª—å/—Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "        model_name = \"blanchefort/rubert-base-cased-sentiment\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # 4) –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "        MAX_LEN = 256      # –±—ã–ª–æ 512 ‚Äî —Å–æ–∫—Ä–∞—â–∞–µ–º –≤ 2 —Ä–∞–∑–∞\n",
    "        BATCH_TRY = [256, 192, 128, 64]  # –≤—ã–±–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≤–æ–∑–º–æ–∂–Ω—ã–π\n",
    "        texts = rv_sampled['text'].tolist()\n",
    "\n",
    "        # 5) –§—É–Ω–∫—Ü–∏—è –±–∞—Ç—á-–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
    "        def infer_batch(batch_texts):\n",
    "            enc = tokenizer(batch_texts,\n",
    "                            padding=True, truncation=True, max_length=MAX_LEN,\n",
    "                            return_tensors=\"pt\")\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            with torch.no_grad():\n",
    "                out = model(**enc)\n",
    "                prob = out.logits.softmax(dim=-1).cpu().numpy()\n",
    "            return prob\n",
    "\n",
    "        # 6) –ü–æ–¥–±–æ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –±–∞—Ç—á–∞ + –ø—Ä–æ–≥–æ–Ω\n",
    "        for BATCH in BATCH_TRY:\n",
    "            try:\n",
    "                _ = infer_batch(texts[:min(BATCH, len(texts))])\n",
    "                break\n",
    "            except RuntimeError as e:\n",
    "                if \"out of memory\" in str(e).lower():\n",
    "                    torch.cuda.empty_cache() if device.type == \"cuda\" else None\n",
    "                    continue\n",
    "                else:\n",
    "                    raise\n",
    "        print(f\"üì¶ batch_size={BATCH}, max_len={MAX_LEN}\")\n",
    "\n",
    "        # 7) –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª\n",
    "        probs = []\n",
    "        for i in tqdm(range(0, len(texts), BATCH), desc=\"üé≠ Sentiment v2\", unit=\"batch\"):\n",
    "            probs.append(infer_batch(texts[i:i+BATCH]))\n",
    "            if device.type == \"mps\":\n",
    "                torch.mps.synchronize()\n",
    "        probs = np.vstack(probs)  # shape: [N, 3]\n",
    "\n",
    "        # 8) –°–æ–±–µ—Ä—ë–º –º–µ—Ç–∫–∏/—Å–∫–æ—Ä—ã\n",
    "        label_ids = probs.argmax(1)\n",
    "        labels_map = {0: \"NEGATIVE\", 1: \"NEUTRAL\", 2: \"POSITIVE\"}  # –ø–æ—Ä—è–¥–æ–∫ —É —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ —Ç–∞–∫–æ–π\n",
    "        labels = [labels_map[i] for i in label_ids]\n",
    "        scores = probs.max(1)\n",
    "\n",
    "        rv_sampled['label'] = labels\n",
    "        rv_sampled['score'] = scores\n",
    "\n",
    "        # 9) –ê–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∑–∞–≤–µ–¥–µ–Ω–∏—é (–∫–∞–∫ —Ä–∞–Ω—å—à–µ + –∏–Ω–¥–µ–∫—Å —Ç–æ–Ω–∞)\n",
    "        g = rv_sampled.groupby('id')\n",
    "        sent_latents = pd.DataFrame({\n",
    "            'sent_n_sampled'     : g.size(),\n",
    "            'sent_pos_cnt'       : g['label'].apply(lambda s: (s=='POSITIVE').sum()),\n",
    "            'sent_neg_cnt'       : g['label'].apply(lambda s: (s=='NEGATIVE').sum()),\n",
    "            'sent_neu_cnt'       : g['label'].apply(lambda s: (s=='NEUTRAL').sum()),\n",
    "            'sent_mean_conf'     : g['score'].mean(),\n",
    "            'sent_strong_cnt'    : g['score'].apply(lambda s: (s>0.8).sum()),\n",
    "            'sent_mean_len_char' : g['len_char'].mean(),\n",
    "        }).reset_index()\n",
    "\n",
    "        total_cnt = reviews.groupby('id').size().rename('sent_total_reviews').reset_index()\n",
    "        sent_latents = sent_latents.merge(total_cnt, on='id', how='left')\n",
    "\n",
    "        sent_latents['sent_pos_share'] = (sent_latents['sent_pos_cnt'] / sent_latents['sent_n_sampled']).fillna(0.0)\n",
    "        total = (sent_latents['sent_pos_cnt'] + sent_latents['sent_neg_cnt'] + sent_latents['sent_neu_cnt']).replace(0, np.nan)\n",
    "        sent_latents['sent_tone_index'] = ((sent_latents['sent_pos_cnt'] - sent_latents['sent_neg_cnt']) / total).fillna(0.0)\n",
    "\n",
    "        sent_latents.to_pickle(FAST_PKL)\n",
    "        print(\"üíæ –°–æ—Ö—Ä–∞–Ω–∏–ª sentiment –∫—ç—à:\", FAST_PKL)\n",
    "\n",
    "    # merge –≤ —Ñ–∏—á–∏\n",
    "    train_fe = train_fe.merge(sent_latents, on='id', how='left')\n",
    "    test_fe  = test_fe.merge(sent_latents,  on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 10: –ì–µ–æ-–æ–∫—Ä—É–∂–µ–Ω–∏–µ (OOF) ‚Äî allcat/samecat cnt + mean/weighted_mean + min_dist/flags\n",
    "# –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç/–∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "train_coords_rad = np.c_[np.deg2rad(train_fe['lat'].astype(float).values),\n",
    "                         np.deg2rad(train_fe['lon'].astype(float).values)]\n",
    "test_coords_rad  = np.c_[np.deg2rad(test_fe['lat'].astype(float).values),\n",
    "                         np.deg2rad(test_fe['lon'].astype(float).values)]\n",
    "train_cat = train_fe['category'].astype('category') if 'category' in train_fe.columns else pd.Series(['_']*len(train_fe)).astype('category')\n",
    "test_cat  = (test_fe['category'].astype('category') if 'category' in test_fe.columns else pd.Series(['_']*len(test_fe)).astype('category')).cat.set_categories(train_cat.cat.categories)\n",
    "train_cat_codes = train_cat.cat.codes.values\n",
    "test_cat_codes  = test_cat.cat.codes.values\n",
    "\n",
    "base_splits, lab_idx = build_base_splits(train_fe, n_splits=5, seed=RANDOM_SEED)\n",
    "y_full = train_fe['target'].astype(float).values\n",
    "valid_y = (y_full > 0) & ~np.isnan(y_full)\n",
    "y_masked = np.where(valid_y, y_full, np.nan)\n",
    "\n",
    "# –∑–∞–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–ª–æ–Ω–æ–∫\n",
    "for r in RADII_M:\n",
    "    for pref in ['geo_allcat','geo_samecat']:\n",
    "        for stat in ['cnt','mean_target','wmean_target','min_dist_m','has_any']:\n",
    "            col = f'{pref}_{stat}_{r}m'\n",
    "            train_fe[col] = np.nan\n",
    "            test_fe[col]  = np.nan\n",
    "\n",
    "# OOF –¥–ª—è train\n",
    "for fold, (tr_idx, va_idx) in enumerate(base_splits, 1):\n",
    "    tree = BallTree(train_coords_rad[tr_idx], metric='haversine')\n",
    "    y_tr  = y_masked[tr_idx]\n",
    "    cat_tr= train_cat_codes[tr_idx]\n",
    "    XY_va = train_coords_rad[va_idx]\n",
    "    cat_va= train_cat_codes[va_idx]\n",
    "\n",
    "    for r in RADII_M:\n",
    "        rad = meters_to_radians(r)\n",
    "        ind_list = tree.query_radius(XY_va, r=rad, return_distance=True)\n",
    "        cnt_all = np.zeros(len(va_idx)); mean_all = np.full(len(va_idx), np.nan)\n",
    "        wmean_all= np.full(len(va_idx), np.nan); mind_all = np.full(len(va_idx), np.nan)\n",
    "        has_all = np.zeros(len(va_idx))\n",
    "        cnt_same = np.zeros(len(va_idx)); mean_same = np.full(len(va_idx), np.nan)\n",
    "        wmean_same= np.full(len(va_idx), np.nan); mind_same = np.full(len(va_idx), np.nan)\n",
    "        has_same = np.zeros(len(va_idx))\n",
    "\n",
    "        for i, (neigh_local, dist) in enumerate(zip(*ind_list)):\n",
    "            # ALL\n",
    "            if neigh_local.size > 0:\n",
    "                vals = y_tr[neigh_local]\n",
    "                cnt_all[i] = neigh_local.size\n",
    "                has_all[i] = 1.0\n",
    "                if np.isfinite(vals).any():\n",
    "                    mean_all[i] = np.nanmean(vals)\n",
    "                    d_m = dist * EARTH_RADIUS_M\n",
    "                    w = 1.0 / (d_m + 50.0)  # 50 –º —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ\n",
    "                    w[np.isnan(vals)] = 0.0\n",
    "                    if 'sent_total_reviews' in train_fe.columns:\n",
    "                        w *= np.log1p(train_fe['sent_total_reviews'].values[tr_idx][neigh_local])\n",
    "                    s = w.sum()\n",
    "                    wmean_all[i] = np.nan if s==0 else float(np.nansum(vals*w)/s)\n",
    "                mind_all[i] = (dist.min()*EARTH_RADIUS_M) if dist.size>0 else np.nan\n",
    "            # SAME CAT\n",
    "            mask_same = (cat_tr[neigh_local] == cat_va[i]) if neigh_local.size>0 else np.array([], dtype=bool)\n",
    "            neigh_same = neigh_local[mask_same]\n",
    "            dist_same  = dist[mask_same]\n",
    "            if neigh_same.size > 0:\n",
    "                vals = y_tr[neigh_same]\n",
    "                cnt_same[i] = neigh_same.size\n",
    "                has_same[i] = 1.0\n",
    "                if np.isfinite(vals).any():\n",
    "                    mean_same[i] = np.nanmean(vals)\n",
    "                    d_m = dist_same * EARTH_RADIUS_M\n",
    "                    w = 1.0 / (d_m + 50.0)\n",
    "                    w[np.isnan(vals)] = 0.0\n",
    "                    if 'sent_total_reviews' in train_fe.columns:\n",
    "                        w *= np.log1p(train_fe['sent_total_reviews'].values[tr_idx][neigh_same])\n",
    "                    s = w.sum()\n",
    "                    wmean_same[i] = np.nan if s==0 else float(np.nansum(vals*w)/s)\n",
    "                mind_same[i] = (dist_same.min()*EARTH_RADIUS_M) if dist_same.size>0 else np.nan\n",
    "\n",
    "        train_fe.loc[va_idx, f'geo_allcat_cnt_{r}m']          = cnt_all\n",
    "        train_fe.loc[va_idx, f'geo_allcat_mean_target_{r}m']  = mean_all\n",
    "        train_fe.loc[va_idx, f'geo_allcat_wmean_target_{r}m'] = wmean_all\n",
    "        train_fe.loc[va_idx, f'geo_allcat_min_dist_m_{r}m']   = mind_all\n",
    "        train_fe.loc[va_idx, f'geo_allcat_has_any_{r}m']      = has_all\n",
    "\n",
    "        train_fe.loc[va_idx, f'geo_samecat_cnt_{r}m']          = cnt_same\n",
    "        train_fe.loc[va_idx, f'geo_samecat_mean_target_{r}m']  = mean_same\n",
    "        train_fe.loc[va_idx, f'geo_samecat_wmean_target_{r}m'] = wmean_same\n",
    "        train_fe.loc[va_idx, f'geo_samecat_min_dist_m_{r}m']   = mind_same\n",
    "        train_fe.loc[va_idx, f'geo_samecat_has_any_{r}m']      = has_same\n",
    "\n",
    "# –î–ª—è test ‚Äî –¥–µ—Ä–µ–≤–æ –Ω–∞ –≤—Å—ë–º train\n",
    "full_tree = BallTree(train_coords_rad, metric='haversine')\n",
    "for r in RADII_M:\n",
    "    rad = meters_to_radians(r)\n",
    "    ind_list = full_tree.query_radius(test_coords_rad, r=rad, return_distance=True)\n",
    "    cnt_all = np.zeros(len(test_fe)); mean_all = np.full(len(test_fe), np.nan)\n",
    "    wmean_all= np.full(len(test_fe), np.nan); mind_all = np.full(len(test_fe), np.nan)\n",
    "    has_all = np.zeros(len(test_fe))\n",
    "    cnt_same= np.zeros(len(test_fe)); mean_same= np.full(len(test_fe), np.nan)\n",
    "    wmean_same= np.full(len(test_fe), np.nan); mind_same= np.full(len(test_fe), np.nan)\n",
    "    has_same = np.zeros(len(test_fe))\n",
    "    for i, (neigh, dist) in enumerate(zip(*ind_list)):\n",
    "        if neigh.size>0:\n",
    "            vals = y_masked[neigh]\n",
    "            cnt_all[i] = neigh.size; has_all[i] = 1.0\n",
    "            if np.isfinite(vals).any():\n",
    "                mean_all[i] = np.nanmean(vals)\n",
    "                d_m = dist * EARTH_RADIUS_M\n",
    "                w = 1.0 / (d_m + 50.0)\n",
    "                w[np.isnan(vals)] = 0.0\n",
    "                if 'sent_total_reviews' in train_fe.columns:\n",
    "                    w *= np.log1p(train_fe['sent_total_reviews'].values[neigh])\n",
    "                s = w.sum()\n",
    "                wmean_all[i] = np.nan if s==0 else float(np.nansum(vals*w)/s)\n",
    "            mind_all[i] = (dist.min()*EARTH_RADIUS_M) if dist.size>0 else np.nan\n",
    "        mask_same = (train_cat_codes[neigh] == test_cat_codes[i]) if neigh.size>0 else np.array([], dtype=bool)\n",
    "        neigh_s = neigh[mask_same]; dist_s = dist[mask_same]\n",
    "        if neigh_s.size>0:\n",
    "            vals = y_masked[neigh_s]\n",
    "            cnt_same[i] = neigh_s.size; has_same[i] = 1.0\n",
    "            if np.isfinite(vals).any():\n",
    "                mean_same[i] = np.nanmean(vals)\n",
    "                d_m = dist_s * EARTH_RADIUS_M\n",
    "                w = 1.0 / (d_m + 50.0)\n",
    "                w[np.isnan(vals)] = 0.0\n",
    "                if 'sent_total_reviews' in train_fe.columns:\n",
    "                    w *= np.log1p(train_fe['sent_total_reviews'].values[neigh_s])\n",
    "                s = w.sum()\n",
    "                wmean_same[i] = np.nan if s==0 else float(np.nansum(vals*w)/s)\n",
    "            mind_same[i] = (dist_s.min()*EARTH_RADIUS_M) if dist_s.size>0 else np.nan\n",
    "\n",
    "    test_fe[f'geo_allcat_cnt_{r}m']          = cnt_all\n",
    "    test_fe[f'geo_allcat_mean_target_{r}m']  = mean_all\n",
    "    test_fe[f'geo_allcat_wmean_target_{r}m'] = wmean_all\n",
    "    test_fe[f'geo_allcat_min_dist_m_{r}m']   = mind_all\n",
    "    test_fe[f'geo_allcat_has_any_{r}m']      = has_all\n",
    "\n",
    "    test_fe[f'geo_samecat_cnt_{r}m']          = cnt_same\n",
    "    test_fe[f'geo_samecat_mean_target_{r}m']  = mean_same\n",
    "    test_fe[f'geo_samecat_wmean_target_{r}m'] = wmean_same\n",
    "    test_fe[f'geo_samecat_min_dist_m_{r}m']   = mind_same\n",
    "    test_fe[f'geo_samecat_has_any_{r}m']      = has_same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 11: –ö–æ—Å–∏–Ω—É—Å –∫ —Ü–µ–Ω—Ç—Ä–æ–∏–¥—É —Å–æ—Å–µ–¥–µ–π —Ç–æ–π –∂–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≤–∑–≤–µ—à–µ–Ω–Ω—ã–π)\n",
    "emb_cols = [c for c in train_fe.columns if c.startswith('rubert_svd_')]\n",
    "if not emb_cols:\n",
    "    emb_cols = [c for c in train_fe.columns if c.startswith('rubert_') and ('_pos' not in c and '_neg' not in c)]\n",
    "\n",
    "if emb_cols:\n",
    "    Z_tr = l2norm(train_fe[emb_cols].fillna(0.0).values.astype('float32'))\n",
    "    Z_te = l2norm(test_fe[emb_cols].fillna(0.0).values.astype('float32'))\n",
    "\n",
    "    for r in RADII_M:\n",
    "        train_fe[f'geo_samecat_sim_{r}m'] = np.nan\n",
    "        test_fe[f'geo_samecat_sim_{r}m']  = np.nan\n",
    "\n",
    "    # OOF\n",
    "    for tr_idx, va_idx in base_splits:\n",
    "        tree = BallTree(train_coords_rad[tr_idx], metric='haversine')\n",
    "        trZ  = Z_tr[tr_idx]\n",
    "        trC  = train_cat_codes[tr_idx]\n",
    "        vaZ  = Z_tr[va_idx]\n",
    "        vaC  = train_cat_codes[va_idx]\n",
    "        vaXY = train_coords_rad[va_idx]\n",
    "        for r in RADII_M:\n",
    "            rad = meters_to_radians(r)\n",
    "            ind_list = tree.query_radius(vaXY, r=rad, return_distance=True)\n",
    "            sims = np.zeros(len(va_idx), dtype='float32')\n",
    "            for i, (neigh_local, dist) in enumerate(zip(*ind_list)):\n",
    "                mask = (trC[neigh_local] == vaC[i])\n",
    "                neigh_local = neigh_local[mask]\n",
    "                dist = dist[mask]\n",
    "                if neigh_local.size == 0:\n",
    "                    sims[i] = 0.0\n",
    "                else:\n",
    "                    w = 1.0 / (dist*EARTH_RADIUS_M + 50.0)\n",
    "                    if 'sent_total_reviews' in train_fe.columns:\n",
    "                        w *= np.log1p(train_fe['sent_total_reviews'].values[tr_idx][neigh_local])\n",
    "                    w = w / (w.sum() + 1e-12)\n",
    "                    centroid = (trZ[neigh_local] * w[:,None]).sum(axis=0)\n",
    "                    sims[i] = float(np.dot(vaZ[i], centroid / (np.linalg.norm(centroid)+1e-12)))\n",
    "            train_fe.loc[va_idx, f'geo_samecat_sim_{r}m'] = sims\n",
    "\n",
    "    # test\n",
    "    full_tree = BallTree(train_coords_rad, metric='haversine')\n",
    "    for r in RADII_M:\n",
    "        rad = meters_to_radians(r)\n",
    "        ind_list = full_tree.query_radius(test_coords_rad, r=rad, return_distance=True)\n",
    "        sims = np.zeros(len(test_fe), dtype='float32')\n",
    "        for i, (neigh, dist) in enumerate(zip(*ind_list)):\n",
    "            mask = (train_cat_codes[neigh] == test_cat_codes[i])\n",
    "            neigh = neigh[mask]; dist = dist[mask]\n",
    "            if neigh.size == 0:\n",
    "                sims[i] = 0.0\n",
    "            else:\n",
    "                w = 1.0 / (dist*EARTH_RADIUS_M + 50.0)\n",
    "                if 'sent_total_reviews' in train_fe.columns:\n",
    "                    w *= np.log1p(train_fe['sent_total_reviews'].values[neigh])\n",
    "                w = w / (w.sum() + 1e-12)\n",
    "                centroid = (Z_tr[neigh] * w[:,None]).sum(axis=0)\n",
    "                sims[i] = float(np.dot(Z_te[i], centroid / (np.linalg.norm(centroid)+1e-12)))\n",
    "        test_fe[f'geo_samecat_sim_{r}m'] = sims\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è –ù–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ‚Äî —à–∞–≥ 11 –ø—Ä–æ–ø—É—â–µ–Ω.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 12: Target Encoding (OOF meta) –¥–ª—è category –∏ category√ógeo_cluster\n",
    "def oof_target_encode_meta(train_df, test_df, cols_key, target_col='target', n_splits=5, seed=42, prior=200):\n",
    "    tr_mask = (train_df[target_col] > 0) & train_df[target_col].notna()\n",
    "    tr = train_df.loc[tr_mask].copy()\n",
    "    key = tr[cols_key].apply(lambda r: tuple(r), axis=1)\n",
    "    gmean = tr[target_col].mean()\n",
    "\n",
    "    # OOF\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    bins = pd.qcut(tr[target_col], q=min(10, tr[target_col].nunique()), duplicates='drop').cat.codes\n",
    "    te_oof = np.zeros(len(tr))\n",
    "    for tr_idx, va_idx in skf.split(tr, bins):\n",
    "        mean_by_key = tr.iloc[tr_idx].groupby(key.iloc[tr_idx])[target_col].agg(['mean','count'])\n",
    "        mean_by_key['smooth'] = (mean_by_key['mean']*mean_by_key['count'] + gmean*prior) / (mean_by_key['count'] + prior)\n",
    "        map_dict = mean_by_key['smooth'].to_dict()\n",
    "        te_oof[va_idx] = [map_dict.get(tuple(k), gmean) for k in key.iloc[va_idx]]\n",
    "\n",
    "    # FULL map -> –ø—Ä–∏–º–µ–Ω—è–µ–º –∫ train/test\n",
    "    key_full_tr = train_df[cols_key].apply(lambda r: tuple(r), axis=1)\n",
    "    key_full_te = test_df[cols_key].apply(lambda r: tuple(r), axis=1)\n",
    "    mean_by_key_full = tr.groupby(key)[target_col].agg(['mean','count'])\n",
    "    mean_by_key_full['smooth'] = (mean_by_key_full['mean']*mean_by_key_full['count'] + gmean*prior) / (mean_by_key_full['count'] + prior)\n",
    "    map_full = mean_by_key_full['smooth'].to_dict()\n",
    "    te_train_full = np.array([map_full.get(tuple(k), gmean) for k in key_full_tr])\n",
    "    te_test_full  = np.array([map_full.get(tuple(k), gmean) for k in key_full_te])\n",
    "\n",
    "    # meta: train=OOF (–Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö), test=FULL\n",
    "    meta_tr = np.zeros(len(train_df))\n",
    "    meta_tr[tr_mask.values] = te_oof\n",
    "    meta_te = te_test_full\n",
    "    return meta_tr, meta_te\n",
    "\n",
    "# TE: category\n",
    "train_fe['TE_category_meta'], test_fe['TE_category_meta'] = oof_target_encode_meta(train_fe, test_fe, ['category'], prior=200)\n",
    "\n",
    "# TE: category √ó geo_cluster (–µ—Å–ª–∏ –µ—Å—Ç—å)\n",
    "if 'geo_cluster' in train_fe.columns:\n",
    "    train_fe['TE_cat_cluster_meta'], test_fe['TE_cat_cluster_meta'] = oof_target_encode_meta(train_fe, test_fe, ['category','geo_cluster'], prior=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 13: TF-IDF (char_wb 3‚Äì5) ‚Üí Ridge (OOF meta –ø—Ä–∏–∑–Ω–∞–∫)\n",
    "MAX_CHARS = 10000\n",
    "place_text = reviews.groupby('id', as_index=False)['text'].apply(lambda s: ' '.join(s.fillna(''))).rename(columns={'text':'agg_text'})\n",
    "place_text['agg_text'] = place_text['agg_text'].str[:MAX_CHARS]\n",
    "\n",
    "tr_text = train_fe[['id']].merge(place_text, on='id', how='left')['agg_text'].fillna('')\n",
    "te_text = test_fe[['id'] ].merge(place_text, on='id', how='left')['agg_text'].fillna('')\n",
    "\n",
    "tfidf = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(3,5), analyzer='char_wb', max_features=200_000)\n",
    "X_tr = tfidf.fit_transform(tr_text)\n",
    "X_te = tfidf.transform(te_text)\n",
    "\n",
    "y_full = train_fe['target'].astype(float).values\n",
    "mask = (y_full > 0) & ~np.isnan(y_full)\n",
    "X_tr_m = X_tr[mask]\n",
    "y_m    = y_full[mask]\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "oof = np.zeros(X_tr_m.shape[0]); test_pred = np.zeros(X_te.shape[0])\n",
    "for tr_idx, va_idx in kf.split(X_tr_m):\n",
    "    model = Ridge(alpha=2.0, random_state=RANDOM_SEED)\n",
    "    model.fit(X_tr_m[tr_idx], y_m[tr_idx])\n",
    "    oof[va_idx] = model.predict(X_tr_m[va_idx])\n",
    "    test_pred += model.predict(X_te) / kf.get_n_splits()\n",
    "\n",
    "train_fe['tfidf_meta'] = 0.0\n",
    "train_fe.loc[mask, 'tfidf_meta'] = oof\n",
    "test_fe['tfidf_meta'] = test_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 14: ¬´–¶–µ–Ω—Ç—Ä –≥–æ—Ä–æ–¥–∞¬ª –±–µ–∑ —Ö–∞—Ä–¥–∫–æ–¥–∞ ‚Äî —á–µ—Ä–µ–∑ KMeans –ø–æ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞–º\n",
    "XY_all = np.c_[np.deg2rad(pd.concat([train_fe['lat'], test_fe['lat']]).astype(float).values),\n",
    "               np.deg2rad(pd.concat([train_fe['lon'], test_fe['lon']]).astype(float).values)]\n",
    "km_c = MiniBatchKMeans(n_clusters=5, random_state=RANDOM_SEED, batch_size=4096).fit(XY_all)\n",
    "centers = km_c.cluster_centers_\n",
    "\n",
    "def min_dist_to_centers(df):\n",
    "    pts = np.c_[np.deg2rad(df['lat'].astype(float).values), np.deg2rad(df['lon'].astype(float).values)]\n",
    "    d = ((pts[:,None,:]-centers[None,:,:])**2).sum(axis=2)**0.5\n",
    "    return d.min(axis=1) * EARTH_RADIUS_KM\n",
    "\n",
    "train_fe['dist_center_km'] = min_dist_to_centers(train_fe)\n",
    "test_fe['dist_center_km']  = min_dist_to_centers(test_fe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 15: –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è train/test, —Å–±–æ—Ä —Å–ø–∏—Å–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "from pandas.api.types import is_numeric_dtype, is_string_dtype, is_categorical_dtype\n",
    "\n",
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏—è —Å—Ç—Ä–æ–∫–æ–π –¥–ª—è CatBoost\n",
    "if 'category' in train_fe.columns:\n",
    "    train_fe['category'] = train_fe['category'].astype(str)\n",
    "    test_fe['category']  = test_fe['category'].astype(str)\n",
    "\n",
    "drop_cols = {'target','name','address','coordinates'}\n",
    "features = [c for c in train_fe.columns if c not in drop_cols]\n",
    "\n",
    "# –ò—Å–∫–ª—é—á–∏–º —è–≤–Ω—ã–µ –Ω–µ-—Å–∫–∞–ª—è—Ä–Ω—ã–µ –ø–æ–ª—è, –µ—Å–ª–∏ —Å–ª—É—á–∞–π–Ω–æ –ø–æ–ø–∞–ª–∏\n",
    "def _is_complex(x): return isinstance(x, (list, tuple, dict, set))\n",
    "complex_cols = []\n",
    "for c in list(features):\n",
    "    if c in train_fe.columns and (train_fe[c].apply(_is_complex).any() or (c in test_fe.columns and test_fe[c].apply(_is_complex).any())):\n",
    "        complex_cols.append(c); features.remove(c)\n",
    "if complex_cols: print(\"üóëÔ∏è –£–±—Ä–∞–Ω—ã –Ω–µ-—Å–∫–∞–ª—è—Ä–Ω—ã–µ —Ñ–∏—á–∏:\", complex_cols[:10], \"‚Ä¶\")\n",
    "\n",
    "# –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –Ω–∞–±–æ—Ä–∞ –∫–æ–ª–æ–Ω–æ–∫\n",
    "common_cols = sorted(list(set(train_fe.columns) & set(test_fe.columns)))\n",
    "if 'target' in common_cols: common_cols.remove('target')\n",
    "train_cols_final = common_cols + (['target'] if 'target' in train_fe.columns else [])\n",
    "train_fe = train_fe[train_cols_final].copy()\n",
    "test_fe  = test_fe[common_cols].copy()\n",
    "\n",
    "# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ NaN: –±–µ—Ä–µ–∂–Ω–æ ‚Äî NaN –≤ geo_/TE_/tfidf_meta –æ—Å—Ç–∞–≤–ª—è–µ–º (CatBoost —É–º–µ–µ—Ç)\n",
    "num_common = [c for c in common_cols if is_numeric_dtype(train_fe[c])]\n",
    "protect_prefixes = ('geo_', 'TE_', 'tfidf_meta')\n",
    "safe_fill = [c for c in num_common if not any(c.startswith(p) for p in protect_prefixes)]\n",
    "train_fe[safe_fill] = train_fe[safe_fill].fillna(0)\n",
    "test_fe[safe_fill]  = test_fe[safe_fill].fillna(0)\n",
    "\n",
    "# –∑–∞—á–∏—Å—Ç–∫–∞ inf\n",
    "for df in (train_fe, test_fe):\n",
    "    arr = df[num_common].to_numpy()\n",
    "    mask_bad = ~np.isfinite(arr)\n",
    "    if mask_bad.any():\n",
    "        arr[mask_bad] = 0.0\n",
    "        df[num_common] = arr\n",
    "\n",
    "# –ü–µ—Ä–µ—Å–±–æ—Ä–∫–∞ features –ø–æ —Ñ–∞–∫—Ç—É\n",
    "features = [c for c in common_cols if c != 'id']\n",
    "num_features = [c for c in features if is_numeric_dtype(train_fe[c])]\n",
    "cat_features = [c for c in features if c not in num_features]\n",
    "\n",
    "# –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫ —Å—Ç—Ä–æ–∫–∞–º/–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º, –±–µ–∑ NaN\n",
    "for df_name, df in (('train', train_fe), ('test', test_fe)):\n",
    "    for c in cat_features:\n",
    "        if df[c].isna().any():\n",
    "            df[c] = df[c].fillna(\"<unknown>\")\n",
    "        if not (is_categorical_dtype(df[c]) or is_string_dtype(df[c])):\n",
    "            df[c] = df[c].astype('string')\n",
    "\n",
    "print(f\"üìä –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã: train={train_fe.shape}, test={test_fe.shape}\")\n",
    "print(f\"üß© –§–∏—á –≤—Å–µ–≥–æ: {len(features)} | —á–∏—Å–ª–æ–≤—ã—Ö: {len(num_features)} | –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {len(cat_features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 16: 5-fold CatBoost CV (MAE) + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ test\n",
    "params = dict(\n",
    "    loss_function='MAE',\n",
    "    eval_metric='MAE',\n",
    "    learning_rate=0.03,\n",
    "    depth=8,\n",
    "    l2_leaf_reg=5.0,\n",
    "    random_seed=RANDOM_SEED,\n",
    "    iterations=10000,\n",
    "    od_type='Iter',\n",
    "    od_wait=200,\n",
    "    verbose=200,\n",
    "    allow_writing_files=False\n",
    ")\n",
    "\n",
    "# –±–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏\n",
    "labeled = train_fe.loc[(train_fe['target']>0) & train_fe['target'].notna()].reset_index(drop=True)\n",
    "X = labeled[features]\n",
    "y = labeled['target'].astype(float).values\n",
    "\n",
    "# —Å—Ç—Ä–∞—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ –±–∏–Ω–∞–º —Ç–∞—Ä–≥–µ—Ç–∞\n",
    "labeled['target_bin'] = pd.qcut(labeled['target'], q=min(10, labeled['target'].nunique()), duplicates='drop').cat.codes\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "oof = np.zeros(len(labeled), dtype=float)\n",
    "test_pred = np.zeros(len(test_fe), dtype=float)\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(skf.split(X, labeled['target_bin']), 1):\n",
    "    tr_pool = Pool(X.iloc[tr_idx], label=y[tr_idx], cat_features=cat_features)\n",
    "    va_pool = Pool(X.iloc[va_idx], label=y[va_idx], cat_features=cat_features)\n",
    "    model = CatBoostRegressor(**params)\n",
    "    print(f\"\\n--- –§–æ–ª–¥ {fold}/5 ---\")\n",
    "    model.fit(tr_pool, eval_set=va_pool, use_best_model=True)\n",
    "    oof[va_idx] = model.predict(va_pool)\n",
    "    test_pred += model.predict(Pool(test_fe[features], cat_features=cat_features)) / 5.0\n",
    "\n",
    "mae = mean_absolute_error(y, oof)\n",
    "print(f\"\\n‚úÖ OOF MAE: {mae:.6f}\")\n",
    "\n",
    "final_pred = np.clip(test_pred, 1, 5)\n",
    "print(\"pred stats:\", float(final_pred.min()), float(final_pred.mean()), float(final_pred.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –®–ê–ì 17: –°–∞–±–º–∏—Ç\n",
    "sub_name = f\"submission_cat_5fold_seed{RANDOM_SEED}_d8.csv\"\n",
    "pd.DataFrame({'id': test_fe['id'].astype(str), 'target': final_pred.astype(float)}).to_csv(sub_name, index=False)\n",
    "print(\"üíæ Saved:\", sub_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
