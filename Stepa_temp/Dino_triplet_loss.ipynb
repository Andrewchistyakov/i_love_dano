{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:19.896685Z",
     "iopub.status.busy": "2025-09-18T08:33:19.896444Z",
     "iopub.status.idle": "2025-09-18T08:33:21.238518Z",
     "shell.execute_reply": "2025-09-18T08:33:21.237812Z",
     "shell.execute_reply.started": "2025-09-18T08:33:19.896662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:21.240079Z",
     "iopub.status.busy": "2025-09-18T08:33:21.239788Z",
     "iopub.status.idle": "2025-09-18T08:33:21.535578Z",
     "shell.execute_reply": "2025-09-18T08:33:21.534797Z",
     "shell.execute_reply.started": "2025-09-18T08:33:21.240062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>train_4028265689</td>\n",
       "      <td>fff1c07ceefc2c970a7964cfb81981c5.jpg</td>\n",
       "      <td>e3cd72389f248f21</td>\n",
       "      <td>Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...</td>\n",
       "      <td>3776555725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34246</th>\n",
       "      <td>train_769054909</td>\n",
       "      <td>fff401691371bdcb382a0d9075dfea6a.jpg</td>\n",
       "      <td>be86851f72e2853c</td>\n",
       "      <td>MamyPoko Pants Royal Soft - S 70 - Popok Celana</td>\n",
       "      <td>2736479533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34247</th>\n",
       "      <td>train_614977732</td>\n",
       "      <td>fff421b78fa7284284724baf249f522e.jpg</td>\n",
       "      <td>ad27f0d08c0fcbf0</td>\n",
       "      <td>KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...</td>\n",
       "      <td>4101248785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34248</th>\n",
       "      <td>train_3630949769</td>\n",
       "      <td>fff51b87916dbfb6d0f8faa01bee67b8.jpg</td>\n",
       "      <td>e3b13bd1d896c05c</td>\n",
       "      <td>Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...</td>\n",
       "      <td>1663538013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34249</th>\n",
       "      <td>train_1792180725</td>\n",
       "      <td>ffffa0ab2ae542357671e96254fa7167.jpg</td>\n",
       "      <td>af8bc4b2d2cf9083</td>\n",
       "      <td>FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...</td>\n",
       "      <td>459464107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34250 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             posting_id                                 image  \\\n",
       "0       train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg   \n",
       "1      train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg   \n",
       "2      train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg   \n",
       "3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n",
       "4      train_3369186413  00136d1cf4edede0203f32f05f660588.jpg   \n",
       "...                 ...                                   ...   \n",
       "34245  train_4028265689  fff1c07ceefc2c970a7964cfb81981c5.jpg   \n",
       "34246   train_769054909  fff401691371bdcb382a0d9075dfea6a.jpg   \n",
       "34247   train_614977732  fff421b78fa7284284724baf249f522e.jpg   \n",
       "34248  train_3630949769  fff51b87916dbfb6d0f8faa01bee67b8.jpg   \n",
       "34249  train_1792180725  ffffa0ab2ae542357671e96254fa7167.jpg   \n",
       "\n",
       "            image_phash                                              title  \\\n",
       "0      94974f937d4c2433                          Paper Bag Victoria Secret   \n",
       "1      af3f9460c2838f0f  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   \n",
       "2      b94cb00ed3e50f78        Maling TTS Canned Pork Luncheon Meat 397 gr   \n",
       "3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n",
       "4      a6f319f924ad708c                  Nescafe \\xc3\\x89clair Latte 220ml   \n",
       "...                 ...                                                ...   \n",
       "34245  e3cd72389f248f21  Masker Bahan Kain Spunbond Non Woven 75 gsm 3 ...   \n",
       "34246  be86851f72e2853c    MamyPoko Pants Royal Soft - S 70 - Popok Celana   \n",
       "34247  ad27f0d08c0fcbf0  KHANZAACC Robot RE101S 1.2mm Subwoofer Bass Me...   \n",
       "34248  e3b13bd1d896c05c  Kaldu NON MSG HALAL Mama Kamu Ayam Kampung , S...   \n",
       "34249  af8bc4b2d2cf9083  FLEX TAPE PELAPIS BOCOR / ISOLASI AJAIB / ANTI...   \n",
       "\n",
       "      label_group  \n",
       "0       249114794  \n",
       "1      2937985045  \n",
       "2      2395904891  \n",
       "3      4093212188  \n",
       "4      3648931069  \n",
       "...           ...  \n",
       "34245  3776555725  \n",
       "34246  2736479533  \n",
       "34247  4101248785  \n",
       "34248  1663538013  \n",
       "34249   459464107  \n",
       "\n",
       "[34250 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"shopee-product-matching/train.csv\")\n",
    "train['label_group'] = train['label_group'].astype(str)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:21.536695Z",
     "iopub.status.busy": "2025-09-18T08:33:21.536428Z",
     "iopub.status.idle": "2025-09-18T08:33:22.476593Z",
     "shell.execute_reply": "2025-09-18T08:33:22.475795Z",
     "shell.execute_reply.started": "2025-09-18T08:33:21.536670Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = train.sample(frac=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:50:26.891800Z",
     "iopub.status.busy": "2025-09-18T08:50:26.891089Z",
     "iopub.status.idle": "2025-09-18T08:50:26.903419Z",
     "shell.execute_reply": "2025-09-18T08:50:26.902772Z",
     "shell.execute_reply.started": "2025-09-18T08:50:26.891776Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>0006c8e5462ae52167402bac1c2e916e.jpg</td>\n",
       "      <td>ecc292392dc7687a</td>\n",
       "      <td>Edufuntoys - CHARACTER PHONE ada lampu dan mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>0007585c4d0f932859339129f709bfdc.jpg</td>\n",
       "      <td>e9968f60d2699e2c</td>\n",
       "      <td>(Beli 1 Free Spatula) Masker Komedo | Blackhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>0008377d3662e83ef44e1881af38b879.jpg</td>\n",
       "      <td>ba81c17e3581cabe</td>\n",
       "      <td>READY Lemonilo Mie instant sehat kuah dan goreng</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id                                 image       image_phash  \\\n",
       "0  test_2255846744  0006c8e5462ae52167402bac1c2e916e.jpg  ecc292392dc7687a   \n",
       "1  test_3588702337  0007585c4d0f932859339129f709bfdc.jpg  e9968f60d2699e2c   \n",
       "2  test_4015706929  0008377d3662e83ef44e1881af38b879.jpg  ba81c17e3581cabe   \n",
       "\n",
       "                                               title  \n",
       "0  Edufuntoys - CHARACTER PHONE ada lampu dan mus...  \n",
       "1  (Beli 1 Free Spatula) Masker Komedo | Blackhea...  \n",
       "2   READY Lemonilo Mie instant sehat kuah dan goreng  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"shopee-product-matching/test.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:22.495142Z",
     "iopub.status.busy": "2025-09-18T08:33:22.494896Z",
     "iopub.status.idle": "2025-09-18T08:33:22.506612Z",
     "shell.execute_reply": "2025-09-18T08:33:22.505780Z",
     "shell.execute_reply.started": "2025-09-18T08:33:22.495102Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_2255846744</td>\n",
       "      <td>test_2255846744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_3588702337</td>\n",
       "      <td>test_3588702337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_4015706929</td>\n",
       "      <td>test_4015706929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        posting_id          matches\n",
       "0  test_2255846744  test_2255846744\n",
       "1  test_3588702337  test_3588702337\n",
       "2  test_4015706929  test_4015706929"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission = pd.read_csv(\"shopee-product-matching/sample_submission.csv\")\n",
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:22.507647Z",
     "iopub.status.busy": "2025-09-18T08:33:22.507440Z",
     "iopub.status.idle": "2025-09-18T08:33:22.518647Z",
     "shell.execute_reply": "2025-09-18T08:33:22.518012Z",
     "shell.execute_reply.started": "2025-09-18T08:33:22.507630Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# num_classes = len(train['label_group'].unique())\n",
    "# num_classes\n",
    "num_classes = 11014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:22.519786Z",
     "iopub.status.busy": "2025-09-18T08:33:22.519510Z",
     "iopub.status.idle": "2025-09-18T08:33:22.534271Z",
     "shell.execute_reply": "2025-09-18T08:33:22.533408Z",
     "shell.execute_reply.started": "2025-09-18T08:33:22.519758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "all_labels = train['label_group'].unique()\n",
    "\n",
    "# Создаем encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(all_labels)\n",
    "\n",
    "# Функции для преобразования\n",
    "def text_to_label(text_labels):\n",
    "    \"\"\"Преобразует текстовые метки в числовые\"\"\"\n",
    "    return label_encoder.transform(text_labels)\n",
    "\n",
    "def label_to_text(numeric_labels):\n",
    "    \"\"\"Преобразует числовые метки обратно в текстовые\"\"\"\n",
    "    return label_encoder.inverse_transform(numeric_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T08:33:22.535690Z",
     "iopub.status.busy": "2025-09-18T08:33:22.535234Z",
     "iopub.status.idle": "2025-09-18T08:33:22.551467Z",
     "shell.execute_reply": "2025-09-18T08:33:22.550604Z",
     "shell.execute_reply.started": "2025-09-18T08:33:22.535662Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=['label_group'])\n",
    "y = train['label_group']\n",
    "y = text_to_label(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.prefix = \"shopee-product-matching/train_images/\"\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        image = Image.open(self.prefix + path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "train_dataset = ImageDataset(X_train['image'].tolist(), y_train.tolist(), transform)\n",
    "val_dataset = ImageDataset(X_val['image'].tolist(), y_val.tolist(), transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMAGE EMBEDDINGS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dinov2_embedder.py\n",
    "from typing import List, Union, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "\n",
    "class DINOv2Embedder(nn.Module):\n",
    "    \"\"\"\n",
    "    DINOv2 (facebook/dinov2-*) — возвращает CLS-эмбеддинг без головы.\n",
    "    Совместим с triplet loss, L2-нормировка опциональна.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"facebook/dinov2-base\",   # small / base / large / giant\n",
    "        device: Optional[str] = None,\n",
    "        dtype: Optional[torch.dtype] = None,\n",
    "        normalize_l2: bool = False,\n",
    "        fp16_infer: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device) if device else torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        self.normalize_l2 = normalize_l2\n",
    "        self.fp16_infer = fp16_infer and self.device.type == \"cuda\"\n",
    "\n",
    "        # Загрузка модели и препроцессора\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        # в DINOv2 \"last_hidden_state[:,0]\" — CLS-токен, головы классификации нет\n",
    "        self.model.eval()\n",
    "\n",
    "        # размер эмбеддинга (768 для base)\n",
    "        self.embed_dim = 768\n",
    "        self.dtype = dtype if dtype is not None else (\n",
    "            torch.float16 if (self.fp16_infer and self.device.type == \"cuda\") else torch.float32\n",
    "        )\n",
    "        if self.dtype == torch.float16:\n",
    "            self.model = self.model.half()\n",
    "\n",
    "    def encode_batch(\n",
    "        self,\n",
    "        images: Union[List[Image.Image], Tensor],\n",
    "        return_numpy: bool = False,\n",
    "    ) -> Union[Tensor, \"np.ndarray\"]:\n",
    "        \"\"\"\n",
    "        images: список PIL.Image (RGB) или тензор (B, C, H, W) в [0,1]/[0,255].\n",
    "        Возвращает (B, D) эмбеддинги CLS.\n",
    "        \"\"\"\n",
    "        if isinstance(images, list):\n",
    "            inputs = self.processor(images, return_tensors=\"pt\")\n",
    "        elif isinstance(images, torch.Tensor):\n",
    "            # если подаём тензор, processor сам умеет работать с torch.Tensor\n",
    "            inputs = self.processor(images=images, return_tensors=\"pt\")\n",
    "        else:\n",
    "            raise TypeError(\"images must be a List[PIL.Image] or a torch.Tensor\")\n",
    "\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        autocast_dtype = torch.float16 if self.fp16_infer else (\n",
    "            torch.bfloat16 if self.device.type == \"cuda\" else None\n",
    "        )\n",
    "        if autocast_dtype is not None:\n",
    "            ctx = torch.autocast(device_type=\"cuda\", dtype=autocast_dtype)\n",
    "        else:\n",
    "            from contextlib import nullcontext\n",
    "            ctx = nullcontext()\n",
    "\n",
    "        with ctx:\n",
    "            outputs = self.model(**inputs)\n",
    "            feats: Tensor = outputs.last_hidden_state[:, 0]  # CLS токен\n",
    "\n",
    "        if self.normalize_l2:\n",
    "            feats = torch.nn.functional.normalize(feats, p=2, dim=1)\n",
    "\n",
    "        if return_numpy:\n",
    "            return feats.detach().cpu().numpy()\n",
    "        return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = DINOv2Embedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBank:\n",
    "    def __init__(self, dim, size=50000, device='cuda'):\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "        self.feats = torch.zeros(size, dim, device=device)\n",
    "        self.labels = torch.full((size,), -1, device=device, dtype=torch.long)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def enqueue(self, feats, labels):\n",
    "        b = feats.size(0)\n",
    "        idx = (torch.arange(b, device=self.device) + self.ptr) % self.size\n",
    "        self.feats[idx] = feats\n",
    "        self.labels[idx] = labels\n",
    "        self.ptr = int((self.ptr + b) % self.size)\n",
    "        if self.ptr == 0:\n",
    "            self.full = True\n",
    "\n",
    "    def valid_mask(self):\n",
    "        return self.labels >= 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mine_triplets(anchor_z, anchor_y, mem: MemoryBank, margin=0.3, pos_k=2, neg_k=5):\n",
    "    # anchor_z: [B, D] (L2-norm), anchor_y: [B]\n",
    "    with torch.no_grad():\n",
    "        mask = mem.valid_mask()\n",
    "        mem_z = mem.feats[mask]          # [M, D]\n",
    "        mem_y = mem.labels[mask]         # [M]\n",
    "\n",
    "        if mem_z.numel() == 0:\n",
    "            return []  # пока память пустая — пропустим шаг лосса\n",
    "\n",
    "        sim = (anchor_z.float() @ mem_z.float().T)\n",
    "        dist = (1 - sim).clamp(0, 2)     # cos distance in [0,2]\n",
    "\n",
    "        triplets = []\n",
    "        for i in range(anchor_z.size(0)):\n",
    "            y = anchor_y[i]\n",
    "            pos_mask = (mem_y == y)\n",
    "            neg_mask = ~pos_mask\n",
    "\n",
    "            pos_d = dist[i][pos_mask]\n",
    "            neg_d = dist[i][neg_mask]\n",
    "            if pos_d.numel() == 0 or neg_d.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            # Позитивы: берём ближайшие pos_k\n",
    "            pos_idx = torch.topk(pos_d, k=min(pos_k, pos_d.numel()), largest=False).indices\n",
    "            pos_sel = torch.nonzero(pos_mask, as_tuple=False).squeeze(1)[pos_idx]\n",
    "\n",
    "            # Негативы: distance-weighted вероятности + semi-hard (d > min_pos_d)\n",
    "            min_pos = pos_d.min()\n",
    "            # semi-hard: d_pos < d_neg < d_pos + margin\n",
    "            neg_window = (neg_d > min_pos) & (neg_d < (min_pos + margin))\n",
    "            cand = neg_d[neg_window]\n",
    "            if cand.numel() == 0:\n",
    "                # fallback: самый близкий негатив (hardest)\n",
    "                neg_idx = torch.topk(neg_d, k=min(neg_k, neg_d.numel()), largest=False).indices\n",
    "                neg_sel = torch.nonzero(neg_mask, as_tuple=False).squeeze(1)[neg_idx]\n",
    "            else:\n",
    "                # можно взять k ближайших в окне\n",
    "                k = min(neg_k, cand.numel())\n",
    "                idx_local = torch.topk(cand, k=k, largest=False).indices\n",
    "                neg_candidates = torch.nonzero(neg_mask, as_tuple=False).squeeze(1)[neg_window]\n",
    "                neg_sel = neg_candidates[idx_local]\n",
    "\n",
    "\n",
    "            for p in pos_sel:\n",
    "                for n in neg_sel:\n",
    "                    triplets.append( (i, p.item(), n.item()) )\n",
    "        return triplets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss_from_indices(anchor_z, anchor_y, mem, triplets, margin=0.3):\n",
    "    if len(triplets) == 0:\n",
    "        return anchor_z.sum()*0  # нулевой лосс, корректный градиент = 0\n",
    "    a_idx = torch.tensor([a for a,_,_ in triplets], device=anchor_z.device)\n",
    "    p_idx = torch.tensor([p for _,p,_ in triplets], device=anchor_z.device)\n",
    "    n_idx = torch.tensor([n for _,_,n in triplets], device=anchor_z.device)\n",
    "\n",
    "    mem_z = mem.feats\n",
    "    sim_ap = (anchor_z[a_idx] * mem_z[p_idx]).sum(-1)\n",
    "    sim_an = (anchor_z[a_idx] * mem_z[n_idx]).sum(-1)\n",
    "    # dist = 1 - sim\n",
    "    loss = F.relu( (1 - sim_ap) - (1 - sim_an) + margin )\n",
    "    # опционально: усреднить top-k hardest\n",
    "    return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemoryBank(dim=768, size=50000, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "\n",
    "total_steps = len(train_dataloader) * batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_embeddings(dataloader):\n",
    "    model.eval()\n",
    "    all_z, all_y = [], []\n",
    "    for image, labels in dataloader:\n",
    "        z = model.encode_batch(image)\n",
    "        all_z.append(z)\n",
    "        all_y.append(labels.to(device))\n",
    "    model.train()\n",
    "    return torch.cat(all_z, dim=0), torch.cat(all_y, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def recall_at_k(z, y, ks=(1,5,10), chunk=2000):\n",
    "    \"\"\"\n",
    "    z: [N, D] L2-норм, y: [N]\n",
    "    Считаем топ-K по косинус-сим, исключая self-match.\n",
    "    Для больших N используем блочно (chunk), чтобы не взорваться по памяти.\n",
    "    \"\"\"\n",
    "    N = z.size(0)\n",
    "    hits = {k:0 for k in ks}\n",
    "    for start in range(0, N, chunk):\n",
    "        end = min(start+chunk, N)\n",
    "        q = z[start:end]                      # [Q, D]\n",
    "        sims = q @ z.T                        # [Q, N]\n",
    "        # исключаем самих себя\n",
    "        idx = torch.arange(start, end, device=z.device)\n",
    "        sims[torch.arange(q.size(0), device=z.device), idx] = -1e9\n",
    "\n",
    "        # топ-K предсказанных индексов\n",
    "        max_k = max(ks)\n",
    "        topk_idx = sims.topk(k=max_k, dim=1, largest=True).indices  # [Q, maxK]\n",
    "        target = y[start:end].unsqueeze(1)                           # [Q, 1]\n",
    "        retrieved = y[topk_idx]                                      # [Q, maxK]\n",
    "        eq = (retrieved == target)                                   # bool\n",
    "\n",
    "        for k in ks:\n",
    "            hits[k] += eq[:, :k].any(dim=1).sum().item()\n",
    "    return {k: hits[k] / N for k in ks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero metricsVal R@1=0.0482 R@5=0.0591 R@10=0.0657\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "      val_z, val_y = compute_embeddings(val_dataloader)\n",
    "      metrics = recall_at_k(val_z, val_y, ks=(1, 5, 10))\n",
    "      print(f\"Zero metrics\"\n",
    "            f\"Val R@1={metrics[1]:.4f} R@5={metrics[5]:.4f} R@10={metrics[10]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "\n",
    "def train_triplet(\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs=3,\n",
    "    lr=2e-5,\n",
    "    weight_decay=1e-4,\n",
    "    margin=0.3,\n",
    "    mem_size=100_000,\n",
    "    pos_k=2,\n",
    "    neg_k=5,\n",
    "    grad_clip=1.0,\n",
    "    log_every=50,\n",
    "    accum_steps=1\n",
    "):\n",
    "    hidden_dim = 768\n",
    "    memory = MemoryBank(dim=hidden_dim, size=mem_size, device=device)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # пересчитываем общее число ШАГОВ оптимизатора (а не мини-батчей!)\n",
    "    total_update_steps = math.ceil(len(train_loader) / accum_steps) * epochs\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optim,\n",
    "        num_warmup_steps=int(0.1 * total_update_steps),\n",
    "        num_training_steps=total_update_steps\n",
    "    )\n",
    "\n",
    "    print(f\"Start training: epochs={epochs}, lr={lr}, mem_size={mem_size}, dim={hidden_dim}\")\n",
    "    best_R1 = -1.0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        running = defaultdict(float)\n",
    "        triplet_cnt = 0\n",
    "        optim.zero_grad()        # <── теперь обнуляем градиенты один раз в начале эпохи\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_loader, 1):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            z = model.encode_batch(images)\n",
    "\n",
    "\n",
    "            # майнинг триплетов\n",
    "            triplets = mine_triplets(z.detach(), labels, memory,\n",
    "                                     margin=margin, pos_k=pos_k, neg_k=neg_k)\n",
    "            if len(triplets) > 0:\n",
    "\n",
    "                # лосс делим на accum_steps\n",
    "                loss = triplet_loss_from_indices(z, labels, memory, triplets, margin=margin)\n",
    "                loss = loss / accum_steps\n",
    "                loss.backward()\n",
    "\n",
    "                running['loss'] += loss.item() * accum_steps  # для логов возвращаем к исходной шкале\n",
    "                triplet_cnt += len(triplets)\n",
    "\n",
    "                # шаг оптимизатора только после accum_steps\n",
    "                if step % accum_steps == 0:\n",
    "                    if grad_clip is not None:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                    optim.step()\n",
    "                    scheduler.step()\n",
    "                    optim.zero_grad()\n",
    "\n",
    "            # обновляем память независимо от оптимизационного шага\n",
    "            with torch.no_grad():\n",
    "                memory.enqueue(z.detach(), labels)\n",
    "            model.train()\n",
    "\n",
    "            # логирование\n",
    "            if step % log_every == 0:\n",
    "                avg_loss = running['loss'] / log_every\n",
    "                avg_trip = triplet_cnt / log_every\n",
    "                print(f\"[epoch {epoch} step {step}] \"\n",
    "                      f\"loss={avg_loss:.4f}  triplets/step={avg_trip:.1f}\")\n",
    "                print(\"lr:\", scheduler.get_last_lr())\n",
    "                running['loss'] = 0.0\n",
    "                triplet_cnt = 0\n",
    "\n",
    "        # если длина даталоадера не кратна accum_steps — делаем «хвостовой» шаг\n",
    "        if len(train_loader) % accum_steps != 0:\n",
    "            if grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optim.step()\n",
    "            scheduler.step()\n",
    "            optim.zero_grad()\n",
    "\n",
    "        # === Валидация ===\n",
    "        with torch.inference_mode():\n",
    "            val_z, val_y = compute_embeddings(val_loader)\n",
    "            metrics = recall_at_k(val_z, val_y, ks=(1, 5, 10))\n",
    "            dt = time.time() - t0\n",
    "            print(f\"Epoch {epoch} done in {dt:.1f}s | \"\n",
    "                f\"Val R@1={metrics[1]:.4f} R@5={metrics[5]:.4f} R@10={metrics[10]:.4f}\")\n",
    "\n",
    "            if metrics[1] > best_R1:\n",
    "                best_R1 = metrics[1]\n",
    "                best_state = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optim.state_dict(),\n",
    "                    \"epoch\": epoch,\n",
    "                    \"metrics\": metrics\n",
    "                }\n",
    "\n",
    "    if best_state is not None:\n",
    "        print(f\"Best Val R@1={best_R1:.4f} @ epoch {best_state['epoch']}\")\n",
    "    return best_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    for (images, labels) in train_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        z = model.encode_batch(images)\n",
    "        memory.enqueue(z.detach(), labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training: epochs=3, lr=2e-05, mem_size=50000, dim=768\n",
      "[epoch 1 step 20] loss=0.7486  triplets/step=5.5\n",
      "lr: [5.882352941176471e-06]\n",
      "[epoch 1 step 40] loss=0.2215  triplets/step=21.0\n",
      "lr: [1.3725490196078432e-05]\n",
      "[epoch 1 step 60] loss=0.0254  triplets/step=36.8\n",
      "lr: [1.9996348616949673e-05]\n",
      "[epoch 1 step 80] loss=243.8707  triplets/step=40.5\n",
      "lr: [1.9868829976729444e-05]\n",
      "[epoch 1 step 100] loss=25.4597  triplets/step=53.0\n",
      "lr: [1.9561399963785586e-05]\n",
      "[epoch 1 step 120] loss=0.0128  triplets/step=62.8\n",
      "lr: [1.9079663108318304e-05]\n",
      "[epoch 1 step 140] loss=0.1606  triplets/step=74.9\n",
      "lr: [1.8432401600228823e-05]\n",
      "[epoch 1 step 160] loss=27.0361  triplets/step=76.2\n",
      "lr: [1.7631415187481818e-05]\n",
      "Epoch 1 done in 159.1s | Val R@1=0.0000 R@5=0.0007 R@10=0.0007\n",
      "[epoch 2 step 20] loss=10.6446  triplets/step=218.2\n",
      "lr: [1.6067494830143014e-05]\n",
      "[epoch 2 step 40] loss=0.0071  triplets/step=222.2\n",
      "lr: [1.4941376675970058e-05]\n",
      "[epoch 2 step 60] loss=0.0063  triplets/step=228.0\n",
      "lr: [1.3725175922034566e-05]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m train_triplet(train_dataloader, val_dataloader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, mem_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50_000\u001b[39m, pos_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, neg_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, grad_clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, log_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 47\u001b[0m, in \u001b[0;36mtrain_triplet\u001b[1;34m(train_loader, val_loader, epochs, lr, weight_decay, margin, mem_size, pos_k, neg_k, grad_clip, log_every, accum_steps)\u001b[0m\n\u001b[0;32m     44\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     45\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 47\u001b[0m z \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_batch(images)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# майнинг триплетов\u001b[39;00m\n\u001b[0;32m     51\u001b[0m triplets \u001b[38;5;241m=\u001b[39m mine_triplets(z\u001b[38;5;241m.\u001b[39mdetach(), labels, memory,\n\u001b[0;32m     52\u001b[0m                          margin\u001b[38;5;241m=\u001b[39mmargin, pos_k\u001b[38;5;241m=\u001b[39mpos_k, neg_k\u001b[38;5;241m=\u001b[39mneg_k)\n",
      "Cell \u001b[1;32mIn[14], line 57\u001b[0m, in \u001b[0;36mDINOv2Embedder.encode_batch\u001b[1;34m(self, images, return_numpy)\u001b[0m\n\u001b[0;32m     54\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(images, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(images, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# если подаём тензор, processor сам умеет работать с torch.Tensor\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessor(images\u001b[38;5;241m=\u001b[39mimages, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages must be a List[PIL.Image] or a torch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\image_processing_utils.py:44\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[1;34m(self, images, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\utils\\generic.py:849\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    840\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    843\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    845\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    846\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    847\u001b[0m     )\n\u001b[1;32m--> 849\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mvalid_kwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\models\\bit\\image_processing_bit.py:300\u001b[0m, in \u001b[0;36mBitImageProcessor.preprocess\u001b[1;34m(self, images, do_resize, size, resample, do_center_crop, crop_size, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_convert_rgb, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_resize:\n\u001b[1;32m--> 300\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresize(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39msize, resample\u001b[38;5;241m=\u001b[39mresample, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m do_center_crop:\n\u001b[0;32m    303\u001b[0m         image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcenter_crop(image\u001b[38;5;241m=\u001b[39mimage, size\u001b[38;5;241m=\u001b[39mcrop_size, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\models\\bit\\image_processing_bit.py:166\u001b[0m, in \u001b[0;36mBitImageProcessor.resize\u001b[1;34m(self, image, size, resample, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSize must contain either \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortest_edge\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidth\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m output_size \u001b[38;5;241m=\u001b[39m get_resize_output_image_size(\n\u001b[0;32m    161\u001b[0m     image,\n\u001b[0;32m    162\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[0;32m    163\u001b[0m     default_to_square\u001b[38;5;241m=\u001b[39mdefault_to_square,\n\u001b[0;32m    164\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[0;32m    165\u001b[0m )\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resize(\n\u001b[0;32m    167\u001b[0m     image,\n\u001b[0;32m    168\u001b[0m     size\u001b[38;5;241m=\u001b[39moutput_size,\n\u001b[0;32m    169\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[0;32m    170\u001b[0m     data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[0;32m    171\u001b[0m     input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    173\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\image_transforms.py:374\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(image, size, resample, reducing_gap, data_format, return_numpy, input_data_format)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[0;32m    373\u001b[0m     do_rescale \u001b[38;5;241m=\u001b[39m _rescale_for_pil_conversion(image)\n\u001b[1;32m--> 374\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_pil_image(image, do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[0;32m    375\u001b[0m height, width \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# PIL images are in the format (width, height)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\transformers\\image_transforms.py:213\u001b[0m, in \u001b[0;36mto_pil_image\u001b[1;34m(image, do_rescale, image_mode, input_data_format)\u001b[0m\n\u001b[0;32m    210\u001b[0m     image \u001b[38;5;241m=\u001b[39m rescale(image, \u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    212\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mfromarray(image, mode\u001b[38;5;241m=\u001b[39mimage_mode)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\PIL\\Image.py:3342\u001b[0m, in \u001b[0;36mfromarray\u001b[1;34m(obj, mode)\u001b[0m\n\u001b[0;32m   3339\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires either tobytes() or tostring()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3340\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 3342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frombuffer(mode, size, obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, rawmode, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\PIL\\Image.py:3244\u001b[0m, in \u001b[0;36mfrombuffer\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3241\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   3242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[1;32m-> 3244\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frombytes(mode, size, data, decoder_name, args)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\PIL\\Image.py:3171\u001b[0m, in \u001b[0;36mfrombytes\u001b[1;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[0;32m   3146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3166\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m   3167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3169\u001b[0m _check_size(size)\n\u001b[1;32m-> 3171\u001b[0m im \u001b[38;5;241m=\u001b[39m new(mode, size)\n\u001b[0;32m   3172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   3173\u001b[0m     decoder_args: Any \u001b[38;5;241m=\u001b[39m args\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\ML_Lyceum\\Lib\\site-packages\\PIL\\Image.py:3136\u001b[0m, in \u001b[0;36mnew\u001b[1;34m(mode, size, color)\u001b[0m\n\u001b[0;32m   3134\u001b[0m         im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m ImagePalette\u001b[38;5;241m.\u001b[39mImagePalette()\n\u001b[0;32m   3135\u001b[0m         color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color_ints)\n\u001b[1;32m-> 3136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(core\u001b[38;5;241m.\u001b[39mfill(mode, size, color))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model_state = train_triplet(train_dataloader, val_dataloader, epochs=3, lr=2e-5, weight_decay=1e-4, margin=0.3, mem_size=50_000, pos_k=2, neg_k=5, grad_clip=1.0, log_every=20)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1878097,
     "sourceId": 24286,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ML_Lyceum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
