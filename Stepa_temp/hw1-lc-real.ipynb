{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# В этой домашке мы поработаем с эмбеддингами текстов и сделаем первое соревнование по нлп"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начнем с подготовки (0.5 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим \"голову\", которая будет отвечать за классификацию на основе эмбеддингов. Для этого обучим CatBoost на заранее подготовленных эмбеддингах с добавленным шумом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:53:30.206188Z",
     "iopub.status.busy": "2025-09-06T06:53:30.206003Z",
     "iopub.status.idle": "2025-09-06T06:53:35.854256Z",
     "shell.execute_reply": "2025-09-06T06:53:35.853350Z",
     "shell.execute_reply.started": "2025-09-06T06:53:30.206171Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bert_dim_0</th>\n",
       "      <th>bert_dim_1</th>\n",
       "      <th>bert_dim_2</th>\n",
       "      <th>bert_dim_3</th>\n",
       "      <th>bert_dim_4</th>\n",
       "      <th>bert_dim_5</th>\n",
       "      <th>bert_dim_6</th>\n",
       "      <th>bert_dim_7</th>\n",
       "      <th>bert_dim_8</th>\n",
       "      <th>bert_dim_9</th>\n",
       "      <th>...</th>\n",
       "      <th>bert_dim_759</th>\n",
       "      <th>bert_dim_760</th>\n",
       "      <th>bert_dim_761</th>\n",
       "      <th>bert_dim_762</th>\n",
       "      <th>bert_dim_763</th>\n",
       "      <th>bert_dim_764</th>\n",
       "      <th>bert_dim_765</th>\n",
       "      <th>bert_dim_766</th>\n",
       "      <th>bert_dim_767</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045296</td>\n",
       "      <td>0.244643</td>\n",
       "      <td>0.146881</td>\n",
       "      <td>-0.101254</td>\n",
       "      <td>-0.354191</td>\n",
       "      <td>-0.580979</td>\n",
       "      <td>1.203313</td>\n",
       "      <td>0.624728</td>\n",
       "      <td>0.039920</td>\n",
       "      <td>-0.574070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052892</td>\n",
       "      <td>0.023292</td>\n",
       "      <td>-0.154176</td>\n",
       "      <td>0.340931</td>\n",
       "      <td>-0.142372</td>\n",
       "      <td>-0.014048</td>\n",
       "      <td>-0.112122</td>\n",
       "      <td>0.264723</td>\n",
       "      <td>-0.180618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.833856</td>\n",
       "      <td>-0.186808</td>\n",
       "      <td>-0.461408</td>\n",
       "      <td>-0.482215</td>\n",
       "      <td>0.191647</td>\n",
       "      <td>0.173931</td>\n",
       "      <td>0.226668</td>\n",
       "      <td>0.462386</td>\n",
       "      <td>-0.625819</td>\n",
       "      <td>-0.180016</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069476</td>\n",
       "      <td>-0.059576</td>\n",
       "      <td>-0.754987</td>\n",
       "      <td>0.476386</td>\n",
       "      <td>-0.163974</td>\n",
       "      <td>-0.251988</td>\n",
       "      <td>-0.121008</td>\n",
       "      <td>0.281774</td>\n",
       "      <td>-0.028574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.553840</td>\n",
       "      <td>-0.868089</td>\n",
       "      <td>0.456105</td>\n",
       "      <td>-0.360056</td>\n",
       "      <td>0.596364</td>\n",
       "      <td>-0.175732</td>\n",
       "      <td>0.661811</td>\n",
       "      <td>0.303952</td>\n",
       "      <td>-0.157333</td>\n",
       "      <td>-0.344494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.411147</td>\n",
       "      <td>0.700571</td>\n",
       "      <td>-0.238418</td>\n",
       "      <td>-0.197669</td>\n",
       "      <td>-0.201720</td>\n",
       "      <td>-0.384008</td>\n",
       "      <td>0.014200</td>\n",
       "      <td>0.099776</td>\n",
       "      <td>0.380442</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.709926</td>\n",
       "      <td>-0.059643</td>\n",
       "      <td>-0.065534</td>\n",
       "      <td>-0.215128</td>\n",
       "      <td>-0.341166</td>\n",
       "      <td>-0.002201</td>\n",
       "      <td>0.697558</td>\n",
       "      <td>0.685646</td>\n",
       "      <td>-0.670927</td>\n",
       "      <td>-0.089965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297650</td>\n",
       "      <td>0.686873</td>\n",
       "      <td>-0.404352</td>\n",
       "      <td>0.475008</td>\n",
       "      <td>0.230612</td>\n",
       "      <td>-0.003604</td>\n",
       "      <td>-0.798723</td>\n",
       "      <td>0.419383</td>\n",
       "      <td>0.010650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.127495</td>\n",
       "      <td>-0.026461</td>\n",
       "      <td>0.160941</td>\n",
       "      <td>0.114806</td>\n",
       "      <td>-0.279382</td>\n",
       "      <td>-0.752050</td>\n",
       "      <td>1.035819</td>\n",
       "      <td>0.550378</td>\n",
       "      <td>0.148431</td>\n",
       "      <td>-0.372095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057954</td>\n",
       "      <td>-0.508310</td>\n",
       "      <td>-0.334053</td>\n",
       "      <td>0.124174</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>-0.142547</td>\n",
       "      <td>-0.883545</td>\n",
       "      <td>0.055623</td>\n",
       "      <td>0.213822</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0.013373</td>\n",
       "      <td>0.053648</td>\n",
       "      <td>-0.172382</td>\n",
       "      <td>-0.199139</td>\n",
       "      <td>-0.172104</td>\n",
       "      <td>0.180905</td>\n",
       "      <td>0.537056</td>\n",
       "      <td>0.874261</td>\n",
       "      <td>-0.208165</td>\n",
       "      <td>-0.178274</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085405</td>\n",
       "      <td>-0.255601</td>\n",
       "      <td>-0.167974</td>\n",
       "      <td>0.482883</td>\n",
       "      <td>0.600995</td>\n",
       "      <td>-0.127468</td>\n",
       "      <td>-0.105035</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.195847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0.113711</td>\n",
       "      <td>0.180877</td>\n",
       "      <td>-0.105192</td>\n",
       "      <td>-0.032074</td>\n",
       "      <td>-0.497120</td>\n",
       "      <td>-0.442231</td>\n",
       "      <td>0.585394</td>\n",
       "      <td>0.361678</td>\n",
       "      <td>0.157206</td>\n",
       "      <td>-0.619585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099414</td>\n",
       "      <td>-0.326833</td>\n",
       "      <td>-0.415066</td>\n",
       "      <td>0.137362</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>-0.309373</td>\n",
       "      <td>-0.617580</td>\n",
       "      <td>0.308346</td>\n",
       "      <td>0.432512</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>-0.218554</td>\n",
       "      <td>0.051987</td>\n",
       "      <td>0.221798</td>\n",
       "      <td>-0.167987</td>\n",
       "      <td>-0.171388</td>\n",
       "      <td>-0.090077</td>\n",
       "      <td>0.484971</td>\n",
       "      <td>0.561129</td>\n",
       "      <td>-0.152184</td>\n",
       "      <td>-0.275283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069041</td>\n",
       "      <td>-0.325151</td>\n",
       "      <td>-0.518002</td>\n",
       "      <td>0.083724</td>\n",
       "      <td>0.004744</td>\n",
       "      <td>0.034347</td>\n",
       "      <td>-0.159692</td>\n",
       "      <td>0.606469</td>\n",
       "      <td>0.353067</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>-0.439928</td>\n",
       "      <td>-0.425846</td>\n",
       "      <td>0.083126</td>\n",
       "      <td>0.157701</td>\n",
       "      <td>-0.386743</td>\n",
       "      <td>0.026565</td>\n",
       "      <td>0.299643</td>\n",
       "      <td>0.243440</td>\n",
       "      <td>-0.287628</td>\n",
       "      <td>-0.245310</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009644</td>\n",
       "      <td>0.338429</td>\n",
       "      <td>0.078797</td>\n",
       "      <td>0.039647</td>\n",
       "      <td>-0.666714</td>\n",
       "      <td>0.179503</td>\n",
       "      <td>0.136507</td>\n",
       "      <td>0.158583</td>\n",
       "      <td>0.207595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>-0.063204</td>\n",
       "      <td>-0.499365</td>\n",
       "      <td>0.168701</td>\n",
       "      <td>-0.093897</td>\n",
       "      <td>-0.494483</td>\n",
       "      <td>-0.441588</td>\n",
       "      <td>0.197109</td>\n",
       "      <td>0.554611</td>\n",
       "      <td>-0.020550</td>\n",
       "      <td>-0.003068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080185</td>\n",
       "      <td>-0.133439</td>\n",
       "      <td>-0.132757</td>\n",
       "      <td>0.474470</td>\n",
       "      <td>0.233145</td>\n",
       "      <td>-0.197760</td>\n",
       "      <td>-0.423012</td>\n",
       "      <td>0.445299</td>\n",
       "      <td>0.270375</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 769 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      bert_dim_0  bert_dim_1  bert_dim_2  bert_dim_3  bert_dim_4  bert_dim_5  \\\n",
       "0      -0.045296    0.244643    0.146881   -0.101254   -0.354191   -0.580979   \n",
       "1      -0.833856   -0.186808   -0.461408   -0.482215    0.191647    0.173931   \n",
       "2      -0.553840   -0.868089    0.456105   -0.360056    0.596364   -0.175732   \n",
       "3      -0.709926   -0.059643   -0.065534   -0.215128   -0.341166   -0.002201   \n",
       "4      -0.127495   -0.026461    0.160941    0.114806   -0.279382   -0.752050   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "7608    0.013373    0.053648   -0.172382   -0.199139   -0.172104    0.180905   \n",
       "7609    0.113711    0.180877   -0.105192   -0.032074   -0.497120   -0.442231   \n",
       "7610   -0.218554    0.051987    0.221798   -0.167987   -0.171388   -0.090077   \n",
       "7611   -0.439928   -0.425846    0.083126    0.157701   -0.386743    0.026565   \n",
       "7612   -0.063204   -0.499365    0.168701   -0.093897   -0.494483   -0.441588   \n",
       "\n",
       "      bert_dim_6  bert_dim_7  bert_dim_8  bert_dim_9  ...  bert_dim_759  \\\n",
       "0       1.203313    0.624728    0.039920   -0.574070  ...     -0.052892   \n",
       "1       0.226668    0.462386   -0.625819   -0.180016  ...     -0.069476   \n",
       "2       0.661811    0.303952   -0.157333   -0.344494  ...     -0.411147   \n",
       "3       0.697558    0.685646   -0.670927   -0.089965  ...      0.297650   \n",
       "4       1.035819    0.550378    0.148431   -0.372095  ...      0.057954   \n",
       "...          ...         ...         ...         ...  ...           ...   \n",
       "7608    0.537056    0.874261   -0.208165   -0.178274  ...     -0.085405   \n",
       "7609    0.585394    0.361678    0.157206   -0.619585  ...     -0.099414   \n",
       "7610    0.484971    0.561129   -0.152184   -0.275283  ...      0.069041   \n",
       "7611    0.299643    0.243440   -0.287628   -0.245310  ...     -0.009644   \n",
       "7612    0.197109    0.554611   -0.020550   -0.003068  ...      0.080185   \n",
       "\n",
       "      bert_dim_760  bert_dim_761  bert_dim_762  bert_dim_763  bert_dim_764  \\\n",
       "0         0.023292     -0.154176      0.340931     -0.142372     -0.014048   \n",
       "1        -0.059576     -0.754987      0.476386     -0.163974     -0.251988   \n",
       "2         0.700571     -0.238418     -0.197669     -0.201720     -0.384008   \n",
       "3         0.686873     -0.404352      0.475008      0.230612     -0.003604   \n",
       "4        -0.508310     -0.334053      0.124174      0.029579     -0.142547   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "7608     -0.255601     -0.167974      0.482883      0.600995     -0.127468   \n",
       "7609     -0.326833     -0.415066      0.137362      0.064457     -0.309373   \n",
       "7610     -0.325151     -0.518002      0.083724      0.004744      0.034347   \n",
       "7611      0.338429      0.078797      0.039647     -0.666714      0.179503   \n",
       "7612     -0.133439     -0.132757      0.474470      0.233145     -0.197760   \n",
       "\n",
       "      bert_dim_765  bert_dim_766  bert_dim_767  target  \n",
       "0        -0.112122      0.264723     -0.180618       1  \n",
       "1        -0.121008      0.281774     -0.028574       1  \n",
       "2         0.014200      0.099776      0.380442       1  \n",
       "3        -0.798723      0.419383      0.010650       1  \n",
       "4        -0.883545      0.055623      0.213822       1  \n",
       "...            ...           ...           ...     ...  \n",
       "7608     -0.105035      0.021592      0.195847       1  \n",
       "7609     -0.617580      0.308346      0.432512       1  \n",
       "7610     -0.159692      0.606469      0.353067       1  \n",
       "7611      0.136507      0.158583      0.207595       1  \n",
       "7612     -0.423012      0.445299      0.270375       1  \n",
       "\n",
       "[7613 rows x 769 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "train = pd.read_csv('FAKE_train.csv').drop(columns=['Unnamed: 0'])\n",
    "test = pd.read_csv('FAKE_test.csv').drop(columns=['Unnamed: 0'])\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:54:29.847714Z",
     "iopub.status.busy": "2025-09-06T06:54:29.847445Z",
     "iopub.status.idle": "2025-09-06T06:54:31.569598Z",
     "shell.execute_reply": "2025-09-06T06:54:31.569017Z",
     "shell.execute_reply.started": "2025-09-06T06:54:29.847692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = train.drop(columns = ['target'])\n",
    "y = train['target']\n",
    "\n",
    "# Сделайте разделение даты на трейн и валидацию с помощью train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:54:37.190625Z",
     "iopub.status.busy": "2025-09-06T06:54:37.190226Z",
     "iopub.status.idle": "2025-09-06T06:54:37.196386Z",
     "shell.execute_reply": "2025-09-06T06:54:37.195636Z",
     "shell.execute_reply.started": "2025-09-06T06:54:37.190602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Эту ячейку не изменяем до определенного этапа\n",
    "\n",
    "class Head_catboost():\n",
    "    def __init__(self,):\n",
    "        # это основные параметры катбуста, которые пока что не изменяйте\n",
    "        self.params = {\n",
    "            'iterations': 1000,\n",
    "            'learning_rate': 0.05,\n",
    "            'depth': 6,\n",
    "            'loss_function': 'Logloss',  # У нас бинарка\n",
    "            'eval_metric': 'F1',  # Оптимизируем F1 как основной скор соревы\n",
    "            'early_stopping_rounds': 150, \n",
    "            'random_seed': 42, # всегда фиксируйте сиды!\n",
    "            'verbose': 100,\n",
    "            'task_type': 'GPU' if torch.cuda.is_available() else 'CPU'\n",
    "        } \n",
    "    \n",
    "    def train(self, X_train, X_val, y_train, y_val):\n",
    "        model = CatBoostClassifier(**self.params)\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=(X_val, y_val),\n",
    "            plot=True, # на кагле не заработает, но на личных машинках должен\n",
    "            use_best_model=True # берется лучшая по скору на валидации\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c68b38d41234404becb04c5d207072b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6447312\ttest: 0.6125654\tbest: 0.6125654 (0)\ttotal: 102ms\tremaining: 1m 42s\n",
      "100:\tlearn: 0.8004977\ttest: 0.7251064\tbest: 0.7308348 (93)\ttotal: 4.39s\tremaining: 39.1s\n",
      "200:\tlearn: 0.8593527\ttest: 0.7443420\tbest: 0.7455919 (196)\ttotal: 8.57s\tremaining: 34.1s\n",
      "300:\tlearn: 0.9058565\ttest: 0.7452282\tbest: 0.7487521 (295)\ttotal: 12.7s\tremaining: 29.6s\n",
      "400:\tlearn: 0.9371181\ttest: 0.7447699\tbest: 0.7522936 (376)\ttotal: 16.9s\tremaining: 25.3s\n",
      "500:\tlearn: 0.9599066\ttest: 0.7485380\tbest: 0.7522936 (376)\ttotal: 21.1s\tremaining: 21s\n",
      "bestTest = 0.752293578\n",
      "bestIteration = 376\n",
      "Shrink model to first 377 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1d985f8c850>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Head_catboost()\n",
    "model.train(X_train, X_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь перейдём к получению сабмита. Условием соревнования является предсказание только классов — 0 или 1. Однако CatBoost по умолчанию возвращает вероятности.\n",
    "\n",
    "Я предложу вам два варианта формирования сабмита. Подумайте (или проверьте на практике), какой из них окажется лучше, и попробуйте объяснить, почему один метод работает лучше другого."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:54:55.535038Z",
     "iopub.status.busy": "2025-09-06T06:54:55.534752Z",
     "iopub.status.idle": "2025-09-06T06:54:55.539649Z",
     "shell.execute_reply": "2025-09-06T06:54:55.538851Z",
     "shell.execute_reply.started": "2025-09-06T06:54:55.535017Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers.trainer import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "def predict_argmax(model, tokenizer, test_df: pd.DataFrame, texts_column='text', batch_size=32, threshold=0.65):\n",
    "    \"\"\"\n",
    "    Делает предсказания для модели BertForSequenceClassification и сохраняет сабмит.\n",
    "    \n",
    "    Args:\n",
    "        model: обученная BertForSequenceClassification\n",
    "        tokenizer: соответствующий токенизатор\n",
    "        sample: DataFrame с колонкой текстов\n",
    "        texts_column: имя колонки с текстами\n",
    "        batch_size: размер батча для предсказаний\n",
    "        threshold: порог для бинарного класса\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    texts = test_df[texts_column].tolist()\n",
    "    predictions = []\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # создаём DataLoader для батчей\n",
    "    class SimpleDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, texts):\n",
    "            self.texts = texts\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.texts[idx]\n",
    "\n",
    "    loader = DataLoader(SimpleDataset(texts), batch_size=batch_size, collate_fn=lambda batch: tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512))\n",
    "\n",
    "    # предсказание по батчам\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]  # вероятность класса 1\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "\n",
    "    # применяем threshold\n",
    "    predictions = (np.array(predictions) >= threshold).astype(int)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_df['id'],\n",
    "        'target': predictions\n",
    "    })\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    submission_df.to_csv(f'Submission_{timestamp}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(сабмиты можно найти во вкладке Output справа в меню кагла, после чего загрузить в соревнование)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пишите ваш ответ тут: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теперь будем создавать эмбеддинги самостоятельно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам предстоит попробовать несколько подходов к построению текстовых представлений.\n",
    "\n",
    "Для каждого текста в трейне создайте эмбеддинг, после чего обучите CatBoost и посмотрите  полученный скор в соревновании. Мы посмотрим, какой метод покажет себя лучше всего."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T06:55:06.912328Z",
     "iopub.status.busy": "2025-09-06T06:55:06.911699Z",
     "iopub.status.idle": "2025-09-06T06:55:06.983425Z",
     "shell.execute_reply": "2025-09-06T06:55:06.982705Z",
     "shell.execute_reply.started": "2025-09-06T06:55:06.912301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_df = pd.read_csv('sample_submission.csv')\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X = train.drop(columns=['target'])\n",
    "y = train['target']\n",
    "\n",
    "# Сделайте разделение даты на трейн и валидацию с помощью train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bag-of-Words (BOW) (0.75 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод создаёт числовое представление текста, где каждое слово превращается в отдельную колонку, а значение указывает частоту его встречаемости. Такой подход полностью игнорирует порядок слов и их смысловые связи, опираясь только на факт их наличия в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_BOW_train = vectorizer.fit_transform(X_train['text'])\n",
    "X_BOW_val = vectorizer.transform(X_val['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61699c5dceb744649e8587b680292463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6263319\ttest: 0.6112327\tbest: 0.6112327 (0)\ttotal: 32ms\tremaining: 32s\n",
      "100:\tlearn: 0.6947935\ttest: 0.6732673\tbest: 0.6732673 (100)\ttotal: 3.08s\tremaining: 27.5s\n",
      "200:\tlearn: 0.7404014\ttest: 0.6993865\tbest: 0.6993865 (200)\ttotal: 5.99s\tremaining: 23.8s\n",
      "300:\tlearn: 0.7682376\ttest: 0.7162630\tbest: 0.7173725 (294)\ttotal: 8.82s\tremaining: 20.5s\n",
      "400:\tlearn: 0.7875936\ttest: 0.7186147\tbest: 0.7217916 (326)\ttotal: 11.6s\tremaining: 17.3s\n",
      "500:\tlearn: 0.7976165\ttest: 0.7256944\tbest: 0.7260156 (480)\ttotal: 14.4s\tremaining: 14.3s\n",
      "600:\tlearn: 0.8065756\ttest: 0.7299652\tbest: 0.7299652 (586)\ttotal: 17.2s\tremaining: 11.4s\n",
      "700:\tlearn: 0.8154011\ttest: 0.7328111\tbest: 0.7376623 (691)\ttotal: 19.8s\tremaining: 8.46s\n",
      "800:\tlearn: 0.8270869\ttest: 0.7312391\tbest: 0.7376623 (691)\ttotal: 22.5s\tremaining: 5.6s\n",
      "bestTest = 0.7376623377\n",
      "bestIteration = 691\n",
      "Shrink model to first 692 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x18d5299da90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Head_catboost()\n",
    "model.train(X_BOW_train, X_BOW_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полученный скор:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF (0.75 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Учитывает не только частоту слова в документе (TF), но и его редкость во всей коллекции документов (IDF), автоматически понижая вес распространенных слов. Это позволяет выделять действительно значимые термины, которые характерны для конкретного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f2c7c4a63d45e1a023f02233d5232a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6231563\ttest: 0.6178862\tbest: 0.6178862 (0)\ttotal: 43.2ms\tremaining: 43.2s\n",
      "100:\tlearn: 0.7004939\ttest: 0.6821429\tbest: 0.6821429 (100)\ttotal: 4.01s\tremaining: 35.7s\n",
      "200:\tlearn: 0.7492355\ttest: 0.6994819\tbest: 0.7038328 (180)\ttotal: 7.78s\tremaining: 30.9s\n",
      "300:\tlearn: 0.7799527\ttest: 0.7103918\tbest: 0.7109974 (286)\ttotal: 11.4s\tremaining: 26.4s\n",
      "400:\tlearn: 0.8005115\ttest: 0.7250000\tbest: 0.7250000 (398)\ttotal: 14.9s\tremaining: 22.2s\n",
      "500:\tlearn: 0.8124603\ttest: 0.7300000\tbest: 0.7300000 (497)\ttotal: 18.4s\tremaining: 18.4s\n",
      "600:\tlearn: 0.8216399\ttest: 0.7283333\tbest: 0.7321131 (588)\ttotal: 21.9s\tremaining: 14.6s\n",
      "700:\tlearn: 0.8310483\ttest: 0.7312187\tbest: 0.7321131 (588)\ttotal: 25.4s\tremaining: 10.9s\n",
      "bestTest = 0.7321131448\n",
      "bestIteration = 588\n",
      "Shrink model to first 589 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x18d078eb2d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_tfidf_train = tfidf_vectorizer.fit_transform(X_train['text'])\n",
    "X_tfidf_val = tfidf_vectorizer.transform(X_val['text'])\n",
    "\n",
    "model = Head_catboost()\n",
    "model.train(X_tfidf_train, X_tfidf_val, y_train, y_val)\n",
    "#ваш выбранный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полученный скор:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec (0.75 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейронная сеть преобразует слова в векторы фиксированной длины так, что семантически близкие слова оказываются рядом в пространстве. Обучение идёт либо в режиме skip-gram (слово → контекст), либо CBOW (контекст → слово), что позволяет сохранять смысловые связи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fefc6e94ba43b1bd682c0e7a92a7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6186891\ttest: 0.6243902\tbest: 0.6243902 (0)\ttotal: 32.4ms\tremaining: 32.4s\n",
      "100:\tlearn: 0.6903775\ttest: 0.6589975\tbest: 0.6606409 (90)\ttotal: 2.7s\tremaining: 24s\n",
      "200:\tlearn: 0.7199309\ttest: 0.6688259\tbest: 0.6709992 (175)\ttotal: 5.24s\tremaining: 20.8s\n",
      "300:\tlearn: 0.7468597\ttest: 0.6693614\tbest: 0.6747377 (222)\ttotal: 7.63s\tremaining: 17.7s\n",
      "400:\tlearn: 0.7623888\ttest: 0.6783719\tbest: 0.6836653 (382)\ttotal: 9.96s\tremaining: 14.9s\n",
      "500:\tlearn: 0.7752857\ttest: 0.6767276\tbest: 0.6836653 (382)\ttotal: 12.3s\tremaining: 12.3s\n",
      "bestTest = 0.6836653386\n",
      "bestIteration = 382\n",
      "Shrink model to first 383 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x18d2068a950>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X_train_tokens = X_train[\"text\"].str.lower().apply(word_tokenize)\n",
    "X_val_tokens = X_val[\"text\"].str.lower().apply(word_tokenize)\n",
    "\n",
    "corpus = X_train_tokens.tolist()\n",
    "\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences=corpus,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    sg=1,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "def text_to_vector(tokens, model, vector_size=100):\n",
    "    vectors = [model.wv[w] for w in tokens if w in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(vector_size)  # если слова нет в словаре\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_w2v_train = np.vstack(X_train_tokens.apply(lambda x: text_to_vector(x, w2v)))\n",
    "X_w2v_val =  np.vstack(X_val_tokens.apply(lambda x: text_to_vector(x, w2v)))\n",
    "model = Head_catboost()\n",
    "model.train(X_w2v_train, X_w2v_val, y_train, y_val)\n",
    "#ваш выбранный метод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полученный скор:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GloVe (0.75 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строит векторные представления слов на основе статистики их совместной встречаемости во всем корпусе текстов, вычисляя, насколько часто пары слов появляются вместе. Оптимизирует вектора так, чтобы их скалярное произведение соответствовало логарифму вероятности совместного появления слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Загружаем предобученные эмбеддинги Glove ===\n",
    "def load_glove(path, dim=300):\n",
    "    embeddings = {}\n",
    "    with open(path, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split()\n",
    "            if len(parts) <= dim:  # если меньше, чем слово + dim чисел → битая строка\n",
    "                continue\n",
    "            word = \" \".join(parts[:-dim])   # всё, что до последних dim токенов\n",
    "            try:\n",
    "                vector = np.asarray(parts[-dim:], dtype=\"float32\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if vector.shape[0] != dim:\n",
    "                continue\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "glove = load_glove(\"wiki_giga_2024_300_MFT20_vectors_seed_2024_alpha_0.75_eta_0.05_combined.txt\")  # скачай заранее с https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "# === 2. Функция: превращаем текст в средний вектор ===\n",
    "def text_to_vector(text, embeddings, dim=300):\n",
    "    words = text.split()\n",
    "    vecs = [embeddings[w] for w in words if w in embeddings]\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "\n",
    "X_glove_train = np.vstack(X_train[\"text\"].apply(lambda x: text_to_vector(str(x), glove, dim=300)))\n",
    "X_glove_val = np.vstack(X_val[\"text\"].apply(lambda x: text_to_vector(str(x), glove, dim=300)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf802981485e45e888afb0a33c6865c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6423510\ttest: 0.6361366\tbest: 0.6361366 (0)\ttotal: 31.4ms\tremaining: 31.4s\n",
      "100:\tlearn: 0.7403291\ttest: 0.6776145\tbest: 0.6798965 (89)\ttotal: 3.26s\tremaining: 29s\n",
      "200:\tlearn: 0.7864407\ttest: 0.6770563\tbest: 0.6827171 (113)\ttotal: 6.41s\tremaining: 25.5s\n",
      "300:\tlearn: 0.8148771\ttest: 0.6804124\tbest: 0.6827586 (250)\ttotal: 9.57s\tremaining: 22.2s\n",
      "400:\tlearn: 0.8373066\ttest: 0.6791809\tbest: 0.6860565 (352)\ttotal: 12.6s\tremaining: 18.8s\n",
      "500:\tlearn: 0.8535270\ttest: 0.6752137\tbest: 0.6860565 (352)\ttotal: 15.6s\tremaining: 15.6s\n",
      "bestTest = 0.6860564585\n",
      "bestIteration = 352\n",
      "Shrink model to first 353 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x18dc16bb7d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Head_catboost()\n",
    "model.train(X_glove_train, X_glove_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Полученный скор:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Трансформерный подход (0.75 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с NLP моделями используйте сайт https://huggingface.co/ и их библиотеку transformers. Сначала будем использовать базовую версию BERT https://huggingface.co/google-bert/bert-base-uncased \n",
    "\n",
    "По желанию можете еще сделать вариант с использованием Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Bert(nn.Module):\n",
    "    def __init__(self, trainable=False):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased').to(self.device) \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.CLS = 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TO DO\n",
    "        \"\"\"\n",
    "        Получаем текстовый вход, токенизирует его и пропускает через BERT-модель,\n",
    "        возвращая векторное представление текста (CLS-токен).\n",
    "        \n",
    "        Args:\n",
    "            input_text (str): Входной текст\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Векторное представление [1, hidden_size]\n",
    "        \"\"\"\n",
    "        self.bert.eval()  # если не тренируем BERT\n",
    "        with torch.no_grad():  # отключаем градиенты для ускорения\n",
    "            encoded = self.tokenizer(\n",
    "                input,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            out = self.bert(**encoded)  # передаем как kwargs\n",
    "            cls_vec = out.last_hidden_state[:, self.CLS, :]  # [batch_size, hidden_size]\n",
    "            \n",
    "        return cls_vec.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_bert = Bert()\n",
    "\n",
    "\n",
    "X_bert_train = np.vstack(X_train[\"text\"].apply(lambda x: base_bert(x)))\n",
    "X_bert_val = np.vstack(X_val[\"text\"].apply(lambda x: base_bert(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd163097b394433b5055826debacd62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6698907\ttest: 0.6605042\tbest: 0.6605042 (0)\ttotal: 58.8ms\tremaining: 58.7s\n",
      "100:\tlearn: 0.8125384\ttest: 0.7545910\tbest: 0.7545910 (100)\ttotal: 4.21s\tremaining: 37.5s\n",
      "200:\tlearn: 0.8628967\ttest: 0.7615894\tbest: 0.7677100 (187)\ttotal: 8.41s\tremaining: 33.4s\n",
      "300:\tlearn: 0.8956121\ttest: 0.7723911\tbest: 0.7727646 (299)\ttotal: 12.4s\tremaining: 28.8s\n",
      "400:\tlearn: 0.9218564\ttest: 0.7725041\tbest: 0.7750411 (306)\ttotal: 16.4s\tremaining: 24.5s\n",
      "bestTest = 0.7750410509\n",
      "bestIteration = 306\n",
      "Shrink model to first 307 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x25519276a50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Head_catboost()\n",
    "model.train(X_bert_train, X_bert_val, y_train, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fine-Tuning (2.75 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сделаем файнтюнинг BERT для нашей задачи, добавим линейный слой на CLS токен и дообучим полученную модель на наши данные. Файнтюнинг самый эффективный метод, если у вас есть ресурсы и время для обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как я показывал на семенаре можно обучать с помощью torch или с помощью Trainer из библиотеки transformers. \n",
    "Реализуйте оба варианта. Но в коротких соревнованиях отдавайте предпочтение Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашем случае мы можем воспринимать задачу как регрессию так и классификацию, это будет зависить от выбранного вами подхода обучения линейного слоя на CLS токене - линейный слой размерностью (d_cls_Berta, 1) - регрессия или же (d_cls_Berta, 2) - классификация. Лично я использовал бы классификацию и обучал бы на CrossEntropyLoss. Пока что реализуйте эту идею, потом можете поэксперементировать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HF Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from transformers import (\n",
    "    BertTokenizer, \n",
    "    BertForSequenceClassification,\n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,  # Количество классов\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    return {'f1': f1}\n",
    "\n",
    "\n",
    "# BertForSequenceClassification автоматически добавляет линейный слой на клс модели с выбранным количеством лейблов\n",
    "#TO DO\n",
    "'''\n",
    "Обучите берт используя Trainer, после чего создайте сабмит с его предсказаниеями и проверьте результат на кагле\n",
    "'''\n",
    "\n",
    "# используйте эти параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    # Основные параметры\n",
    "    output_dir='./bert-binary-classifier',  # Директория для сохранения\n",
    "    \n",
    "    # Параметры обучения\n",
    "    num_train_epochs=3,                     # Количество эпох\n",
    "    per_device_train_batch_size=64,         # Размер батча для обучения\n",
    "    per_device_eval_batch_size=64,          # Размер батча для валидации\n",
    "    learning_rate=2e-5,                     # Learning rate\n",
    "    warmup_ratio = 0.1,                     # 10% от общего числа шагов для вармапа или warmup_steps = int(0.1 * total_training_steps)\n",
    "    lr_scheduler_type = 'cosine',           # Можете посмотреть на них в \n",
    "                                            # https://www.kaggle.com/code/snnclsr/learning-rate-schedulers \n",
    "                                            # соответсвующий ему будет get_cosine_schedule_with_warmup\n",
    "    \n",
    "    # Сохранение и логирование\n",
    "    logging_dir='./logs',                   # Директория для логов\n",
    "    logging_steps=100,                      # Частота логирования\n",
    "    save_steps=500,                         # Частота сохранения\n",
    "    save_total_limit=2,                     # Максимум чекпоинтов\n",
    "    save_strategy='epoch',                  # Стратегия сохранения\n",
    "    \n",
    "    # Валидация\n",
    "    eval_strategy='epoch',            # Стратегия валидации\n",
    "    eval_steps=500,                         # Шаги для валидации\n",
    "    load_best_model_at_end=True,            # Загружать лучшую модель\n",
    "    metric_for_best_model='f1',             # Метрика для выбора лучшей\n",
    "    greater_is_better=True,                 # Больше значение = лучше\n",
    "\n",
    "    # воспроизводимость\n",
    "    seed=42,                                # Seed для воспроизводимости\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reset_index(drop=True)\n",
    "X_val = X_val.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_val = y_val.reset_index(drop=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    X_train['text'].tolist(), \n",
    "    padding=True, \n",
    "    truncation=True\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    X_val['text'].tolist(), \n",
    "    padding=True, \n",
    "    truncation=True\n",
    ")\n",
    "test_encodings = tokenizer(\n",
    "    test['text'].tolist(), \n",
    "    padding=True, \n",
    "    truncation=True\n",
    ")\n",
    "train_dataset = CustomDataset(train_encodings, y_train)\n",
    "val_dataset = CustomDataset(val_encodings, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.418596</td>\n",
       "      <td>0.787365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.536600</td>\n",
       "      <td>0.419562</td>\n",
       "      <td>0.799090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369300</td>\n",
       "      <td>0.408295</td>\n",
       "      <td>0.806880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "#TO DO\n",
    "'''\n",
    "Обучите берт используя Torch используйте параметры обучения из ячейки выше,\n",
    "после чего создайте сабмит с его предсказаниеями и проверьте результат на кагле\n",
    "'''\n",
    "\n",
    "\n",
    "def train():\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, test_df: pd.DataFrame, texts_column='text', batch_size=32, threshold=0.65):\n",
    "    \"\"\"\n",
    "    Делает предсказания для модели BertForSequenceClassification и сохраняет сабмит.\n",
    "    \n",
    "    Args:\n",
    "        model: обученная BertForSequenceClassification\n",
    "        tokenizer: соответствующий токенизатор\n",
    "        sample: DataFrame с колонкой текстов\n",
    "        texts_column: имя колонки с текстами\n",
    "        batch_size: размер батча для предсказаний\n",
    "        threshold: порог для бинарного класса\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    texts = test_df[texts_column].tolist()\n",
    "    predictions = []\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # создаём DataLoader для батчей\n",
    "    class SimpleDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, texts):\n",
    "            self.texts = texts\n",
    "        def __len__(self):\n",
    "            return len(self.texts)\n",
    "        def __getitem__(self, idx):\n",
    "            return self.texts[idx]\n",
    "\n",
    "    loader = DataLoader(SimpleDataset(texts), batch_size=batch_size, collate_fn=lambda batch: tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512))\n",
    "\n",
    "    # предсказание по батчам\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(**batch).logits\n",
    "            probs = torch.softmax(logits, dim=1)[:, 1]  # вероятность класса 1\n",
    "            predictions.extend(probs.cpu().numpy())\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict(model, tokenizer, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_argmax(model, AutoTokenizer.from_pretrained('bert-base-uncased'), test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Для самых пытливых, которые хотят участвовать и выигрывать в соревнованиях (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь я даю вам полную свободу в эксперементах, записывайте логи и идеи ниже, попробуйте занять хорошее место в этой соревке. \n",
    "\n",
    "Первым делом я бы предложил поперебирать модели. Для удобства в подборе можно использовать AutoModelForSequenceClassification. \n",
    "\n",
    "Для выбора моделей воспользуйтесь рейтингом MTEB - https://huggingface.co/spaces/mteb/leaderboard (новая версия) / https://huggingface.co/spaces/mteb/leaderboard_legacy (легаси версия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете еще посмотреть библиотеку SentenceTransformer (является оберткой для Transformers от сбера), но я ей не то чтобы много пользовался, также у нее куча проблем в плане совместимостей и на всеросе она мне куду положила)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6892dfea1a4773a6a697ed0fdb3cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\NLP\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Степан\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-Embedding-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11fd6dd1f4154affb76c7b19e8fabe45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993ad859d2984448bb0bfef779c768a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73eab5f7a49449d881865baab5b074bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9dfc68b3094ccf9c9dfa445067fbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/336M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc48e1daca7346f481dedaf3b6769162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import (\n",
    "    SentenceTransformer, SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments\n",
    ")\n",
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "from datasets import load_dataset\n",
    "\n",
    "model = SentenceTransformer(\"Qwen/Qwen3-Embedding-8B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка баллов за этот пункт будет делиться на две части, \n",
    "\n",
    "- качество проведенных эксперементов максимум один балл \n",
    "\n",
    "- баллы за соревнование 2 * (1 - (Ваше_место - 1)/(количество_участников - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Описание экспериментов**\n",
    "- Обучаем Qwen3-Embedding-0.6B (взят с ЛБ) с такими же параметрами. Результат 0.82\n",
    "- Обучаем Qwen3-Embedding-0.6B. Предварительно делаем ей более корректную предсказывающую голову: Linear - Relu - Dropout - Linear. Отдельно обучаем предсказывающую голову с большим LR, затем всю модель вместе с маленьким LR (1e-5). Результат 0.81\n",
    "- Достаем эмбеддинги из обученной Qwen3-Embedding-0.6B и используем их как вход в CatBoost. также используем колонки keyword и location, данные нам изначально. Результат 0.83\n",
    "- Усредняем предсказания прошлого эксперимента с предсказаниями BERTа из baseline. Результат 0.84"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    },
    {
     "datasetId": 8093573,
     "sourceId": 12800844,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
