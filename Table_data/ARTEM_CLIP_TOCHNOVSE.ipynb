{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "843c7fbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:03.086174Z",
     "iopub.status.busy": "2025-11-18T21:06:03.085874Z",
     "iopub.status.idle": "2025-11-18T21:06:03.092231Z",
     "shell.execute_reply": "2025-11-18T21:06:03.091443Z",
     "shell.execute_reply.started": "2025-11-18T21:06:03.086153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "LOAD_LIBS = False\n",
    "if LOAD_LIBS:\n",
    "    !pip install -q -U catboost\n",
    "    !pip install -q -U lightgbm\n",
    "    !pip install -q -U xgboost\n",
    "    !pip install -q -U geopy\n",
    "    !pip install -q -U phik\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f3d9bdca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:06.502868Z",
     "iopub.status.busy": "2025-11-18T21:06:06.502122Z",
     "iopub.status.idle": "2025-11-18T21:06:06.647628Z",
     "shell.execute_reply": "2025-11-18T21:06:06.647047Z",
     "shell.execute_reply.started": "2025-11-18T21:06:06.502839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModel, CLIPModel, CLIPProcessor\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    accuracy_score,\n",
    "    log_loss,\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# Гео-бэкенд: geopy (если установлен) или haversine\n",
    "try:\n",
    "    from geopy.distance import geodesic\n",
    "    GEO_BACKEND = \"geopy\"\n",
    "except ImportError:\n",
    "    GEO_BACKEND = \"haversine\"\n",
    "\n",
    "\n",
    "try:\n",
    "    import phik\n",
    "    HAS_PHIK = True\n",
    "except ImportError:\n",
    "    HAS_PHIK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6814b40a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:06.672304Z",
     "iopub.status.busy": "2025-11-18T21:06:06.671855Z",
     "iopub.status.idle": "2025-11-18T21:06:06.677070Z",
     "shell.execute_reply": "2025-11-18T21:06:06.676312Z",
     "shell.execute_reply.started": "2025-11-18T21:06:06.672270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('geopy', True)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GEO_BACKEND, HAS_PHIK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82061097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:06.774923Z",
     "iopub.status.busy": "2025-11-18T21:06:06.774467Z",
     "iopub.status.idle": "2025-11-18T21:06:06.778566Z",
     "shell.execute_reply": "2025-11-18T21:06:06.777885Z",
     "shell.execute_reply.started": "2025-11-18T21:06:06.774906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# для текста, никак не трогать\n",
    "TEXT_MODEL = None\n",
    "TEXT_TOKENIZER = None\n",
    "TEXT_DEVICE = None\n",
    "\n",
    "# TF-IDF и SVD отдельно для каждого текстового поля (чтобы не мешать разные источники)\n",
    "TFIDF_VECTORIZERS = {}  # field_name -> vectorizer\n",
    "TFIDF_SVDS = {}         # field_name -> svd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4654d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:07.034682Z",
     "iopub.status.busy": "2025-11-18T21:06:07.034019Z",
     "iopub.status.idle": "2025-11-18T21:06:07.047542Z",
     "shell.execute_reply": "2025-11-18T21:06:07.046800Z",
     "shell.execute_reply.started": "2025-11-18T21:06:07.034664Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DATA_FOLDER_PATH = '.'\n",
    "# CONFIG = {\n",
    "#     # ---------- Paths ----------\n",
    "#     \"train_path\": os.path.join(DATA_FOLDER_PATH, \"/kaggle/input/petfinder-pawpularity-score/train.csv\"),\n",
    "#     \"test_path\": os.path.join(DATA_FOLDER_PATH, '/kaggle/input/petfinder-pawpularity-score/test.csv'),\n",
    "#     \"sep\": \",\",\n",
    "#     \"id_column\": \"Id\",\n",
    "#     \"target_column\": \"Pawpularity\",\n",
    "#     \"datetime_columns\": [],\n",
    "#     \"output_dir\": \"tabular_boosting_output\",\n",
    "\n",
    "#     # ---------- Task ----------\n",
    "#     # \"binary\" / \"multiclass\" / \"regression\"\n",
    "#     \"task_type\": \"regression\",\n",
    "\n",
    "#     # ---------- Basic features ----------\n",
    "#     \"basic_drop_columns\": [],\n",
    "#     \"basic_as_categorical\": [],\n",
    "#     \"basic_max_cat_unique\": 64,\n",
    "#     \"basic_datetime_expand\": False,\n",
    "#     \"basic_datetime_features\": [\"year\", \"month\", \"day\", \"dow\", \"hour\"],\n",
    "\n",
    "#     # ---------- Categorical processing ----------\n",
    "#     # минимальный \"размер\" категории (по числу объектов и/или доле),\n",
    "#     # все категории меньше порога сливаются в 'other'\n",
    "#     \"cat_min_count\": 20,          # минимальное кол-во объектов в категории\n",
    "#     \"cat_min_freq\": 0.0,          # минимальная доля (0..1); 0 = игнорировать\n",
    "#     # если доля \"редких\" категорий в признаке >= этого порога — признак удаляется\n",
    "#     \"cat_max_rare_share\": 0.98,\n",
    "#     # если после объединения в other уникальных всё ещё слишком много — признак удаляется\n",
    "#     \"cat_max_unique_after_group\": 500,\n",
    "\n",
    "#     # ---------- Post-feature service columns ----------\n",
    "#     # служебные колонки, которые нужно удалить после генерации фич\n",
    "#     # (исходные координаты, raw id, сырые адреса и т.п.)\n",
    "#     \"post_feature_drop_columns\": [], # может быть удалить category_geo_ref_lat, category_geo_ref_lon\n",
    "\n",
    "#     # ---------- Address processing ----------\n",
    "#     # колонка-адрес, из которой нужно извлечь город\n",
    "#     \"address_column\": None,  \n",
    "\n",
    "#     # индекс элемента после split (например [-1] = последний)\n",
    "#     # можно указывать как отрицательный индекс\n",
    "#     \"address_city_index\": -1,  \n",
    "\n",
    "#     # разделитель в строке адреса\n",
    "#     \"address_split_sep\": \",\",  \n",
    "\n",
    "#     # ---------- Aggregates ----------\n",
    "#     \"agg_enable\": False,\n",
    "#     \"agg_groupby_cols\": [],\n",
    "#     \"agg_numeric_cols\": [],\n",
    "#     \"agg_aggs\": [\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\", \"nunique\", \"count\"],\n",
    "#     \"agg_prefix\": \"agg\",\n",
    "\n",
    "#     # ---------- Geo features ----------\n",
    "#     \"geo_enable\": False, \n",
    "#     \"geo_lat_from_col\": \"pickup_lat\",\n",
    "#     \"geo_lon_from_col\": \"pickup_lon\",\n",
    "#     \"geo_lat_to_col\": \"dropoff_lat\",\n",
    "#     \"geo_lon_to_col\": \"dropoff_lon\",\n",
    "#     \"geo_ref_lat\": 'category_geo_ref_lat',\n",
    "#     \"geo_ref_lon\": 'category_geo_ref_lon',\n",
    "#     \"geo_prefix\": \"geo\",\n",
    "#     # дополнительные фичи (bearing, manhattan, dlat/dlon, midpoint)\n",
    "#     \"geo_extra_enable\": False,\n",
    "\n",
    "#     \"geo_from_coord_col\": None,\n",
    "#     \"geo_to_coord_col\": None,\n",
    "#     \"geo_coord_string_sep\": \",\",\n",
    "\n",
    "#     # ---------- Geo reference config ----------\n",
    "#     \"geo_reference\": {\n",
    "#         \"enabled\": False,\n",
    "#         \"category_column\": \"category\",\n",
    "#         \"coordinates_column\": \"coordinates\",  # строка \"lat,lon\", если lat/lon не заданы\n",
    "#         \"lat_column\": None,                  # <- если задать, берём отсюда широту\n",
    "#         \"lon_column\": None,                  # <- если задать, берём отсюда долготу\n",
    "#         \"output_column\": \"category_geo_ref\",\n",
    "#         \"output_lat_column\": \"category_geo_ref_lat\",\n",
    "#         \"output_lon_column\": \"category_geo_ref_lon\",\n",
    "#     },\n",
    "\n",
    "#     # ---------- Correlation-based feature filtering ----------\n",
    "#     \"corr_enable\": False,          # включить/выключить фильтрацию по корреляции\n",
    "#     \"corr_pearson_min_abs\": 0.95,   # порог |Pearson|; 0.1, 0.05 и т.п.\n",
    "#     \"corr_use_phik\": False,        # считать ли phik\n",
    "#     \"corr_phik_min_abs\": 0.0,      # порог |phik|\n",
    "    \n",
    "#     #----------images---------\n",
    "#     # папка с train-картинками (если используем image-фичи)\n",
    "#     \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n",
    "#     # папка с test-картинками (если используем image-фичи)\n",
    "#     \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n",
    "#     # Если в таблице image_id уже содержит \".jpg\", поставь IMAGE_EXT = \"\"\n",
    "#     # расширение файлов картинок (если в таблице только id без расширения)\n",
    "#     \"image_ext\":\".jpg\",\n",
    "#     # колонка в таблице с именем файла картинки\n",
    "#     \"file_names_column\": \"Id\",\n",
    "#     # размер батча при обработке картинок\n",
    "#     \"batch_size\": 32,\n",
    "#     \"clip_model_name\": \"openai/clip-vit-large-patch14\",\n",
    "\n",
    "    \n",
    "#     # ---------- Text features ----------\n",
    "#     # включить/выключить построение текстовых эмбеддингов\n",
    "#     \"text_enable\": False,  # включить/выключить текстовые фичи\n",
    "\n",
    "#     # любые текстовые колонки в основной train/test таблице\n",
    "#     \"text_columns\": [],  # например [\"title\", \"description\"]\n",
    "\n",
    "#     # внешние текстовые таблицы (id, text) в parquet/csv/tsv\n",
    "#     # каждая запись:\n",
    "#     # {\n",
    "#     #   \"name\": \"comments\",        # логическое имя (для названия фич)\n",
    "#     #   \"train_path\": \"train_comments.parquet\",\n",
    "#     #   \"test_path\": \"test_comments.parquet\",\n",
    "#     #   \"format\": \"parquet\",       # \"parquet\" / \"csv\" / \"tsv\" / \"auto\"\n",
    "#     #   \"id_column\": \"index\",      # колонка id в этой таблице\n",
    "#     #   \"text_column\": \"text\",     # колонка с текстом в этой таблице\n",
    "#     #   \"output_column\": \"comments_text\"  # как назвать колонку в общей таблице (опц.)\n",
    "#     # }\n",
    "#     \"text_external_tables\": [{\n",
    "#         \"name\": \"reviews\",\n",
    "#         \"train_path\": \"reviews.tsv\",\n",
    "#         \"test_path\": \"reviews.tsv\",\n",
    "#         \"format\": \"tsv\",       # или \"auto\"\n",
    "#         # имя колонки с уникальным идентификатором объекта\n",
    "#     \"id_column\": \"id\",      # в этих файлах\n",
    "#         \"text_column\": \"text\",     # колонка с текстом\n",
    "#         \"output_column\": \"comments_text\"\n",
    "#     }],\n",
    "\n",
    "#     # тип эмбеддингов\n",
    "#     # \"bert\"      — трансформер-эмбеддинги\n",
    "#     # \"tfidf_svd\" — TF-IDF + TruncatedSVD\n",
    "#     # тип текстовой модели: \"bert\" или \"tfidf_svd\"\n",
    "#     \"text_model_type\": \"tfidf_svd\",\n",
    "\n",
    "#     # варианты моделей (комментами, чтобы можно было быстро переключать)\n",
    "#     # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # RU + EN (универсал)\n",
    "#     # \"DeepPavlov/rubert-base-cased-sentence\"                        # ru-only\n",
    "#     # \"sentence-transformers/all-mpnet-base-v2\"                      # en-only\n",
    "#     \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n",
    "\n",
    "#     # настройки BERT\n",
    "#     \"text_max_length\": 256,\n",
    "#     \"text_batch_size\": 32,\n",
    "#     # настройки TF-IDF + SVD\n",
    "#     \"text_tfidf_max_features\": 50000,\n",
    "#     \"text_svd_n_components\": 256,\n",
    "\n",
    "#     # ---------- CV ----------\n",
    "#     \"cv_n_splits\": 5,\n",
    "#     \"cv_random_state\": 42,\n",
    "#     \"cv_shuffle\": True,\n",
    "#     \"cv_stratified\": True,\n",
    "\n",
    "#     # ---------- Models ----------\n",
    "#     \"use_catboost\": True,\n",
    "#     \"use_lightgbm\": False,\n",
    "#     \"use_xgboost\": False,\n",
    "\n",
    "#     \"catboost_params\": {\n",
    "#         \"iterations\": 3500,\n",
    "#         \"learning_rate\": 0.05,\n",
    "#         \"depth\": 6,\n",
    "#         \"loss_function\": \"MultiClass\", # \"Logloss\"\n",
    "#         \"eval_metric\": \"AUC\",\n",
    "#         \"random_seed\": 42,\n",
    "#         \"verbose\": 100\n",
    "#     },\n",
    "\n",
    "\n",
    "#     \"lgb_params\": {\n",
    "#         \"objective\": \"multiclass\", # \"binary\"\n",
    "#         \"eval_metric\": \"auc\", #[\"auc\", \"binary_logloss\"],\n",
    "#         \"learning_rate\": 0.05,\n",
    "#         \"num_leaves\": 31,\n",
    "#         \"feature_fraction\": 0.9,\n",
    "#         \"bagging_fraction\": 0.8,\n",
    "#         \"bagging_freq\": 5,\n",
    "#         \"lambda_l2\": 1.0,\n",
    "#         \"num_threads\": 0,\n",
    "#         \"verbose\": -1\n",
    "#     },\n",
    "\n",
    "#     \"xgb_params\": {\n",
    "#         \"objective\": \"multi:softmax\", #\"binary:logistic\",\n",
    "#         \"eval_metric\": \"auc\",\n",
    "#         \"learning_rate\": 0.05,\n",
    "#         \"max_depth\": 6,\n",
    "#         \"subsample\": 0.8,\n",
    "#         \"colsample_bytree\": 0.8,\n",
    "#         \"lambda\": 1.0,\n",
    "#         \"alpha\": 0.0,\n",
    "#         \"tree_method\": \"hist\"\n",
    "#     },\n",
    "\n",
    "#     # ---------- Blending ----------\n",
    "#     \"blend_weights\": {\n",
    "#         \"catboost\": 1,\n",
    "#         \"lightgbm\": 0.35,\n",
    "#         \"xgboost\": 0.15\n",
    "#     },\n",
    "\n",
    "#     # ---------- Feature importance ----------\n",
    "#     \"compute_feature_importance\": True,\n",
    "#     \"top_features_to_show\": 50,\n",
    "\n",
    "#     # ---------- Early stopping ----------\n",
    "#     # параметр ранней остановки для бустингов\n",
    "#     \"early_stopping_rounds\": 100,# Количество раундов без улучшения для ранней остановки\n",
    "#     \"device\": \"cuda\",\n",
    "\n",
    "#     \"seed\": 42,\n",
    "#     \"verbose\": True\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34452e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATA_FOLDER_PATH = '.'  # базовая директория проекта; все пути ниже будут строиться относительно неё\n",
    "\n",
    "CONFIG = {\n",
    "    # ---------- Paths ----------\n",
    "    # Полный путь к train-файлу с табличными данными.\n",
    "    # Обычно это CSV/TSV с признаками + target.\n",
    "    \"train_path\": os.path.join(DATA_FOLDER_PATH, \"/kaggle/input/petfinder-pawpularity-score/train.csv\"),\n",
    "\n",
    "    # Полный путь к test-файлу. Структура колонок должна в целом совпадать с train,\n",
    "    # но без target (Pawpularity).\n",
    "    \"test_path\": os.path.join(DATA_FOLDER_PATH, '/kaggle/input/petfinder-pawpularity-score/test.csv'),\n",
    "\n",
    "    # Разделитель в текстовых файлах (здесь стандартный CSV).\n",
    "    \"sep\": \",\",\n",
    "\n",
    "    # Имя колонки с уникальным идентификатором объекта.\n",
    "    # Используется для:\n",
    "    #  - мёрджа с внешними таблицами (тексты, гео и т.п.),\n",
    "    #  - формирования сабмишна (Id, prediction).\n",
    "    \"id_column\": \"Id\",\n",
    "\n",
    "    # Имя колонки с целевой переменной (target).\n",
    "    # Для Pawpularity — это \"Pawpularity\".\n",
    "    \"target_column\": \"Pawpularity\",\n",
    "\n",
    "    # Список колонок, которые нужно интерпретировать как datetime.\n",
    "    # Если есть поля с датой/временем (created_at, date, timestamp) — добавляешь сюда.\n",
    "    \"datetime_columns\": [],\n",
    "\n",
    "    # Папка, куда будут сохраняться:\n",
    "    #  - обученные модели,\n",
    "    #  - логи,\n",
    "    #  - важности признаков,\n",
    "    #  - промежуточные артефакты.\n",
    "    \"output_dir\": \"tabular_boosting_output\",\n",
    "\n",
    "    # ---------- Task ----------\n",
    "    # \"binary\" / \"multiclass\" / \"regression\"\n",
    "    # Тип задачи:\n",
    "    #  - \"regression\" — численный таргет (как здесь),\n",
    "    #  - \"binary\" — бинарная классификация,\n",
    "    #  - \"multiclass\" — мультикласс.\n",
    "    \"task_type\": \"regression\",\n",
    "\n",
    "    # ---------- Basic features ----------\n",
    "    # Колонки, которые нужно сразу удалить из датафрейма после чтения,\n",
    "    # до любой обработки. Например:\n",
    "    #  - дубликаты id,\n",
    "    #  - явно мусорные поля,\n",
    "    #  - текст, который не планируется использовать.\n",
    "    \"basic_drop_columns\": [],\n",
    "\n",
    "    # Список колонок, которые нужно принудительно трактовать как категориальные,\n",
    "    # даже если по типу/кол-ву уникальных они не выглядят как категории.\n",
    "    # Например, бинарные флаги в 0/1, коды, id-шники категорий и т.п.\n",
    "    \"basic_as_categorical\": [],\n",
    "\n",
    "    # Порог \"максимум уникальных значений\" для того, чтобы числовую колонку\n",
    "    # можно было считать категориальной (если auto-логика это использует).\n",
    "    # Если уникальных значений > этого числа, колонка скорее всего будет считаться\n",
    "    # числовой/текстовой, а не категориальной.\n",
    "    \"basic_max_cat_unique\": 64,\n",
    "\n",
    "    # Если True, для всех колонок из \"datetime_columns\" (и/или авто-определённых дат)\n",
    "    # будут созданы дополнительные фичи: год, месяц, день, день недели и т.д.\n",
    "    \"basic_datetime_expand\": False,\n",
    "\n",
    "    # Какие именно компоненты даты генерировать, если basic_datetime_expand = True:\n",
    "    #  - \"year\"  — год\n",
    "    #  - \"month\" — месяц\n",
    "    #  - \"day\"   — день месяца\n",
    "    #  - \"dow\"   — день недели\n",
    "    #  - \"hour\"  — час\n",
    "    \"basic_datetime_features\": [\"year\", \"month\", \"day\", \"dow\", \"hour\"],\n",
    "\n",
    "    # ---------- Categorical processing ----------\n",
    "    # минимальный \"размер\" категории (по числу объектов и/или доле),\n",
    "    # все категории меньше порога сливаются в 'other'\n",
    "\n",
    "    # Минимальное число объектов в категории.\n",
    "    # Категории с count < cat_min_count → объединяются в одну категорию 'other'.\n",
    "    \"cat_min_count\": 20,          # минимальное кол-во объектов в категории\n",
    "\n",
    "    # Минимальная доля объектов в категории (в долях от 0 до 1).\n",
    "    # Если > 0, то категория с freq < cat_min_freq также считается редкой.\n",
    "    # При 0.0 условие по частоте отключено (оставляем только порог по count).\n",
    "    \"cat_min_freq\": 0.0,          # минимальная доля (0..1); 0 = игнорировать\n",
    "\n",
    "    # если доля \"редких\" категорий в признаке >= этого порога — признак удаляется\n",
    "    # То есть: после слияния редких категорий в 'other',\n",
    "    # если доля 'other' >= cat_max_rare_share, колонка считается бесполезной\n",
    "    # (слишком много мусорных значений) и выкидывается.\n",
    "    \"cat_max_rare_share\": 0.98,\n",
    "\n",
    "    # если после объединения в other уникальных всё ещё слишком много — признак удаляется\n",
    "    # Например, если после группировки в 'other' всё равно > 500 уникальных категорий,\n",
    "    # колонка воспринимается как слишком \"широкая\" и тоже дропается.\n",
    "    \"cat_max_unique_after_group\": 500,\n",
    "\n",
    "    # ---------- Post-feature service columns ----------\n",
    "    # служебные колонки, которые нужно удалить после генерации фич\n",
    "    # (исходные координаты, raw id, сырые адреса и т.п.)\n",
    "    # Сюда можно добавить, например, 'category_geo_ref_lat', 'category_geo_ref_lon',\n",
    "    # если после строительства geo-фич они больше не нужны в данных.\n",
    "    \"post_feature_drop_columns\": [], # может быть удалить category_geo_ref_lat, category_geo_ref_lon\n",
    "\n",
    "    # ---------- Address processing ----------\n",
    "    # колонка-адрес, из которой нужно извлечь город\n",
    "    # Если у тебя есть колонка-строка с полным адресом (улица, дом, город),\n",
    "    # её имя задаётся тут. Если None — адресные фичи не считаются.\n",
    "    \"address_column\": None,  \n",
    "\n",
    "    # индекс элемента после split (например [-1] = последний)\n",
    "    # можно указывать как отрицательный индекс\n",
    "    # Пример: \"ул. Пушкина, дом Колотушкина, Москва\" split(\",\")\n",
    "    # при address_city_index = -1 → \"Москва\".\n",
    "    \"address_city_index\": -1,  \n",
    "\n",
    "    # разделитель в строке адреса\n",
    "    # По этому сепаратору адрес split-ится на части.\n",
    "    \"address_split_sep\": \",\",  \n",
    "\n",
    "    # ---------- Aggregates ----------\n",
    "    # Включить ли генерацию агрегатных фич:\n",
    "    # группировки по колонкам и расчёт mean/std/min/max/... по числовым признакам.\n",
    "    \"agg_enable\": False,\n",
    "\n",
    "    # Список колонок, по которым делаем groupby.\n",
    "    # Например: [\"user_id\"] или [\"city\", \"category\"].\n",
    "    \"agg_groupby_cols\": [],\n",
    "\n",
    "    # Список числовых колонок, для которых считаем агрегаты внутри групп.\n",
    "    \"agg_numeric_cols\": [],\n",
    "\n",
    "    # Какие агрегаты считать. Стандартные агрегирующие функции pandas:\n",
    "    #  - \"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\", \"nunique\", \"count\"\n",
    "    \"agg_aggs\": [\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\", \"nunique\", \"count\"],\n",
    "\n",
    "    # Префикс, который будет добавлен к именам новых агрегатных фич.\n",
    "    # Например: \"agg_user_id_amount_mean\".\n",
    "    \"agg_prefix\": \"agg\",\n",
    "\n",
    "    # ---------- Geo features ----------\n",
    "    # Включить/выключить гео-фичи. Если False — все поля ниже игнорируются.\n",
    "    \"geo_enable\": False, \n",
    "\n",
    "    # Название колонки с широтой точки отправления (lat_from).\n",
    "    \"geo_lat_from_col\": \"pickup_lat\",\n",
    "\n",
    "    # Название колонки с долготой точки отправления (lon_from).\n",
    "    \"geo_lon_from_col\": \"pickup_lon\",\n",
    "\n",
    "    # Название колонки с широтой точки назначения (lat_to).\n",
    "    \"geo_lat_to_col\": \"dropoff_lat\",\n",
    "\n",
    "    # Название колонки с долготой точки назначения (lon_to).\n",
    "    \"geo_lon_to_col\": \"dropoff_lon\",\n",
    "\n",
    "    # Колонка с \"референсной\" широтой (например, центроид категории).\n",
    "    # Обычно это создаётся через geo_reference (см. ниже).\n",
    "    \"geo_ref_lat\": 'category_geo_ref_lat',\n",
    "\n",
    "    # Колонка с \"референсной\" долготой.\n",
    "    \"geo_ref_lon\": 'category_geo_ref_lon',\n",
    "\n",
    "    # Префикс для всех сгенерированных гео-фич (дистанции, углы и т.п.).\n",
    "    \"geo_prefix\": \"geo\",\n",
    "\n",
    "    # дополнительные фичи (bearing, manhattan, dlat/dlon, midpoint)\n",
    "    # Если True — помимо базового расстояния считаем:\n",
    "    #  - bearing (азимут),\n",
    "    #  - manhattan distance (по широте/долготе),\n",
    "    #  - dlat/dlon (дельты координат),\n",
    "    #  - midpoint (середина между точками).\n",
    "    \"geo_extra_enable\": False,\n",
    "\n",
    "    # Если координаты хранятся не в отдельных колонках, а в одной строке вида \"lat,lon\",\n",
    "    # можно использовать geo_from_coord_col / geo_to_coord_col и geo_coord_string_sep.\n",
    "    # Тогда lat/lon будут парситься из этих строк.\n",
    "    \"geo_from_coord_col\": None,\n",
    "    \"geo_to_coord_col\": None,\n",
    "    \"geo_coord_string_sep\": \",\",\n",
    "\n",
    "    # ---------- Geo reference config ----------\n",
    "    # Конфиг для вычисления \"референсных\" координат по категориям.\n",
    "    \"geo_reference\": {\n",
    "        # Включено ли построение референсных координат.\n",
    "        \"enabled\": False,\n",
    "\n",
    "        # Колонка с категорией (например, \"city\", \"region\", \"shop_id\"),\n",
    "        # по которой будем усреднять координаты.\n",
    "        \"category_column\": \"category\",\n",
    "\n",
    "        # Название колонки со строкой координат \"lat,lon\" (если отдельные lat/lon не заданы).\n",
    "        \"coordinates_column\": \"coordinates\",  # строка \"lat,lon\", если lat/lon не заданы\n",
    "\n",
    "        # Колонка с широтой, если координаты уже разбиты по отдельным полям.\n",
    "        # Если не None, берём широту отсюда, а не из coordinates_column.\n",
    "        \"lat_column\": None,                  # <- если задать, берём отсюда широту\n",
    "\n",
    "        # Колонка с долготой, аналогично lat_column.\n",
    "        \"lon_column\": None,                  # <- если задать, берём отсюда долготу\n",
    "\n",
    "        # Итоговое имя новой колонки с идентификатором/названием референсной точки.\n",
    "        \"output_column\": \"category_geo_ref\",\n",
    "\n",
    "        # Имя колонки с референсной широтой, которая потом может использоваться\n",
    "        # как geo_ref_lat в основном конфиге.\n",
    "        \"output_lat_column\": \"category_geo_ref_lat\",\n",
    "\n",
    "        # Имя колонки с референсной долготой (аналогично широте).\n",
    "        \"output_lon_column\": \"category_geo_ref_lon\",\n",
    "    },\n",
    "\n",
    "    # ---------- Correlation-based feature filtering ----------\n",
    "    # включить/выключить фильтрацию по корреляции\n",
    "    # Если True — будет строиться матрица корреляции между признаками,\n",
    "    # и сильно скоррелированные фичи выше порога могут удаляться.\n",
    "    \"corr_enable\": False,          \n",
    "\n",
    "    # порог |Pearson|; 0.1, 0.05 и т.п.\n",
    "    # Чем ниже порог — тем больше признаков будет считаться \"сильно коррелирующими\".\n",
    "    # Здесь 0.95 — довольно высокий порог, то есть удаляться будут только почти дубликаты.\n",
    "    \"corr_pearson_min_abs\": 0.95,   \n",
    "\n",
    "    # считать ли phik\n",
    "    # phik — более продвинутый аналог корреляции, хорошо работающий с категориальными фичами.\n",
    "    \"corr_use_phik\": False,        \n",
    "\n",
    "    # порог |phik| (аналогично Pearson, но для phik-метрики).\n",
    "    \"corr_phik_min_abs\": 0.0,      \n",
    "\n",
    "    #----------images---------\n",
    "    # папка с train-картинками (если используем image-фичи)\n",
    "    \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n",
    "\n",
    "    # папка с test-картинками (если используем image-фичи)\n",
    "    \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n",
    "\n",
    "    # Если в таблице image_id уже содержит \".jpg\", поставь IMAGE_EXT = \"\"\n",
    "    # расширение файлов картинок (если в таблице только id без расширения)\n",
    "    \"image_ext\": \".jpg\",\n",
    "\n",
    "    # колонка в таблице с именем файла картинки\n",
    "    # Обычно совпадает с id, к которому дописывается image_ext.\n",
    "    \"file_names_column\": \"Id\",\n",
    "\n",
    "    # размер батча при обработке картинок\n",
    "    # Чем больше батч, тем быстрее, но тем больше память видеокарты/CPU.\n",
    "    \"batch_size\": 32,\n",
    "\n",
    "    # имя CLIP-модели в HuggingFace, которая используется для извлечения image-эмбеддингов.\n",
    "    \"clip_model_name\": \"openai/clip-vit-large-patch14\",\n",
    "\n",
    "    \n",
    "    # ---------- Text features ----------\n",
    "    # включить/выключить построение текстовых эмбеддингов\n",
    "    \"text_enable\": False,  # включить/выключить текстовые фичи\n",
    "\n",
    "    # любые текстовые колонки в основной train/test таблице\n",
    "    # Пример: [\"title\", \"description\"], если в train.csv есть такие поля.\n",
    "    \"text_columns\": [],  # например [\"title\", \"description\"]\n",
    "\n",
    "    # внешние текстовые таблицы (id, text) в parquet/csv/tsv\n",
    "    # каждая запись:\n",
    "    # {\n",
    "    #   \"name\": \"comments\",        # логическое имя (для названия фич)\n",
    "    #   \"train_path\": \"train_comments.parquet\",\n",
    "    #   \"test_path\": \"test_comments.parquet\",\n",
    "    #   \"format\": \"parquet\",       # \"parquet\" / \"csv\" / \"tsv\" / \"auto\"\n",
    "    #   \"id_column\": \"index\",      # колонка id в этой таблице\n",
    "    #   \"text_column\": \"text\",     # колонка с текстом в этой таблице\n",
    "    #   \"output_column\": \"comments_text\"  # как назвать колонку в общей таблице (опц.)\n",
    "    # }\n",
    "    \"text_external_tables\": [{\n",
    "        \"name\": \"reviews\",            # логическое имя внешней текстовой таблицы\n",
    "        \"train_path\": \"reviews.tsv\",  # путь к текстам для train\n",
    "        \"test_path\": \"reviews.tsv\",   # путь к текстам для test (может быть тот же файл)\n",
    "        \"format\": \"tsv\",              # или \"auto\" — формат будет определён по расширению\n",
    "        # имя колонки с уникальным идентификатором объекта\n",
    "        \"id_column\": \"id\",            # в этих файлах\n",
    "        # колонка с текстом\n",
    "        \"text_column\": \"text\",        # колонка с самим текстом\n",
    "        # имя новой колонки в общей табличке после merge по id_column\n",
    "        \"output_column\": \"comments_text\"\n",
    "    }],\n",
    "\n",
    "    # тип эмбеддингов\n",
    "    # \"bert\"      — трансформер-эмбеддинги\n",
    "    # \"tfidf_svd\" — TF-IDF + TruncatedSVD\n",
    "    # тип текстовой модели: \"bert\" или \"tfidf_svd\"\n",
    "    \"text_model_type\": \"tfidf_svd\",\n",
    "\n",
    "    # варианты моделей (комментами, чтобы можно было быстро переключать)\n",
    "    # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # RU + EN (универсал)\n",
    "    # \"DeepPavlov/rubert-base-cased-sentence\"                        # ru-only\n",
    "    # \"sentence-transformers/all-mpnet-base-v2\"                      # en-only\n",
    "    \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n",
    "\n",
    "    # настройки BERT\n",
    "    # Максимальная длина токенизированного текста (обрежет слишком длинные).\n",
    "    \"text_max_length\": 256,\n",
    "    # Размер батча при прогоне текстов через BERT.\n",
    "    \"text_batch_size\": 32,\n",
    "\n",
    "    # настройки TF-IDF + SVD\n",
    "    # Максимальное число признаков TF-IDF (размер словаря).\n",
    "    \"text_tfidf_max_features\": 50000,\n",
    "    # Размерность итогового текстового эмбеддинга после SVD.\n",
    "    \"text_svd_n_components\": 256,\n",
    "\n",
    "    # ---------- CV ----------\n",
    "    # Количество фолдов в кросс-валидации (KFold или StratifiedKFold).\n",
    "    \"cv_n_splits\": 5,\n",
    "\n",
    "    # random_state для генератора случайных чисел в KFold'е.\n",
    "    \"cv_random_state\": 42,\n",
    "\n",
    "    # Перемешивать ли данные перед разбиением на фолды.\n",
    "    \"cv_shuffle\": True,\n",
    "\n",
    "    # Использовать ли стратификацию по target (актуально для классификации).\n",
    "    # Для регрессии обычно False, но можно оставить True, если реализована stratified-regression логика.\n",
    "    \"cv_stratified\": True,\n",
    "\n",
    "    # ---------- Models ----------\n",
    "    # Флажки, какие модели участвуют в обучении/блендинге.\n",
    "    \"use_catboost\": True,\n",
    "    \"use_lightgbm\": False,\n",
    "    \"use_xgboost\": False,\n",
    "\n",
    "    # Параметры CatBoost.\n",
    "    # ⚠️ Сейчас loss_function=\"MultiClass\", eval_metric=\"AUC\",\n",
    "    # что больше подходит для мультикласса/классификации.\n",
    "    # Для task_type=\"regression\" обычно:\n",
    "    #  - loss_function=\"RMSE\" / \"MAE\"\n",
    "    #  - eval_metric=\"RMSE\" / \"MAE\" и т.п.\n",
    "    \"catboost_params\": {\n",
    "        \"iterations\": 3500,           # количество деревьев/итераций бустинга\n",
    "        \"learning_rate\": 0.05,        # шаг обучения\n",
    "        \"depth\": 6,                   # глубина деревьев\n",
    "        \"loss_function\": \"MultiClass\", # \"Logloss\" / \"RMSE\" / \"MAE\" и др.\n",
    "        \"eval_metric\": \"AUC\",         # метрика для отслеживания качества\n",
    "        \"random_seed\": 42,            # сид для воспроизводимости\n",
    "        \"verbose\": 100                # лог каждые 100 итераций\n",
    "    },\n",
    "\n",
    "    # Параметры LightGBM.\n",
    "    # ⚠️ Здесь objective=\"multiclass\", eval_metric=\"auc\" — опять же конфиг под классификацию.\n",
    "    # Для регрессии:\n",
    "    #  - objective=\"regression\"\n",
    "    #  - eval_metric=\"rmse\" / \"mae\" и т.п.\n",
    "    \"lgb_params\": {\n",
    "        \"objective\": \"multiclass\",    # \"binary\" / \"regression\" / \"multiclass\"\n",
    "        \"eval_metric\": \"auc\",         # [\"auc\", \"binary_logloss\"] и т.п. для классификации\n",
    "        \"learning_rate\": 0.05,        # шаг обучения\n",
    "        \"num_leaves\": 31,             # сложность деревьев (кол-во листьев)\n",
    "        \"feature_fraction\": 0.9,      # доля фич, используемых в каждом дереве\n",
    "        \"bagging_fraction\": 0.8,      # доля объектов для bagging\n",
    "        \"bagging_freq\": 5,            # как часто делать bagging\n",
    "        \"lambda_l2\": 1.0,             # L2-регуляризация\n",
    "        \"num_threads\": 0,             # число потоков (0 = авто)\n",
    "        \"verbose\": -1                 # уровень логгирования\n",
    "    },\n",
    "\n",
    "    # Параметры XGBoost.\n",
    "    # ⚠️ objective=\"multi:softmax\", eval_metric=\"auc\" — под мультикласс.\n",
    "    # Для регрессии:\n",
    "    #  - objective=\"reg:squarederror\"\n",
    "    #  - eval_metric=\"rmse\" / \"mae\".\n",
    "    \"xgb_params\": {\n",
    "        \"objective\": \"multi:softmax\", # \"binary:logistic\", \"reg:squarederror\" и др.\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,        # шаг обучения\n",
    "        \"max_depth\": 6,               # глубина деревьев\n",
    "        \"subsample\": 0.8,             # доля объектов для каждого дерева\n",
    "        \"colsample_bytree\": 0.8,      # доля признаков для каждого дерева\n",
    "        \"lambda\": 1.0,                # L2-регуляризация\n",
    "        \"alpha\": 0.0,                 # L1-регуляризация\n",
    "        \"tree_method\": \"hist\"         # метод построения деревьев (hist — быстрый/памяте-эффективный)\n",
    "    },\n",
    "\n",
    "    # ---------- Blending ----------\n",
    "    # Веса моделей в финальном ансамбле.\n",
    "    # Если модель отключена (use_* = False), её вес можно игнорировать.\n",
    "    # Итоговый предикт примерно: w_cat * y_cat + w_lgb * y_lgb + w_xgb * y_xgb.\n",
    "    \"blend_weights\": {\n",
    "        \"catboost\": 1,\n",
    "        \"lightgbm\": 0.35,\n",
    "        \"xgboost\": 0.15\n",
    "    },\n",
    "\n",
    "    # ---------- Feature importance ----------\n",
    "    # Считать ли важности признаков после обучения бустингов.\n",
    "    \"compute_feature_importance\": True,\n",
    "\n",
    "    # Сколько топовых признаков показывать/логировать.\n",
    "    \"top_features_to_show\": 50,\n",
    "\n",
    "    # ---------- Early stopping ----------\n",
    "    # параметр ранней остановки для бустингов\n",
    "    # Количество итераций без улучшения метрики,\n",
    "    # после которого обучение будет остановлено.\n",
    "    \"early_stopping_rounds\": 100,  # Количество раундов без улучшения для ранней остановки\n",
    "\n",
    "    # На каком устройстве считать:\n",
    "    #  - \"cuda\" — GPU (если доступно),\n",
    "    #  - \"cpu\" — процессор,\n",
    "    #  - \"mps\" — Apple Silicon.\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # Общий сид для воспроизводимости (используется в разных местах).\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Если True — включено детальное логгирование (print/logging).\n",
    "    \"verbose\": True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af431dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:07.079974Z",
     "iopub.status.busy": "2025-11-18T21:06:07.079572Z",
     "iopub.status.idle": "2025-11-18T21:06:07.086734Z",
     "shell.execute_reply": "2025-11-18T21:06:07.086046Z",
     "shell.execute_reply.started": "2025-11-18T21:06:07.079957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def setup_logging(output_dir: str) -> logging.Logger:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    log_path = os.path.join(output_dir, \"training.log\")\n",
    "\n",
    "    logger = logging.getLogger(\"TABULAR_BOOSTING\")\n",
    "    logger.setLevel(logging.INFO)\n",
    "    logger.handlers = []\n",
    "\n",
    "    # Используем UTF-8 кодировку для файла, чтобы поддерживать эмодзи\n",
    "    fh = logging.FileHandler(log_path, encoding='utf-8')\n",
    "    fh.setLevel(logging.INFO)\n",
    "    \n",
    "    # Для консоли также используем UTF-8, если возможно\n",
    "    import sys\n",
    "    if sys.stdout.encoding != 'utf-8':\n",
    "        # Если консоль не поддерживает UTF-8, создаем обертку\n",
    "        class UTF8StreamHandler(logging.StreamHandler):\n",
    "            def emit(self, record):\n",
    "                try:\n",
    "                    msg = self.format(record)\n",
    "                    # Убираем эмодзи для консоли, если она не поддерживает UTF-8\n",
    "                    import re\n",
    "                    msg = re.sub(r'[^\\x00-\\x7F]+', '', msg)  # Удаляем не-ASCII символы\n",
    "                    stream = self.stream\n",
    "                    stream.write(msg + self.terminator)\n",
    "                    self.flush()\n",
    "                except Exception:\n",
    "                    self.handleError(record)\n",
    "        ch = UTF8StreamHandler()\n",
    "    else:\n",
    "        ch = logging.StreamHandler()\n",
    "    ch.setLevel(logging.INFO)\n",
    "\n",
    "    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "    fh.setFormatter(fmt)\n",
    "    ch.setFormatter(fmt)\n",
    "\n",
    "    logger.addHandler(fh)\n",
    "    logger.addHandler(ch)\n",
    "\n",
    "    logger.info(\"Logger initialized.\")\n",
    "    return logger\n",
    "\n",
    "\n",
    "def log_config(logger: logging.Logger, config: Dict):\n",
    "    logger.info(\"========== CONFIG ==========\")\n",
    "    logger.info(json.dumps(config, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8b7b9c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:07.231539Z",
     "iopub.status.busy": "2025-11-18T21:06:07.231079Z",
     "iopub.status.idle": "2025-11-18T21:06:07.243098Z",
     "shell.execute_reply": "2025-11-18T21:06:07.242400Z",
     "shell.execute_reply.started": "2025-11-18T21:06:07.231520Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:06:07,234 - INFO - Logger initialized.\n",
      "INFO:TABULAR_BOOSTING:Logger initialized.\n",
      "2025-11-18 21:06:07,236 - INFO - ========== CONFIG ==========\n",
      "INFO:TABULAR_BOOSTING:========== CONFIG ==========\n",
      "2025-11-18 21:06:07,238 - INFO - {\n",
      "  \"train_path\": \"/kaggle/input/petfinder-pawpularity-score/train.csv\",\n",
      "  \"test_path\": \"/kaggle/input/petfinder-pawpularity-score/test.csv\",\n",
      "  \"sep\": \",\",\n",
      "  \"id_column\": \"Id\",\n",
      "  \"target_column\": \"Pawpularity\",\n",
      "  \"datetime_columns\": [],\n",
      "  \"output_dir\": \"tabular_boosting_output\",\n",
      "  \"task_type\": \"regression\",\n",
      "  \"basic_drop_columns\": [],\n",
      "  \"basic_as_categorical\": [],\n",
      "  \"basic_max_cat_unique\": 64,\n",
      "  \"basic_datetime_expand\": false,\n",
      "  \"basic_datetime_features\": [\n",
      "    \"year\",\n",
      "    \"month\",\n",
      "    \"day\",\n",
      "    \"dow\",\n",
      "    \"hour\"\n",
      "  ],\n",
      "  \"cat_min_count\": 20,\n",
      "  \"cat_min_freq\": 0.0,\n",
      "  \"cat_max_rare_share\": 0.98,\n",
      "  \"cat_max_unique_after_group\": 500,\n",
      "  \"post_feature_drop_columns\": [],\n",
      "  \"address_column\": null,\n",
      "  \"address_city_index\": -1,\n",
      "  \"address_split_sep\": \",\",\n",
      "  \"agg_enable\": false,\n",
      "  \"agg_groupby_cols\": [],\n",
      "  \"agg_numeric_cols\": [],\n",
      "  \"agg_aggs\": [\n",
      "    \"mean\",\n",
      "    \"std\",\n",
      "    \"min\",\n",
      "    \"max\",\n",
      "    \"sum\",\n",
      "    \"median\",\n",
      "    \"nunique\",\n",
      "    \"count\"\n",
      "  ],\n",
      "  \"agg_prefix\": \"agg\",\n",
      "  \"geo_enable\": false,\n",
      "  \"geo_lat_from_col\": \"pickup_lat\",\n",
      "  \"geo_lon_from_col\": \"pickup_lon\",\n",
      "  \"geo_lat_to_col\": \"dropoff_lat\",\n",
      "  \"geo_lon_to_col\": \"dropoff_lon\",\n",
      "  \"geo_ref_lat\": \"category_geo_ref_lat\",\n",
      "  \"geo_ref_lon\": \"category_geo_ref_lon\",\n",
      "  \"geo_prefix\": \"geo\",\n",
      "  \"geo_extra_enable\": false,\n",
      "  \"geo_from_coord_col\": null,\n",
      "  \"geo_to_coord_col\": null,\n",
      "  \"geo_coord_string_sep\": \",\",\n",
      "  \"geo_reference\": {\n",
      "    \"enabled\": false,\n",
      "    \"category_column\": \"category\",\n",
      "    \"coordinates_column\": \"coordinates\",\n",
      "    \"lat_column\": null,\n",
      "    \"lon_column\": null,\n",
      "    \"output_column\": \"category_geo_ref\",\n",
      "    \"output_lat_column\": \"category_geo_ref_lat\",\n",
      "    \"output_lon_column\": \"category_geo_ref_lon\"\n",
      "  },\n",
      "  \"corr_enable\": false,\n",
      "  \"corr_pearson_min_abs\": 0.95,\n",
      "  \"corr_use_phik\": false,\n",
      "  \"corr_phik_min_abs\": 0.0,\n",
      "  \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n",
      "  \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n",
      "  \"image_ext\": \".jpg\",\n",
      "  \"file_names_column\": \"Id\",\n",
      "  \"batch_size\": 32,\n",
      "  \"clip_model_name\": \"openai/clip-vit-large-patch14\",\n",
      "  \"text_enable\": false,\n",
      "  \"text_columns\": [],\n",
      "  \"text_external_tables\": [\n",
      "    {\n",
      "      \"name\": \"reviews\",\n",
      "      \"train_path\": \"reviews.tsv\",\n",
      "      \"test_path\": \"reviews.tsv\",\n",
      "      \"format\": \"tsv\",\n",
      "      \"id_column\": \"id\",\n",
      "      \"text_column\": \"text\",\n",
      "      \"output_column\": \"comments_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"text_model_type\": \"tfidf_svd\",\n",
      "  \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n",
      "  \"text_max_length\": 256,\n",
      "  \"text_batch_size\": 32,\n",
      "  \"text_tfidf_max_features\": 50000,\n",
      "  \"text_svd_n_components\": 256,\n",
      "  \"cv_n_splits\": 5,\n",
      "  \"cv_random_state\": 42,\n",
      "  \"cv_shuffle\": true,\n",
      "  \"cv_stratified\": true,\n",
      "  \"use_catboost\": true,\n",
      "  \"use_lightgbm\": false,\n",
      "  \"use_xgboost\": false,\n",
      "  \"catboost_params\": {\n",
      "    \"iterations\": 3500,\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"depth\": 6,\n",
      "    \"loss_function\": \"MultiClass\",\n",
      "    \"eval_metric\": \"AUC\",\n",
      "    \"random_seed\": 42,\n",
      "    \"verbose\": 100\n",
      "  },\n",
      "  \"lgb_params\": {\n",
      "    \"objective\": \"multiclass\",\n",
      "    \"eval_metric\": \"auc\",\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"num_leaves\": 31,\n",
      "    \"feature_fraction\": 0.9,\n",
      "    \"bagging_fraction\": 0.8,\n",
      "    \"bagging_freq\": 5,\n",
      "    \"lambda_l2\": 1.0,\n",
      "    \"num_threads\": 0,\n",
      "    \"verbose\": -1\n",
      "  },\n",
      "  \"xgb_params\": {\n",
      "    \"objective\": \"multi:softmax\",\n",
      "    \"eval_metric\": \"auc\",\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"max_depth\": 6,\n",
      "    \"subsample\": 0.8,\n",
      "    \"colsample_bytree\": 0.8,\n",
      "    \"lambda\": 1.0,\n",
      "    \"alpha\": 0.0,\n",
      "    \"tree_method\": \"hist\"\n",
      "  },\n",
      "  \"blend_weights\": {\n",
      "    \"catboost\": 1,\n",
      "    \"lightgbm\": 0.35,\n",
      "    \"xgboost\": 0.15\n",
      "  },\n",
      "  \"compute_feature_importance\": true,\n",
      "  \"top_features_to_show\": 50,\n",
      "  \"early_stopping_rounds\": 100,\n",
      "  \"device\": \"cuda\",\n",
      "  \"seed\": 42,\n",
      "  \"verbose\": true\n",
      "}\n",
      "INFO:TABULAR_BOOSTING:{\n",
      "  \"train_path\": \"/kaggle/input/petfinder-pawpularity-score/train.csv\",\n",
      "  \"test_path\": \"/kaggle/input/petfinder-pawpularity-score/test.csv\",\n",
      "  \"sep\": \",\",\n",
      "  \"id_column\": \"Id\",\n",
      "  \"target_column\": \"Pawpularity\",\n",
      "  \"datetime_columns\": [],\n",
      "  \"output_dir\": \"tabular_boosting_output\",\n",
      "  \"task_type\": \"regression\",\n",
      "  \"basic_drop_columns\": [],\n",
      "  \"basic_as_categorical\": [],\n",
      "  \"basic_max_cat_unique\": 64,\n",
      "  \"basic_datetime_expand\": false,\n",
      "  \"basic_datetime_features\": [\n",
      "    \"year\",\n",
      "    \"month\",\n",
      "    \"day\",\n",
      "    \"dow\",\n",
      "    \"hour\"\n",
      "  ],\n",
      "  \"cat_min_count\": 20,\n",
      "  \"cat_min_freq\": 0.0,\n",
      "  \"cat_max_rare_share\": 0.98,\n",
      "  \"cat_max_unique_after_group\": 500,\n",
      "  \"post_feature_drop_columns\": [],\n",
      "  \"address_column\": null,\n",
      "  \"address_city_index\": -1,\n",
      "  \"address_split_sep\": \",\",\n",
      "  \"agg_enable\": false,\n",
      "  \"agg_groupby_cols\": [],\n",
      "  \"agg_numeric_cols\": [],\n",
      "  \"agg_aggs\": [\n",
      "    \"mean\",\n",
      "    \"std\",\n",
      "    \"min\",\n",
      "    \"max\",\n",
      "    \"sum\",\n",
      "    \"median\",\n",
      "    \"nunique\",\n",
      "    \"count\"\n",
      "  ],\n",
      "  \"agg_prefix\": \"agg\",\n",
      "  \"geo_enable\": false,\n",
      "  \"geo_lat_from_col\": \"pickup_lat\",\n",
      "  \"geo_lon_from_col\": \"pickup_lon\",\n",
      "  \"geo_lat_to_col\": \"dropoff_lat\",\n",
      "  \"geo_lon_to_col\": \"dropoff_lon\",\n",
      "  \"geo_ref_lat\": \"category_geo_ref_lat\",\n",
      "  \"geo_ref_lon\": \"category_geo_ref_lon\",\n",
      "  \"geo_prefix\": \"geo\",\n",
      "  \"geo_extra_enable\": false,\n",
      "  \"geo_from_coord_col\": null,\n",
      "  \"geo_to_coord_col\": null,\n",
      "  \"geo_coord_string_sep\": \",\",\n",
      "  \"geo_reference\": {\n",
      "    \"enabled\": false,\n",
      "    \"category_column\": \"category\",\n",
      "    \"coordinates_column\": \"coordinates\",\n",
      "    \"lat_column\": null,\n",
      "    \"lon_column\": null,\n",
      "    \"output_column\": \"category_geo_ref\",\n",
      "    \"output_lat_column\": \"category_geo_ref_lat\",\n",
      "    \"output_lon_column\": \"category_geo_ref_lon\"\n",
      "  },\n",
      "  \"corr_enable\": false,\n",
      "  \"corr_pearson_min_abs\": 0.95,\n",
      "  \"corr_use_phik\": false,\n",
      "  \"corr_phik_min_abs\": 0.0,\n",
      "  \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n",
      "  \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n",
      "  \"image_ext\": \".jpg\",\n",
      "  \"file_names_column\": \"Id\",\n",
      "  \"batch_size\": 32,\n",
      "  \"clip_model_name\": \"openai/clip-vit-large-patch14\",\n",
      "  \"text_enable\": false,\n",
      "  \"text_columns\": [],\n",
      "  \"text_external_tables\": [\n",
      "    {\n",
      "      \"name\": \"reviews\",\n",
      "      \"train_path\": \"reviews.tsv\",\n",
      "      \"test_path\": \"reviews.tsv\",\n",
      "      \"format\": \"tsv\",\n",
      "      \"id_column\": \"id\",\n",
      "      \"text_column\": \"text\",\n",
      "      \"output_column\": \"comments_text\"\n",
      "    }\n",
      "  ],\n",
      "  \"text_model_type\": \"tfidf_svd\",\n",
      "  \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n",
      "  \"text_max_length\": 256,\n",
      "  \"text_batch_size\": 32,\n",
      "  \"text_tfidf_max_features\": 50000,\n",
      "  \"text_svd_n_components\": 256,\n",
      "  \"cv_n_splits\": 5,\n",
      "  \"cv_random_state\": 42,\n",
      "  \"cv_shuffle\": true,\n",
      "  \"cv_stratified\": true,\n",
      "  \"use_catboost\": true,\n",
      "  \"use_lightgbm\": false,\n",
      "  \"use_xgboost\": false,\n",
      "  \"catboost_params\": {\n",
      "    \"iterations\": 3500,\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"depth\": 6,\n",
      "    \"loss_function\": \"MultiClass\",\n",
      "    \"eval_metric\": \"AUC\",\n",
      "    \"random_seed\": 42,\n",
      "    \"verbose\": 100\n",
      "  },\n",
      "  \"lgb_params\": {\n",
      "    \"objective\": \"multiclass\",\n",
      "    \"eval_metric\": \"auc\",\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"num_leaves\": 31,\n",
      "    \"feature_fraction\": 0.9,\n",
      "    \"bagging_fraction\": 0.8,\n",
      "    \"bagging_freq\": 5,\n",
      "    \"lambda_l2\": 1.0,\n",
      "    \"num_threads\": 0,\n",
      "    \"verbose\": -1\n",
      "  },\n",
      "  \"xgb_params\": {\n",
      "    \"objective\": \"multi:softmax\",\n",
      "    \"eval_metric\": \"auc\",\n",
      "    \"learning_rate\": 0.05,\n",
      "    \"max_depth\": 6,\n",
      "    \"subsample\": 0.8,\n",
      "    \"colsample_bytree\": 0.8,\n",
      "    \"lambda\": 1.0,\n",
      "    \"alpha\": 0.0,\n",
      "    \"tree_method\": \"hist\"\n",
      "  },\n",
      "  \"blend_weights\": {\n",
      "    \"catboost\": 1,\n",
      "    \"lightgbm\": 0.35,\n",
      "    \"xgboost\": 0.15\n",
      "  },\n",
      "  \"compute_feature_importance\": true,\n",
      "  \"top_features_to_show\": 50,\n",
      "  \"early_stopping_rounds\": 100,\n",
      "  \"device\": \"cuda\",\n",
      "  \"seed\": 42,\n",
      "  \"verbose\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "output_dir = CONFIG[\"output_dir\"]\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger = setup_logging(output_dir)\n",
    "log_config(logger, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8795137b",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87f51611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:07.597608Z",
     "iopub.status.busy": "2025-11-18T21:06:07.597398Z",
     "iopub.status.idle": "2025-11-18T21:06:07.607632Z",
     "shell.execute_reply": "2025-11-18T21:06:07.606835Z",
     "shell.execute_reply.started": "2025-11-18T21:06:07.597592Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def _ensure_lat_lon_from_single_column(\n",
    "    df: pd.DataFrame,\n",
    "    coord_col: str,\n",
    "    lat_name: str,\n",
    "    lon_name: str,\n",
    "    sep: str = \",\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    coord_col может быть:\n",
    "      - строка вида \"55.75,37.62\"\n",
    "      - список/кортеж [55.75, 37.62]\n",
    "    создаём/перезаписываем колонки lat_name, lon_name\n",
    "    \"\"\"\n",
    "    if coord_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    def _parse_one(x):\n",
    "        if isinstance(x, (list, tuple)) and len(x) >= 2:\n",
    "            return x[0], x[1]\n",
    "        if isinstance(x, str):\n",
    "            parts = x.split(sep)\n",
    "            if len(parts) >= 2:\n",
    "                try:\n",
    "                    return float(parts[0]), float(parts[1])\n",
    "                except Exception:\n",
    "                    return np.nan, np.nan\n",
    "        return np.nan, np.nan\n",
    "\n",
    "    lat_list, lon_list = zip(*df[coord_col].map(_parse_one))\n",
    "    df = df.copy()\n",
    "    df[lat_name] = lat_list\n",
    "    df[lon_name] = lon_list\n",
    "    return df\n",
    "\n",
    "# NEW: очистка имён фичей для бустингов (XGBoost особенно строгий)\n",
    "def sanitize_feature_names(\n",
    "    df: pd.DataFrame,\n",
    "    logger: logging.Logger\n",
    ") -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Преобразует имена колонок в строки без символов [, ], <, >,\n",
    "    следит за уникальностью имён и возвращает:\n",
    "      - новый DataFrame с переименованными колонками\n",
    "      - mapping {старое_имя -> новое_имя}\n",
    "    \"\"\"\n",
    "    old_cols = list(df.columns)\n",
    "    new_cols: List[str] = []\n",
    "    mapping: Dict = {}\n",
    "    changed = False\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    for col in old_cols:\n",
    "        new = str(col)\n",
    "\n",
    "        # запрещённые символы (можно расширить при необходимости)\n",
    "        new = new.replace(\"[\", \"(\").replace(\"]\", \")\")\n",
    "        new = new.replace(\"<\", \"_lt_\").replace(\">\", \"_gt_\")\n",
    "\n",
    "        # на всякий случай избавимся от очень \"экзотики\"\n",
    "        # (например, пустые имена)\n",
    "        if new.strip() == \"\":\n",
    "            new = \"feature\"\n",
    "\n",
    "        base = new\n",
    "        k = 1\n",
    "        # обеспечиваем уникальность\n",
    "        while new in new_cols:\n",
    "            new = f\"{base}__{k}\"\n",
    "            k += 1\n",
    "\n",
    "        if new != col:\n",
    "            changed = True\n",
    "        mapping[col] = new\n",
    "        new_cols.append(new)\n",
    "\n",
    "    df.columns = new_cols\n",
    "\n",
    "    if changed:\n",
    "        logger.info(\"Sanitized feature names for boosting models.\")\n",
    "        # при желании можно залогировать часть маппинга\n",
    "        logger.info(\n",
    "            \"Example of feature name mapping: \" +\n",
    "            \", \".join(\n",
    "                f\"{k} -> {v}\" for k, v in list(mapping.items())[:10]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return df, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa0054b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:07.719390Z",
     "iopub.status.busy": "2025-11-18T21:06:07.719167Z",
     "iopub.status.idle": "2025-11-18T21:06:07.723950Z",
     "shell.execute_reply": "2025-11-18T21:06:07.723133Z",
     "shell.execute_reply.started": "2025-11-18T21:06:07.719374Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ImagesDataset:\n",
    "    def __init__(self, file_paths, transform=None):\n",
    "        self.file_paths = file_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ### read image\n",
    "\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        # DataLoader сам создаст batch dimension, поэтому unsqueeze не нужен\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bb95ee92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:12.170864Z",
     "iopub.status.busy": "2025-11-18T21:06:12.170178Z",
     "iopub.status.idle": "2025-11-18T21:06:12.191967Z",
     "shell.execute_reply": "2025-11-18T21:06:12.191325Z",
     "shell.execute_reply.started": "2025-11-18T21:06:12.170836Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def detect_categorical_columns(\n",
    "    df: pd.DataFrame,\n",
    "    max_unique: int,\n",
    "    force_categorical: List[str]\n",
    ") -> List[str]:\n",
    "    # Проверяем, что max_unique не None\n",
    "    if max_unique is None:\n",
    "        max_unique = 64  # значение по умолчанию\n",
    "    \n",
    "    # Фильтруем None из force_categorical\n",
    "    cats = set(c for c in force_categorical if c is not None and isinstance(c, str))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # Пропускаем None или не-строковые имена колонок\n",
    "        if col is None or not isinstance(col, str):\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            if df[col].dtype == \"object\":\n",
    "                cats.add(col)\n",
    "            else:\n",
    "                try:\n",
    "                    nunique_val = df[col].nunique(dropna=True)\n",
    "                    # Проверяем, что nunique_val не None и является числом\n",
    "                    # Также проверяем, что max_unique - это число\n",
    "                    if (nunique_val is not None and \n",
    "                        isinstance(nunique_val, (int, np.integer)) and\n",
    "                        isinstance(max_unique, (int, np.integer, float))):\n",
    "                        if int(nunique_val) <= int(max_unique):\n",
    "                            cats.add(col)\n",
    "                except (TypeError, ValueError) as e:\n",
    "                    # Если не можем вычислить nunique, пропускаем колонку\n",
    "                    continue\n",
    "        except Exception:\n",
    "            # Если возникают проблемы с колонкой, пропускаем её\n",
    "            continue\n",
    "    \n",
    "    # Фильтруем None и не-строковые значения перед сортировкой\n",
    "    # Также убеждаемся, что все значения - строки\n",
    "    cats_filtered = []\n",
    "    for c in cats:\n",
    "        if c is not None:\n",
    "            # Преобразуем в строку, если это не строка\n",
    "            try:\n",
    "                c_str = str(c) if not isinstance(c, str) else c\n",
    "                if c_str and c_str.strip():  # Проверяем, что строка не пустая\n",
    "                    cats_filtered.append(c_str)\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    # Сортируем только строки\n",
    "    try:\n",
    "        return sorted(cats_filtered)\n",
    "    except TypeError as e:\n",
    "        # Если все еще ошибка сортировки, возвращаем без сортировки\n",
    "        return list(cats_filtered)\n",
    "\n",
    "\n",
    "def process_categorical_features(\n",
    "    X: pd.DataFrame,\n",
    "    test: Optional[pd.DataFrame],\n",
    "    cat_cols: List[str],\n",
    "    config: Dict,\n",
    "    logger: logging.Logger\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame], List[str]]:\n",
    "    \"\"\"\n",
    "    Для каждой категориальной фичи:\n",
    "      1) категории с малой частотой/долей сливаются в 'other'\n",
    "      2) если доля редких категорий слишком велика или\n",
    "         даже после слияния слишком много уникальных — фича выкидывается.\n",
    "\n",
    "    Возвращает:\n",
    "      X_new, test_new, updated_cat_cols\n",
    "    \"\"\"\n",
    "    if not cat_cols:\n",
    "        return X, test, cat_cols\n",
    "\n",
    "    min_count = int(config.get(\"cat_min_count\", 0) or 0)\n",
    "    min_freq = float(config.get(\"cat_min_freq\", 0.0) or 0.0)\n",
    "    max_rare_share = float(config.get(\"cat_max_rare_share\", 1.0) or 1.0)\n",
    "    max_unique_after = int(config.get(\"cat_max_unique_after_group\", 10**9) or 10**9)\n",
    "\n",
    "    if min_count <= 0 and min_freq <= 0.0 and max_rare_share >= 1.0:\n",
    "        # фактически фильтрация отключена\n",
    "        logger.info(\"Categorical processing: thresholds are trivial, skipping.\")\n",
    "        return X, test, cat_cols\n",
    "\n",
    "    logger.info(\n",
    "        \"Categorical processing: \"\n",
    "        f\"cat_min_count={min_count}, cat_min_freq={min_freq}, \"\n",
    "        f\"cat_max_rare_share={max_rare_share}, \"\n",
    "        f\"cat_max_unique_after_group={max_unique_after}\"\n",
    "    )\n",
    "\n",
    "    X_new = X.copy()\n",
    "    test_new = test.copy() if test is not None else None\n",
    "    to_drop = []\n",
    "\n",
    "    n = len(X_new)\n",
    "\n",
    "    for col in tqdm(cat_cols, desc=\"Processing categorical features\"):\n",
    "        if col not in X_new.columns:\n",
    "            continue\n",
    "\n",
    "        vc = X_new[col].value_counts(dropna=False)\n",
    "        total = vc.sum()\n",
    "\n",
    "        # определяем \"редкие\" категории\n",
    "        rare_mask = np.zeros(len(vc), dtype=bool)\n",
    "        if min_count > 0:\n",
    "            rare_mask |= (vc.values < min_count)\n",
    "        if min_freq > 0.0:\n",
    "            rare_mask |= (vc.values / total < min_freq)\n",
    "\n",
    "        rare_cats = vc.index[rare_mask]\n",
    "        rare_share = vc[rare_cats].sum() / total if len(rare_cats) > 0 else 0.0\n",
    "\n",
    "        logger.info(\n",
    "            f\"[cat] {col}: unique={vc.size}, rare_cats={len(rare_cats)}, \"\n",
    "            f\"rare_share={rare_share:.4f}\"\n",
    "        )\n",
    "\n",
    "        # если редких слишком много — фича бесполезна, выкидываем\n",
    "        if rare_share >= max_rare_share:\n",
    "            logger.info(\n",
    "                f\"[cat] {col}: rare_share={rare_share:.4f} >= {max_rare_share}, \"\n",
    "                f\"dropping whole feature.\"\n",
    "            )\n",
    "            to_drop.append(col)\n",
    "            continue\n",
    "\n",
    "        if len(rare_cats) == 0:\n",
    "            # нечего объединять\n",
    "            continue\n",
    "\n",
    "        # объединяем редкие в 'other'\n",
    "        other_label = \"__OTHER__\"\n",
    "        X_new[col] = X_new[col].where(~X_new[col].isin(rare_cats), other_label)\n",
    "        if test_new is not None:\n",
    "            test_new[col] = test_new[col].where(~test_new[col].isin(rare_cats), other_label)\n",
    "\n",
    "        # пересчитываем уникальные после объединения\n",
    "        new_unique = X_new[col].nunique(dropna=True)\n",
    "        if new_unique > max_unique_after:\n",
    "            logger.info(\n",
    "                f\"[cat] {col}: unique_after={new_unique} > \"\n",
    "                f\"cat_max_unique_after_group={max_unique_after}, dropping feature.\"\n",
    "            )\n",
    "            to_drop.append(col)\n",
    "\n",
    "    if to_drop:\n",
    "        logger.info(f\"Dropping categorical features: {to_drop}\")\n",
    "        X_new = X_new.drop(columns=[c for c in to_drop if c in X_new.columns])\n",
    "        if test_new is not None:\n",
    "            test_new = test_new.drop(columns=[c for c in to_drop if c in test_new.columns])\n",
    "        cat_cols = [c for c in cat_cols if c not in to_drop]\n",
    "\n",
    "    return X_new, test_new, cat_cols\n",
    "\n",
    "\n",
    "def expand_datetime_columns(\n",
    "    df: pd.DataFrame,\n",
    "    datetime_cols: List[str],\n",
    "    features: List[str]\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in datetime_cols:\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "        s = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "        if \"year\" in features:\n",
    "            df[f\"{col}_year\"] = s.dt.year.astype(\"Int64\")\n",
    "        if \"month\" in features:\n",
    "            df[f\"{col}_month\"] = s.dt.month.astype(\"Int64\")\n",
    "        if \"day\" in features:\n",
    "            df[f\"{col}_day\"] = s.dt.day.astype(\"Int64\")\n",
    "        if \"dow\" in features:\n",
    "            df[f\"{col}_dow\"] = s.dt.dayofweek.astype(\"Int64\")\n",
    "        if \"hour\" in features:\n",
    "            df[f\"{col}_hour\"] = s.dt.hour.astype(\"Int64\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def haversine_distance(\n",
    "    lat1, lon1, lat2, lon2, radius: float = 6371.0\n",
    ") -> np.ndarray:\n",
    "    lat1 = np.radians(lat1.astype(float))\n",
    "    lon1 = np.radians(lon1.astype(float))\n",
    "    lat2 = np.radians(lat2.astype(float))\n",
    "    lon2 = np.radians(lon2.astype(float))\n",
    "\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "\n",
    "    a = np.sin(dlat / 2.0) ** 2 + \\\n",
    "        np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    return radius * c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6493bc28",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "504a78c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:17.445718Z",
     "iopub.status.busy": "2025-11-18T21:06:17.444952Z",
     "iopub.status.idle": "2025-11-18T21:06:17.451039Z",
     "shell.execute_reply": "2025-11-18T21:06:17.450006Z",
     "shell.execute_reply.started": "2025-11-18T21:06:17.445695Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_data(config: Dict, logger: logging.Logger) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    train_path = config[\"train_path\"]\n",
    "    test_path = config[\"test_path\"]\n",
    "\n",
    "    logger.info(f\"📥 Loading train from {train_path}\")\n",
    "    train = pd.read_csv(train_path, sep=config[\"sep\"])\n",
    "\n",
    "    if test_path:\n",
    "        logger.info(f\"📥 Loading test from {test_path}\")\n",
    "        test = pd.read_csv(test_path, sep=config[\"sep\"])\n",
    "    else:\n",
    "        test = None\n",
    "\n",
    "    logger.info(f\"Train shape: {train.shape}\")\n",
    "    if test is not None:\n",
    "        logger.info(f\"Test  shape: {test.shape}\")\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99b6a7f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:21.701769Z",
     "iopub.status.busy": "2025-11-18T21:06:21.701489Z",
     "iopub.status.idle": "2025-11-18T21:06:21.732071Z",
     "shell.execute_reply": "2025-11-18T21:06:21.731556Z",
     "shell.execute_reply.started": "2025-11-18T21:06:21.701749Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:06:21,703 - INFO -  Loading train from /kaggle/input/petfinder-pawpularity-score/train.csv\n",
      "INFO:TABULAR_BOOSTING:📥 Loading train from /kaggle/input/petfinder-pawpularity-score/train.csv\n",
      "2025-11-18 21:06:21,722 - INFO -  Loading test from /kaggle/input/petfinder-pawpularity-score/test.csv\n",
      "INFO:TABULAR_BOOSTING:📥 Loading test from /kaggle/input/petfinder-pawpularity-score/test.csv\n",
      "2025-11-18 21:06:21,725 - INFO - Train shape: (9912, 14)\n",
      "INFO:TABULAR_BOOSTING:Train shape: (9912, 14)\n",
      "2025-11-18 21:06:21,726 - INFO - Test  shape: (8, 13)\n",
      "INFO:TABULAR_BOOSTING:Test  shape: (8, 13)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = load_data(CONFIG, logger)\n",
    "\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "target_col = CONFIG[\"target_column\"]\n",
    "\n",
    "test_ids = None\n",
    "if test_df is not None and id_col in test_df.columns:\n",
    "    test_ids = test_df[id_col].copy()\n",
    "\n",
    "y = train_df[target_col]\n",
    "X = train_df.drop(columns=[target_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "43910515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:40.135740Z",
     "iopub.status.busy": "2025-11-18T21:06:40.134983Z",
     "iopub.status.idle": "2025-11-18T21:06:40.149854Z",
     "shell.execute_reply": "2025-11-18T21:06:40.148980Z",
     "shell.execute_reply.started": "2025-11-18T21:06:40.135708Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Добавь сюда создание reference колонки для гео-фичей\n",
    "# На основе усреднения координат при группировке по категориальной колонке\n",
    "def fit_geo_reference(df: pd.DataFrame, config: Dict, logger: Optional[logging.Logger] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Строит DataFrame geo_ref_df, который сопоставляет каждой категории средние координаты.\n",
    "    Здесь создаётся reference-колонка для гео-фичей:\n",
    "      - <output_lat_column>, <output_lon_column> (float)\n",
    "      - <output_column> как строка \"lat,lon\".\n",
    "    \"\"\"\n",
    "    geo_cfg = config.get(\"geo_reference\", {})\n",
    "    cat_col = geo_cfg.get(\"category_column\", \"category\")\n",
    "    coord_col = geo_cfg.get(\"coordinates_column\", \"coordinates\")\n",
    "    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n",
    "    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n",
    "    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n",
    "\n",
    "    lat_col_cfg = geo_cfg.get(\"lat_column\")\n",
    "    lon_col_cfg = geo_cfg.get(\"lon_column\")\n",
    "\n",
    "    df_work = df.copy()\n",
    "\n",
    "    # 1) Берём lat/lon либо из явных колонок, либо парсим строку \"lat,lon\"\n",
    "    if (\n",
    "        lat_col_cfg\n",
    "        and lon_col_cfg\n",
    "        and lat_col_cfg in df_work.columns\n",
    "        and lon_col_cfg in df_work.columns\n",
    "    ):\n",
    "        # Явные колонки широты/долготы\n",
    "        df_work[\"_lat\"] = pd.to_numeric(df_work[lat_col_cfg], errors=\"coerce\")\n",
    "        df_work[\"_lon\"] = pd.to_numeric(df_work[lon_col_cfg], errors=\"coerce\")\n",
    "    else:\n",
    "        # Парсим coordinates как строку \"lat,lon\" (можно с пробелами и знаками)\n",
    "        coords_parsed = (\n",
    "            df_work[coord_col]\n",
    "            .astype(str)\n",
    "            .str.extract(r\"([+-]?\\d+\\.?\\d*)\\s*,\\s*([+-]?\\d+\\.?\\d*)\")\n",
    "        )\n",
    "        df_work[\"_lat\"] = pd.to_numeric(coords_parsed[0], errors=\"coerce\")\n",
    "        df_work[\"_lon\"] = pd.to_numeric(coords_parsed[1], errors=\"coerce\")\n",
    "\n",
    "    # 2) Считаем средние координаты по группам\n",
    "    geo_ref_df = (\n",
    "        df_work.groupby(cat_col)[[\"_lat\", \"_lon\"]]\n",
    "        .mean()\n",
    "        .rename(columns={\"_lat\": out_lat_col, \"_lon\": out_lon_col})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # 3) Строковая колонка \"lat,lon\" (для отладки/экспорта)\n",
    "    geo_ref_df[out_col] = (\n",
    "        geo_ref_df[out_lat_col].round(6).astype(str)\n",
    "        + \",\" +\n",
    "        geo_ref_df[out_lon_col].round(6).astype(str)\n",
    "    )\n",
    "\n",
    "    if logger:\n",
    "        logger.info(\n",
    "            f\"🌍 Обучили reference-гео-центры по {cat_col}, всего групп: {geo_ref_df.shape[0]}\"\n",
    "        )\n",
    "    return geo_ref_df\n",
    "\n",
    "def add_geo_reference_column(\n",
    "    df: pd.DataFrame,\n",
    "    geo_ref_df: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    logger: Optional[logging.Logger] = None\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Добавляет reference-гео-колонку(и) к df, используя geo_ref_df, посчитанную на train.\n",
    "    \"\"\"\n",
    "    geo_cfg = config.get(\"geo_reference\", {})\n",
    "    cat_col = geo_cfg.get(\"category_column\", \"category\")\n",
    "    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n",
    "    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n",
    "    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n",
    "\n",
    "    df = df.merge(geo_ref_df[[cat_col, out_col, out_lat_col, out_lon_col]], how=\"left\", on=cat_col)\n",
    "    n_missing = df[out_col].isna().sum()\n",
    "    if logger:\n",
    "        logger.info(\n",
    "            f\"✅ Добавлены reference-гео-колонки ({out_col}, {out_lat_col}, {out_lon_col}) \"\n",
    "            f\"к df (geo reference по {cat_col}). Пропусков: {n_missing}\"\n",
    "        )\n",
    "    return df\n",
    "\n",
    "\n",
    "def maybe_apply_geo_reference(\n",
    "    df: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    logger: Optional[logging.Logger] = None,\n",
    "    fit: bool = False,\n",
    "    geo_ref_df: Optional[pd.DataFrame] = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Обертка для train/test: если fit=True, вычисляет geo reference и добавляет в train,\n",
    "    если fit=False (test), просто применяет geo_ref_df к df.\n",
    "    Возвращает:\n",
    "      - при fit=True: (df_with_ref, geo_ref_df)\n",
    "      - при fit=False: df_with_ref\n",
    "    \"\"\"\n",
    "    geo_cfg = config.get(\"geo_reference\", {})\n",
    "    if not geo_cfg.get(\"enabled\", False):\n",
    "        if logger:\n",
    "            logger.info(\"🌎 Гео-reference выключен (config['geo_reference']['enabled']=False)\")\n",
    "        return (df, None) if fit else df\n",
    "\n",
    "    if fit:\n",
    "        geo_ref_df = fit_geo_reference(df, config, logger)\n",
    "        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n",
    "        return df, geo_ref_df\n",
    "    else:\n",
    "        if geo_ref_df is None:\n",
    "            raise ValueError(\"geo_ref_df must be passed for applying on test set\")\n",
    "        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n",
    "        return df\n",
    "\n",
    "# --- Применяем reference-гео-фичи к train и test, если нужно ---\n",
    "if CONFIG.get(\"geo_reference\", {}).get(\"enabled\", False):\n",
    "    logger.info(\"🌎 Применяем гео-reference features...\")\n",
    "    train_df, geo_ref_df = maybe_apply_geo_reference(train_df, CONFIG, logger, fit=True)\n",
    "    test_df = maybe_apply_geo_reference(test_df, CONFIG, logger, fit=False, geo_ref_df=geo_ref_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "925eb654",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:43.575624Z",
     "iopub.status.busy": "2025-11-18T21:06:43.575349Z",
     "iopub.status.idle": "2025-11-18T21:06:43.579461Z",
     "shell.execute_reply": "2025-11-18T21:06:43.578623Z",
     "shell.execute_reply.started": "2025-11-18T21:06:43.575606Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if CONFIG['task_type'] in ['multiclass', 'binary']:\n",
    "    y = y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b12d371",
   "metadata": {},
   "source": [
    "УДАЛЕНИЕ ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ec651d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:44.512206Z",
     "iopub.status.busy": "2025-11-18T21:06:44.511642Z",
     "iopub.status.idle": "2025-11-18T21:06:44.515305Z",
     "shell.execute_reply": "2025-11-18T21:06:44.514601Z",
     "shell.execute_reply.started": "2025-11-18T21:06:44.512185Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NOTE: ID column is intentionally NOT dropped here.\n",
    "# Оно будет удалено позже, непосредственно перед обучением бустингов,\n",
    "# чтобы сохранить ID для генерации фич (картинки/тексты/агрегации)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f2c77",
   "metadata": {},
   "source": [
    "### Features generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44b85bc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:45.327556Z",
     "iopub.status.busy": "2025-11-18T21:06:45.326929Z",
     "iopub.status.idle": "2025-11-18T21:06:45.352953Z",
     "shell.execute_reply": "2025-11-18T21:06:45.352190Z",
     "shell.execute_reply.started": "2025-11-18T21:06:45.327535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_basic_features(\n",
    "    df: pd.DataFrame,\n",
    "    config: Dict,\n",
    "    logger: Optional[logging.Logger] = None\n",
    ") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    drop_cols = config[\"basic_drop_columns\"]\n",
    "    for c in drop_cols:\n",
    "        if c in df.columns:\n",
    "            if logger:\n",
    "                logger.info(f\"Dropping column: {c}\")\n",
    "            df = df.drop(columns=[c])\n",
    "\n",
    "    if config[\"basic_datetime_expand\"] and config[\"datetime_columns\"]:\n",
    "        if logger:\n",
    "            logger.info(f\"Expanding datetime columns: {config['datetime_columns']}\")\n",
    "        df = expand_datetime_columns(\n",
    "            df,\n",
    "            datetime_cols=config[\"datetime_columns\"],\n",
    "            features=config[\"basic_datetime_features\"]\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def generate_aggregate_features(\n",
    "    train: pd.DataFrame,\n",
    "    test: Optional[pd.DataFrame],\n",
    "    config: Dict,\n",
    "    logger: Optional[logging.Logger] = None\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    if not config[\"agg_enable\"]:\n",
    "        if logger:\n",
    "            logger.info(\"Aggregate features disabled in CONFIG.\")\n",
    "        return train, test\n",
    "\n",
    "    groupby_cols = config[\"agg_groupby_cols\"]\n",
    "    if not groupby_cols:\n",
    "        if logger:\n",
    "            logger.info(\"No agg_groupby_cols specified — skipping aggregates.\")\n",
    "        return train, test\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Generating aggregate features by {groupby_cols}\")\n",
    "\n",
    "    all_df = train if test is None else pd.concat([train, test], axis=0, ignore_index=True)\n",
    "\n",
    "    if config[\"agg_numeric_cols\"]:\n",
    "        num_cols = [c for c in config[\"agg_numeric_cols\"] if c in all_df.columns]\n",
    "    else:\n",
    "        num_cols = [\n",
    "            c for c in all_df.columns\n",
    "            if pd.api.types.is_numeric_dtype(all_df[c])\n",
    "        ]\n",
    "        for c in [config[\"target_column\"], config[\"id_column\"]]:\n",
    "            if c in num_cols:\n",
    "                num_cols.remove(c)\n",
    "\n",
    "    aggs = config[\"agg_aggs\"]\n",
    "    prefix = config[\"agg_prefix\"]\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Aggregating numeric cols: {num_cols}\")\n",
    "        logger.info(f\"Using aggs: {aggs}, prefix: {prefix}\")\n",
    "\n",
    "    grouped = all_df.groupby(groupby_cols)[num_cols].agg(aggs)\n",
    "    grouped.columns = [\n",
    "        f\"{prefix}_\" + \"_\".join(map(str, col)).strip()\n",
    "        for col in grouped.columns.to_flat_index()\n",
    "    ]\n",
    "    grouped = grouped.reset_index()\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Aggregate frame shape: {grouped.shape}\")\n",
    "\n",
    "    train_merged = train.merge(grouped, on=groupby_cols, how=\"left\")\n",
    "    test_merged = None\n",
    "    if test is not None:\n",
    "        test_merged = test.merge(grouped, on=groupby_cols, how=\"left\")\n",
    "\n",
    "    return train_merged, test_merged\n",
    "\n",
    "\n",
    "def generate_geo_features(\n",
    "    train: pd.DataFrame,\n",
    "    test: Optional[pd.DataFrame],\n",
    "    config: Dict,\n",
    "    logger: Optional[logging.Logger] = None\n",
    ") -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Базовые фичи:\n",
    "      - geo_dist_from_to_km\n",
    "      - geo_dist_from_ref_km, geo_dist_to_ref_km (если задан ref)\n",
    "      - geo_lat_from_abs / geo_lon_from_abs / geo_lat_to_abs / geo_lon_to_abs\n",
    "\n",
    "    Если geo_extra_enable=True:\n",
    "      - geo_dlat / geo_dlon\n",
    "      - geo_manhattan_km\n",
    "      - geo_bearing_deg\n",
    "      - geo_mid_lat / geo_mid_lon\n",
    "    \"\"\"\n",
    "    if not config[\"geo_enable\"]:\n",
    "        if logger:\n",
    "            logger.info(\"Geo features disabled in CONFIG.\")\n",
    "        return train, test\n",
    "\n",
    "    lat_from_col = config[\"geo_lat_from_col\"]\n",
    "    lon_from_col = config[\"geo_lon_from_col\"]\n",
    "    lat_to_col = config[\"geo_lat_to_col\"]\n",
    "    lon_to_col = config[\"geo_lon_to_col\"]\n",
    "    prefix = config[\"geo_prefix\"]\n",
    "\n",
    "    from_coord_col = config.get(\"geo_from_coord_col\")\n",
    "    to_coord_col = config.get(\"geo_to_coord_col\")\n",
    "    coord_sep = config.get(\"geo_coord_string_sep\", \",\")\n",
    "\n",
    "    if from_coord_col is not None:\n",
    "        train = _ensure_lat_lon_from_single_column(\n",
    "            train, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n",
    "        )\n",
    "        if test is not None:\n",
    "            test = _ensure_lat_lon_from_single_column(\n",
    "                test, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n",
    "            )\n",
    "\n",
    "    if to_coord_col is not None:\n",
    "        train = _ensure_lat_lon_from_single_column(\n",
    "            train, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n",
    "        )\n",
    "        if test is not None:\n",
    "            test = _ensure_lat_lon_from_single_column(\n",
    "                test, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n",
    "            )\n",
    "\n",
    "\n",
    "    for col in [lat_from_col, lon_from_col, lat_to_col, lon_to_col]:\n",
    "        if col and col not in train.columns:\n",
    "            raise ValueError(f\"Column {col} not found in train for geo features\")\n",
    "\n",
    "    if logger:\n",
    "        logger.info(f\"Generating geo features with backend: {GEO_BACKEND}\")\n",
    "\n",
    "    def _distance_rowwise(\n",
    "        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        if GEO_BACKEND == \"geopy\":\n",
    "            def _one(a, b, c, d):\n",
    "                if pd.isna(a) or pd.isna(b) or pd.isna(c) or pd.isna(d):\n",
    "                    return np.nan\n",
    "                return geodesic((a, b), (c, d)).km\n",
    "            return np.vectorize(_one)(lat1, lon1, lat2, lon2)\n",
    "        else:\n",
    "            return haversine_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    def _bearing(\n",
    "        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n",
    "    ) -> np.ndarray:\n",
    "        # initial bearing (degrees)\n",
    "        lat1_r = np.radians(lat1.astype(float))\n",
    "        lat2_r = np.radians(lat2.astype(float))\n",
    "        dlon_r = np.radians(lon2.astype(float) - lon1.astype(float))\n",
    "\n",
    "        x = np.sin(dlon_r) * np.cos(lat2_r)\n",
    "        y = np.cos(lat1_r) * np.sin(lat2_r) - np.sin(lat1_r) * np.cos(lat2_r) * np.cos(dlon_r)\n",
    "        brng = np.degrees(np.arctan2(x, y))\n",
    "        brng = (brng + 360) % 360\n",
    "        return brng\n",
    "\n",
    "    def _apply_geo(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "\n",
    "        df[f\"{prefix}_dist_from_to_km\"] = _distance_rowwise(\n",
    "            df[lat_from_col], df[lon_from_col],\n",
    "            df[lat_to_col], df[lon_to_col]\n",
    "        )\n",
    "\n",
    "        ref_lat = config[\"geo_ref_lat\"]\n",
    "        ref_lon = config[\"geo_ref_lon\"]\n",
    "        if ref_lat is not None and ref_lon is not None:\n",
    "            df[f\"{prefix}_dist_from_ref_km\"] = _distance_rowwise(\n",
    "                df[lat_from_col], df[lon_from_col],\n",
    "                pd.Series(ref_lat, index=df.index),\n",
    "                pd.Series(ref_lon, index=df.index),\n",
    "            )\n",
    "            df[f\"{prefix}_dist_to_ref_km\"] = _distance_rowwise(\n",
    "                df[lat_to_col], df[lon_to_col],\n",
    "                pd.Series(ref_lat, index=df.index),\n",
    "                pd.Series(ref_lon, index=df.index),\n",
    "            )\n",
    "\n",
    "        df[f\"{prefix}_lat_from_abs\"] = df[lat_from_col].abs()\n",
    "        df[f\"{prefix}_lon_from_abs\"] = df[lon_from_col].abs()\n",
    "        df[f\"{prefix}_lat_to_abs\"] = df[lat_to_col].abs()\n",
    "        df[f\"{prefix}_lon_to_abs\"] = df[lon_to_col].abs()\n",
    "\n",
    "        if config.get(\"geo_extra_enable\", False):\n",
    "            df[f\"{prefix}_dlat\"] = df[lat_to_col] - df[lat_from_col]\n",
    "            df[f\"{prefix}_dlon\"] = df[lon_to_col] - df[lon_from_col]\n",
    "\n",
    "            df[f\"{prefix}_manhattan_km\"] = (\n",
    "                haversine_distance(df[lat_from_col], df[lon_from_col], df[lat_to_col], df[lon_from_col]) +\n",
    "                haversine_distance(df[lat_to_col], df[lon_from_col], df[lat_to_col], df[lon_to_col])\n",
    "            )\n",
    "\n",
    "            df[f\"{prefix}_bearing_deg\"] = _bearing(\n",
    "                df[lat_from_col], df[lon_from_col],\n",
    "                df[lat_to_col], df[lon_to_col]\n",
    "            )\n",
    "\n",
    "            df[f\"{prefix}_mid_lat\"] = (df[lat_from_col] + df[lat_to_col]) / 2.0\n",
    "            df[f\"{prefix}_mid_lon\"] = (df[lon_from_col] + df[lon_to_col]) / 2.0\n",
    "\n",
    "        return df\n",
    "\n",
    "    train_geo = _apply_geo(train)\n",
    "    test_geo = _apply_geo(test) if test is not None else None\n",
    "    return train_geo, test_geo\n",
    "\n",
    "\n",
    "def process_address_extract_city(\n",
    "    df: pd.DataFrame,\n",
    "    column: str,\n",
    "    city_index: int,\n",
    "    sep: str,\n",
    "    logger: logging.Logger\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Здесь извлекается колонка 'город':\n",
    "      - address -> список частей (split)\n",
    "      - из списка берём city_index (по умолчанию -1 = последний элемент)\n",
    "      - создаём новую колонку address_city\n",
    "      - она впоследствии становится категориальной и проходит\n",
    "        такую же обработку, как и остальные категориальные фичи\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        logger.warning(f\"Address column '{column}' not found, skipping city extraction.\")\n",
    "        return df\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    def _extract(addr):\n",
    "        if not isinstance(addr, str):\n",
    "            return None\n",
    "        parts = [p.strip() for p in addr.split(sep)]\n",
    "        if len(parts) == 0:\n",
    "            return None\n",
    "        try:\n",
    "            return parts[city_index]\n",
    "        except Exception:\n",
    "            # если индекс вне диапазона\n",
    "            return parts[-1]\n",
    "\n",
    "    df[\"address_city\"] = df[column].map(_extract)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0dd84223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:54.651313Z",
     "iopub.status.busy": "2025-11-18T21:06:54.650599Z",
     "iopub.status.idle": "2025-11-18T21:06:54.665080Z",
     "shell.execute_reply": "2025-11-18T21:06:54.664290Z",
     "shell.execute_reply.started": "2025-11-18T21:06:54.651291Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def _normalize_image_name(name: str, default_ext: str | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Приводим имя файла к нормальному виду:\n",
    "    - убираем пробелы/переводы строк\n",
    "    - если нет расширения, добавляем default_ext или CONFIG[\"image_ext\"]\n",
    "    \"\"\"\n",
    "    name = str(name).strip()\n",
    "    base = os.path.basename(name)\n",
    "    root, ext = os.path.splitext(base)\n",
    "    if ext == \"\":\n",
    "        if default_ext is None:\n",
    "            default_ext = CONFIG.get(\"image_ext\", \"\")\n",
    "        return root + (default_ext or \"\")\n",
    "    return root + ext\n",
    "\n",
    "\n",
    "def images_features(X, test_df):\n",
    "    \"\"\"\n",
    "    Добавляет к табличным фичам эмбеддинги картинок, посчитанные CLIP-моделью от OpenAI\n",
    "    (через HuggingFace Transformers, только image-encoder).\n",
    "\n",
    "    Ожидается, что в CONFIG заданы:\n",
    "      - 'file_names_column'   — колонка с именами файлов\n",
    "      - 'train_images_dir'    — папка с train-картинками\n",
    "      - 'test_images_dir'     — папка с test-картинками\n",
    "      - 'batch_size'          — размер батча для инференса\n",
    "      - 'clip_model_name'     — имя CLIP-модели в HuggingFace (по умолчанию ViT-L/14)\n",
    "    \"\"\"\n",
    "    file_name_column = CONFIG['file_names_column']\n",
    "\n",
    "    if file_name_column not in X.columns:\n",
    "        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} нет в train\")\n",
    "\n",
    "    if test_df is not None and file_name_column not in test_df.columns:\n",
    "        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} нет в test\")\n",
    "\n",
    "    # ----------------- CLIP BACKBONE (image encoder only) -----------------\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    clip_model_name = CONFIG.get(\"clip_model_name\", \"openai/clip-vit-large-patch14\")\n",
    "    # Загружаем модель и процессор CLIP\n",
    "    model = CLIPModel.from_pretrained(clip_model_name)\n",
    "    processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ----------------- TRAIN IMAGES -----------------\n",
    "    train_file_paths = [\n",
    "        os.path.join(\n",
    "            CONFIG['train_images_dir'],\n",
    "            _normalize_image_name(file_name, CONFIG.get(\"image_ext\", \"\"))\n",
    "        )\n",
    "        for file_name in X[file_name_column]\n",
    "    ]\n",
    "    train_images_part = process_images(train_file_paths, model, processor, device, name=\"train_clip\")\n",
    "    train_images_part.index = X.index\n",
    "    X = pd.concat([X, train_images_part], axis=1)\n",
    "\n",
    "    # ----------------- TEST IMAGES ------------------\n",
    "    if test_df is not None:\n",
    "        test_file_paths = [\n",
    "            os.path.join(\n",
    "                CONFIG['test_images_dir'],\n",
    "                _normalize_image_name(file_name, CONFIG.get(\"image_ext\", \"\"))\n",
    "            )\n",
    "            for file_name in test_df[file_name_column]\n",
    "        ]\n",
    "        test_images_part = process_images(test_file_paths, model, processor, device, name=\"test_clip\")\n",
    "        test_images_part.index = test_df.index\n",
    "        test_df = pd.concat([test_df, test_images_part], axis=1)\n",
    "\n",
    "    # (опционально) выбрасываем колонку с именем файла, чтобы она не ушла в бустинги\n",
    "    for df in (X, test_df):\n",
    "        if df is not None and file_name_column in df.columns:\n",
    "            df.drop(columns=[file_name_column], inplace=True)\n",
    "\n",
    "    return X, test_df\n",
    "\n",
    "\n",
    "def process_images(file_paths, model, processor, device, name):\n",
    "    \"\"\"\n",
    "    Считает эмбеддинги для списка путей к картинкам одной CLIP-моделью.\n",
    "    Использует CLIPProcessor + CLIPModel.get_image_features.\n",
    "    \"\"\"\n",
    "    batch_size = CONFIG.get(\"batch_size\", 32)\n",
    "\n",
    "    collect_embs = []\n",
    "    if not file_paths:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    for start in tqdm(\n",
    "        range(0, len(file_paths), batch_size),\n",
    "        total=(len(file_paths) + batch_size - 1) // batch_size,\n",
    "        desc=f\"Images {name} process...\"\n",
    "    ):\n",
    "        batch_paths = file_paths[start:start + batch_size]\n",
    "        images = []\n",
    "        for path in batch_paths:\n",
    "            try:\n",
    "                img = Image.open(path).convert(\"RGB\")\n",
    "            except Exception:\n",
    "                # если не удалось открыть картинку — используем заглушку\n",
    "                img = Image.new(\"RGB\", (224, 224), color=0)\n",
    "            images.append(img)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "            embs = model.get_image_features(**inputs)\n",
    "\n",
    "            # L2-нормализация по строкам (num_samples x dim)\n",
    "            embs = embs / (embs.norm(dim=-1, keepdim=True) + 1e-12)\n",
    "\n",
    "        collect_embs.extend(embs.cpu().numpy().tolist())\n",
    "\n",
    "    if not collect_embs:\n",
    "        # На всякий случай, чтобы не упасть на пустом списке\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    n_feat = len(collect_embs[0])\n",
    "    return pd.DataFrame(collect_embs, columns=[f\"emb_{i}\" for i in range(n_feat)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f360d267",
   "metadata": {},
   "source": [
    "ТЕКСТОВЫЕ ФИЧИ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e964e493",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:06:57.741577Z",
     "iopub.status.busy": "2025-11-18T21:06:57.741239Z",
     "iopub.status.idle": "2025-11-18T21:06:57.767047Z",
     "shell.execute_reply": "2025-11-18T21:06:57.766175Z",
     "shell.execute_reply.started": "2025-11-18T21:06:57.741559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_text_device():\n",
    "    global TEXT_DEVICE\n",
    "    if TEXT_DEVICE is None:\n",
    "        TEXT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return TEXT_DEVICE\n",
    "\n",
    "\n",
    "def init_bert_model():\n",
    "    \"\"\"\n",
    "    Инициализируем BERT-модель и токенайзер один раз.\n",
    "    \"\"\"\n",
    "    global TEXT_MODEL, TEXT_TOKENIZER, TEXT_DEVICE\n",
    "\n",
    "    if TEXT_MODEL is not None:\n",
    "        return\n",
    "\n",
    "    model_name = CONFIG.get(\n",
    "        \"text_bert_model_name\",\n",
    "        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    )\n",
    "    TEXT_DEVICE = get_text_device()\n",
    "\n",
    "    TEXT_TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n",
    "    TEXT_MODEL = AutoModel.from_pretrained(model_name)\n",
    "    TEXT_MODEL.to(TEXT_DEVICE)\n",
    "    TEXT_MODEL.eval()\n",
    "    \n",
    "def _detect_format_from_path(path: str) -> str:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".parquet\":\n",
    "        return \"parquet\"\n",
    "    if ext == \".csv\":\n",
    "        return \"csv\"\n",
    "    if ext == \".tsv\":\n",
    "        return \"tsv\"\n",
    "    # по умолчанию считаем csv\n",
    "    return \"csv\"\n",
    "\n",
    "\n",
    "def load_text_table(path: str, fmt: str | None, id_column: str, text_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Загружаем внешнюю таблицу с текстом.\n",
    "    Поддерживаем parquet/csv/tsv. format=\"auto\" -> по расширению.\n",
    "    Возвращаем только столбцы [id_column, text_column].\n",
    "    \"\"\"\n",
    "    if fmt is None or fmt == \"auto\":\n",
    "        fmt = _detect_format_from_path(path)\n",
    "\n",
    "    if fmt == \"parquet\":\n",
    "        df = pd.read_parquet(path)\n",
    "    elif fmt == \"tsv\":\n",
    "        df = pd.read_csv(path, sep=\"\\t\")\n",
    "    else:  # \"csv\" и всё остальное по-умолчанию\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "    # Явно проверяем, что нужные колонки есть\n",
    "    missing = [c for c in (id_column, text_column) if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"Columns {missing} not found in external text table {path}. \"\n",
    "            f\"Available columns: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    return df[[id_column, text_column]].copy()\n",
    "\n",
    "def attach_external_text_tables(X: pd.DataFrame, test_df: pd.DataFrame):\n",
    "    \"\"\"Подцепляем внешние текстовые таблицы (CONFIG['text_external_tables']) к X и test_df.\n",
    "\n",
    "    Важно: если во внешней таблице несколько строк на один и тот же id,\n",
    "    мы сначала агрегируем их в ОДНУ строку (конкатенируем тексты через пробел),\n",
    "    чтобы merge НЕ раздувал количество строк в основной таблице.\n",
    "    \"\"\"\n",
    "    external_cfgs = CONFIG.get(\"text_external_tables\", []) or []\n",
    "    if not external_cfgs:\n",
    "        return X, test_df, []\n",
    "\n",
    "    main_id_col = CONFIG.get(\"id_column\", \"index\")\n",
    "    added_cols: list[str] = []\n",
    "\n",
    "    for cfg in external_cfgs:\n",
    "        name = cfg.get(\"name\", \"ext\")\n",
    "\n",
    "        train_path = cfg[\"train_path\"]\n",
    "        # если test_path не указан — используем тот же файл\n",
    "        test_path = cfg.get(\"test_path\", train_path)\n",
    "\n",
    "        fmt = cfg.get(\"format\", \"auto\")\n",
    "\n",
    "        # ID-колонка в external-таблице: по умолчанию такая же, как в основной\n",
    "        ext_id_col = cfg.get(\"id_column\", main_id_col)\n",
    "\n",
    "        text_col_ext = cfg[\"text_column\"]\n",
    "        output_col = cfg.get(\"output_column\", f\"text_ext_{name}\")\n",
    "\n",
    "        # --- TRAIN ---\n",
    "        ext_train = load_text_table(train_path, fmt, ext_id_col, text_col_ext)\n",
    "        ext_train = ext_train.rename(columns={text_col_ext: output_col})\n",
    "\n",
    "        # NEW: агрегируем несколько текстов по одному id в одну строку\n",
    "        if ext_train[ext_id_col].duplicated().any():\n",
    "            ext_train = (\n",
    "                ext_train\n",
    "                .groupby(ext_id_col, as_index=False)[output_col]\n",
    "                .agg(lambda s: \" \".join(map(str, s)))\n",
    "            )\n",
    "\n",
    "        # Проверяем, есть ли main_id_col в колонках X\n",
    "        # Если нет, используем индекс для merge\n",
    "        if main_id_col in X.columns:\n",
    "            # ID колонка есть в X\n",
    "            if main_id_col == ext_id_col:\n",
    "                # Имя ID одинаковое в основной и внешней таблице\n",
    "                X = X.merge(\n",
    "                    ext_train[[ext_id_col, output_col]],\n",
    "                    on=ext_id_col,\n",
    "                    how=\"left\"\n",
    "                )\n",
    "            else:\n",
    "                # Имя ID в основной и внешней таблицах разное\n",
    "                X = X.merge(\n",
    "                    ext_train[[ext_id_col, output_col]],\n",
    "                    left_on=main_id_col,\n",
    "                    right_on=ext_id_col,\n",
    "                    how=\"left\"\n",
    "                ).drop(columns=[ext_id_col])\n",
    "        else:\n",
    "            # ID колонки нет в X, используем индекс\n",
    "            # Устанавливаем ext_id_col как индекс во внешней таблице\n",
    "            ext_train_indexed = ext_train.set_index(ext_id_col)[[output_col]]\n",
    "            X = X.merge(\n",
    "                ext_train_indexed,\n",
    "                left_index=True,\n",
    "                right_index=True,\n",
    "                how=\"left\"\n",
    "            )\n",
    "\n",
    "        # --- TEST ---\n",
    "        if test_df is not None:\n",
    "            ext_test = load_text_table(test_path, fmt, ext_id_col, text_col_ext)\n",
    "            ext_test = ext_test.rename(columns={text_col_ext: output_col})\n",
    "\n",
    "            # NEW: агрегируем несколько текстов по одному id в одну строку\n",
    "            if ext_test[ext_id_col].duplicated().any():\n",
    "                ext_test = (\n",
    "                    ext_test\n",
    "                    .groupby(ext_id_col, as_index=False)[output_col]\n",
    "                    .agg(lambda s: \" \".join(map(str, s)))\n",
    "                )\n",
    "\n",
    "            # Проверяем, есть ли main_id_col в колонках test_df\n",
    "            if main_id_col in test_df.columns:\n",
    "                if main_id_col == ext_id_col:\n",
    "                    test_df = test_df.merge(\n",
    "                        ext_test[[ext_id_col, output_col]],\n",
    "                        on=ext_id_col,\n",
    "                        how=\"left\"\n",
    "                    )\n",
    "                else:\n",
    "                    test_df = test_df.merge(\n",
    "                        ext_test[[ext_id_col, output_col]],\n",
    "                        left_on=main_id_col,\n",
    "                        right_on=ext_id_col,\n",
    "                        how=\"left\"\n",
    "                    ).drop(columns=[ext_id_col])\n",
    "            else:\n",
    "                # ID колонки нет в test_df, используем индекс\n",
    "                # Устанавливаем ext_id_col как индекс во внешней таблице\n",
    "                ext_test_indexed = ext_test.set_index(ext_id_col)[[output_col]]\n",
    "                test_df = test_df.merge(\n",
    "                    ext_test_indexed,\n",
    "                    left_index=True,\n",
    "                    right_index=True,\n",
    "                    how=\"left\"\n",
    "                )\n",
    "\n",
    "        added_cols.append(output_col)\n",
    "\n",
    "    return X, test_df, added_cols\n",
    "\n",
    "\n",
    "def build_bert_embeddings_for_field(field_name: str,\n",
    "                                    train_texts: list[str],\n",
    "                                    test_texts: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Строим эмбеддинги BERT для одного логического текстового поля.\n",
    "    BERT-модель общая для всех полей, но считаем отдельно для каждого.\n",
    "    \"\"\"\n",
    "    init_bert_model()\n",
    "\n",
    "    device = get_text_device()\n",
    "    max_len = CONFIG.get(\"text_max_length\", 256)\n",
    "    batch_size = CONFIG.get(\"text_batch_size\", 32)\n",
    "\n",
    "    def encode_list(texts: list[str], desc: str) -> np.ndarray:\n",
    "        all_embs = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=f\"{desc} [{field_name}]\"):\n",
    "            batch = texts[i:i + batch_size]\n",
    "\n",
    "            enc = TEXT_TOKENIZER(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = TEXT_MODEL(**enc)\n",
    "                last_hidden = outputs.last_hidden_state  # [bs, seq_len, hidden]\n",
    "\n",
    "                mask = enc[\"attention_mask\"].unsqueeze(-1).expand(last_hidden.size())\n",
    "                masked = last_hidden * mask\n",
    "\n",
    "                summed = masked.sum(dim=1)\n",
    "                counts = mask.sum(dim=1).clamp(min=1)\n",
    "                mean_pooled = summed / counts  # [bs, hidden]\n",
    "\n",
    "                embs = mean_pooled.cpu().numpy()\n",
    "\n",
    "                # L2 нормализация\n",
    "                norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "                norms = np.clip(norms, 1e-12, None)\n",
    "                embs = embs / norms\n",
    "\n",
    "                all_embs.append(embs)\n",
    "\n",
    "        return np.vstack(all_embs) if len(all_embs) > 0 else np.zeros((0, TEXT_MODEL.config.hidden_size))\n",
    "\n",
    "    train_embs = encode_list(train_texts, \"BERT text embeddings (train)\")\n",
    "    test_embs = encode_list(test_texts, \"BERT text embeddings (test)\")\n",
    "\n",
    "    return train_embs, test_embs\n",
    "def build_tfidf_svd_embeddings_for_field(field_name: str,\n",
    "                                         train_texts: list[str],\n",
    "                                         test_texts: list[str],\n",
    "                                         logger: logging.Logger) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TF-IDF (uni+bi-grams) -> TruncatedSVD до компактного вектора -> L2-нормализация.\n",
    "    Для каждого текстового поля свой TF-IDF и SVD, чтобы не мешать разные источники.\n",
    "    \"\"\"\n",
    "    max_features = CONFIG.get(\"text_tfidf_max_features\", 50000)\n",
    "    n_components = CONFIG.get(\"text_svd_n_components\", 256)\n",
    "    random_state = CONFIG.get(\"seed\", 42)\n",
    "\n",
    "    vect = TFIDF_VECTORIZERS.get(field_name)\n",
    "    svd = TFIDF_SVDS.get(field_name)\n",
    "\n",
    "    if vect is None or svd is None:\n",
    "        vect = TfidfVectorizer(\n",
    "            max_features=max_features,\n",
    "            ngram_range=(1, 2)\n",
    "        )\n",
    "        logger.info(f\"TF-IDF fitting for field '{field_name}'...\")\n",
    "        train_tfidf = vect.fit_transform(train_texts)\n",
    "\n",
    "        svd = TruncatedSVD(\n",
    "            n_components=n_components,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        logger.info(f\"SVD fitting for field '{field_name}'...\")\n",
    "        train_embs = svd.fit_transform(train_tfidf)\n",
    "\n",
    "        TFIDF_VECTORIZERS[field_name] = vect\n",
    "        TFIDF_SVDS[field_name] = svd\n",
    "    else:\n",
    "        train_tfidf = vect.transform(train_texts)\n",
    "        train_embs = svd.transform(train_tfidf)\n",
    "\n",
    "    test_tfidf = vect.transform(test_texts)\n",
    "    test_embs = svd.transform(test_tfidf)\n",
    "\n",
    "    # L2-нормализация\n",
    "    def l2_norm(x: np.ndarray) -> np.ndarray:\n",
    "        norms = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "        norms = np.where(norms == 0, 1, norms)\n",
    "        return x / norms\n",
    "\n",
    "    train_embs = l2_norm(train_embs)\n",
    "    test_embs = l2_norm(test_embs)\n",
    "\n",
    "    return train_embs, test_embs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8e1e2baa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:07:01.445860Z",
     "iopub.status.busy": "2025-11-18T21:07:01.445595Z",
     "iopub.status.idle": "2025-11-18T21:07:01.456764Z",
     "shell.execute_reply": "2025-11-18T21:07:01.455959Z",
     "shell.execute_reply.started": "2025-11-18T21:07:01.445841Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_features(X: pd.DataFrame, test_df: pd.DataFrame, logger: logging.Logger):\n",
    "    \"\"\"\n",
    "    1) Приклеиваем внешние текстовые таблицы (parquet/csv/tsv) по id.\n",
    "    2) Для каждой текстовой колонки строим отдельный эмбеддинг (BERT или TF-IDF+SVD).\n",
    "    3) Конкатим всё к X и test_df.\n",
    "    \"\"\"\n",
    "    if not CONFIG.get(\"text_enable\", False):\n",
    "        return X, test_df\n",
    "\n",
    "    logger.info(\"Text feature extraction starts...\")\n",
    "\n",
    "    # 1. Подцепляем внешние текстовые таблицы\n",
    "    X, test_df, external_cols = attach_external_text_tables(X, test_df)\n",
    "\n",
    "    # 2. Собираем финальный список текстовых колонок\n",
    "    base_text_cols = CONFIG.get(\"text_columns\", []) or []\n",
    "    # фильтруем только те, которые реально есть в таблице\n",
    "    base_text_cols = [c for c in base_text_cols if c in X.columns]\n",
    "\n",
    "    all_text_cols = base_text_cols + external_cols\n",
    "\n",
    "    if not all_text_cols:\n",
    "        logger.warning(\"No text columns found for text_features. Skipping.\")\n",
    "        return X, test_df\n",
    "\n",
    "    model_type = CONFIG.get(\"text_model_type\", \"bert\")\n",
    "\n",
    "    # 3. Для каждой текстовой колонки строим свои эмбеддинги и конкатим\n",
    "    for col in tqdm(all_text_cols, desc=\"Building text embeddings\"):\n",
    "        logger.info(f\"Building text embeddings for column: {col}\")\n",
    "\n",
    "        # собираем тексты (строка -> str) - ВАЖНО: сохраняем порядок через индексы\n",
    "        train_texts = X[col].fillna(\"\").astype(str).tolist()\n",
    "        test_texts = test_df[col].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "        # Проверка размеров перед созданием эмбеддингов\n",
    "        if len(train_texts) != len(X):\n",
    "            raise ValueError(f\"Train texts length {len(train_texts)} != X length {len(X)} for column {col}\")\n",
    "        if len(test_texts) != len(test_df):\n",
    "            raise ValueError(f\"Test texts length {len(test_texts)} != test_df length {len(test_df)} for column {col}\")\n",
    "\n",
    "        # проверка, а не все ли пустые\n",
    "        if all(t == \"\" for t in train_texts) and all(t == \"\" for t in test_texts):\n",
    "            logger.warning(f\"Column '{col}' has only empty texts. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        field_name = col  # можно потом маппить/переименовать если хочешь\n",
    "\n",
    "        if model_type == \"bert\":\n",
    "            train_embs, test_embs = build_bert_embeddings_for_field(field_name, train_texts, test_texts)\n",
    "        elif model_type == \"tfidf_svd\":\n",
    "            train_embs, test_embs = build_tfidf_svd_embeddings_for_field(field_name, train_texts, test_texts, logger)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown text_model_type: {model_type}\")\n",
    "\n",
    "        # Проверка размеров эмбеддингов\n",
    "        if train_embs.shape[0] != len(X):\n",
    "            raise ValueError(f\"Train embeddings shape {train_embs.shape[0]} != X length {len(X)} for column {col}\")\n",
    "        if test_embs.shape[0] != len(test_df):\n",
    "            raise ValueError(f\"Test embeddings shape {test_embs.shape[0]} != test_df length {len(test_df)} for column {col}\")\n",
    "\n",
    "        dim = train_embs.shape[1]\n",
    "        # префикс с названием колонки, чтобы понимать источник\n",
    "        prefix = f\"text_{field_name}_emb\"\n",
    "        cols = [f\"{prefix}_{i}\" for i in range(dim)]\n",
    "\n",
    "        # Создаем DataFrame с правильными индексами\n",
    "        train_text_df = pd.DataFrame(train_embs, columns=cols, index=X.index)\n",
    "        test_text_df = pd.DataFrame(test_embs, columns=cols, index=test_df.index)\n",
    "\n",
    "        X = pd.concat([X, train_text_df], axis=1)\n",
    "        test_df = pd.concat([test_df, test_text_df], axis=1)\n",
    "\n",
    "        logger.info(f\"Text features for '{col}' added: {dim} dims ({prefix}_*)\")\n",
    "\n",
    "    return X, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "517c555f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:07:01.787269Z",
     "iopub.status.busy": "2025-11-18T21:07:01.786821Z",
     "iopub.status.idle": "2025-11-18T21:07:01.807688Z",
     "shell.execute_reply": "2025-11-18T21:07:01.806950Z",
     "shell.execute_reply.started": "2025-11-18T21:07:01.787247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prepare_all_features(X, test_df, config, logger):\n",
    "    # КРИТИЧЕСКАЯ ПРОВЕРКА: сохраняем исходные размеры\n",
    "    original_X_size = len(X)\n",
    "    original_test_size = len(test_df) if test_df is not None else 0\n",
    "    logger.info(f\"🔍 ORIGINAL SIZES: X={original_X_size}, test_df={original_test_size}\")\n",
    "    \n",
    "    # ----- Features -----  \n",
    "    logger.info(\"🔧 Generating basic features...\")\n",
    "    logger.info(f\"X shape before basic: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape before basic: {test_df.shape}\")\n",
    "    \n",
    "    X = generate_basic_features(X, config, logger)\n",
    "    if test_df is not None:\n",
    "        test_df = generate_basic_features(test_df, config, logger)\n",
    "    \n",
    "    logger.info(f\"X shape after basic: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape after basic: {test_df.shape}\")\n",
    "        if len(test_df) != original_test_size:\n",
    "            raise ValueError(f\"❌ test_df size changed in generate_basic_features: {original_test_size} -> {len(test_df)}\")\n",
    "\n",
    "    logger.info(\"📊 Generating aggregate features...\")\n",
    "    logger.info(f\"X shape before aggregate: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape before aggregate: {test_df.shape}\")\n",
    "    \n",
    "    X, test_df = generate_aggregate_features(X, test_df, config, logger)\n",
    "    \n",
    "    logger.info(f\"X shape after aggregate: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape after aggregate: {test_df.shape}\")\n",
    "        if len(test_df) != original_test_size:\n",
    "            raise ValueError(f\"❌ test_df size changed in generate_aggregate_features: {original_test_size} -> {len(test_df)}\")\n",
    "\n",
    "    logger.info(\"🗺  Generating geo features...\")\n",
    "    logger.info(f\"X shape before geo: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape before geo: {test_df.shape}\")\n",
    "    \n",
    "    X, test_df = generate_geo_features(X, test_df, config, logger)\n",
    "    \n",
    "    logger.info(f\"X shape after geo: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape after geo: {test_df.shape}\")\n",
    "        if len(test_df) != original_test_size:\n",
    "            raise ValueError(f\"❌ test_df size changed in generate_geo_features: {original_test_size} -> {len(test_df)}\")\n",
    "    \n",
    "    if config['train_images_dir']:\n",
    "        logger.info(\"Images process starts...\")\n",
    "        X, test_df = images_features(X, test_df)\n",
    "    \n",
    "    if CONFIG.get(\"text_enable\", False):\n",
    "        logger.info(\"Text process starts...\")\n",
    "        logger.info(f\"X shape before text: {X.shape}\")\n",
    "        if test_df is not None:\n",
    "            logger.info(f\"test_df shape before text: {test_df.shape}\")\n",
    "        \n",
    "        X, test_df = text_features(X, test_df, logger)\n",
    "        \n",
    "        logger.info(f\"X shape after text: {X.shape}\")\n",
    "        if test_df is not None:\n",
    "            logger.info(f\"test_df shape after text: {test_df.shape}\")\n",
    "            if len(test_df) != original_test_size:\n",
    "                raise ValueError(f\"❌ test_df size changed in text_features: {original_test_size} -> {len(test_df)}\")\n",
    "\n",
    "    # ----- Address → city extraction -----\n",
    "    addr_col = config.get(\"address_column\")\n",
    "    if addr_col:\n",
    "        logger.info(f\"🏙  Extracting city from address column '{addr_col}' ...\")\n",
    "        logger.info(f\"X shape before address: {X.shape}\")\n",
    "        if test_df is not None:\n",
    "            logger.info(f\"test_df shape before address: {test_df.shape}\")\n",
    "        \n",
    "        city_index = config.get(\"address_city_index\", -1)\n",
    "        sep = config.get(\"address_split_sep\", \",\")\n",
    "        X = process_address_extract_city(X, addr_col, city_index, sep, logger)\n",
    "        if test_df is not None:\n",
    "            test_df = process_address_extract_city(test_df, addr_col, city_index, sep, logger)\n",
    "        \n",
    "        logger.info(f\"X shape after address: {X.shape}\")\n",
    "        if test_df is not None:\n",
    "            logger.info(f\"test_df shape after address: {test_df.shape}\")\n",
    "            if len(test_df) != original_test_size:\n",
    "                raise ValueError(f\"❌ test_df size changed in process_address_extract_city: {original_test_size} -> {len(test_df)}\")\n",
    "\n",
    "    # ----- Categorical detection -----\n",
    "    logger.info(\"🔎 Detecting categorical columns...\")\n",
    "    logger.info(f\"X shape before categorical detection: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape before categorical detection: {test_df.shape}\")\n",
    "    \n",
    "    concat_df = pd.concat([X] if test_df is None else [X, test_df], ignore_index=False)\n",
    "    cat_cols = detect_categorical_columns(\n",
    "        concat_df,\n",
    "        max_unique=config[\"basic_max_cat_unique\"],\n",
    "        force_categorical=config[\"basic_as_categorical\"]\n",
    "    )\n",
    "    cat_cols = [c for c in cat_cols if c in X.columns]\n",
    "    logger.info(f\"Categorical columns: {cat_cols}\")\n",
    "\n",
    "    # ----- Categorical post-processing: rare → 'other' -----\n",
    "    logger.info(\"🧩 Processing categorical features (merge rare into 'other')...\")\n",
    "    logger.info(f\"X shape before categorical processing: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape before categorical processing: {test_df.shape}\")\n",
    "    \n",
    "    X, test_df, cat_cols = process_categorical_features(X, test_df, cat_cols, config, logger)\n",
    "    logger.info(f\"Categorical columns after processing: {cat_cols}\")\n",
    "    \n",
    "    logger.info(f\"X shape after categorical processing: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"test_df shape after categorical processing: {test_df.shape}\")\n",
    "        if len(test_df) != original_test_size:\n",
    "            raise ValueError(f\"❌ test_df size changed in process_categorical_features: {original_test_size} -> {len(test_df)}\")\n",
    "\n",
    "    # ----- Post-feature service columns drop -----\n",
    "    post_drop = config.get(\"post_feature_drop_columns\", [])\n",
    "    if post_drop:\n",
    "        logger.info(f\"🧹 Dropping post-feature service columns: {post_drop}\")\n",
    "        X = X.drop(columns=[c for c in post_drop if c in X.columns])\n",
    "        if test_df is not None:\n",
    "            test_df = test_df.drop(columns=[c for c in post_drop if c in test_df.columns])\n",
    "        # обновляем список категориальных\n",
    "        cat_cols = [c for c in cat_cols if c not in post_drop]\n",
    "\n",
    "    # Обработка NaN: сначала категориальные, потом числовые\n",
    "    # Для категориальных используем специальное значение\n",
    "    for col_name in cat_cols:\n",
    "        if col_name in X.columns:\n",
    "            X[col_name] = X[col_name].fillna(\"__MISSING__\").astype(str)\n",
    "        if test_df is not None and col_name in test_df.columns:\n",
    "            test_df[col_name] = test_df[col_name].fillna(\"__MISSING__\").astype(str)\n",
    "    \n",
    "    # Для числовых колонок заполняем NaN\n",
    "    numeric_cols = [c for c in X.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(X[c])]\n",
    "    if numeric_cols:\n",
    "        X[numeric_cols] = X[numeric_cols].fillna(-99999999)\n",
    "        if test_df is not None:\n",
    "            # Проверяем, что колонки есть в test_df\n",
    "            numeric_cols_test = [c for c in numeric_cols if c in test_df.columns]\n",
    "            if numeric_cols_test:\n",
    "                test_df[numeric_cols_test] = test_df[numeric_cols_test].fillna(-99999999)\n",
    "    \n",
    "    # Финальная проверка размеров\n",
    "    logger.info(f\"Final X shape: {X.shape}\")\n",
    "    if test_df is not None:\n",
    "        logger.info(f\"Final test_df shape: {test_df.shape}\")\n",
    "        \n",
    "        # КРИТИЧЕСКАЯ ПРОВЕРКА: размер test_df не должен был измениться\n",
    "        if len(test_df) != original_test_size:\n",
    "            raise ValueError(\n",
    "                f\"❌ CRITICAL: test_df size changed from {original_test_size} to {len(test_df)} during prepare_all_features! \"\n",
    "                f\"This should not happen. Please check the code above.\"\n",
    "            )\n",
    "        \n",
    "        # Проверяем, что колонки совпадают (кроме возможных различий в категориальных)\n",
    "        X_cols = set(X.columns)\n",
    "        test_cols = set(test_df.columns)\n",
    "        missing_in_test = X_cols - test_cols\n",
    "        extra_in_test = test_cols - X_cols\n",
    "        if missing_in_test:\n",
    "            logger.warning(f\"Columns in X but not in test_df: {list(missing_in_test)[:10]}\")\n",
    "        if extra_in_test:\n",
    "            logger.warning(f\"Columns in test_df but not in X: {list(extra_in_test)[:10]}\")\n",
    "    \n",
    "    return X, test_df, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defcb0fb",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ed9b13f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:19:39.063882Z",
     "iopub.status.busy": "2025-11-18T21:19:39.063576Z",
     "iopub.status.idle": "2025-11-18T21:19:39.236934Z",
     "shell.execute_reply": "2025-11-18T21:19:39.235947Z",
     "shell.execute_reply.started": "2025-11-18T21:19:39.063861Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:19:39,064 - INFO -  ORIGINAL SIZES: X=9912, test_df=8\n",
      "INFO:TABULAR_BOOSTING:🔍 ORIGINAL SIZES: X=9912, test_df=8\n",
      "2025-11-18 21:19:39,067 - INFO -  Generating basic features...\n",
      "INFO:TABULAR_BOOSTING:🔧 Generating basic features...\n",
      "2025-11-18 21:19:39,069 - INFO - X shape before basic: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape before basic: (9912, 780)\n",
      "2025-11-18 21:19:39,070 - INFO - test_df shape before basic: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape before basic: (8, 780)\n",
      "2025-11-18 21:19:39,159 - INFO - X shape after basic: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape after basic: (9912, 780)\n",
      "2025-11-18 21:19:39,160 - INFO - test_df shape after basic: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape after basic: (8, 780)\n",
      "2025-11-18 21:19:39,162 - INFO -  Generating aggregate features...\n",
      "INFO:TABULAR_BOOSTING:📊 Generating aggregate features...\n",
      "2025-11-18 21:19:39,163 - INFO - X shape before aggregate: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape before aggregate: (9912, 780)\n",
      "2025-11-18 21:19:39,164 - INFO - test_df shape before aggregate: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape before aggregate: (8, 780)\n",
      "2025-11-18 21:19:39,165 - INFO - Aggregate features disabled in CONFIG.\n",
      "INFO:TABULAR_BOOSTING:Aggregate features disabled in CONFIG.\n",
      "2025-11-18 21:19:39,167 - INFO - X shape after aggregate: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape after aggregate: (9912, 780)\n",
      "2025-11-18 21:19:39,168 - INFO - test_df shape after aggregate: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape after aggregate: (8, 780)\n",
      "2025-11-18 21:19:39,169 - INFO -   Generating geo features...\n",
      "INFO:TABULAR_BOOSTING:🗺  Generating geo features...\n",
      "2025-11-18 21:19:39,171 - INFO - X shape before geo: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape before geo: (9912, 780)\n",
      "2025-11-18 21:19:39,172 - INFO - test_df shape before geo: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape before geo: (8, 780)\n",
      "2025-11-18 21:19:39,173 - INFO - Geo features disabled in CONFIG.\n",
      "INFO:TABULAR_BOOSTING:Geo features disabled in CONFIG.\n",
      "2025-11-18 21:19:39,175 - INFO - X shape after geo: (9912, 780)\n",
      "INFO:TABULAR_BOOSTING:X shape after geo: (9912, 780)\n",
      "2025-11-18 21:19:39,176 - INFO - test_df shape after geo: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape after geo: (8, 780)\n",
      "2025-11-18 21:19:39,177 - INFO - Images process starts...\n",
      "INFO:TABULAR_BOOSTING:Images process starts...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"CONFIG['file_names_column']='Id' нет в train\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/1274007552.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_48/1398052482.py\u001b[0m in \u001b[0;36mprepare_all_features\u001b[0;34m(X, test_df, config, logger)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_images_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Images process starts...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_enable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_48/725627518.py\u001b[0m in \u001b[0;36mimages_features\u001b[0;34m(X, test_df)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile_name_column\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"CONFIG['file_names_column']={file_name_column!r} нет в train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfile_name_column\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"CONFIG['file_names_column']='Id' нет в train\""
     ]
    }
   ],
   "source": [
    "X, test_df, cat_cols = prepare_all_features(X, test_df, CONFIG, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60425ace",
   "metadata": {},
   "source": [
    "### Features filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "78cfa405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:21:09.180542Z",
     "iopub.status.busy": "2025-11-18T21:21:09.179964Z",
     "iopub.status.idle": "2025-11-18T21:21:09.190784Z",
     "shell.execute_reply": "2025-11-18T21:21:09.189920Z",
     "shell.execute_reply.started": "2025-11-18T21:21:09.180509Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def filter_features_by_correlation(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    config: Dict,\n",
    "    logger: logging.Logger\n",
    ") -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    1) Pearson по числовым признакам\n",
    "    2) phik (если включено и установлен phik)\n",
    "    Возвращает X с отобранными признаками и список их имён.\n",
    "    \"\"\"\n",
    "    selected = list(X.columns)\n",
    "    task_type = config[\"task_type\"]\n",
    "\n",
    "    # --- Pearson ---\n",
    "    pearson_thr = float(config.get(\"corr_pearson_min_abs\", 0.0) or 0.0)\n",
    "    if pearson_thr > 0.0:\n",
    "        logger.info(f\"Applying Pearson filter with |corr| >= {pearson_thr}\")\n",
    "        corr_vals = {}\n",
    "        y_series = pd.Series(y)\n",
    "\n",
    "        for col in tqdm(selected, desc=\"Computing Pearson correlations\"):\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                try:\n",
    "                    c = X[col].corr(y_series)\n",
    "                except Exception:\n",
    "                    c = np.nan\n",
    "            else:\n",
    "                c = np.nan\n",
    "            corr_vals[col] = c\n",
    "\n",
    "        corr_series = pd.Series(corr_vals)\n",
    "        keep_mask = corr_series.abs() < pearson_thr\n",
    "        kept = corr_series.index[keep_mask].tolist()\n",
    "        logger.info(\n",
    "            f\"Pearson filter: kept {len(kept)} / {len(corr_series)} features \"\n",
    "            f\"(min abs corr {corr_series.abs().min():.6f}, max {corr_series.abs().max():.6f})\"\n",
    "        )\n",
    "        selected = kept\n",
    "\n",
    "    # --- phik ---\n",
    "    use_phik = bool(config.get(\"corr_use_phik\", False))\n",
    "    phik_thr = float(config.get(\"corr_phik_min_abs\", 0.0) or 0.0)\n",
    "\n",
    "    if use_phik and phik_thr > 0.0:\n",
    "        if not HAS_PHIK:\n",
    "            logger.warning(\"corr_use_phik=True, но библиотека phik не установлена — пропускаем phik-фильтр.\")\n",
    "        else:\n",
    "            logger.info(f\"Applying phik filter with |phik| >= {phik_thr}\")\n",
    "            df_phik = X[selected].copy()\n",
    "            tmp_target_col = \"__target_for_phik__\"\n",
    "            df_phik[tmp_target_col] = y.values\n",
    "\n",
    "            # числовые для interval_cols\n",
    "            interval_cols = [\n",
    "                c for c in df_phik.columns\n",
    "                if pd.api.types.is_numeric_dtype(df_phik[c])\n",
    "            ]\n",
    "            try:\n",
    "                phik_matrix = df_phik.phik_matrix(interval_cols=interval_cols)\n",
    "                target_corr = phik_matrix[tmp_target_col].drop(\n",
    "                    labels=[tmp_target_col], errors=\"ignore\"\n",
    "                )\n",
    "                keep_mask = target_corr.abs() >= phik_thr\n",
    "                kept_phik = target_corr.index[keep_mask].tolist()\n",
    "                logger.info(\n",
    "                    f\"phik filter: kept {len(kept_phik)} / {len(target_corr)} features \"\n",
    "                    f\"(min abs phik {target_corr.abs().min():.6f}, \"\n",
    "                    f\"max {target_corr.abs().max():.6f})\"\n",
    "                )\n",
    "                # пересечение с уже отфильтрованными по Pearson\n",
    "                selected = [c for c in selected if c in kept_phik]\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"phik computation failed, skip phik filter. Error: {e}\")\n",
    "\n",
    "    logger.info(f\"Total features after correlation filtering: {len(selected)}\")\n",
    "    return X[selected], selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b75dce25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:21:11.096205Z",
     "iopub.status.busy": "2025-11-18T21:21:11.095597Z",
     "iopub.status.idle": "2025-11-18T21:21:11.102404Z",
     "shell.execute_reply": "2025-11-18T21:21:11.101615Z",
     "shell.execute_reply.started": "2025-11-18T21:21:11.096181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# NEW: фильтрация по корреляции\n",
    "if CONFIG.get(\"corr_enable\", False):\n",
    "    logger.info(\"📉 Applying correlation-based feature filtering...\")\n",
    "    # КРИТИЧЕСКАЯ ПРОВЕРКА: сохраняем размер перед фильтрацией\n",
    "    test_size_before_corr = len(test_df) if test_df is not None else 0\n",
    "    logger.info(f\"🔍 test_df size BEFORE correlation filter: {test_size_before_corr}\")\n",
    "    \n",
    "    X, selected_cols = filter_features_by_correlation(X, y, CONFIG, logger)\n",
    "\n",
    "    if test_df is not None:\n",
    "        missing_in_test = [c for c in selected_cols if c not in test_df.columns]\n",
    "        if missing_in_test:\n",
    "            logger.warning(\n",
    "                f\"{len(missing_in_test)} features selected by correlation \"\n",
    "                f\"missing in test: {missing_in_test[:10]}...\"\n",
    "            )\n",
    "        # оставляем только те, что есть в тесте\n",
    "        keep_for_test = [c for c in selected_cols if c in test_df.columns]\n",
    "        test_df = test_df[keep_for_test]\n",
    "        \n",
    "        # КРИТИЧЕСКАЯ ПРОВЕРКА: размер не должен измениться при фильтрации колонок\n",
    "        logger.info(f\"🔍 test_df size AFTER correlation filter: {len(test_df)}\")\n",
    "        if len(test_df) != test_size_before_corr:\n",
    "            raise ValueError(\n",
    "                f\"❌ CRITICAL: test_df size changed from {test_size_before_corr} to {len(test_df)} \"\n",
    "                f\"during correlation filtering! This should not happen when filtering columns.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e440026",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:21:12.943361Z",
     "iopub.status.busy": "2025-11-18T21:21:12.943044Z",
     "iopub.status.idle": "2025-11-18T21:21:13.053354Z",
     "shell.execute_reply": "2025-11-18T21:21:13.052592Z",
     "shell.execute_reply.started": "2025-11-18T21:21:12.943340Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:21:12,945 - INFO -  Sanitizing feature names...\n",
      "INFO:TABULAR_BOOSTING:🧾 Sanitizing feature names...\n",
      "2025-11-18 21:21:12,948 - INFO -  test_df size BEFORE sanitize: 8\n",
      "INFO:TABULAR_BOOSTING:🔍 test_df size BEFORE sanitize: 8\n",
      "2025-11-18 21:21:13,048 - INFO -  test_df size AFTER sanitize: 8\n",
      "INFO:TABULAR_BOOSTING:🔍 test_df size AFTER sanitize: 8\n"
     ]
    }
   ],
   "source": [
    " # NEW: sanitize feature names для всех моделей (особенно XGBoost)\n",
    "logger.info(\"🧾 Sanitizing feature names...\")\n",
    "# КРИТИЧЕСКАЯ ПРОВЕРКА: сохраняем размер перед sanitize\n",
    "test_size_before_sanitize = len(test_df) if test_df is not None else 0\n",
    "logger.info(f\"🔍 test_df size BEFORE sanitize: {test_size_before_sanitize}\")\n",
    "\n",
    "X, feature_name_mapping = sanitize_feature_names(X, logger)\n",
    "\n",
    "if test_df is not None:\n",
    "    # сначала переименуем колонки теста по тому же маппингу\n",
    "    test_df = test_df.rename(columns=feature_name_mapping)\n",
    "    # если в тесте остались какие-то дополнительные колонки (в реальности редко),\n",
    "    # можно ещё раз прогнать sanitize, но имена уже будут безопасные\n",
    "    # test, _ = sanitize_feature_names(test, logger)\n",
    "    \n",
    "    # КРИТИЧЕСКАЯ ПРОВЕРКА: размер не должен измениться при переименовании\n",
    "    logger.info(f\"🔍 test_df size AFTER sanitize: {len(test_df)}\")\n",
    "    if len(test_df) != test_size_before_sanitize:\n",
    "        raise ValueError(\n",
    "            f\"❌ CRITICAL: test_df size changed from {test_size_before_sanitize} to {len(test_df)} \"\n",
    "            f\"during sanitize! This should not happen when renaming columns.\"\n",
    "        )\n",
    "\n",
    "# обновляем список категориальных колонок под новые имена\n",
    "cat_cols = [feature_name_mapping.get(c, c) for c in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104dfaa",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "476f2b4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:21:18.938770Z",
     "iopub.status.busy": "2025-11-18T21:21:18.938133Z",
     "iopub.status.idle": "2025-11-18T21:21:18.988970Z",
     "shell.execute_reply": "2025-11-18T21:21:18.988103Z",
     "shell.execute_reply.started": "2025-11-18T21:21:18.938745Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_folds(\n",
    "    y: pd.Series,\n",
    "    config: Dict\n",
    "):\n",
    "    n_splits = config[\"cv_n_splits\"]\n",
    "    seed = config[\"cv_random_state\"]\n",
    "    shuffle = config[\"cv_shuffle\"]\n",
    "    stratified = config[\"cv_stratified\"]\n",
    "    task_type = config[\"task_type\"]\n",
    "\n",
    "    if stratified and task_type in [\"binary\", \"multiclass\"]:\n",
    "        splitter = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=shuffle,\n",
    "            random_state=seed\n",
    "        )\n",
    "    else:\n",
    "        splitter = KFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=shuffle,\n",
    "            random_state=seed\n",
    "        )\n",
    "    return list(splitter.split(np.zeros(len(y)), y))\n",
    "\n",
    "\n",
    "def build_catboost_model(\n",
    "    config: Dict\n",
    "):\n",
    "    params = config[\"catboost_params\"].copy()\n",
    "    \n",
    "    # Добавляем device в параметры (CatBoost использует \"GPU\" или \"CPU\")\n",
    "    device_str = config.get(\"device\", \"CPU\")\n",
    "    if device_str.upper() == \"CUDA\":\n",
    "        params[\"task_type\"] = \"GPU\"\n",
    "    elif device_str.upper() == \"GPU\":\n",
    "        params[\"task_type\"] = \"GPU\"\n",
    "    else:\n",
    "        params[\"task_type\"] = \"CPU\"\n",
    "    \n",
    "    if config[\"task_type\"] == \"regression\":\n",
    "        params[\"loss_function\"] = \"RMSE\"\n",
    "        params[\"eval_metric\"] = \"RMSE\"\n",
    "        model = CatBoostRegressor(**params)\n",
    "    else:\n",
    "        if config[\"task_type\"] == \"multiclass\":\n",
    "            params[\"loss_function\"] = \"MultiClass\"\n",
    "            params[\"eval_metric\"] = \"MultiClass\"\n",
    "        model = CatBoostClassifier(**params)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_lgb_params(\n",
    "    config: Dict,\n",
    "    num_classes: Optional[int] = None\n",
    "):\n",
    "    params = config[\"lgb_params\"].copy()\n",
    "    task_type = config[\"task_type\"]\n",
    "\n",
    "    # ---------- Device (CPU / GPU) ----------\n",
    "    device_str = config.get(\"device\", \"CPU\").upper()\n",
    "    if device_str in [\"CUDA\", \"GPU\"]:\n",
    "        # для LightGBM правильно \"device_type\"\n",
    "        params[\"device_type\"] = \"gpu\"\n",
    "    else:\n",
    "        params[\"device_type\"] = \"cpu\"\n",
    "\n",
    "    # ---------- Objective + metric под тип задачи ----------\n",
    "    if task_type == \"regression\":\n",
    "        # обычная регрессия\n",
    "        params[\"objective\"] = \"regression\"\n",
    "        # метрики LightGBM задаются ключом \"metric\"\n",
    "        params[\"metric\"] = [\"mae\"]   # или [\"rmse\"]\n",
    "    elif task_type == \"binary\":\n",
    "        params[\"objective\"] = \"binary\"\n",
    "        params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n",
    "    else:  # multiclass\n",
    "        if num_classes is None or num_classes <= 1:\n",
    "            raise ValueError(\"num_classes must be > 1 for multiclass\")\n",
    "        params[\"objective\"] = \"multiclass\"\n",
    "        params[\"num_class\"] = num_classes\n",
    "        params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "def build_xgb_params(\n",
    "    config: Dict,\n",
    "    num_classes: Optional[int] = None,\n",
    "):\n",
    "    params = config[\"xgb_params\"].copy()\n",
    "    task_type = config[\"task_type\"]\n",
    "\n",
    "    # ----- чистим возможный мусор -----\n",
    "    # если в конфиге было что-то вроде \"device\": 0 — убираем\n",
    "    if \"device\" in params and not isinstance(params[\"device\"], str):\n",
    "        params.pop(\"device\")\n",
    "\n",
    "    # ----- CPU / GPU -----\n",
    "    device_str = config.get(\"device\", \"CPU\").upper()\n",
    "    if device_str in [\"CUDA\", \"GPU\"]:\n",
    "        # XGBoost 2.x хочет строковый device\n",
    "        params[\"device\"] = \"cuda\"      # можно \"cuda:0\"\n",
    "        # на GPU обычно достаточно tree_method=\"hist\" или \"gpu_hist\"\n",
    "        # оставим \"gpu_hist\", если в конфиге уже задан, иначе поставим мы\n",
    "        params.setdefault(\"tree_method\", \"gpu_hist\")\n",
    "    else:\n",
    "        params[\"device\"] = \"cpu\"\n",
    "        params.setdefault(\"tree_method\", \"hist\")\n",
    "\n",
    "    # потоков по умолчанию — все ядра\n",
    "    params.setdefault(\"nthread\", 0)\n",
    "\n",
    "    # ----- тип задачи -----\n",
    "    if task_type == \"regression\":\n",
    "        params[\"objective\"] = \"reg:squarederror\"\n",
    "        params[\"eval_metric\"] = \"mae\"           # можно поменять на \"rmse\"\n",
    "    elif task_type == \"binary\":\n",
    "        params[\"objective\"] = \"binary:logistic\"\n",
    "        params[\"eval_metric\"] = \"auc\"\n",
    "    else:\n",
    "        # multiclass\n",
    "        if num_classes is None:\n",
    "            raise ValueError(\"num_classes must be provided for multiclass\")\n",
    "        params[\"objective\"] = \"multi:softprob\"\n",
    "        params[\"num_class\"] = num_classes\n",
    "        params[\"eval_metric\"] = \"mlogloss\"\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _evaluate_predictions(\n",
    "    y_true: pd.Series,\n",
    "    preds: np.ndarray,\n",
    "    task_type: str,\n",
    "    name_prefix: str,\n",
    "    logger: logging.Logger\n",
    ") -> Dict[str, float]:\n",
    "    metrics = {}\n",
    "    if preds is None:\n",
    "        return metrics\n",
    "\n",
    "    if task_type == \"regression\":\n",
    "        rmse = mean_squared_error(y_true, preds, squared=False)\n",
    "        mae = mean_absolute_error(y_true, preds)\n",
    "        metrics[f\"{name_prefix}_RMSE\"] = rmse\n",
    "        metrics[f\"{name_prefix}_MAE\"] = rmse\n",
    "        \n",
    "        logger.info(f\"{name_prefix} RMSE: {rmse:.6f} MAE: {mae:.6f}\")\n",
    "    elif task_type == \"binary\":\n",
    "        auc = roc_auc_score(y_true, preds)\n",
    "        logloss = log_loss(y_true, preds)\n",
    "\n",
    "        ## можно сделать подбор оптимального порога\n",
    "        preds_label = (preds >= 0.5).astype(int)\n",
    "        acc = accuracy_score(y_true, preds_label)\n",
    "        cm = confusion_matrix(y_true, preds_label)\n",
    "        metrics[f\"{name_prefix}_AUC\"] = auc\n",
    "        metrics[f\"{name_prefix}_LogLoss\"] = logloss\n",
    "        metrics[f\"{name_prefix}_ACC\"] = acc\n",
    "        logger.info(f\"{name_prefix} AUC: {auc:.6f}, LogLoss: {logloss:.6f}, ACC: {acc:.6f}\")\n",
    "        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n",
    "    else:\n",
    "        # multiclass\n",
    "        # если пришли вероятности (n_samples, n_classes) — берём argmax\n",
    "        if preds.ndim == 2:\n",
    "            preds_label = np.argmax(preds, axis=1)\n",
    "        else:\n",
    "            preds_label = preds\n",
    "\n",
    "        acc = accuracy_score(y_true, preds_label)\n",
    "        cm = confusion_matrix(y_true, preds_label)\n",
    "        metrics[f\"{name_prefix}_ACC\"] = acc\n",
    "        logger.info(f\"{name_prefix} ACC: {acc:.6f}\")\n",
    "        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def train_and_evaluate(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    cat_cols: List[str],\n",
    "    config: Dict,\n",
    "    logger: logging.Logger\n",
    ") -> Dict:\n",
    "    task_type = config[\"task_type\"]\n",
    "    folds = make_folds(y, config)\n",
    "    logger.info(f\"Using {len(folds)} folds.\")\n",
    "\n",
    "    cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n",
    "\n",
    "    # СНАЧАЛА считаем classes_\n",
    "    classes_ = np.unique(y) if task_type == \"multiclass\" else None\n",
    "\n",
    "    n = len(y)\n",
    "    if task_type == \"multiclass\":\n",
    "        n_classes = len(classes_)\n",
    "        oof_preds_cat = np.zeros((n, n_classes)) if config[\"use_catboost\"] else None\n",
    "        oof_preds_lgb = np.zeros((n, n_classes)) if config[\"use_lightgbm\"] else None\n",
    "        oof_preds_xgb = np.zeros((n, n_classes)) if config[\"use_xgboost\"] else None\n",
    "    else:\n",
    "        oof_preds_cat = np.zeros(n) if config[\"use_catboost\"] else None\n",
    "        oof_preds_lgb = np.zeros(n) if config[\"use_lightgbm\"] else None\n",
    "        oof_preds_xgb = np.zeros(n) if config[\"use_xgboost\"] else None\n",
    "\n",
    "    models_cat: List = []\n",
    "    models_lgb: List = []\n",
    "    models_xgb: List = []\n",
    "\n",
    "    feature_importances_lgb = []\n",
    "    feature_importances_xgb = []\n",
    "\n",
    "    for fold_idx, (tr_idx, val_idx) in enumerate(tqdm(folds, desc=\"Cross-validation folds\")):\n",
    "        logger.info(f\"========== Fold {fold_idx + 1}/{len(folds)} ==========\")\n",
    "\n",
    "        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
    "\n",
    "        if config[\"use_catboost\"]:\n",
    "            logger.info(\"Training CatBoost...\")\n",
    "            train_pool = Pool(\n",
    "                X_tr, y_tr,\n",
    "                cat_features=cat_features_idx\n",
    "            )\n",
    "            val_pool = Pool(\n",
    "                X_val, y_val,\n",
    "                cat_features=cat_features_idx\n",
    "            )\n",
    "            model_cat = build_catboost_model(config)\n",
    "            # Early stopping для CatBoost\n",
    "            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n",
    "            model_cat.fit(\n",
    "                train_pool, \n",
    "                eval_set=val_pool,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                use_best_model=True\n",
    "            )\n",
    "\n",
    "            if task_type == \"regression\":\n",
    "                preds_val = model_cat.predict(val_pool)\n",
    "            else:\n",
    "                proba = model_cat.predict_proba(val_pool)\n",
    "                if task_type == \"binary\":\n",
    "                    preds_val = proba[:, 1]\n",
    "                else:\n",
    "                    oof_preds_cat[val_idx, :] = proba\n",
    "                    preds_val = proba  # для единообразия, если вдруг где-то используешь\n",
    "\n",
    "            if task_type != \"multiclass\":\n",
    "                oof_preds_cat[val_idx] = preds_val\n",
    "            models_cat.append(model_cat)\n",
    "\n",
    "        if config[\"use_lightgbm\"]:\n",
    "            logger.info(\"Training LightGBM...\")\n",
    "\n",
    "            num_classes = len(classes_) if task_type == \"multiclass\" else None\n",
    "            lgb_params = build_lgb_params(config, num_classes=num_classes)\n",
    "\n",
    "            X_tr_lgb = X_tr.copy()\n",
    "            X_val_lgb = X_val.copy()\n",
    "            for c in cat_cols:\n",
    "                if c in X_tr_lgb.columns:\n",
    "                    X_tr_lgb[c] = X_tr_lgb[c].astype(\"category\")\n",
    "                    X_val_lgb[c] = X_val_lgb[c].astype(\"category\")\n",
    "\n",
    "            lgb_train = lgb.Dataset(X_tr_lgb, label=y_tr)\n",
    "            lgb_val = lgb.Dataset(X_val_lgb, label=y_val)\n",
    "            print(lgb_params)\n",
    "            # Early stopping для LightGBM\n",
    "            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n",
    "            model_lgb = lgb.train(\n",
    "                lgb_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[lgb_train, lgb_val],\n",
    "                valid_names=[\"train\", \"valid\"],\n",
    "                callbacks=[\n",
    "                    lgb.early_stopping(stopping_rounds=early_stopping_rounds),\n",
    "                    lgb.log_evaluation(100)\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if task_type == \"regression\":\n",
    "                preds_val = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n",
    "            else:\n",
    "                proba = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n",
    "                if task_type == \"binary\":\n",
    "                    preds_val = proba\n",
    "                else:\n",
    "                    # multiclass: сохраняем вероятности\n",
    "                    oof_preds_lgb[val_idx, :] = proba\n",
    "                    preds_val = proba\n",
    "\n",
    "            if task_type != \"multiclass\":\n",
    "                oof_preds_lgb[val_idx] = preds_val\n",
    "            models_lgb.append(model_lgb)\n",
    "            feature_importances_lgb.append(model_lgb.feature_importance(importance_type=\"gain\"))\n",
    "\n",
    "        if config[\"use_xgboost\"]:\n",
    "            logger.info(\"Training XGBoost...\")\n",
    "            num_classes = len(classes_) if task_type == \"multiclass\" else None\n",
    "            xgb_params = build_xgb_params(config, num_classes=num_classes)\n",
    "\n",
    "            X_tr_xgb = X_tr.copy()\n",
    "            X_val_xgb = X_val.copy()\n",
    "            for c in X_tr_xgb.columns:\n",
    "                if X_tr_xgb[c].dtype == \"object\":\n",
    "                    X_tr_xgb[c] = X_tr_xgb[c].astype(\"category\")\n",
    "                    X_val_xgb[c] = X_val_xgb[c].astype(\"category\")\n",
    "\n",
    "            dtrain = xgb.DMatrix(X_tr_xgb, label=y_tr, enable_categorical=True)\n",
    "            dvalid = xgb.DMatrix(X_val_xgb, label=y_val, enable_categorical=True)\n",
    "\n",
    "            evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "            print(dtrain, dvalid)\n",
    "            print(xgb_params)\n",
    "            # Early stopping для XGBoost\n",
    "            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n",
    "            model_xgb = xgb.train(\n",
    "                params=xgb_params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=1000,\n",
    "                evals=evals,\n",
    "                early_stopping_rounds=early_stopping_rounds,\n",
    "                verbose_eval=100\n",
    "            )\n",
    "\n",
    "            if task_type == \"regression\":\n",
    "                preds_val = model_xgb.predict(\n",
    "                    dvalid,\n",
    "                    iteration_range=(0, model_xgb.best_iteration + 1)\n",
    "                )\n",
    "            else:\n",
    "                proba = model_xgb.predict(\n",
    "                    dvalid,\n",
    "                    iteration_range=(0, model_xgb.best_iteration + 1)\n",
    "                )\n",
    "                if task_type == \"binary\":\n",
    "                    preds_val = proba\n",
    "                else:\n",
    "                    # multiclass: сохраняем вероятности\n",
    "                    oof_preds_xgb[val_idx, :] = proba\n",
    "                    preds_val = proba\n",
    "\n",
    "            if task_type != \"multiclass\":\n",
    "                oof_preds_xgb[val_idx] = preds_val\n",
    "            models_xgb.append(model_xgb)\n",
    "\n",
    "            fi_xgb = model_xgb.get_score(importance_type=\"gain\")\n",
    "            fi_vec = np.array([fi_xgb.get(f, 0.0) for f in X.columns])\n",
    "            feature_importances_xgb.append(fi_vec)\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    metrics_all: Dict[str, float] = {}\n",
    "    metrics_all.update(_evaluate_predictions(y, oof_preds_cat, task_type, \"CatBoost\", logger))\n",
    "    metrics_all.update(_evaluate_predictions(y, oof_preds_lgb, task_type, \"LightGBM\", logger))\n",
    "    metrics_all.update(_evaluate_predictions(y, oof_preds_xgb, task_type, \"XGBoost\", logger))\n",
    "\n",
    "    logger.info(\"Evaluating blended OOF predictions...\")\n",
    "    blend_weights = config[\"blend_weights\"]\n",
    "    \n",
    "    blend_preds = None\n",
    "    if task_type == \"multiclass\":\n",
    "        # определяем число классов по первой не-None матрице\n",
    "        num_classes = None\n",
    "        for arr in [oof_preds_cat, oof_preds_lgb, oof_preds_xgb]:\n",
    "            if arr is not None:\n",
    "                num_classes = arr.shape[1]\n",
    "                break\n",
    "        if num_classes is None:\n",
    "            logger.warning(\"No OOF predictions for multiclass blend.\")\n",
    "        else:\n",
    "            blend_num = np.zeros((n, num_classes), dtype=float)\n",
    "            blend_den = np.zeros(n, dtype=float)\n",
    "\n",
    "            if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n",
    "                w = blend_weights[\"catboost\"]\n",
    "                blend_num += w * oof_preds_cat\n",
    "                blend_den += w\n",
    "            if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n",
    "                w = blend_weights[\"lightgbm\"]\n",
    "                blend_num += w * oof_preds_lgb\n",
    "                blend_den += w\n",
    "            if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n",
    "                w = blend_weights[\"xgboost\"]\n",
    "                blend_num += w * oof_preds_xgb\n",
    "                blend_den += w\n",
    "\n",
    "            valid_mask = blend_den > 0\n",
    "            if valid_mask.any():\n",
    "                blend_preds = np.zeros_like(blend_num)\n",
    "                # нормируем по весам, broadcast по axis=1\n",
    "                blend_preds[valid_mask] = (\n",
    "                    blend_num[valid_mask] /\n",
    "                    blend_den[valid_mask][:, None]\n",
    "                )\n",
    "                metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n",
    "            else:\n",
    "                logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n",
    "    else:\n",
    "        # binary / regression — как раньше (1D)\n",
    "        blend_num = np.zeros(n, dtype=float)\n",
    "        blend_den = np.zeros(n, dtype=float)\n",
    "\n",
    "        if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n",
    "            w = blend_weights[\"catboost\"]\n",
    "            blend_num += w * oof_preds_cat\n",
    "            blend_den += w\n",
    "        if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n",
    "            w = blend_weights[\"lightgbm\"]\n",
    "            blend_num += w * oof_preds_lgb\n",
    "            blend_den += w\n",
    "        if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n",
    "            w = blend_weights[\"xgboost\"]\n",
    "            blend_num += w * oof_preds_xgb\n",
    "            blend_den += w\n",
    "\n",
    "        valid_mask = blend_den > 0\n",
    "        if valid_mask.any():\n",
    "            blend_preds = np.zeros_like(blend_num)\n",
    "            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n",
    "            metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n",
    "        else:\n",
    "            logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n",
    "    logger.info(\"========== OOF metrics ==========\")\n",
    "    logger.info(metrics_all)\n",
    "\n",
    "    results = {\n",
    "        \"oof_catboost\": oof_preds_cat,\n",
    "        \"oof_lightgbm\": oof_preds_lgb,\n",
    "        \"oof_xgboost\": oof_preds_xgb,\n",
    "        \"oof_blend\": blend_preds,\n",
    "        \"metrics\": metrics_all,\n",
    "        \"models_catboost\": models_cat,\n",
    "        \"models_lightgbm\": models_lgb,\n",
    "        \"models_xgboost\": models_xgb\n",
    "    }\n",
    "\n",
    "    if feature_importances_lgb:\n",
    "        fi_lgb = np.mean(feature_importances_lgb, axis=0)\n",
    "        results[\"feature_importance_lgb\"] = fi_lgb\n",
    "\n",
    "    if feature_importances_xgb:\n",
    "        fi_xgb_mean = np.mean(feature_importances_xgb, axis=0)\n",
    "        results[\"feature_importance_xgb\"] = fi_xgb_mean\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def ensemble_predict(\n",
    "    models: Dict[str, List],\n",
    "    X: pd.DataFrame,\n",
    "    cat_cols: List[str],\n",
    "    config: Dict\n",
    ") -> Dict[str, Optional[np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Делает предсказания ансамблем:\n",
    "      - по каждому бустингу усредняет предсказания по фолдам\n",
    "      - формирует blended предсказания по весам\n",
    "\n",
    "    Возвращает:\n",
    "      {\n",
    "        \"catboost\": np.ndarray или None,\n",
    "        \"lightgbm\": np.ndarray или None,\n",
    "        \"xgboost\": np.ndarray или None,\n",
    "        \"blend\": np.ndarray или None,\n",
    "      }\n",
    "    \"\"\"\n",
    "    task_type = config[\"task_type\"]\n",
    "    blend_weights = config[\"blend_weights\"]\n",
    "\n",
    "    n = len(X)\n",
    "    preds_cat = None\n",
    "    preds_lgb = None\n",
    "    preds_xgb = None\n",
    "\n",
    "    X_lgb = X.copy()\n",
    "    for c in cat_cols:\n",
    "        if c in X_lgb.columns:\n",
    "            X_lgb[c] = X_lgb[c].astype(\"category\")\n",
    "\n",
    "    # CatBoost\n",
    "    if models.get(\"catboost\"):\n",
    "        cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n",
    "        test_pool = Pool(X, cat_features=cat_features_idx)\n",
    "        for m in models[\"catboost\"]:\n",
    "            if task_type == \"regression\":\n",
    "                proba = m.predict(test_pool)              # (n,)\n",
    "            else:\n",
    "                proba_full = m.predict_proba(test_pool)   # (n,2) или (n,C)\n",
    "                if task_type == \"binary\":\n",
    "                    proba = proba_full[:, 1]              # (n,)\n",
    "                else:\n",
    "                    proba = proba_full                    # (n,C)\n",
    "            if preds_cat is None:\n",
    "                preds_cat = np.zeros_like(proba, dtype=float)\n",
    "            preds_cat += proba\n",
    "        preds_cat /= len(models[\"catboost\"])\n",
    "\n",
    "    if models.get(\"lightgbm\"):\n",
    "        for m in models[\"lightgbm\"]:\n",
    "            if task_type == \"regression\":\n",
    "                proba = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,)\n",
    "            else:\n",
    "                proba_full = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,) или (n,C)\n",
    "                if task_type == \"binary\":\n",
    "                    proba = proba_full                                      # (n,)\n",
    "                else:\n",
    "                    proba = proba_full                                      # (n,C)\n",
    "            if preds_lgb is None:\n",
    "                preds_lgb = np.zeros_like(proba, dtype=float)\n",
    "            preds_lgb += proba\n",
    "        preds_lgb /= len(models[\"lightgbm\"])\n",
    "\n",
    "    # XGBoost\n",
    "    if models.get(\"xgboost\"):\n",
    "        X_xgb = X.copy()\n",
    "        for c in X_xgb.columns:\n",
    "            if X_xgb[c].dtype == \"object\":\n",
    "                X_xgb[c] = X_xgb[c].astype(\"category\")\n",
    "        dtest = xgb.DMatrix(X_xgb, enable_categorical=True)\n",
    "\n",
    "        for m in models[\"xgboost\"]:\n",
    "            if task_type == \"regression\":\n",
    "                proba = m.predict(\n",
    "                    dtest,\n",
    "                    iteration_range=(0, m.best_iteration + 1)\n",
    "                )  # shape: (n,)\n",
    "            else:\n",
    "                proba_full = m.predict(\n",
    "                    dtest,\n",
    "                    iteration_range=(0, m.best_iteration + 1)\n",
    "                )  # shape: (n,) for binary, (n, C) for multiclass\n",
    "                if task_type == \"binary\":\n",
    "                    proba = proba_full          # (n,)\n",
    "                else:\n",
    "                    proba = proba_full          # (n, C)\n",
    "\n",
    "            if preds_xgb is None:\n",
    "                preds_xgb = np.zeros_like(proba, dtype=float)\n",
    "            preds_xgb += proba\n",
    "\n",
    "        preds_xgb /= len(models[\"xgboost\"])\n",
    "\n",
    "    # Blending\n",
    "    blend_preds = None\n",
    "    if task_type == \"multiclass\":\n",
    "        # определяем форму по первой не-None матрице\n",
    "        base = None\n",
    "        for arr in [preds_cat, preds_lgb, preds_xgb]:\n",
    "            if arr is not None:\n",
    "                base = arr\n",
    "                break\n",
    "        if base is not None:\n",
    "            blend_num = np.zeros_like(base, dtype=float)  # (n,C)\n",
    "            blend_den = np.zeros(n, dtype=float)\n",
    "\n",
    "            if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n",
    "                w = blend_weights[\"catboost\"]\n",
    "                blend_num += w * preds_cat\n",
    "                blend_den += w\n",
    "            if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n",
    "                w = blend_weights[\"lightgbm\"]\n",
    "                blend_num += w * preds_lgb\n",
    "                blend_den += w\n",
    "            if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n",
    "                w = blend_weights[\"xgboost\"]\n",
    "                blend_num += w * preds_xgb\n",
    "                blend_den += w\n",
    "\n",
    "            valid_mask = blend_den > 0\n",
    "            if valid_mask.any():\n",
    "                blend_preds = np.zeros_like(blend_num)\n",
    "                blend_preds[valid_mask] = (\n",
    "                    blend_num[valid_mask] /\n",
    "                    blend_den[valid_mask][:, None]\n",
    "                )\n",
    "    else:\n",
    "        blend_num = np.zeros(n, dtype=float)\n",
    "        blend_den = np.zeros(n, dtype=float)\n",
    "        if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n",
    "            w = blend_weights[\"catboost\"]\n",
    "            blend_num += w * preds_cat\n",
    "            blend_den += w\n",
    "        if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n",
    "            w = blend_weights[\"lightgbm\"]\n",
    "            blend_num += w * preds_lgb\n",
    "            blend_den += w\n",
    "        if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n",
    "            w = blend_weights[\"xgboost\"]\n",
    "            blend_num += w * preds_xgb\n",
    "            blend_den += w\n",
    "\n",
    "        valid_mask = blend_den > 0\n",
    "        if valid_mask.any():\n",
    "            blend_preds = np.zeros_like(blend_num)\n",
    "            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n",
    "\n",
    "    return {\n",
    "        \"catboost\": preds_cat,\n",
    "        \"lightgbm\": preds_lgb,\n",
    "        \"xgboost\": preds_xgb,\n",
    "        \"blend\": blend_preds\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f4c59c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:21:27.336839Z",
     "iopub.status.busy": "2025-11-18T21:21:27.336539Z",
     "iopub.status.idle": "2025-11-18T21:23:37.899585Z",
     "shell.execute_reply": "2025-11-18T21:23:37.898768Z",
     "shell.execute_reply.started": "2025-11-18T21:21:27.336818Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:21:27,345 - INFO - Using 5 folds.\n",
      "INFO:TABULAR_BOOSTING:Using 5 folds.\n",
      "Cross-validation folds:   0%|          | 0/5 [00:00<?, ?it/s]2025-11-18 21:21:27,349 - INFO - ========== Fold 1/5 ==========\n",
      "INFO:TABULAR_BOOSTING:========== Fold 1/5 ==========\n",
      "2025-11-18 21:21:27,409 - INFO - Training CatBoost...\n",
      "INFO:TABULAR_BOOSTING:Training CatBoost...\n",
      "Warning: less than 75% GPU memory available for training. Free: 10822.125 Total: 15095.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.3802004\ttest: 20.9300699\tbest: 20.9300699 (0)\ttotal: 175ms\tremaining: 10m 12s\n",
      "100:\tlearn: 16.9138527\ttest: 18.2101740\tbest: 18.2101740 (100)\ttotal: 3.44s\tremaining: 1m 55s\n",
      "200:\tlearn: 15.8099983\ttest: 17.9217795\tbest: 17.9217795 (200)\ttotal: 6.48s\tremaining: 1m 46s\n",
      "300:\tlearn: 14.9926065\ttest: 17.8779569\tbest: 17.8674114 (288)\ttotal: 9.46s\tremaining: 1m 40s\n",
      "400:\tlearn: 14.2649801\ttest: 17.8180804\tbest: 17.8180804 (400)\ttotal: 12.4s\tremaining: 1m 36s\n",
      "500:\tlearn: 13.6613756\ttest: 17.8031024\tbest: 17.8031024 (500)\ttotal: 15.4s\tremaining: 1m 32s\n",
      "600:\tlearn: 13.0777474\ttest: 17.7944839\tbest: 17.7931555 (555)\ttotal: 18.4s\tremaining: 1m 28s\n",
      "700:\tlearn: 12.5647572\ttest: 17.7754482\tbest: 17.7710601 (661)\ttotal: 21.4s\tremaining: 1m 25s\n",
      "800:\tlearn: 12.1154208\ttest: 17.7859340\tbest: 17.7701902 (711)\ttotal: 24.4s\tremaining: 1m 22s\n",
      "bestTest = 17.77019016\n",
      "bestIteration = 711\n",
      "Shrink model to first 712 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation folds:  20%|██        | 1/5 [00:26<01:47, 26.76s/it]2025-11-18 21:21:54,109 - INFO - ========== Fold 2/5 ==========\n",
      "INFO:TABULAR_BOOSTING:========== Fold 2/5 ==========\n",
      "2025-11-18 21:21:54,141 - INFO - Training CatBoost...\n",
      "INFO:TABULAR_BOOSTING:Training CatBoost...\n",
      "Warning: less than 75% GPU memory available for training. Free: 10820.125 Total: 15095.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.4785146\ttest: 20.4554206\tbest: 20.4554206 (0)\ttotal: 32.6ms\tremaining: 1m 54s\n",
      "100:\tlearn: 16.7732724\ttest: 18.3915519\tbest: 18.3915519 (100)\ttotal: 3.29s\tremaining: 1m 50s\n",
      "200:\tlearn: 15.7253562\ttest: 18.2091892\tbest: 18.2049887 (193)\ttotal: 6.35s\tremaining: 1m 44s\n",
      "300:\tlearn: 14.8481892\ttest: 18.1212907\tbest: 18.1212907 (300)\ttotal: 9.36s\tremaining: 1m 39s\n",
      "400:\tlearn: 14.1574546\ttest: 18.0780146\tbest: 18.0780146 (400)\ttotal: 12.3s\tremaining: 1m 35s\n",
      "500:\tlearn: 13.5480314\ttest: 18.0440833\tbest: 18.0440833 (500)\ttotal: 15.3s\tremaining: 1m 31s\n",
      "600:\tlearn: 13.0442520\ttest: 18.0214352\tbest: 18.0166609 (592)\ttotal: 18.2s\tremaining: 1m 27s\n",
      "bestTest = 18.01666088\n",
      "bestIteration = 592\n",
      "Shrink model to first 593 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation folds:  40%|████      | 2/5 [00:49<01:13, 24.34s/it]2025-11-18 21:22:16,752 - INFO - ========== Fold 3/5 ==========\n",
      "INFO:TABULAR_BOOSTING:========== Fold 3/5 ==========\n",
      "2025-11-18 21:22:16,781 - INFO - Training CatBoost...\n",
      "INFO:TABULAR_BOOSTING:Training CatBoost...\n",
      "Warning: less than 75% GPU memory available for training. Free: 10820.125 Total: 15095.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.5822643\ttest: 19.9837464\tbest: 19.9837464 (0)\ttotal: 34.4ms\tremaining: 2m\n",
      "100:\tlearn: 16.9202528\ttest: 17.6497878\tbest: 17.6497878 (100)\ttotal: 3.2s\tremaining: 1m 47s\n",
      "200:\tlearn: 15.8322165\ttest: 17.4748576\tbest: 17.4745526 (199)\ttotal: 6.2s\tremaining: 1m 41s\n",
      "300:\tlearn: 14.9584270\ttest: 17.3997634\tbest: 17.3983742 (286)\ttotal: 9.13s\tremaining: 1m 37s\n",
      "400:\tlearn: 14.2589074\ttest: 17.3542038\tbest: 17.3542038 (400)\ttotal: 12.1s\tremaining: 1m 33s\n",
      "500:\tlearn: 13.6320402\ttest: 17.3116140\tbest: 17.3067771 (471)\ttotal: 15.1s\tremaining: 1m 30s\n",
      "600:\tlearn: 13.0749181\ttest: 17.3100665\tbest: 17.2988913 (578)\ttotal: 18s\tremaining: 1m 26s\n",
      "700:\tlearn: 12.5923537\ttest: 17.2975751\tbest: 17.2974019 (637)\ttotal: 20.9s\tremaining: 1m 23s\n",
      "800:\tlearn: 12.1236250\ttest: 17.2985960\tbest: 17.2908715 (719)\ttotal: 23.9s\tremaining: 1m 20s\n",
      "bestTest = 17.29087149\n",
      "bestIteration = 719\n",
      "Shrink model to first 720 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation folds:  60%|██████    | 3/5 [01:15<00:50, 25.16s/it]2025-11-18 21:22:42,895 - INFO - ========== Fold 4/5 ==========\n",
      "INFO:TABULAR_BOOSTING:========== Fold 4/5 ==========\n",
      "2025-11-18 21:22:42,931 - INFO - Training CatBoost...\n",
      "INFO:TABULAR_BOOSTING:Training CatBoost...\n",
      "Warning: less than 75% GPU memory available for training. Free: 10820.125 Total: 15095.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.4585323\ttest: 20.4690535\tbest: 20.4690535 (0)\ttotal: 34.7ms\tremaining: 2m 1s\n",
      "100:\tlearn: 16.8805237\ttest: 18.1264409\tbest: 18.1264409 (100)\ttotal: 3.18s\tremaining: 1m 47s\n",
      "200:\tlearn: 15.8272461\ttest: 17.8878863\tbest: 17.8878863 (200)\ttotal: 6.21s\tremaining: 1m 41s\n",
      "300:\tlearn: 14.9553950\ttest: 17.8225005\tbest: 17.8109696 (282)\ttotal: 9.19s\tremaining: 1m 37s\n",
      "400:\tlearn: 14.1952750\ttest: 17.7575274\tbest: 17.7575274 (400)\ttotal: 12.2s\tremaining: 1m 34s\n",
      "500:\tlearn: 13.5452484\ttest: 17.7425565\tbest: 17.7310216 (450)\ttotal: 15.2s\tremaining: 1m 30s\n",
      "600:\tlearn: 13.0230631\ttest: 17.7235745\tbest: 17.7211040 (595)\ttotal: 18.1s\tremaining: 1m 27s\n",
      "700:\tlearn: 12.5094466\ttest: 17.7015735\tbest: 17.6963550 (693)\ttotal: 21.1s\tremaining: 1m 24s\n",
      "800:\tlearn: 12.0392505\ttest: 17.6709593\tbest: 17.6694870 (796)\ttotal: 24.1s\tremaining: 1m 21s\n",
      "900:\tlearn: 11.6328813\ttest: 17.6516994\tbest: 17.6502952 (898)\ttotal: 27.1s\tremaining: 1m 18s\n",
      "1000:\tlearn: 11.2662990\ttest: 17.6421161\tbest: 17.6421161 (1000)\ttotal: 30s\tremaining: 1m 14s\n",
      "1100:\tlearn: 10.9369654\ttest: 17.6335899\tbest: 17.6321261 (1063)\ttotal: 33s\tremaining: 1m 11s\n",
      "bestTest = 17.63212609\n",
      "bestIteration = 1063\n",
      "Shrink model to first 1064 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation folds:  80%|████████  | 4/5 [01:52<00:29, 29.71s/it]2025-11-18 21:23:19,569 - INFO - ========== Fold 5/5 ==========\n",
      "INFO:TABULAR_BOOSTING:========== Fold 5/5 ==========\n",
      "2025-11-18 21:23:19,602 - INFO - Training CatBoost...\n",
      "INFO:TABULAR_BOOSTING:Training CatBoost...\n",
      "Warning: less than 75% GPU memory available for training. Free: 10820.125 Total: 15095.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 20.4366613\ttest: 20.5950846\tbest: 20.5950846 (0)\ttotal: 32.5ms\tremaining: 1m 53s\n",
      "100:\tlearn: 16.8436958\ttest: 18.1491553\tbest: 18.1491553 (100)\ttotal: 3.25s\tremaining: 1m 49s\n",
      "200:\tlearn: 15.7681799\ttest: 17.9643999\tbest: 17.9589224 (199)\ttotal: 6.31s\tremaining: 1m 43s\n",
      "300:\tlearn: 14.9546672\ttest: 17.8921413\tbest: 17.8921413 (300)\ttotal: 9.31s\tremaining: 1m 38s\n",
      "400:\tlearn: 14.1890763\ttest: 17.8643958\tbest: 17.8604943 (395)\ttotal: 12.3s\tremaining: 1m 35s\n",
      "500:\tlearn: 13.5733099\ttest: 17.8742912\tbest: 17.8584594 (447)\ttotal: 15.3s\tremaining: 1m 31s\n",
      "bestTest = 17.85845937\n",
      "bestIteration = 447\n",
      "Shrink model to first 448 iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cross-validation folds: 100%|██████████| 5/5 [02:10<00:00, 26.10s/it]\n",
      "2025-11-18 21:23:37,873 - INFO - CatBoost RMSE: 17.715399 MAE: 12.937462\n",
      "INFO:TABULAR_BOOSTING:CatBoost RMSE: 17.715399 MAE: 12.937462\n",
      "2025-11-18 21:23:37,875 - INFO - Evaluating blended OOF predictions...\n",
      "INFO:TABULAR_BOOSTING:Evaluating blended OOF predictions...\n",
      "2025-11-18 21:23:37,881 - INFO - Blend RMSE: 17.715399 MAE: 12.937462\n",
      "INFO:TABULAR_BOOSTING:Blend RMSE: 17.715399 MAE: 12.937462\n",
      "2025-11-18 21:23:37,883 - INFO - ========== OOF metrics ==========\n",
      "INFO:TABULAR_BOOSTING:========== OOF metrics ==========\n",
      "2025-11-18 21:23:37,885 - INFO - {'CatBoost_RMSE': 17.715398906717056, 'CatBoost_MAE': 17.715398906717056, 'Blend_RMSE': 17.715398906717056, 'Blend_MAE': 17.715398906717056}\n",
      "INFO:TABULAR_BOOSTING:{'CatBoost_RMSE': 17.715398906717056, 'CatBoost_MAE': 17.715398906717056, 'Blend_RMSE': 17.715398906717056, 'Blend_MAE': 17.715398906717056}\n",
      "2025-11-18 21:23:37,892 - INFO -  Saving artifacts...\n",
      "INFO:TABULAR_BOOSTING:💾 Saving artifacts...\n"
     ]
    }
   ],
   "source": [
    "# Удаляем ID только перед обучением бустингов,\n",
    "# чтобы ID не утек в модели, но остался для генерации фич (картинки/тексты и т.п.)\n",
    "id_col = CONFIG[\"id_column\"]\n",
    "\n",
    "if id_col in X.columns:\n",
    "    logger.info(f\"Dropping ID column '{id_col}' before boosting\")\n",
    "    X = X.drop(columns=[id_col])\n",
    "\n",
    "if test_df is not None and id_col in test_df.columns:\n",
    "    test_df = test_df.drop(columns=[id_col])\n",
    "\n",
    "# На всякий случай исключаем ID из списка категориальных колонок\n",
    "if \"cat_cols\" in globals():\n",
    "    cat_cols = [c for c in cat_cols if c != id_col]\n",
    "\n",
    "results = train_and_evaluate(X, y, cat_cols, CONFIG, logger)\n",
    "\n",
    "# ----- Save OOF, metrics, FI -----\n",
    "logger.info(\"💾 Saving artifacts...\")\n",
    "if results[\"oof_catboost\"] is not None:\n",
    "    np.save(os.path.join(output_dir, \"oof_catboost.npy\"), results[\"oof_catboost\"])\n",
    "if results[\"oof_lightgbm\"] is not None:\n",
    "    np.save(os.path.join(output_dir, \"oof_lightgbm.npy\"), results[\"oof_lightgbm\"])\n",
    "if results[\"oof_xgboost\"] is not None:\n",
    "    np.save(os.path.join(output_dir, \"oof_xgboost.npy\"), results[\"oof_xgboost\"])\n",
    "if results[\"oof_blend\"] is not None:\n",
    "    np.save(os.path.join(output_dir, \"oof_blend.npy\"), results[\"oof_blend\"])\n",
    "\n",
    "with open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results[\"metrics\"], f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe648f2",
   "metadata": {},
   "source": [
    "### Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9194b81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:23:56.995008Z",
     "iopub.status.busy": "2025-11-18T21:23:56.994198Z",
     "iopub.status.idle": "2025-11-18T21:23:57.000538Z",
     "shell.execute_reply": "2025-11-18T21:23:56.999831Z",
     "shell.execute_reply.started": "2025-11-18T21:23:56.994981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if \"feature_importance_lgb\" in results:\n",
    "    fi = results[\"feature_importance_lgb\"]\n",
    "    fi_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"importance_gain\": fi\n",
    "    }).sort_values(\"importance_gain\", ascending=False)\n",
    "    fi_df.to_csv(os.path.join(output_dir, \"feature_importance_lgb.csv\"), index=False)\n",
    "    logger.info(\"Top features (LGB, gain):\")\n",
    "    logger.info(fi_df.head(CONFIG[\"top_features_to_show\"]))\n",
    "\n",
    "if \"feature_importance_xgb\" in results:\n",
    "    fi_xgb = results[\"feature_importance_xgb\"]\n",
    "    fi_xgb_df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"importance_gain\": fi_xgb\n",
    "    }).sort_values(\"importance_gain\", ascending=False)\n",
    "    fi_xgb_df.to_csv(os.path.join(output_dir, \"feature_importance_xgb.csv\"), index=False)\n",
    "    logger.info(\"Top features (XGB, gain):\")\n",
    "    logger.info(fi_xgb_df.head(CONFIG[\"top_features_to_show\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e844bbe",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfd7f5d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:23:58.003500Z",
     "iopub.status.busy": "2025-11-18T21:23:58.002697Z",
     "iopub.status.idle": "2025-11-18T21:23:58.126585Z",
     "shell.execute_reply": "2025-11-18T21:23:58.125919Z",
     "shell.execute_reply.started": "2025-11-18T21:23:58.003474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:23:58,013 - INFO -  Aligning categorical values in test with train for XGBoost...\n",
      "INFO:TABULAR_BOOSTING:🧩 Aligning categorical values in test with train for XGBoost...\n",
      "2025-11-18 21:23:58,040 - INFO -  Making ensemble predictions for test...\n",
      "INFO:TABULAR_BOOSTING:📤 Making ensemble predictions for test...\n",
      "2025-11-18 21:23:58,042 - INFO - test_df shape before ensemble_predict: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:test_df shape before ensemble_predict: (8, 780)\n",
      "2025-11-18 21:23:58,044 - INFO - test_ids length: 8\n",
      "INFO:TABULAR_BOOSTING:test_ids length: 8\n",
      "2025-11-18 21:23:58,103 - INFO - Test DataFrame shape: (8, 780)\n",
      "INFO:TABULAR_BOOSTING:Test DataFrame shape: (8, 780)\n",
      "2025-11-18 21:23:58,105 - INFO - Test IDs length: 8\n",
      "INFO:TABULAR_BOOSTING:Test IDs length: 8\n",
      "2025-11-18 21:23:58,106 - INFO - Predictions 'catboost' shape: (8,), length: 8\n",
      "INFO:TABULAR_BOOSTING:Predictions 'catboost' shape: (8,), length: 8\n",
      "2025-11-18 21:23:58,115 - INFO - Saved predictions: pred_catboost.csv\n",
      "INFO:TABULAR_BOOSTING:Saved predictions: pred_catboost.csv\n",
      "2025-11-18 21:23:58,117 - INFO - Predictions 'blend' shape: (8,), length: 8\n",
      "INFO:TABULAR_BOOSTING:Predictions 'blend' shape: (8,), length: 8\n",
      "2025-11-18 21:23:58,120 - INFO - Saved predictions: pred_blend.csv\n",
      "INFO:TABULAR_BOOSTING:Saved predictions: pred_blend.csv\n",
      "2025-11-18 21:23:58,122 - INFO -  Finished in 1070.9 seconds\n",
      "INFO:TABULAR_BOOSTING:⏱ Finished in 1070.9 seconds\n"
     ]
    }
   ],
   "source": [
    "# ----- Test predictions (ensemble) -----\n",
    "if test_df is not None:\n",
    "    # NEW: выравниваем категориальные значения теста по train,\n",
    "    # маппим все unseen → \"other\" (если она есть), иначе → мода трейна\n",
    "    if cat_cols:\n",
    "        logger.info(\"🧩 Aligning categorical values in test with train for XGBoost...\")\n",
    "        for c in cat_cols:\n",
    "            if c in test_df.columns and c in X.columns:\n",
    "\n",
    "                train_vals = X[c].dropna()\n",
    "                if train_vals.empty:\n",
    "                    continue\n",
    "\n",
    "                known = set(train_vals.unique())\n",
    "                mask_new = ~test_df[c].isin(known)\n",
    "\n",
    "                if mask_new.any():\n",
    "                    # проверяем существование категорий \"other\"\n",
    "                    known_lower = {str(v).lower() for v in known}\n",
    "                    if \"other\" in known_lower:\n",
    "                        # находим точное значение \"other\" в train\n",
    "                        other_value = next(v for v in known if str(v).lower() == \"other\")\n",
    "                        replacement = other_value\n",
    "                    else:\n",
    "                        # если \"other\" нет — fallback на моду\n",
    "                        replacement = train_vals.mode(dropna=True).iloc[0]\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"🔄 Column '{c}': mapping {mask_new.sum()} unseen categories → '{replacement}'\"\n",
    "                    )\n",
    "                    test_df.loc[mask_new, c] = replacement\n",
    "\n",
    "    logger.info(\"📤 Making ensemble predictions for test...\")\n",
    "    # КРИТИЧЕСКАЯ ПРОВЕРКА: убеждаемся, что test_df имеет правильный размер\n",
    "    logger.info(f\"test_df shape before ensemble_predict: {test_df.shape}\")\n",
    "    logger.info(f\"test_ids length: {len(test_ids) if test_ids is not None else 'None'}\")\n",
    "    \n",
    "    # КРИТИЧЕСКАЯ ПРОВЕРКА: если размер test_df не совпадает с test_ids, это ошибка\n",
    "    if test_ids is not None:\n",
    "        expected_test_size = len(test_ids)\n",
    "        actual_test_size = len(test_df)\n",
    "        if actual_test_size != expected_test_size:\n",
    "            error_msg = (\n",
    "                f\"❌ CRITICAL ERROR: test_df size ({actual_test_size}) != test_ids size ({expected_test_size})! \"\n",
    "                f\"This indicates a problem in data processing. \"\n",
    "                f\"test_df should have {expected_test_size} rows, but has {actual_test_size}. \"\n",
    "                f\"Please check your data processing pipeline, especially generate_aggregate_features.\"\n",
    "            )\n",
    "            logger.error(error_msg)\n",
    "            raise ValueError(error_msg)\n",
    "    \n",
    "    pred_dict = ensemble_predict(\n",
    "        models={\n",
    "            \"catboost\": results[\"models_catboost\"],\n",
    "            \"lightgbm\": results[\"models_lightgbm\"],\n",
    "            \"xgboost\": results[\"models_xgboost\"],\n",
    "        },\n",
    "        X=test_df,\n",
    "        cat_cols=cat_cols,\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "    if test_ids is None:\n",
    "        test_ids_series = pd.Series(np.arange(len(test_df)), name=id_col)\n",
    "    else:\n",
    "        test_ids_series = test_ids\n",
    "\n",
    "    # Проверка размеров перед созданием submission\n",
    "    logger.info(f\"Test DataFrame shape: {test_df.shape}\")\n",
    "    logger.info(f\"Test IDs length: {len(test_ids_series)}\")\n",
    "\n",
    "    for name, preds in pred_dict.items():\n",
    "        if preds is None:\n",
    "            continue\n",
    "\n",
    "        # Проверка размеров предсказаний\n",
    "        if isinstance(preds, np.ndarray):\n",
    "            pred_len = preds.shape[0] if preds.ndim == 1 else preds.shape[0]\n",
    "            logger.info(f\"Predictions '{name}' shape: {preds.shape}, length: {pred_len}\")\n",
    "            \n",
    "            if pred_len != len(test_df):\n",
    "                raise ValueError(\n",
    "                    f\"Predictions length {pred_len} != test_df length {len(test_df)} for {name}. \"\n",
    "                    f\"Test_df shape: {test_df.shape}, predictions shape: {preds.shape}\"\n",
    "                )\n",
    "\n",
    "        if isinstance(preds, np.ndarray) and preds.ndim == 2:\n",
    "            n_classes = preds.shape[1]\n",
    "            data = {\n",
    "                id_col: test_ids_series.values,  # Используем .values для гарантии правильного размера\n",
    "                \"prediction\": np.argmax(preds, axis=1),  # предсказанный класс\n",
    "            }\n",
    "            for k in range(n_classes):\n",
    "                data[f\"proba_class_{k}\"] = preds[:, k]\n",
    "            out_df = pd.DataFrame(data)\n",
    "        else:\n",
    "            # Убеждаемся, что размеры совпадают\n",
    "            preds_flat = preds.flatten() if isinstance(preds, np.ndarray) else preds\n",
    "            if len(preds_flat) != len(test_ids_series):\n",
    "                raise ValueError(\n",
    "                    f\"Predictions length {len(preds_flat)} != test_ids length {len(test_ids_series)} for {name}\"\n",
    "                )\n",
    "            out_df = pd.DataFrame({\n",
    "                id_col: test_ids_series.values,  # Используем .values\n",
    "                \"prediction\": preds_flat\n",
    "            })\n",
    "\n",
    "        out_df.to_csv(os.path.join(output_dir, f\"pred_{name}.csv\"), index=False)\n",
    "        logger.info(f\"Saved predictions: pred_{name}.csv\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "logger.info(f\"⏱ Finished in {elapsed:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "146d23f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:24:05.077364Z",
     "iopub.status.busy": "2025-11-18T21:24:05.077068Z",
     "iopub.status.idle": "2025-11-18T21:24:05.083329Z",
     "shell.execute_reply": "2025-11-18T21:24:05.082608Z",
     "shell.execute_reply.started": "2025-11-18T21:24:05.077343Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'catboost': array([42.73166499, 45.94307196, 39.43541628, 40.92841088, 36.26719054,\n",
       "        41.97345426, 40.59874338, 41.76970097]),\n",
       " 'lightgbm': None,\n",
       " 'xgboost': None,\n",
       " 'blend': array([42.73166499, 45.94307196, 39.43541628, 40.92841088, 36.26719054,\n",
       "        41.97345426, 40.59874338, 41.76970097])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9783588e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:24:09.239933Z",
     "iopub.status.busy": "2025-11-18T21:24:09.239268Z",
     "iopub.status.idle": "2025-11-18T21:24:09.253734Z",
     "shell.execute_reply": "2025-11-18T21:24:09.252780Z",
     "shell.execute_reply.started": "2025-11-18T21:24:09.239912Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 21:24:09,243 - INFO -  Preparing final submission file...\n",
      "INFO:TABULAR_BOOSTING:📝 Preparing final submission file...\n",
      "2025-11-18 21:24:09,248 - INFO -  Submission file saved: tabular_boosting_output/submission.csv\n",
      "INFO:TABULAR_BOOSTING:✅ Submission file saved: tabular_boosting_output/submission.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Id  Pawpularity\n",
      "0  4128bae22183829d2b5fea10effdb0c3    42.731665\n",
      "1  43a2262d7738e3d420d453815151079e    45.943072\n",
      "2  4e429cead1848a298432a0acad014c9d    39.435416\n",
      "3  80bc3ccafcc51b66303c2c263aa38486    40.928411\n",
      "4  8f49844c382931444e68dffbe20228f4    36.267191\n"
     ]
    }
   ],
   "source": [
    "# ----- FINAL SUBMISSION -----\n",
    "logger.info(\"📝 Preparing final submission file...\")\n",
    "\n",
    "# Найдём итоговый ансамблевый prediction\n",
    "final_preds = pred_dict.get(\"catboost\")\n",
    "\n",
    "if final_preds is None:\n",
    "    logger.warning(\"Blend predictions are None. Falling back to first available model.\")\n",
    "    for name in [\"catboost\", \"lightgbm\", \"xgboost\"]:\n",
    "        if pred_dict.get(name) is not None:\n",
    "            final_preds = pred_dict[name]\n",
    "            logger.info(f\"Using '{name}' predictions for submission.\")\n",
    "            break\n",
    "\n",
    "if final_preds is None:\n",
    "    raise ValueError(\"No predictions available to form submission.\")\n",
    "\n",
    "# Создаём DataFrame с id\n",
    "submission = pd.DataFrame({id_col: test_ids_series})\n",
    "\n",
    "# Теперь добавляем target\n",
    "task_type = CONFIG[\"task_type\"]\n",
    "\n",
    "if task_type == \"multiclass\":\n",
    "    # final_preds — матрица (n, C)\n",
    "    submission[CONFIG[\"target_column\"]] = np.argmax(final_preds, axis=1)\n",
    "\n",
    "elif task_type == \"binary\":\n",
    "    # финальный prediction — вероятность класса 1\n",
    "    # если нужен label, заложи порог = 0.5\n",
    "    submission[CONFIG[\"target_column\"]] = (final_preds >= 0.5).astype(int)\n",
    "\n",
    "else:  # regression\n",
    "    submission[CONFIG[\"target_column\"]] = final_preds\n",
    "\n",
    "# Сохраняем\n",
    "sub_path = os.path.join(output_dir, \"submission.csv\")\n",
    "if \"index\" in submission.columns:\n",
    "    submission.drop(columns=\"index\", inplace=True)\n",
    "submission.to_csv(sub_path, index=False)\n",
    "\n",
    "logger.info(f\"✅ Submission file saved: {sub_path}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fcbc2f2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:24:12.532133Z",
     "iopub.status.busy": "2025-11-18T21:24:12.531838Z",
     "iopub.status.idle": "2025-11-18T21:24:12.540711Z",
     "shell.execute_reply": "2025-11-18T21:24:12.539789Z",
     "shell.execute_reply.started": "2025-11-18T21:24:12.532110Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Pawpularity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4128bae22183829d2b5fea10effdb0c3</td>\n",
       "      <td>42.731665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43a2262d7738e3d420d453815151079e</td>\n",
       "      <td>45.943072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4e429cead1848a298432a0acad014c9d</td>\n",
       "      <td>39.435416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n",
       "      <td>40.928411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8f49844c382931444e68dffbe20228f4</td>\n",
       "      <td>36.267191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b03f7041962238a7c9d6537e22f9b017</td>\n",
       "      <td>41.973454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>c978013571258ed6d4637f6e8cc9d6a3</td>\n",
       "      <td>40.598743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e0de453c1bffc20c22b072b34b54e50f</td>\n",
       "      <td>41.769701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Id  Pawpularity\n",
       "0  4128bae22183829d2b5fea10effdb0c3    42.731665\n",
       "1  43a2262d7738e3d420d453815151079e    45.943072\n",
       "2  4e429cead1848a298432a0acad014c9d    39.435416\n",
       "3  80bc3ccafcc51b66303c2c263aa38486    40.928411\n",
       "4  8f49844c382931444e68dffbe20228f4    36.267191\n",
       "5  b03f7041962238a7c9d6537e22f9b017    41.973454\n",
       "6  c978013571258ed6d4637f6e8cc9d6a3    40.598743\n",
       "7  e0de453c1bffc20c22b072b34b54e50f    41.769701"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff6fe35",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-11-18T21:06:00.016488Z",
     "iopub.status.idle": "2025-11-18T21:06:00.016760Z",
     "shell.execute_reply": "2025-11-18T21:06:00.016650Z",
     "shell.execute_reply.started": "2025-11-18T21:06:00.016637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Optuna: CPU-only hyperparameter tuning\n",
    "# ============================\n",
    "import optuna\n",
    "\n",
    "def _metric_for_optuna(y_true, y_pred, task_type: str):\n",
    "    \"\"\"\n",
    "    Внутренняя метрика для Optuna: чем БОЛЬШЕ, тем лучше.\n",
    "    - regression:  -RMSE\n",
    "    - binary/multiclass: accuracy\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "    if task_type == \"regression\":\n",
    "        rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "        return -rmse\n",
    "    else:\n",
    "        if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:\n",
    "            y_hat = np.argmax(y_pred, axis=1)\n",
    "        else:\n",
    "            y_hat = y_pred\n",
    "        acc = accuracy_score(y_true, y_hat)\n",
    "        return acc\n",
    "\n",
    "\n",
    "def tune_all_boostings_optuna(\n",
    "    X,\n",
    "    y,\n",
    "    cat_cols,\n",
    "    CONFIG,\n",
    "    logger,\n",
    "    timeout: int = 3600,\n",
    "    tune_cat: bool = True,\n",
    "    tune_lgb: bool = True,\n",
    "    tune_xgb: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Подбор гиперов CatBoost, LightGBM и XGBoost на CPU с помощью Optuna.\n",
    "\n",
    "    tune_cat / tune_lgb / tune_xgb — флаги, какие модели тюнить.\n",
    "    Например, чтобы тюнить только CatBoost:\n",
    "        tune_cat=True, tune_lgb=False, tune_xgb=False\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import StratifiedKFold, KFold\n",
    "    from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "    import lightgbm as lgb\n",
    "    import xgboost as xgb\n",
    "\n",
    "    task_type = CONFIG.get(\"task_type\", \"regression\")\n",
    "\n",
    "    # ====================\n",
    "    # CV-сплиттер\n",
    "    # ====================\n",
    "    n_splits = CONFIG.get(\"cv_n_splits\", 3)\n",
    "    cv_shuffle = CONFIG.get(\"cv_shuffle\", True)\n",
    "    cv_random_state = CONFIG.get(\"cv_random_state\", 42)\n",
    "    cv_stratified = CONFIG.get(\"cv_stratified\", True)\n",
    "\n",
    "    if task_type == \"regression\" or not cv_stratified:\n",
    "        kf = KFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=cv_shuffle,\n",
    "            random_state=cv_random_state,\n",
    "        )\n",
    "    else:\n",
    "        kf = StratifiedKFold(\n",
    "            n_splits=n_splits,\n",
    "            shuffle=cv_shuffle,\n",
    "            random_state=cv_random_state,\n",
    "        )\n",
    "\n",
    "    # ======================================================\n",
    "    # 1) CatBoost (CPU, с автоматической настройкой loss/metric)\n",
    "    # ======================================================\n",
    "    if tune_cat and CONFIG.get(\"use_catboost\", False):\n",
    "        logger.info(\"🔍 Optuna tuning for CatBoost (CPU)...\")\n",
    "\n",
    "        def objective_cat(trial):\n",
    "            base_params = CONFIG[\"catboost_params\"].copy()\n",
    "\n",
    "            # принудительно на CPU (Optuna не трогает твоё GPU-обучение)\n",
    "            base_params[\"task_type\"] = \"CPU\"\n",
    "            base_params[\"thread_count\"] = 0\n",
    "            base_params[\"verbose\"] = False\n",
    "\n",
    "            # гиперы для тюнинга\n",
    "            params = {\n",
    "                **base_params,\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 4, 10),\n",
    "                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0, log=True),\n",
    "                \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n",
    "                \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "            }\n",
    "\n",
    "            # ----- loss_function / eval_metric под task_type -----\n",
    "            if task_type == \"regression\":\n",
    "                params[\"loss_function\"] = \"MAE\"\n",
    "                params[\"eval_metric\"] = \"MAE\"\n",
    "                ModelCls = CatBoostRegressor\n",
    "            elif task_type == \"binary\":\n",
    "                params[\"loss_function\"] = \"Logloss\"\n",
    "                params[\"eval_metric\"] = \"AUC\"\n",
    "                ModelCls = CatBoostClassifier\n",
    "            elif task_type == \"multiclass\":\n",
    "                params[\"loss_function\"] = \"MultiClass\"\n",
    "                params[\"eval_metric\"] = \"MultiClass\"\n",
    "                ModelCls = CatBoostClassifier\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported task_type '{task_type}' for CatBoost\")\n",
    "\n",
    "            oof_preds = None\n",
    "\n",
    "            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n",
    "                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "                train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n",
    "                valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n",
    "\n",
    "                model = ModelCls(**params)\n",
    "\n",
    "                model.fit(\n",
    "                    train_pool,\n",
    "                    eval_set=valid_pool,\n",
    "                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n",
    "                    verbose=False,\n",
    "                )\n",
    "\n",
    "                if task_type == \"regression\":\n",
    "                    preds = model.predict(X_va)\n",
    "                else:\n",
    "                    proba = model.predict_proba(X_va)\n",
    "                    if task_type == \"binary\":\n",
    "                        preds = proba[:, 1]\n",
    "                    else:\n",
    "                        preds = proba\n",
    "\n",
    "                if oof_preds is None:\n",
    "                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n",
    "                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n",
    "                    else:\n",
    "                        oof_preds = np.zeros(len(y), dtype=float)\n",
    "                oof_preds[va_idx] = preds\n",
    "\n",
    "            score = _metric_for_optuna(y.values, oof_preds, task_type)\n",
    "            return score\n",
    "\n",
    "        study_cat = optuna.create_study(direction=\"maximize\")\n",
    "        study_cat.optimize(objective_cat, timeout=timeout)\n",
    "        logger.info(f\"CatBoost best value: {study_cat.best_value}\")\n",
    "        logger.info(f\"CatBoost best params: {study_cat.best_params}\")\n",
    "\n",
    "        CONFIG[\"catboost_params\"].update({\n",
    "            \"learning_rate\": study_cat.best_params[\"learning_rate\"],\n",
    "            \"depth\": study_cat.best_params[\"depth\"],\n",
    "            \"l2_leaf_reg\": study_cat.best_params[\"l2_leaf_reg\"],\n",
    "            \"bagging_temperature\": study_cat.best_params[\"bagging_temperature\"],\n",
    "            \"border_count\": study_cat.best_params[\"border_count\"],\n",
    "        })\n",
    "\n",
    "    # ======================================================\n",
    "    # 2) LightGBM (CPU, objective/metric по task_type)\n",
    "    # ======================================================\n",
    "    if tune_lgb and CONFIG.get(\"use_lightgbm\", False):\n",
    "        logger.info(\"🔍 Optuna tuning for LightGBM (CPU)...\")\n",
    "\n",
    "        def objective_lgb(trial):\n",
    "            base_params = CONFIG[\"lgb_params\"].copy()\n",
    "\n",
    "            base_params[\"device_type\"] = \"cpu\"\n",
    "            base_params[\"num_threads\"] = 0\n",
    "\n",
    "            if task_type == \"regression\":\n",
    "                base_params[\"objective\"] = \"regression\"\n",
    "                base_params[\"metric\"] = [\"mae\"]\n",
    "            elif task_type == \"binary\":\n",
    "                base_params[\"objective\"] = \"binary\"\n",
    "                base_params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n",
    "            elif task_type == \"multiclass\":\n",
    "                num_classes = len(np.unique(y))\n",
    "                base_params[\"objective\"] = \"multiclass\"\n",
    "                base_params[\"num_class\"] = num_classes\n",
    "                base_params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported task_type '{task_type}' for LightGBM\")\n",
    "\n",
    "            params = {\n",
    "                **base_params,\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n",
    "                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 10),\n",
    "                \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n",
    "                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n",
    "            }\n",
    "\n",
    "            oof_preds = None\n",
    "            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n",
    "                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "                train_set = lgb.Dataset(X_tr, label=y_tr)\n",
    "                valid_set = lgb.Dataset(X_va, label=y_va)\n",
    "\n",
    "                model = lgb.train(\n",
    "                    params,\n",
    "                    train_set,\n",
    "                    num_boost_round=5000,\n",
    "                    valid_sets=[valid_set],\n",
    "                    valid_names=[\"valid\"],\n",
    "                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "\n",
    "                preds = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "\n",
    "                if oof_preds is None:\n",
    "                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n",
    "                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n",
    "                    else:\n",
    "                        oof_preds = np.zeros(len(y), dtype=float)\n",
    "                oof_preds[va_idx] = preds\n",
    "\n",
    "            score = _metric_for_optuna(y.values, oof_preds, task_type)\n",
    "            return score\n",
    "\n",
    "        study_lgb = optuna.create_study(direction=\"maximize\")\n",
    "        study_lgb.optimize(objective_lgb, timeout=timeout)\n",
    "        logger.info(f\"LightGBM best value: {study_lgb.best_value}\")\n",
    "        logger.info(f\"LightGBM best params: {study_lgb.best_params}\")\n",
    "\n",
    "        CONFIG[\"lgb_params\"].update({\n",
    "            \"learning_rate\": study_lgb.best_params[\"learning_rate\"],\n",
    "            \"num_leaves\": study_lgb.best_params[\"num_leaves\"],\n",
    "            \"feature_fraction\": study_lgb.best_params[\"feature_fraction\"],\n",
    "            \"bagging_fraction\": study_lgb.best_params[\"bagging_fraction\"],\n",
    "            \"bagging_freq\": study_lgb.best_params[\"bagging_freq\"],\n",
    "            \"min_data_in_leaf\": study_lgb.best_params[\"min_data_in_leaf\"],\n",
    "            \"lambda_l1\": study_lgb.best_params[\"lambda_l1\"],\n",
    "            \"lambda_l2\": study_lgb.best_params[\"lambda_l2\"],\n",
    "        })\n",
    "\n",
    "    # ======================================================\n",
    "    # 3) XGBoost (CPU, objective/metric по task_type)\n",
    "    # ======================================================\n",
    "    if tune_xgb and CONFIG.get(\"use_xgboost\", False):\n",
    "        logger.info(\"🔍 Optuna tuning for XGBoost (CPU)...\")\n",
    "\n",
    "        def objective_xgb(trial):\n",
    "            base_params = CONFIG[\"xgb_params\"].copy()\n",
    "\n",
    "            base_params[\"device\"] = \"cpu\"\n",
    "            base_params.setdefault(\"tree_method\", \"hist\")\n",
    "            base_params.setdefault(\"nthread\", 0)\n",
    "\n",
    "            if task_type == \"regression\":\n",
    "                base_params[\"objective\"] = \"reg:squarederror\"\n",
    "                base_params[\"eval_metric\"] = \"mae\"\n",
    "            elif task_type == \"binary\":\n",
    "                base_params[\"objective\"] = \"binary:logistic\"\n",
    "                base_params[\"eval_metric\"] = \"auc\"\n",
    "            elif task_type == \"multiclass\":\n",
    "                num_classes = len(np.unique(y))\n",
    "                base_params[\"objective\"] = \"multi:softprob\"\n",
    "                base_params[\"num_class\"] = num_classes\n",
    "                base_params[\"eval_metric\"] = \"mlogloss\"\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported task_type '{task_type}' for XGBoost\")\n",
    "\n",
    "            params = {\n",
    "                **base_params,\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "                \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n",
    "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "                \"lambda\": trial.suggest_float(\"lambda\", 0.0, 10.0),\n",
    "                \"alpha\": trial.suggest_float(\"alpha\", 0.0, 10.0),\n",
    "            }\n",
    "\n",
    "            oof_preds = None\n",
    "            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n",
    "                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "                dtrain = xgb.DMatrix(X_tr, label=y_tr)\n",
    "                dvalid = xgb.DMatrix(X_va, label=y_va)\n",
    "\n",
    "                model = xgb.train(\n",
    "                    params=params,\n",
    "                    dtrain=dtrain,\n",
    "                    num_boost_round=4000,\n",
    "                    evals=[(dvalid, \"valid\")],\n",
    "                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n",
    "                    verbose_eval=False,\n",
    "                )\n",
    "\n",
    "                preds = model.predict(dvalid, iteration_range=(0, model.best_iteration + 1))\n",
    "\n",
    "                if oof_preds is None:\n",
    "                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n",
    "                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n",
    "                    else:\n",
    "                        oof_preds = np.zeros(len(y), dtype=float)\n",
    "                oof_preds[va_idx] = preds\n",
    "\n",
    "            score = _metric_for_optuna(y.values, oof_preds, task_type)\n",
    "            return score\n",
    "\n",
    "        study_xgb = optuna.create_study(direction=\"maximize\")\n",
    "        study_xgb.optimize(objective_xgb, timeout=timeout)\n",
    "        logger.info(f\"XGBoost best value: {study_xgb.best_value}\")\n",
    "        logger.info(f\"XGBoost best params: {study_xgb.best_params}\")\n",
    "\n",
    "        CONFIG[\"xgb_params\"].update({\n",
    "            \"learning_rate\": study_xgb.best_params[\"learning_rate\"],\n",
    "            \"max_depth\": study_xgb.best_params[\"max_depth\"],\n",
    "            \"min_child_weight\": study_xgb.best_params[\"min_child_weight\"],\n",
    "            \"subsample\": study_xgb.best_params[\"subsample\"],\n",
    "            \"colsample_bytree\": study_xgb.best_params[\"colsample_bytree\"],\n",
    "            \"lambda\": study_xgb.best_params[\"lambda\"],\n",
    "            \"alpha\": study_xgb.best_params[\"alpha\"],\n",
    "        })\n",
    "\n",
    "    logger.info(\"✅ Optuna tuning finished. CONFIG обновлён лучшими гиперпараметрами.\")\n",
    "    return CONFIG\n",
    "\n",
    "# Пример вызова ТОЛЬКО для CatBoost:\n",
    "#CONFIG = tune_all_boostings_optuna(X, y, cat_cols, CONFIG, logger, timeout=3600,tune_cat=True, tune_lgb=False, tune_xgb=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa3d34",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 2684322,
     "isSourceIdPinned": false,
     "sourceId": 25383,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
