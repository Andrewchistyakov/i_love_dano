{"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"23dfc36f","cell_type":"markdown","source":"# –í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ —Ä–µ—à–µ–Ω–∏–µ c catboost\n*–û–≥–ª–∞–≤–ª–µ–Ω–∏–µ:*\n1. –ü–æ–ª–Ω—ã–π –ø–∞–π–Ω–ª–∞–π–Ω —Å –∫–∞—Ç–±—É—Å—Ç–æ–º: –∫–∞—Ç–±—É—Å—Ç –∏ –ø–∞–º—è—Ç–∫–∞ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º\n2. –ü–æ–¥–±–æ—Ä –≥–∏–ø–æ–≤\n3. –ò–¥–µ–∏ –¥–ª—è —Ñ–∏—á–µ–π –ø–æ–¥ —Ä–∞–∑–Ω—ã–µ –æ–±–ª–∞—Å—Ç–∏","metadata":{}},{"id":"8fd5ab6e-eb9d-4ec3-98cd-73ded99e9178","cell_type":"code","source":"!pip install catboost","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6f116f96-21e5-4006-a62e-1b9c931bfdab","cell_type":"markdown","source":"# –°—Ç–∞—Ä—Ç–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω","metadata":{}},{"id":"793b5b14-0b47-46a9-a0a1-3f5c01389646","cell_type":"code","source":"import pandas as pd\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"17b0a199","cell_type":"markdown","source":"# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –∫–∞—Ç–±—É—Å–æ–º","metadata":{}},{"id":"682b2854","cell_type":"code","source":"!pip install catboost","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting catboost\n","  Using cached catboost-1.2.8-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Collecting graphviz (from catboost)\n","  Using cached graphviz-0.21-py3-none-any.whl.metadata (12 kB)\n","Collecting matplotlib (from catboost)\n","  Using cached matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Collecting numpy<3.0,>=1.16.0 (from catboost)\n","  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n","Collecting pandas>=0.24 (from catboost)\n","  Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n","Collecting scipy (from catboost)\n","  Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","Collecting plotly (from catboost)\n","  Using cached plotly-6.4.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Collecting pytz>=2020.1 (from pandas>=0.24->catboost)\n","  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n","Collecting tzdata>=2022.7 (from pandas>=0.24->catboost)\n","  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n","Collecting contourpy>=1.0.1 (from matplotlib->catboost)\n","  Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n","Collecting cycler>=0.10 (from matplotlib->catboost)\n","  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n","Collecting fonttools>=4.22.0 (from matplotlib->catboost)\n","  Using cached fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (112 kB)\n","Collecting kiwisolver>=1.3.1 (from matplotlib->catboost)\n","  Using cached kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.3 kB)\n","Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->catboost) (25.0)\n","Collecting pillow>=8 (from matplotlib->catboost)\n","  Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n","Collecting pyparsing>=3 (from matplotlib->catboost)\n","  Using cached pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n","Collecting narwhals>=1.15.1 (from plotly->catboost)\n","  Using cached narwhals-2.11.0-py3-none-any.whl.metadata (11 kB)\n","Using cached catboost-1.2.8-cp310-cp310-manylinux2014_x86_64.whl (99.2 MB)\n","Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n","Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n","Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n","Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n","Using cached graphviz-0.21-py3-none-any.whl (47 kB)\n","Using cached matplotlib-3.10.7-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n","Using cached contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n","Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n","Using cached fonttools-4.60.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n","Using cached kiwisolver-1.4.9-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n","Using cached pillow-12.0.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n","Using cached pyparsing-3.2.5-py3-none-any.whl (113 kB)\n","Using cached plotly-6.4.0-py3-none-any.whl (9.9 MB)\n","Using cached narwhals-2.11.0-py3-none-any.whl (423 kB)\n","Using cached scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n","Installing collected packages: pytz, tzdata, pyparsing, pillow, numpy, narwhals, kiwisolver, graphviz, fonttools, cycler, scipy, plotly, pandas, contourpy, matplotlib, catboost\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16/16\u001b[0m [catboost]/16\u001b[0m [catboost]b]\n","\u001b[1A\u001b[2KSuccessfully installed catboost-1.2.8 contourpy-1.3.2 cycler-0.12.1 fonttools-4.60.1 graphviz-0.21 kiwisolver-1.4.9 matplotlib-3.10.7 narwhals-2.11.0 numpy-2.2.6 pandas-2.3.3 pillow-12.0.0 plotly-6.4.0 pyparsing-3.2.5 pytz-2025.2 scipy-1.15.3 tzdata-2025.2\n"]}],"execution_count":1},{"id":"58ba028e","cell_type":"code","source":"pip install scikit-learn","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting scikit-learn\n","  Using cached scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n","Requirement already satisfied: numpy>=1.22.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n","Requirement already satisfied: scipy>=1.8.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n","Collecting joblib>=1.2.0 (from scikit-learn)\n","  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n","Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n","  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n","Using cached scikit_learn-1.7.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n","Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n","Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n","Installing collected packages: threadpoolctl, joblib, scikit-learn\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [scikit-learn][0m [scikit-learn]\n","\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.2 scikit-learn-1.7.2 threadpoolctl-3.6.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"execution_count":5},{"id":"62ab69e8","cell_type":"code","source":"from catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd","metadata":{},"outputs":[],"execution_count":6},{"id":"df057aa0","cell_type":"code","source":"#–†–ï–ì–†–ï–°–°–ò–Ø\ndata_path = '–≤—Å—Ç–∞–≤–∏—Ç—å'\ntarget = '–≤—Å—Ç–∞–≤–∏—Ç—å'\n# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\ndata = pd.read_csv(data_path)\n\ntext_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n\nsafe_cat = []\nremove_text = []\n\nfor col in text_cols:\n    if data[col].str.len().mean() > 50:   # –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã ‚Üí —É–¥–∞–ª—è–µ–º\n        remove_text.append(col)\n    else:\n        safe_cat.append(col)\n\nprint(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", safe_cat)\nprint(\"–£–¥–∞–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", remove_text)\n\ndata = data.drop(columns=remove_text)\n\ncat_features = safe_cat\n\n# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\nX = data.drop(target, axis=1)\ny = data[target]\n\n# 3. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\ntrain_pool = Pool(X_train, y_train, cat_features=cat_features)\ntest_pool = Pool(X_test, y_test, cat_features=cat_features)\n\n# 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\nmodel = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    verbose=100\n)\n\n# 6. –û–±—É—á–µ–Ω–∏–µ\nmodel.fit(train_pool, eval_set=test_pool, use_best_model=True)\n\n# 7. –û—Ü–µ–Ω–∫–∞\npreds = model.predict(X_test)\nmse = mean_squared_error(y_test, preds)\nprint(\"RMSE:\", mse ** 0.5)\n\n# 8. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\nmodel.save_model(\"catboost_regressor.cbm\")\npreds = model.predict(X_test)\n\nsubmission = pd.DataFrame({\n    \"id\": test[\"id\"],\n    \"target\": preds\n})\nsubmission.to_csv(\"submission_reg.csv\", index=False)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"288d5576","cell_type":"code","source":"#–ë–ò–ù–ê–†–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport pandas as pd","metadata":{},"outputs":[],"execution_count":7},{"id":"b1f3177a","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom catboost import CatBoostClassifier, Pool\n\n# ===========================\n# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n# ===========================\n\ndata_path = '–≤—Å—Ç–∞–≤–∏—Ç—å'\ntarget = '–≤—Å—Ç–∞–≤–∏—Ç—å'\n\ndata = pd.read_csv(data_path)\n\n# –í—ã–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã\ntext_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n\nsafe_cat = []\nremove_text = []\n\nfor col in text_cols:\n    if data[col].str.len().mean() > 50:   # –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã ‚Üí —É–¥–∞–ª—è–µ–º\n        remove_text.append(col)\n    else:\n        safe_cat.append(col)\n\nprint(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", safe_cat)\nprint(\"–£–¥–∞–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", remove_text)\n\n# –£–¥–∞–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\ndata = data.drop(columns=remove_text)\n\ncat_features = safe_cat\n\n# ===========================\n# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤/—Ü–µ–ª–∏\n# ===========================\n\nX = data.drop(target, axis=1)\ny = data[target]\n\n# ===========================\n# 3. Train/Test Split\n# ===========================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\ntrain_pool = Pool(X_train, y_train, cat_features=cat_features)\ntest_pool = Pool(X_test, y_test, cat_features=cat_features)\n\n# ===========================\n# 4. –ú–æ–¥–µ–ª—å (CatBoostClassifier)\n# ===========================\n\nmodel = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='Logloss',   # –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n    eval_metric='AUC',         # –º–æ–∂–Ω–æ 'Accuracy'\n    verbose=100\n)\n\n# ===========================\n# 5. –û–±—É—á–µ–Ω–∏–µ\n# ===========================\n\nmodel.fit(train_pool, eval_set=test_pool, use_best_model=True)\n\n# ===========================\n# 6. –û—Ü–µ–Ω–∫–∞\n# ===========================\n\npreds_proba = model.predict_proba(X_test)[:, 1]   # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\npreds = (preds_proba >= 0.5).astype(int)          # –∫–ª–∞—Å—Å—ã\n\nacc = accuracy_score(y_test, preds)\nauc = roc_auc_score(y_test, preds_proba)\n\nprint(\"Accuracy:\", acc)\nprint(\"AUC:\", auc)\n\n# ===========================\n# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n# ===========================\n\nmodel.save_model(\"catboost_binary_classifier.cbm\")\n\n# ===========================\n# 8. –°–∞–±–º–∏—Ç\n# ===========================\n\n# –ü—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –µ—Å—Ç—å test.csv:\n# submission = pd.DataFrame({\n#     \"id\": test[\"id\"],\n#     \"target\": model.predict_proba(test)[:, 1]\n# })\n# submission.to_csv(\"submission_binary.csv\", index=False)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"b55263fe","cell_type":"code","source":"#–ú–£–õ–¨–¢–ò–ö–õ–ê–°–°–û–í–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport pandas as pd","metadata":{},"outputs":[],"execution_count":8},{"id":"425b9159-157f-4132-9b2f-815c12ea7b84","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom catboost import CatBoostClassifier, Pool\n\n# ===========================\n# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n# ===========================\n\ndata_path = '–≤—Å—Ç–∞–≤–∏—Ç—å'\ntarget = '–≤—Å—Ç–∞–≤–∏—Ç—å'\n\ndata = pd.read_csv(data_path)\n\n# –í—ã–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã\ntext_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n\nsafe_cat = []\nremove_text = []\n\nfor col in text_cols:\n    if data[col].str.len().mean() > 50:   # –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã ‚Üí —É–¥–∞–ª—è–µ–º\n        remove_text.append(col)\n    else:\n        safe_cat.append(col)\n\nprint(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", safe_cat)\nprint(\"–£–¥–∞–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", remove_text)\n\n# –£–¥–∞–ª—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\ndata = data.drop(columns=remove_text)\n\ncat_features = safe_cat\n\n# ===========================\n# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤/—Ü–µ–ª–∏\n# ===========================\n\nX = data.drop(target, axis=1)\ny = data[target]\n\n# ===========================\n# 3. Train/Test Split\n# ===========================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\ntrain_pool = Pool(X_train, y_train, cat_features=cat_features)\ntest_pool = Pool(X_test, y_test, cat_features=cat_features)\n\n# ===========================\n# 4. –ú–æ–¥–µ–ª—å (CatBoostClassifier)\n# ===========================\n\nmodel = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='MultiClass',   # –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n    eval_metric='MultiClass',\n    verbose=100\n)\n\n# ===========================\n# 5. –û–±—É—á–µ–Ω–∏–µ\n# ===========================\n\nmodel.fit(train_pool, eval_set=test_pool, use_best_model=True)\n\n# ===========================\n# 6. –û—Ü–µ–Ω–∫–∞\n# ===========================\n\npreds = model.predict(X_test)          # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Å—ã\npreds = preds.reshape(-1)              # CatBoost –¥–∞—ë—Ç —Å—Ç–æ–ª–±–µ—Ü ‚Äî —Ä–∞—Å–ø—Ä—è–º–ª—è–µ–º\n\nacc = accuracy_score(y_test, preds)\n\nprint(\"Accuracy:\", acc)\n\n# ===========================\n# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n# ===========================\n\nmodel.save_model(\"catboost_multiclass.cbm\")\n\n# ===========================\n# 8. –°–∞–±–º–∏—Ç\n# ===========================\n\n# –ü—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –µ—Å—Ç—å test.csv:\n# submission = pd.DataFrame({\n#     \"id\": test[\"id\"],\n#     \"target\": model.predict(test)\n# })\n# submission.to_csv(\"submission_multiclass.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"19383141","cell_type":"code","source":"#–†–ê–ù–ñ–ò–†–û–í–ê–ù–ò–ï\nfrom catboost import CatBoostRanker, Pool\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","metadata":{},"outputs":[],"execution_count":null},{"id":"1707bedd","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRanker, Pool\nfrom sklearn.metrics import ndcg_score\n\n# ===========================\n# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n# ===========================\n\ndata_path = '–≤—Å—Ç–∞–≤–∏—Ç—å'\ntarget = '–≤—Å—Ç–∞–≤–∏—Ç—å'        # —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å\ngroup_col = 'group_id'     # –ì–õ–ê–í–ù–û–ï –ø–æ–ª–µ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è\n\ndata = pd.read_csv(data_path)\n\n# ===========================\n# 1.1. –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n# ===========================\n\ntext_cols = data.select_dtypes(include=[\"object\"]).columns.tolist()\n\nsafe_cat = []\nremove_text = []\n\nfor col in text_cols:\n    if data[col].str.len().mean() > 50:   # –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã ‚Üí —É–¥–∞–ª—è–µ–º\n        remove_text.append(col)\n    else:\n        safe_cat.append(col)\n\nprint(\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\", safe_cat)\nprint(\"–£–¥–∞–ª—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–µ:\", remove_text)\n\ndata = data.drop(columns=remove_text)\n\ncat_features = safe_cat\n\n# ===========================\n# 2. –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ü–µ–ª–∏\n# ===========================\n\nX = data.drop([target], axis=1)\ny = data[target]\n\n# ===========================\n# 3. Train/Test Split\n# ===========================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=X[group_col]\n)\n\n# –í–∞–∂–Ω–æ: CatBoost —Ç—Ä–µ–±—É–µ—Ç –º–∞—Å—Å–∏–≤ —Ä–∞–∑–º–µ—Ä–∞ –≥—Ä—É–ø–ø\ntrain_group = X_train[group_col].values\ntest_group = X_test[group_col].values\n\n# –£–¥–∞–ª—è–µ–º group_id –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–æ–Ω –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ñ–∏—á–µ–π)\nX_train = X_train.drop(columns=[group_col])\nX_test = X_test.drop(columns=[group_col])\n\ntrain_pool = Pool(\n    X_train, y_train,\n    group_id=train_group,\n    cat_features=cat_features\n)\n\ntest_pool = Pool(\n    X_test, y_test,\n    group_id=test_group,\n    cat_features=cat_features\n)\n\n# ===========================\n# 4. –ú–æ–¥–µ–ª—å CatBoostRanker\n# ===========================\n\nmodel = CatBoostRanker(\n    iterations=1500,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='YetiRank',   # –ª—É—á—à–∏–π LTR –ª–æ—Å—Å\n    eval_metric='NDCG:top=10',\n    verbose=100\n)\n\n# ===========================\n# 5. –û–±—É—á–µ–Ω–∏–µ\n# ===========================\n\nmodel.fit(train_pool, eval_set=test_pool, use_best_model=True)\n\n# ===========================\n# 6. –û—Ü–µ–Ω–∫–∞\n# ===========================\n\npreds = model.predict(X_test)\n\n# –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –ø–æ group_id\ntest_df = X_test.copy()\ntest_df[\"target\"] = y_test\ntest_df[\"pred\"] = preds\ntest_df[\"group\"] = test_group\n\nndcgs = []\nfor g, df_g in test_df.groupby(\"group\"):\n    if df_g[\"target\"].nunique() > 1:\n        ndcgs.append(\n            ndcg_score([df_g[\"target\"].values], [df_g[\"pred\"].values], k=10)\n        )\n\nprint(\"–°—Ä–µ–¥–Ω–∏–π NDCG@10:\", sum(ndcgs) / len(ndcgs))\n\n# ===========================\n# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n# ===========================\n\nmodel.save_model(\"catboost_ranker.cbm\")\n\n# ===========================\n# 8. –°–∞–±–º–∏—Ç (–ø—Ä–∏–º–µ—Ä)\n# ===========================\n\n# submission = pd.DataFrame({\n#     \"group_id\": test_group,\n#     \"score\": preds\n# })\n# submission.to_csv(\"submission_rank.csv\", index=False)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"857e25c8","cell_type":"markdown","source":"–ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n- –ß–µ–º –º–µ–Ω–µ–µ –≥–ª—É–±–æ–∫–∞—è –º–æ–¥–µ–ª—å, —Ç–µ–º –±–æ–ª—å—à–µ iterations\n\niterations –∏ learning_rate –≤—Å–µ–≥–¥–∞ –∏–¥—É—Ç –≤ –ø–∞—Ä–µ:\n- –º–µ–Ω—å—à–µ learning_rate ‚Üí –Ω—É–∂–Ω–æ –±–æ–ª—å—à–µ iterations.\n- –±–æ–ª—å—à–µ learning_rate ‚Üí –Ω—É–∂–Ω–æ –º–µ–Ω—å—à–µ iterations\n\n–ö–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏—Ç—å (depth=8‚Äì10):\n- –î–∞–Ω–Ω—ã–µ —Å–ª–æ–∂–Ω—ã–µ, –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏, –º–Ω–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –±–æ–ª—å—à–æ–π ‚Üí —Ä–∏—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–µ–Ω—å—à–µ.\n–ö–æ–≥–¥–∞ —É–º–µ–Ω—å—à–∏—Ç—å (depth=4‚Äì6):\n- –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö.\n- –ü—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–æ—Å—Ç—ã–µ –∏–ª–∏ —É–∂–µ —Ö–æ—Ä–æ—à–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ feature engineering).\n- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.\n\nl2_leaf_reg ‚Äî L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è (—à—Ç—Ä–∞—Ñ –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏)\n–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:\n–°–≥–ª–∞–∂–∏–≤–∞–µ—Ç –≤–µ—Å–∞, –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º.\n–ö–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10, 20, 50):\n- –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è.\n- –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –Ω–µ–±–æ–ª—å—à–æ–π –∏–ª–∏ —à—É–º–Ω—ã–π.\n–ö–æ–≥–¥–∞ —É–º–µ–Ω—å—à–∏—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1, 2, 3):\n-–ú–æ–¥–µ–ª—å –Ω–µ–¥–æ–æ–±—É—á–∞–µ—Ç—Å—è.\n- –î–∞–Ω–Ω—ã–µ –±–æ–ª—å—à–∏–µ –∏ \"—á–∏—Å—Ç—ã–µ\".\n\nbagging_temperature ‚Äî —Å—Ç–µ–ø–µ–Ω—å —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ (–∞–Ω–∞–ª–æ–≥ subsample)\n–ß—Ç–æ –¥–µ–ª–∞–µ—Ç:\n–î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç–æ—Ö–∞—Å—Ç–∏—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏.\n–ö–æ–≥–¥–∞ —É–≤–µ–ª–∏—á–∏—Ç—å (1‚Äì2):\n- –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ ‚Üí –¥–æ–±–∞–≤–ª—è–µ–º –±–æ–ª—å—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏.\n- –£–ª—É—á—à–∞–µ—Ç –æ–±–æ–±—â–∞—é—â—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å.\n–ö–æ–≥–¥–∞ —É–º–µ–Ω—å—à–∏—Ç—å (0‚Äì0.5):\n- –ù–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ ‚Üí —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏—é, –¥–µ–ª–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å—Ç–∞–±–∏–ª—å–Ω–µ–µ.\n\ngrow_policy\n'SymmetricTree' ‚Äî —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è, –±—ã—Å—Ç—Ä–æ).\n'Depthwise' –∏–ª–∏ 'Lossguide' ‚Äî –≥–∏–±—á–µ, –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ª—É—á—à–µ –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.\n–ö–æ–≥–¥–∞ –º–µ–Ω—è—Ç—å:\n- 'SymmetricTree' —Ö–æ—Ä–æ—à–æ –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á.\n- 'Lossguide' –ø–æ–ª–µ–∑–µ–Ω, –µ—Å–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ > 1000.\n\n–ü—Ä–∏–º–µ—Ä—ã –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π\n–°–∏—Ç—É–∞—Ü–∏—è\t\n–ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–æ–±—É—á–∞–µ—Ç—Å—è\t‚Üì depth, ‚Üì learning_rate, ‚Üë l2_leaf_reg, ‚Üë bagging_temperature\n–ú–æ–¥–µ–ª—å –Ω–µ–¥–æ–æ–±—É—á–∞–µ—Ç—Å—è\t‚Üë depth, ‚Üë iterations, ‚Üì l2_leaf_reg\n–ú–æ–¥–µ–ª—å –æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ –æ–±—É—á–∞–µ—Ç—Å—è\t‚Üë learning_rate, ‚Üì iterations, ‚Üì depth\n–ú–Ω–æ–≥–æ —à—É–º–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö\t‚Üë l2_leaf_reg, ‚Üì depth, ‚Üë bagging_temperature\n–ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\t‚Üì one_hot_max_size, ‚Üë l2_leaf_reg","metadata":{}},{"id":"71c4ee41-81a0-4de0-9956-b93334c18cfc","cell_type":"markdown","source":"# –ü–æ–¥–±–æ—Ä –≥–∏–ø–æ–≤","metadata":{}},{"id":"5471bff9","cell_type":"code","source":"# --Grid search--\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nmodel = CatBoostClassifier(\n    iterations=500,\n    cat_features=cat_features,\n    verbose=0\n)\n\nparam_grid = {\n    'depth': [4, 6, 8],\n    'learning_rate': [0.03, 0.05, 0.1],\n    'l2_leaf_reg': [1, 3, 5, 10],\n    'bagging_temperature': [0, 1, 2]\n}\n\ngrid = GridSearchCV(model, param_grid, scoring='roc_auc', cv=3, n_jobs=-1)\ngrid.fit(X, y)\nprint(\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\", grid.best_params_)\n\n# --Randomized Search--\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom catboost import CatBoostClassifier\nfrom scipy.stats import randint, uniform\n\nmodel = CatBoostClassifier(\n    iterations=1000,\n    cat_features=cat_features,\n    verbose=0\n)\n\nparam_dist = {\n    'depth': randint(4, 10),\n    'learning_rate': uniform(0.01, 0.1),\n    'l2_leaf_reg': uniform(1, 10),\n    'bagging_temperature': uniform(0, 2),\n}\n\nsearch = RandomizedSearchCV(\n    model, \n    param_distributions=param_dist,\n    n_iter=30,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª—É—á–∞–π–Ω—ã—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π\n    scoring='roc_auc',\n    cv=3,\n    random_state=42,\n    n_jobs=-1\n)\n\nsearch.fit(X, y)\nprint(\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\", search.best_params_)\n\n# --Grid search –≤ CatBoost\nfrom catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(\n    iterations=500,\n    loss_function='Logloss',\n    eval_metric='AUC',\n    cat_features=cat_features,\n    verbose=False\n)\n\ngrid = {\n    'depth': [4, 6, 8],\n    'learning_rate': [0.03, 0.05, 0.1],\n    'l2_leaf_reg': [1, 3, 5, 10]\n}\n\ngrid_result = model.grid_search(grid, X=X, y=y, cv=3, shuffle=True, stratified=True)\nprint(grid_result['params'])\n\n# --Optuna--\n!pip install optuna\nimport optuna\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    params = {\n        'iterations': 1000,\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n        'depth': trial.suggest_int('depth', 4, 10),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 15, log=True),\n        'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 2),\n        'random_strength': trial.suggest_float('random_strength', 0, 5),\n        'border_count': trial.suggest_int('border_count', 32, 256),\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': 0\n    }\n\n    model = CatBoostClassifier(**params)\n    score = cross_val_score(model, X, y, cv=3, scoring='roc_auc', n_jobs=-1)\n    return score.mean()\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=30)\n\nprint(\"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\", study.best_params)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"2e43d821","cell_type":"code","source":"# --Early stopping--\nmodel = CatBoostClassifier(\n    iterations=2000,              # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π\n    learning_rate=0.05,\n    depth=8,\n    loss_function='Logloss',\n    eval_metric='AUC',\n    od_type='Iter',               # —Ç–∏–ø –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ (–ø–æ —á–∏—Å–ª—É –∏—Ç–µ—Ä–∞—Ü–∏–π)\n    od_wait=50,                   # –µ—Å–ª–∏ 50 –∏—Ç–µ—Ä–∞—Ü–∏–π –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è ‚Äî —Å—Ç–æ–ø\n    random_seed=42,\n    verbose=100\n)\n# –ü–µ—Ä–µ–¥–∞—ë–º eval_set (–≤–∞–ª–∏–¥–∞—Ü–∏—é)\nmodel.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"ebd28f88","cell_type":"code","source":"# --Cross Validation--\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom catboost import CatBoostClassifier\n\nmodel = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='Logloss',\n    cat_features=cat_features,\n    verbose=0\n)\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=kfold, scoring='roc_auc', n_jobs=-1)\n\nprint(\"AUC –ø–æ —Ñ–æ–ª–¥–∞–º:\", scores)\nprint(\"–°—Ä–µ–¥–Ω–∏–π AUC:\", scores.mean())\n\n# —Ä–µ–≥—Ä–µ—Å—Å–∏—è\nfrom catboost import CatBoostRegressor, Pool, cv\n\nparams = {\n    'iterations': 1500,\n    'learning_rate': 0.03,\n    'depth': 6,\n    'loss_function': 'RMSE',\n    'eval_metric': 'RMSE',\n    'od_type': 'Iter',\n    'od_wait': 50,\n    'verbose': 100\n}\n\ntrain_pool = Pool(X, y, cat_features=cat_features)\n\ncv_data = cv(\n    pool=train_pool,\n    params=params,\n    fold_count=5,\n    shuffle=True,\n    partition_random_seed=42\n)\n\nprint(\"‚úÖ Best RMSE:\", cv_data[\"test-RMSE-mean\"].min())\nprint(\"‚úÖ Best iteration:\", cv_data[\"test-RMSE-mean\"].idxmin())","metadata":{},"outputs":[],"execution_count":null},{"id":"a6875330-ff92-47ac-8a98-735f80ba08c2","cell_type":"markdown","source":"# Feature Engineering","metadata":{}},{"id":"29d7d0fb","cell_type":"code","source":"#—Ä–∞–±–æ—Ç–∞ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ \nX = X.fillna(X.median(numeric_only=True))\n#—Ä–∞–±–æ—Ç–∞ —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\n#—Å–æ–∑–¥–∞–Ω–∏–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π\n#–¥–∞—Ç–∞ –∏ –≤—Ä–µ–º—è \nX[\"date\"] = pd.to_datetime(X[\"date\"])\nX[\"year\"] = X[\"date\"].dt.year\nX[\"month\"] = X[\"date\"].dt.month\nX[\"day\"] = X[\"date\"].dt.day\nX[\"dayofweek\"] = X[\"date\"].dt.dayofweek\nX[\"is_weekend\"] = X[\"dayofweek\"].isin([5, 6]).astype(int)\nX[\"hour\"] = X[\"date\"].dt.hour\nX[\"month_sin\"] = np.sin(2 * np.pi * X[\"month\"]/12)\nX[\"month_cos\"] = np.cos(2 * np.pi * X[\"month\"]/12)\n#–≥—Ä—É–ø–ø–æ–≤—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã\ngrouped = X.groupby('customer_id')['amount'].agg(['mean', 'sum', 'std', 'count']).reset_index()\ngrouped.columns = ['customer_id', 'amount_mean', 'amount_sum', 'amount_std', 'amount_count']\nX = X.merge(grouped, on='customer_id', how='left')\n#–æ—Ç–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n\n#–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\nfor col in cat_features:\n    freqs = X[col].value_counts()\n    rare = freqs[freqs < 50].index\n    X[col] = X[col].replace(rare, 'rare')","metadata":{},"outputs":[],"execution_count":null},{"id":"2fd6d1a6","cell_type":"markdown","source":"# –ò–¥–µ–∏ –¥–ª—è —Ñ–∏—á–µ–π –≤ —É–∑–∫–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö\n    1. –ö—Ä–µ–¥–∏—Ç–Ω—ã–π —Å–∫–æ—Ä–∏–Ω–≥ / –ë–∞–Ω–∫–æ–≤—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n–¶–µ–ª—å: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –¥–µ—Ñ–æ–ª—Ç / –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ–≤–æ–∑–≤—Ä–∞—Ç–∞.\n–ò–¥–µ–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\nincome_to_loan_ratio ‚Äî –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–∞ –∫ —Å—É–º–º–µ –∫—Ä–µ–¥–∏—Ç–∞\nage_group ‚Äî –≤–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\nemployment_length ‚Äî —Å—Ç–∞–∂\ncredit_utilization ‚Äî —Ç–µ–∫—É—â–∏–µ –¥–æ–ª–≥–∏ / –∫—Ä–µ–¥–∏—Ç–Ω—ã–π –ª–∏–º–∏—Ç\ndebt_to_income ‚Äî –¥–æ–ª–≥–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞\nprevious_loans_count ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—à–ª—ã—Ö –∫—Ä–µ–¥–∏—Ç–æ–≤\navg_payment_delay ‚Äî —Å—Ä–µ–¥–Ω—è—è –∑–∞–¥–µ—Ä–∂–∫–∞ –ø–ª–∞—Ç–µ–∂–µ–π\nnum_open_accounts ‚Äî –∞–∫—Ç–∏–≤–Ω—ã–µ —Å—á–µ—Ç–∞\nhas_mortgage, has_car_loan ‚Äî –±–∏–Ω–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n","metadata":{}},{"id":"f16e1bd8-6caa-4dcc-b3fb-3468c2cbbcbd","cell_type":"code","source":"X['income_to_loan'] = X['income'] / (X['loan_amount'] + 1)\nX['debt_to_income'] = X['total_debt'] / (X['income'] + 1)\nX['credit_utilization'] = X['used_credit'] / (X['credit_limit'] + 1)\nX['employment_years'] = (pd.Timestamp('today').year - X['employment_start_year'])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c4eda107-ec5c-4ab3-9ae0-37dbf2da31b8","cell_type":"markdown","source":"\n    2. –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≥–µ–æ-–¥–∞–Ω–Ω—ã–µ)\n–ü—Ä–∏–º–µ–Ω–∏–º–æ –∫ –¥–æ—Å—Ç–∞–≤–∫–∞–º, –ª–æ–≥–∏—Å—Ç–∏–∫–µ, –≥–µ–æ–∞–Ω–∞–ª–∏—Ç–∏–∫–µ.\n–ò–¥–µ–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\ndistance ‚Äî –≥–µ–æ-–¥–∏—Å—Ç–∞–Ω—Ü–∏—è (Haversine formula)\nregion, city, postal_code\ndistance_to_center\navg_order_distance_by_user","metadata":{}},{"id":"0971b557-1888-482a-875d-a6b96a99ee75","cell_type":"code","source":"import numpy as np\n\ndef haversine(lat1, lon1, lat2, lon2):\n    R = 6371  # –∫–º\n    d_lat = np.radians(lat2 - lat1)\n    d_lon = np.radians(lon2 - lon1)\n    a = np.sin(d_lat / 2)**2 + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * np.sin(d_lon / 2)**2\n    return 2 * R * np.arcsin(np.sqrt(a))\n\nX[\"distance_km\"] = haversine(X[\"pickup_lat\"], X[\"pickup_lon\"], X[\"drop_lat\"], X[\"drop_lon\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d538cf8-3a6a-4f09-8dc0-da0e548e7cfa","cell_type":"markdown","source":"\n    3. –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã\n–î–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ø—Ä–æ—Å–∞, –ø—Ä–æ–¥–∞–∂, —Ç—Ä–∞—Ñ–∏–∫–∞, –∫–ª–∏–∫–æ–≤ –∏ —Ç.–¥.\n–ò–¥–µ–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\n–ª–∞–≥–∏: y(t-1), y(t-7)\n—Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ: rolling_mean, rolling_std\n—Ç—Ä–µ–Ω–¥ (diff, pct_change)\n–ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –¥–Ω–∏, —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å","metadata":{}},{"id":"8dd1f765-5602-4c80-b89c-f0a9ab58ad35","cell_type":"code","source":"X[\"lag_1\"] = X[\"target\"].shift(1)\nX[\"lag_7\"] = X[\"target\"].shift(7)\nX[\"rolling_mean_7\"] = X[\"target\"].shift(1).rolling(7).mean()\nX[\"rolling_std_7\"] = X[\"target\"].shift(1).rolling(7).std()\nX[\"diff\"] = X[\"target\"].diff()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e41b17c3-f468-443a-972e-1a1ceb76385c","cell_type":"markdown","source":"    4. E-commerce / Retail\n–¶–µ–ª—å: –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å –ø–æ–∫—É–ø–∫–∏, –≤—ã—Ä—É—á–∫—É, –æ—Ç—Ç–æ–∫ –∫–ª–∏–µ–Ω—Ç–∞.\n–ò–¥–µ–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\navg_purchase_value\ntotal_orders, unique_products\ndays_since_last_purchase\navg_time_between_purchases\nis_weekend_purchase","metadata":{}},{"id":"f43376d2-c2ee-44fb-a67a-e656a89c0239","cell_type":"code","source":"X['days_since_last_purchase'] = (X['current_date'] - X['last_purchase_date']).dt.days\nX['avg_order_value'] = X['total_spent'] / (X['orders_count'] + 1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c8fd4000-3325-4d66-b8ab-9666e47ddeae","cell_type":"markdown","source":"    5. –õ–æ–≥–∏—Å—Ç–∏–∫–∞ / –¢—Ä–∞–Ω—Å–ø–æ—Ä—Ç\n–í—Ä–µ–º—è –ø—Ä–∏–±—ã—Ç–∏—è, –º–∞—Ä—à—Ä—É—Ç—ã, –∑–∞–¥–µ—Ä–∂–∫–∏.\n–ò–¥–µ–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n\nroute_length_km\navg_speed = distance / duration\ndelay = actual_arrival - scheduled_arrival\nis_rush_hour, is_weekend","metadata":{}},{"id":"7a72f978-9fd0-41b8-afeb-d21cbd1826fd","cell_type":"code","source":"X[\"avg_speed\"] = X[\"distance_km\"] / (X[\"duration_min\"]/60)\nX[\"delay_min\"] = (X[\"actual_arrival\"] - X[\"scheduled_arrival\"]).dt.total_seconds() / 60","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b6b83ec2-8a04-493c-88e6-fefb4f1c4870","cell_type":"markdown","source":"    6. –ë–∞–∑–æ–≤—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏","metadata":{}},{"id":"0466dd65-264e-47b2-8ff9-1c1afbbb9274","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\n\ndef basic_text_features(df, text_col):\n    df[f\"{text_col}_len\"] = df[text_col].astype(str).apply(len)\n    df[f\"{text_col}_word_count\"] = df[text_col].astype(str).apply(lambda x: len(x.split()))\n    df[f\"{text_col}_unique_words\"] = df[text_col].astype(str).apply(lambda x: len(set(x.split())))\n    df[f\"{text_col}_punct_count\"] = df[text_col].astype(str).apply(lambda x: len(re.findall(r'[^\\w\\s]', x)))\n    df[f\"{text_col}_caps_ratio\"] = df[text_col].astype(str).apply(\n        lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1)\n    )\n    df[f\"{text_col}_avg_word_len\"] = df[text_col].astype(str).apply(\n        lambda x: np.mean([len(w) for w in x.split()]) if len(x.split()) > 0 else 0\n    )\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"849f5125-10f7-4179-b10c-41a296e55712","cell_type":"markdown","source":"    7. –ü—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞","metadata":{}},{"id":"55164145-34d4-448b-b319-dbd445b770ae","cell_type":"code","source":"pip install textblob\nfrom textblob import TextBlob\n\ndef sentiment_features(df, text_col):\n    df[f\"{text_col}_polarity\"] = df[text_col].astype(str).apply(lambda x: TextBlob(x).sentiment.polarity)\n    df[f\"{text_col}_subjectivity\"] = df[text_col].astype(str).apply(lambda x: TextBlob(x).sentiment.subjectivity)\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c8b08f3e-95e8-4354-86dd-c29f101b4948","cell_type":"markdown","source":"    8. –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ (topic modeling ‚Äî LDA)\n–¢–∞–∫–∏–µ —Ñ–∏—á–∏ —Ö–æ—Ä–æ—à–æ –æ—Ç—Ä–∞–∂–∞—é—Ç \"–æ —á—ë–º —Ç–µ–∫—Å—Ç\", –æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.","metadata":{}},{"id":"6ecdaf07-ad67-4340-a5e5-9808681528f0","cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef topic_features(df, text_col, n_topics=5, max_features=1000):\n    cv = CountVectorizer(max_features=max_features, stop_words='english')\n    bow = cv.fit_transform(df[text_col].fillna(''))\n    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n    topics = lda.fit_transform(bow)\n    topic_df = pd.DataFrame(topics, columns=[f\"topic_{i}\" for i in range(n_topics)])\n    return pd.concat([df.reset_index(drop=True), topic_df.reset_index(drop=True)], axis=1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"202b4408-0ad3-417c-a6df-fbaf47f6bb51","cell_type":"markdown","source":"    9. –ò–¥–µ–∏ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∏—á –ø–æ —Ç–∏–ø–∞–º –∑–∞–¥–∞—á\n\n–¢–∏–ø –∑–∞–¥–∞—á–∏\t                       –ü–æ–ª–µ–∑–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n–û—Ç–∑—ã–≤—ã (Sentiment)\t               –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞, polarity, subjectivity, TF-IDF, —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ ‚Äúgood‚Äù, ‚Äúbad‚Äù\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–∏–∫–µ—Ç—ã / support\t   TF-IDF, —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ ‚Äúerror‚Äù, ‚Äúfail‚Äù, ‚Äúcrash‚Äù, –¥–ª–∏–Ω–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ !, ?\nE-commerce –æ–ø–∏—Å–∞–Ω–∏—è\t               –¥–ª–∏–Ω–∞, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞, —á–∞—Å—Ç–æ—Ç—ã ‚Äúnew‚Äù, ‚Äúsale‚Äù, ‚Äúdiscount‚Äù, topic features\n–ù–æ–≤–æ—Å—Ç–∏ / —Å—Ç–∞—Ç—å–∏\t               LDA —Ç–µ–º—ã, TF-IDF, sentence embeddings\n–†–µ–∑—é–º–µ / –≤–∞–∫–∞–Ω—Å–∏–∏\t               —á–∞—Å—Ç–æ—Ç—ã –ø–æ –ø—Ä–æ—Ñ–µ—Å—Å–∏—è–º, –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞, topic modeling (‚Äúskills‚Äù, ‚Äúexperience‚Äù)\n–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã\t               –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—É–Ω–∫—Ç–æ–≤, –¥–ª–∏–Ω–∞, –¥–æ–ª—è —á–∏—Å–µ–ª, topic modeling –ø–æ —Ç–µ—Ä–º–∏–Ω–∞–º\n–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\t   polarity, caps ratio, emoji count, sentiment, avg word length","metadata":{}},{"id":"bf15eddf","cell_type":"code","source":"#–û—Ç–∑—ã–≤—ã (Sentiment)\nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# ==== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ====\ndef safe_text(x):\n    return str(x) if not pd.isna(x) else \"\"\n\ndef word_tokens(text):\n    return re.findall(r\"\\b[\\w']+\\b\", safe_text(text).lower())\n\ndef lexicon_sentiment(text):\n    POS = {\"good\", \"great\", \"excellent\", \"love\", \"best\", \"nice\", \"amazing\"}\n    NEG = {\"bad\", \"terrible\", \"awful\", \"hate\", \"poor\", \"worst\"}\n    SUBJ = {\"think\", \"feel\", \"believe\", \"maybe\", \"might\", \"seems\"}\n    toks = word_tokens(text)\n    if not toks:\n        return 0, 0\n    pos = sum(t in POS for t in toks)\n    neg = sum(t in NEG for t in toks)\n    subj = sum(t in SUBJ for t in toks)\n    return (pos - neg) / len(toks), subj / len(toks)\n\n# ==== –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –¥–ª—è –æ—Ç–∑—ã–≤–æ–≤ ====\nclass ReviewTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", tfidf_max_features=300):\n        self.text_col = text_col\n        self.tfidf_max_features = tfidf_max_features\n        self.tfidf_vec = None\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        self.tfidf_vec = TfidfVectorizer(max_features=self.tfidf_max_features, stop_words=\"english\")\n        self.tfidf_vec.fit(texts)\n        return self\n\n    def transform(self, X):\n        df = X.copy()\n        texts = df[self.text_col].fillna(\"\").astype(str).tolist()\n        feats = {}\n\n        feats[\"len\"] = [len(t) for t in texts]\n        polsub = [lexicon_sentiment(t) for t in texts]\n        feats[\"polarity\"] = [p for p, s in polsub]\n        feats[\"subjectivity\"] = [s for p, s in polsub]\n        feats[\"freq_good\"] = [t.lower().count(\"good\") for t in texts]\n        feats[\"freq_bad\"] = [t.lower().count(\"bad\") for t in texts]\n\n        tfidf = self.tfidf_vec.transform(texts)\n        tfidf_df = pd.DataFrame(\n            tfidf.toarray(),\n            columns=[f\"tfidf_{w}\" for w in self.tfidf_vec.get_feature_names_out()]\n        )\n\n        feat_df = pd.concat([pd.DataFrame(feats), tfidf_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"b8024930","cell_type":"code","source":"# –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–∏–∫–µ—Ç—ã / support\t   TF-IDF, —á–∞—Å—Ç–æ—Ç—ã —Å–ª–æ–≤ ‚Äúerror‚Äù, ‚Äúfail‚Äù, ‚Äúcrash‚Äù, –¥–ª–∏–Ω–∞, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ !, ?\nclass SupportTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", tfidf_max_features=300):\n        self.text_col = text_col\n        self.tfidf_max_features = tfidf_max_features\n        self.tfidf_vec = None\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        self.tfidf_vec = TfidfVectorizer(max_features=self.tfidf_max_features, stop_words=\"english\")\n        self.tfidf_vec.fit(texts)\n        return self\n\n    def transform(self, X):\n        df = X.copy()\n        texts = df[self.text_col].fillna(\"\").astype(str).tolist()\n        feats = {}\n\n        feats[\"len\"] = [len(t) for t in texts]\n        feats[\"excl_count\"] = [t.count(\"!\") for t in texts]\n        feats[\"quest_count\"] = [t.count(\"?\") for t in texts]\n        feats[\"freq_error\"] = [t.lower().count(\"error\") for t in texts]\n        feats[\"freq_fail\"] = [t.lower().count(\"fail\") for t in texts]\n        feats[\"freq_crash\"] = [t.lower().count(\"crash\") for t in texts]\n\n        tfidf = self.tfidf_vec.transform(texts)\n        tfidf_df = pd.DataFrame(\n            tfidf.toarray(),\n            columns=[f\"tfidf_{w}\" for w in self.tfidf_vec.get_feature_names_out()]\n        )\n\n        feat_df = pd.concat([pd.DataFrame(feats), tfidf_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"088790ab","cell_type":"code","source":"#–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n# –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\nreviews_df = pd.DataFrame({\n    \"text\": [\n        \"I love this product, it's really good and amazing!\",\n        \"Terrible experience, bad quality and poor support.\",\n        \"Good price but not the best design.\"\n    ]\n})\n\nsupport_df = pd.DataFrame({\n    \"text\": [\n        \"App crash after update! Please fix this error asap!\",\n        \"Getting fail message when trying to login??\",\n        \"Error 404 on page load\"\n    ]\n})\n\n# --- –û—Ç–∑—ã–≤—ã ---\nrev_trans = ReviewTextFeatures(text_col=\"text\")\nrev_features = rev_trans.fit_transform(reviews_df)\nprint(\"–û—Ç–∑—ã–≤—ã (Sentiment):\")\nprint(rev_features.head())\n\n# --- Support —Ç–∏–∫–µ—Ç—ã ---\nsup_trans = SupportTextFeatures(text_col=\"text\")\nsup_features = sup_trans.fit_transform(support_df)\nprint(\"\\n–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–∏–∫–µ—Ç—ã:\")\nprint(sup_features.head())\n","metadata":{},"outputs":[],"execution_count":null},{"id":"251a5769","cell_type":"code","source":"#E-commerce –æ–ø–∏—Å–∞–Ω–∏—è\nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# ===== –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =====\ndef safe_text(x):\n    return str(x) if not pd.isna(x) else \"\"\n\ndef unique_word_count(text):\n    words = re.findall(r\"\\b[a-zA-Z]+\\b\", safe_text(text).lower())\n    return len(set(words))\n\n# ==========================================\n# üõçÔ∏è E-commerce —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n# ==========================================\nclass EcommerceTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", lda_topics=5, max_features=500):\n        self.text_col = text_col\n        self.lda_topics = lda_topics\n        self.max_features = max_features\n        self.count_vec = None\n        self.lda = None\n        self.target_words = [\"new\", \"sale\", \"discount\"]\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        self.count_vec = CountVectorizer(max_features=self.max_features, stop_words=\"english\")\n        bow = self.count_vec.fit_transform(texts)\n        self.lda = LatentDirichletAllocation(n_components=self.lda_topics, random_state=42)\n        self.lda.fit(bow)\n        return self\n\n    def transform(self, X):\n        texts = X[self.text_col].fillna(\"\").astype(str).tolist()\n\n        feats = {}\n        feats[\"len\"] = [len(t) for t in texts]\n        feats[\"unique_words\"] = [unique_word_count(t) for t in texts]\n        for w in self.target_words:\n            feats[f\"freq_{w}\"] = [t.lower().count(w) for t in texts]\n\n        bow = self.count_vec.transform(texts)\n        topics = self.lda.transform(bow)\n        topic_df = pd.DataFrame(topics, columns=[f\"topic_{i}\" for i in range(topics.shape[1])])\n\n        feat_df = pd.concat([pd.DataFrame(feats), topic_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)\n","metadata":{},"outputs":[],"execution_count":null},{"id":"07c532d2","cell_type":"code","source":"#–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n# ==== –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö ====\necom_df = pd.DataFrame({\n    \"text\": [\n        \"New stylish summer dress on sale! Get a 20% discount now.\",\n        \"Discount on new arrivals: men's jackets and coats available online.\",\n        \"Buy our best-selling shoes ‚Äî elegant, comfortable, and new design.\",\n        \"Exclusive sale! Modern laptop bag with sleek style and functionality.\"\n    ]\n})\n\n# --- E-commerce ---\necom_trans = EcommerceTextFeatures(text_col=\"text\")\necom_features = ecom_trans.fit_transform(ecom_df)\n\nprint(\"üõçÔ∏è E-commerce —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:\")\nprint(ecom_features.head())\n","metadata":{},"outputs":[],"execution_count":null},{"id":"b82b16a4","cell_type":"code","source":"#–ù–æ–≤–æ—Å—Ç–∏ / —Å—Ç–∞—Ç—å–∏\t  \nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation\n\ndef safe_text(x):\n    return str(x) if not pd.isna(x) else \"\"\n\nclass NewsTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", tfidf_max_features=500, embed_dim=16, lda_topics=5):\n        self.text_col = text_col\n        self.tfidf_max_features = tfidf_max_features\n        self.embed_dim = embed_dim\n        self.lda_topics = lda_topics\n\n        self.tfidf_vec = None\n        self.svd = None\n        self.count_vec = None\n        self.lda = None\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        # TF-IDF\n        self.tfidf_vec = TfidfVectorizer(max_features=self.tfidf_max_features, stop_words=\"english\")\n        tfidf_mat = self.tfidf_vec.fit_transform(texts)\n\n        # Sentence embeddings —á–µ—Ä–µ–∑ SVD\n        dim = min(self.embed_dim, tfidf_mat.shape[1]-1 if tfidf_mat.shape[1] > 1 else 1)\n        self.svd = TruncatedSVD(n_components=dim, random_state=42)\n        self.svd.fit(tfidf_mat)\n\n        # LDA –ø–æ Bag of Words\n        self.count_vec = CountVectorizer(max_features=self.tfidf_max_features, stop_words=\"english\")\n        bow = self.count_vec.fit_transform(texts)\n        self.lda = LatentDirichletAllocation(n_components=self.lda_topics, random_state=42)\n        self.lda.fit(bow)\n        return self\n\n    def transform(self, X):\n        texts = X[self.text_col].fillna(\"\").astype(str).tolist()\n\n        # TF-IDF\n        tfidf = self.tfidf_vec.transform(texts)\n        tfidf_df = pd.DataFrame(tfidf.toarray(), columns=[f\"tfidf_{w}\" for w in self.tfidf_vec.get_feature_names_out()])\n\n        # Sentence embeddings\n        emb = self.svd.transform(tfidf)\n        emb_df = pd.DataFrame(emb, columns=[f\"embed_{i}\" for i in range(emb.shape[1])])\n\n        # LDA topics\n        bow = self.count_vec.transform(texts)\n        topics = self.lda.transform(bow)\n        topic_df = pd.DataFrame(topics, columns=[f\"topic_{i}\" for i in range(topics.shape[1])])\n\n        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n        feat_df = pd.concat([tfidf_df, emb_df, topic_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"bf2f26ca","cell_type":"code","source":"# –†–µ–∑—é–º–µ / –≤–∞–∫–∞–Ω—Å–∏–∏\t           \nclass ResumeTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", tfidf_max_features=300, lda_topics=4):\n        self.text_col = text_col\n        self.tfidf_max_features = tfidf_max_features\n        self.lda_topics = lda_topics\n\n        self.count_vec = None\n        self.lda = None\n\n        self.professions = [\"engineer\", \"developer\", \"manager\", \"analyst\", \"designer\", \"consultant\"]\n        self.skills = [\"python\", \"sql\", \"excel\", \"communication\", \"leadership\", \"project\", \"experience\"]\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        self.count_vec = CountVectorizer(max_features=self.tfidf_max_features, stop_words=\"english\")\n        bow = self.count_vec.fit_transform(texts)\n        self.lda = LatentDirichletAllocation(n_components=self.lda_topics, random_state=42)\n        self.lda.fit(bow)\n        return self\n\n    def transform(self, X):\n        texts = X[self.text_col].fillna(\"\").astype(str).tolist()\n\n        feats = {}\n        feats[\"len\"] = [len(t) for t in texts]\n        feats[\"prof_count\"] = [sum(t.lower().count(p) for p in self.professions) for t in texts]\n        feats[\"skills_count\"] = [sum(t.lower().count(s) for s in self.skills) for t in texts]\n\n        # Topic modeling\n        bow = self.count_vec.transform(texts)\n        topics = self.lda.transform(bow)\n        topic_df = pd.DataFrame(topics, columns=[f\"topic_{i}\" for i in range(topics.shape[1])])\n\n        feat_df = pd.concat([pd.DataFrame(feats), topic_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)\n        \n    ","metadata":{},"outputs":[],"execution_count":null},{"id":"50f0b8c5","cell_type":"code","source":"# --–ü–†–ò–ú–ï–ù–ï–ù–ò–ï--\n    # ==== –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö ====\nnews_df = pd.DataFrame({\n    \"text\": [\n        \"The government passed a new economic reform bill today.\",\n        \"Stock markets saw a sharp decline after the inflation report.\",\n        \"Scientists discovered a new planet similar to Earth.\",\n        \"The local football team won the championship last night.\"\n    ]\n})\n\nresume_df = pd.DataFrame({\n    \"text\": [\n        \"Experienced software engineer with strong Python and SQL skills. Managed multiple projects.\",\n        \"Marketing manager with 10 years of leadership experience and strong communication skills.\",\n        \"Junior data analyst skilled in Excel and data visualization.\",\n        \"Project designer experienced in architecture and planning.\"\n    ]\n})\n\n# --- –ù–æ–≤–æ—Å—Ç–∏ ---\nnews_trans = NewsTextFeatures(text_col=\"text\")\nnews_features = news_trans.fit_transform(news_df)\nprint(\"–ù–æ–≤–æ—Å—Ç–∏ / —Å—Ç–∞—Ç—å–∏:\")\nprint(news_features.head())\n\n# --- –†–µ–∑—é–º–µ ---\nresume_trans = ResumeTextFeatures(text_col=\"text\")\nresume_features = resume_trans.fit_transform(resume_df)\nprint(\"\\n–†–µ–∑—é–º–µ / –≤–∞–∫–∞–Ω—Å–∏–∏:\")\nprint(resume_features.head())","metadata":{},"outputs":[],"execution_count":null},{"id":"8f4afd61","cell_type":"code","source":"#–Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã\nimport re\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# ======= –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ =======\ndef safe_text(x):\n    return str(x) if not pd.isna(x) else \"\"\n\ndef word_tokens(text):\n    return re.findall(r\"\\b[\\w']+\\b\", safe_text(text).lower())\n\ndef caps_ratio(text):\n    s = safe_text(text)\n    if not s:\n        return 0\n    return sum(1 for c in s if c.isupper()) / (len(s) + 1e-9)\n\ndef emoji_count(text):\n    return sum(1 for c in safe_text(text) if ord(c) in range(0x1F300, 0x1F9FF))\n\ndef avg_word_len(text):\n    toks = word_tokens(text)\n    return np.mean([len(t) for t in toks]) if toks else 0\n\ndef digits_fraction(text):\n    s = safe_text(text)\n    return sum(1 for c in s if c.isdigit()) / (len(s) + 1e-9)\n\ndef lexicon_sentiment(text):\n    POS = {\"good\", \"great\", \"excellent\", \"love\", \"best\", \"nice\", \"amazing\"}\n    NEG = {\"bad\", \"terrible\", \"awful\", \"hate\", \"poor\", \"worst\"}\n    toks = word_tokens(text)\n    if not toks:\n        return 0\n    score = sum((1 if t in POS else -1 if t in NEG else 0) for t in toks)\n    return score / len(toks)\n\n# ==========================================\n# ‚öñÔ∏è –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã\n# ==========================================\nclass LegalTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\", lda_topics=5, max_features=500):\n        self.text_col = text_col\n        self.lda_topics = lda_topics\n        self.max_features = max_features\n\n        self.count_vec = None\n        self.lda = None\n        self.legal_terms = [\n            \"agreement\", \"contract\", \"party\", \"clause\", \"warranty\",\n            \"liability\", \"condition\", \"term\", \"obligation\", \"notice\"\n        ]\n\n    def fit(self, X, y=None):\n        texts = X[self.text_col].fillna(\"\").astype(str)\n        self.count_vec = CountVectorizer(max_features=self.max_features, stop_words=\"english\")\n        bow = self.count_vec.fit_transform(texts)\n        self.lda = LatentDirichletAllocation(n_components=self.lda_topics, random_state=42)\n        self.lda.fit(bow)\n        return self\n\n    def transform(self, X):\n        texts = X[self.text_col].fillna(\"\").astype(str).tolist()\n        feats = {}\n        feats[\"len\"] = [len(t) for t in texts]\n        feats[\"num_points\"] = [len(re.findall(r\"\\b\\d+\\.\", t)) for t in texts]\n        feats[\"digits_frac\"] = [digits_fraction(t) for t in texts]\n        feats[\"legal_term_count\"] = [sum(t.lower().count(term) for term in self.legal_terms) for t in texts]\n\n        bow = self.count_vec.transform(texts)\n        topics = self.lda.transform(bow)\n        topic_df = pd.DataFrame(topics, columns=[f\"topic_{i}\" for i in range(topics.shape[1])])\n\n        feat_df = pd.concat([pd.DataFrame(feats), topic_df], axis=1)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"9c3b424e","cell_type":"code","source":"#–°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏\nclass SocialTextFeatures(BaseEstimator, TransformerMixin):\n    def __init__(self, text_col=\"text\"):\n        self.text_col = text_col\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        texts = X[self.text_col].fillna(\"\").astype(str).tolist()\n        feats = {}\n        feats[\"polarity\"] = [lexicon_sentiment(t) for t in texts]\n        feats[\"caps_ratio\"] = [caps_ratio(t) for t in texts]\n        feats[\"emoji_count\"] = [emoji_count(t) for t in texts]\n        feats[\"avg_word_len\"] = [avg_word_len(t) for t in texts]\n        feats[\"sentiment_score\"] = [np.sign(lexicon_sentiment(t)) for t in texts]\n\n        feat_df = pd.DataFrame(feats)\n        feat_df.columns = [f\"{self.text_col}_{c}\" for c in feat_df.columns]\n        return feat_df.reset_index(drop=True)","metadata":{},"outputs":[],"execution_count":null},{"id":"0f6ff82e","cell_type":"code","source":"#–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ\n# ==== –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö ====\nlegal_df = pd.DataFrame({\n    \"text\": [\n        \"1. This agreement is made between the two parties. 2. Each party has obligations under clause 5.\",\n        \"The contract includes warranty terms and liability conditions.\",\n        \"Parties agree to provide notice of termination in writing.\"\n    ]\n})\n\nsocial_df = pd.DataFrame({\n    \"text\": [\n        \"OMG I LOVE this üòçüòç AMAZING!!!\",\n        \"bad service, hate it :((\",\n        \"Wow GREAT job team!!! So proud üí™üî•\"\n    ]\n})\n\n# --- –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã ---\nlegal_trans = LegalTextFeatures(text_col=\"text\")\nlegal_features = legal_trans.fit_transform(legal_df)\nprint(\"‚öñÔ∏è –Æ—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ —Ç–µ–∫—Å—Ç—ã:\")\nprint(legal_features.head())\n\n# --- –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ ---\nsocial_trans = SocialTextFeatures(text_col=\"text\")\nsocial_features = social_trans.fit_transform(social_df)\nprint(\"\\nüí¨ –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–µ—Ç–∏ / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏:\")\nprint(social_features.head())\n","metadata":{},"outputs":[],"execution_count":null}]}