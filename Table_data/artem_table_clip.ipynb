{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":25383,"databundleVersionId":2684322,"isSourceIdPinned":false,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"843c7fbe","cell_type":"code","source":"LOAD_LIBS = False\nif LOAD_LIBS:\n    !pip install -q -U catboost\n    !pip install -q -U lightgbm\n    !pip install -q -U xgboost\n    !pip install -q -U geopy\n    !pip install -q -U phik\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:31.991543Z","iopub.execute_input":"2025-11-17T22:38:31.992220Z","iopub.status.idle":"2025-11-17T22:38:31.997864Z","shell.execute_reply.started":"2025-11-17T22:38:31.992193Z","shell.execute_reply":"2025-11-17T22:38:31.997034Z"}},"outputs":[],"execution_count":143},{"id":"f3d9bdca","cell_type":"code","source":"import os\nimport gc\nimport json\nimport time\nimport logging\nimport argparse\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    log_loss,\n    mean_squared_error,\n    mean_absolute_error,\n    confusion_matrix\n)\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nimport open_clip\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n# –ì–µ–æ-–±—ç–∫–µ–Ω–¥: geopy (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω) –∏–ª–∏ haversine\ntry:\n    from geopy.distance import geodesic\n    GEO_BACKEND = \"geopy\"\nexcept ImportError:\n    GEO_BACKEND = \"haversine\"\n\n\ntry:\n    import phik\n    HAS_PHIK = True\nexcept ImportError:\n    HAS_PHIK = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:31.999127Z","iopub.execute_input":"2025-11-17T22:38:31.999333Z","iopub.status.idle":"2025-11-17T22:38:32.195749Z","shell.execute_reply.started":"2025-11-17T22:38:31.999318Z","shell.execute_reply":"2025-11-17T22:38:32.195003Z"}},"outputs":[],"execution_count":144},{"id":"6814b40a","cell_type":"code","source":"GEO_BACKEND, HAS_PHIK","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.197086Z","iopub.execute_input":"2025-11-17T22:38:32.197310Z","iopub.status.idle":"2025-11-17T22:38:32.209220Z","shell.execute_reply.started":"2025-11-17T22:38:32.197295Z","shell.execute_reply":"2025-11-17T22:38:32.208505Z"}},"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"('geopy', True)"},"metadata":{}}],"execution_count":145},{"id":"82061097","cell_type":"code","source":"# –¥–ª—è —Ç–µ–∫—Å—Ç–∞, –Ω–∏–∫–∞–∫ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å\nTEXT_MODEL = None\nTEXT_TOKENIZER = None\nTEXT_DEVICE = None\n\n# TF-IDF –∏ SVD –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è (—á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)\nTFIDF_VECTORIZERS = {}  # field_name -> vectorizer\nTFIDF_SVDS = {}         # field_name -> svd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.209918Z","iopub.execute_input":"2025-11-17T22:38:32.210167Z","iopub.status.idle":"2025-11-17T22:38:32.218162Z","shell.execute_reply.started":"2025-11-17T22:38:32.210146Z","shell.execute_reply":"2025-11-17T22:38:32.217544Z"}},"outputs":[],"execution_count":146},{"id":"9f4654d2","cell_type":"code","source":"DATA_FOLDER_PATH = '.'\nCONFIG = {\n    # ---------- Paths ----------\n    \"train_path\": os.path.join(DATA_FOLDER_PATH, \"/kaggle/input/petfinder-pawpularity-score/train.csv\"),\n    \"test_path\": os.path.join(DATA_FOLDER_PATH, '/kaggle/input/petfinder-pawpularity-score/test.csv'),\n    \"sep\": \",\",\n    \"id_column\": \"Id\",\n    \"target_column\": \"Pawpularity\",\n    \"datetime_columns\": [],\n    \"output_dir\": \"tabular_boosting_output\",\n\n    # ---------- Task ----------\n    # \"binary\" / \"multiclass\" / \"regression\"\n    \"task_type\": \"regression\",\n\n    # ---------- Basic features ----------\n    \"basic_drop_columns\": [],\n    \"basic_as_categorical\": [],\n    \"basic_max_cat_unique\": 64,\n    \"basic_datetime_expand\": False,\n    \"basic_datetime_features\": [\"year\", \"month\", \"day\", \"dow\", \"hour\"],\n\n    # ---------- Categorical processing ----------\n    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π \"—Ä–∞–∑–º–µ—Ä\" –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø–æ —á–∏—Å–ª—É –æ–±—ä–µ–∫—Ç–æ–≤ –∏/–∏–ª–∏ –¥–æ–ª–µ),\n    # –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞ —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ 'other'\n    \"cat_min_count\": 20,          # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n    \"cat_min_freq\": 0.0,          # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è (0..1); 0 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å\n    # –µ—Å–ª–∏ –¥–æ–ª—è \"—Ä–µ–¥–∫–∏—Ö\" –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ø—Ä–∏–∑–Ω–∞–∫–µ >= —ç—Ç–æ–≥–æ –ø–æ—Ä–æ–≥–∞ ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ —É–¥–∞–ª—è–µ—Ç—Å—è\n    \"cat_max_rare_share\": 0.98,\n    # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ other —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ —É–¥–∞–ª—è–µ—Ç—Å—è\n    \"cat_max_unique_after_group\": 500,\n\n    # ---------- Post-feature service columns ----------\n    # —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏—á\n    # (–∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, raw id, —Å—ã—Ä—ã–µ –∞–¥—Ä–µ—Å–∞ –∏ —Ç.–ø.)\n    \"post_feature_drop_columns\": [], # –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–∞–ª–∏—Ç—å category_geo_ref_lat, category_geo_ref_lon\n\n    # ---------- Address processing ----------\n    # –∫–æ–ª–æ–Ω–∫–∞-–∞–¥—Ä–µ—Å, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –≥–æ—Ä–æ–¥\n    \"address_column\": None,  \n\n    # –∏–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ split (–Ω–∞–ø—Ä–∏–º–µ—Ä [-1] = –ø–æ—Å–ª–µ–¥–Ω–∏–π)\n    # –º–æ–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n    \"address_city_index\": -1,  \n\n    # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ —Å—Ç—Ä–æ–∫–µ –∞–¥—Ä–µ—Å–∞\n    \"address_split_sep\": \",\",  \n\n    # ---------- Aggregates ----------\n    \"agg_enable\": False,\n    \"agg_groupby_cols\": [],\n    \"agg_numeric_cols\": [],\n    \"agg_aggs\": [\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\", \"nunique\", \"count\"],\n    \"agg_prefix\": \"agg\",\n\n    # ---------- Geo features ----------\n    \"geo_enable\": False, \n    \"geo_lat_from_col\": \"pickup_lat\",\n    \"geo_lon_from_col\": \"pickup_lon\",\n    \"geo_lat_to_col\": \"dropoff_lat\",\n    \"geo_lon_to_col\": \"dropoff_lon\",\n    \"geo_ref_lat\": 'category_geo_ref_lat',\n    \"geo_ref_lon\": 'category_geo_ref_lon',\n    \"geo_prefix\": \"geo\",\n    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (bearing, manhattan, dlat/dlon, midpoint)\n    \"geo_extra_enable\": False,\n\n    \"geo_from_coord_col\": None,\n    \"geo_to_coord_col\": None,\n    \"geo_coord_string_sep\": \",\",\n\n    # ---------- Geo reference config ----------\n    \"geo_reference\": {\n        \"enabled\": False,\n        \"category_column\": \"category\",\n        \"coordinates_column\": \"coordinates\",  # —Å—Ç—Ä–æ–∫–∞ \"lat,lon\", –µ—Å–ª–∏ lat/lon –Ω–µ –∑–∞–¥–∞–Ω—ã\n        \"lat_column\": None,                  # <- –µ—Å–ª–∏ –∑–∞–¥–∞—Ç—å, –±–µ—Ä—ë–º –æ—Ç—Å—é–¥–∞ —à–∏—Ä–æ—Ç—É\n        \"lon_column\": None,                  # <- –µ—Å–ª–∏ –∑–∞–¥–∞—Ç—å, –±–µ—Ä—ë–º –æ—Ç—Å—é–¥–∞ –¥–æ–ª–≥–æ—Ç—É\n        \"output_column\": \"category_geo_ref\",\n        \"output_lat_column\": \"category_geo_ref_lat\",\n        \"output_lon_column\": \"category_geo_ref_lon\",\n    },\n\n    # ---------- Correlation-based feature filtering ----------\n    \"corr_enable\": False,          # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n    \"corr_pearson_min_abs\": 0.95,   # –ø–æ—Ä–æ–≥ |Pearson|; 0.1, 0.05 –∏ —Ç.–ø.\n    \"corr_use_phik\": False,        # —Å—á–∏—Ç–∞—Ç—å –ª–∏ phik\n    \"corr_phik_min_abs\": 0.0,      # –ø–æ—Ä–æ–≥ |phik|\n    \n    #----------images---------\n    # –ø–∞–ø–∫–∞ —Å train-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º image-—Ñ–∏—á–∏)\n    \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n    # –ø–∞–ø–∫–∞ —Å test-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º image-—Ñ–∏—á–∏)\n    \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n    # –ï—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ image_id —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç \".jpg\", –ø–æ—Å—Ç–∞–≤—å IMAGE_EXT = \"\"\n    # —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (–µ—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ —Ç–æ–ª—å–∫–æ id –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)\n    \"image_ext\":\".jpg\",\n    # –∫–æ–ª–æ–Ω–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏\n    \"file_names_column\": \"Id\",\n    # —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞—Ä—Ç–∏–Ω–æ–∫\n    \"batch_size\": 32,\n    \n    # ---------- Text features ----------\n    # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n    \"text_enable\": False,  # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏\n\n    # –ª—é–±—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π train/test —Ç–∞–±–ª–∏—Ü–µ\n    \"text_columns\": [],  # –Ω–∞–ø—Ä–∏–º–µ—Ä [\"title\", \"description\"]\n\n    # –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (id, text) –≤ parquet/csv/tsv\n    # –∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å:\n    # {\n    #   \"name\": \"comments\",        # –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏–º—è (–¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á)\n    #   \"train_path\": \"train_comments.parquet\",\n    #   \"test_path\": \"test_comments.parquet\",\n    #   \"format\": \"parquet\",       # \"parquet\" / \"csv\" / \"tsv\" / \"auto\"\n    #   \"id_column\": \"index\",      # –∫–æ–ª–æ–Ω–∫–∞ id –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ\n    #   \"text_column\": \"text\",     # –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ\n    #   \"output_column\": \"comments_text\"  # –∫–∞–∫ –Ω–∞–∑–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É –≤ –æ–±—â–µ–π —Ç–∞–±–ª–∏—Ü–µ (–æ–ø—Ü.)\n    # }\n    \"text_external_tables\": [{\n        \"name\": \"reviews\",\n        \"train_path\": \"reviews.tsv\",\n        \"test_path\": \"reviews.tsv\",\n        \"format\": \"tsv\",       # –∏–ª–∏ \"auto\"\n        # –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –æ–±—ä–µ–∫—Ç–∞\n    \"id_column\": \"id\",      # –≤ —ç—Ç–∏—Ö —Ñ–∞–π–ª–∞—Ö\n        \"text_column\": \"text\",     # –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º\n        \"output_column\": \"comments_text\"\n    }],\n\n    # —Ç–∏–ø —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n    # \"bert\"      ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n    # \"tfidf_svd\" ‚Äî TF-IDF + TruncatedSVD\n    # —Ç–∏–ø —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏: \"bert\" –∏–ª–∏ \"tfidf_svd\"\n    \"text_model_type\": \"tfidf_svd\",\n\n    # –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π (–∫–æ–º–º–µ–Ω—Ç–∞–º–∏, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å)\n    # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # RU + EN (—É–Ω–∏–≤–µ—Ä—Å–∞–ª)\n    # \"DeepPavlov/rubert-base-cased-sentence\"                        # ru-only\n    # \"sentence-transformers/all-mpnet-base-v2\"                      # en-only\n    \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n\n    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ BERT\n    \"text_max_length\": 256,\n    \"text_batch_size\": 32,\n    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ TF-IDF + SVD\n    \"text_tfidf_max_features\": 50000,\n    \"text_svd_n_components\": 256,\n\n    # ---------- CV ----------\n    \"cv_n_splits\": 5,\n    \"cv_random_state\": 42,\n    \"cv_shuffle\": True,\n    \"cv_stratified\": True,\n\n    # ---------- Models ----------\n    \"use_catboost\": True,\n    \"use_lightgbm\": False,\n    \"use_xgboost\": False,\n\n    \"catboost_params\": {\n        \"iterations\": 3500,\n        \"learning_rate\": 0.05,\n        \"depth\": 6,\n        \"loss_function\": \"MultiClass\", # \"Logloss\"\n        \"eval_metric\": \"AUC\",\n        \"random_seed\": 42,\n        \"verbose\": 100\n    },\n\n\n    \"lgb_params\": {\n        \"objective\": \"multiclass\", # \"binary\"\n        \"eval_metric\": \"auc\", #[\"auc\", \"binary_logloss\"],\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.9,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"lambda_l2\": 1.0,\n        \"num_threads\": 0,\n        \"verbose\": -1\n    },\n\n    \"xgb_params\": {\n        \"objective\": \"multi:softmax\", #\"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"learning_rate\": 0.05,\n        \"max_depth\": 6,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"lambda\": 1.0,\n        \"alpha\": 0.0,\n        \"tree_method\": \"hist\"\n    },\n\n    # ---------- Blending ----------\n    \"blend_weights\": {\n        \"catboost\": 1,\n        \"lightgbm\": 0.35,\n        \"xgboost\": 0.15\n    },\n\n    # ---------- Feature importance ----------\n    \"compute_feature_importance\": True,\n    \"top_features_to_show\": 50,\n\n    # ---------- Early stopping ----------\n    # –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –±—É—Å—Ç–∏–Ω–≥–æ–≤\n    \"early_stopping_rounds\": 100,# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n    \"device\": \"cuda\",\n\n    \"seed\": 42,\n    \"verbose\": True\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.220067Z","iopub.execute_input":"2025-11-17T22:38:32.220243Z","iopub.status.idle":"2025-11-17T22:38:32.233479Z","shell.execute_reply.started":"2025-11-17T22:38:32.220229Z","shell.execute_reply":"2025-11-17T22:38:32.232828Z"}},"outputs":[],"execution_count":147},{"id":"af431dbd","cell_type":"code","source":"def setup_logging(output_dir: str) -> logging.Logger:\n    os.makedirs(output_dir, exist_ok=True)\n    log_path = os.path.join(output_dir, \"training.log\")\n\n    logger = logging.getLogger(\"TABULAR_BOOSTING\")\n    logger.setLevel(logging.INFO)\n    logger.handlers = []\n\n    # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —ç–º–æ–¥–∑–∏\n    fh = logging.FileHandler(log_path, encoding='utf-8')\n    fh.setLevel(logging.INFO)\n    \n    # –î–ª—è –∫–æ–Ω—Å–æ–ª–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º UTF-8, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ\n    import sys\n    if sys.stdout.encoding != 'utf-8':\n        # –ï—Å–ª–∏ –∫–æ–Ω—Å–æ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç UTF-8, —Å–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É\n        class UTF8StreamHandler(logging.StreamHandler):\n            def emit(self, record):\n                try:\n                    msg = self.format(record)\n                    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç UTF-8\n                    import re\n                    msg = re.sub(r'[^\\x00-\\x7F]+', '', msg)  # –£–¥–∞–ª—è–µ–º –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã\n                    stream = self.stream\n                    stream.write(msg + self.terminator)\n                    self.flush()\n                except Exception:\n                    self.handleError(record)\n        ch = UTF8StreamHandler()\n    else:\n        ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n\n    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    fh.setFormatter(fmt)\n    ch.setFormatter(fmt)\n\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n\n    logger.info(\"Logger initialized.\")\n    return logger\n\n\ndef log_config(logger: logging.Logger, config: Dict):\n    logger.info(\"========== CONFIG ==========\")\n    logger.info(json.dumps(config, indent=2, ensure_ascii=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.234068Z","iopub.execute_input":"2025-11-17T22:38:32.234236Z","iopub.status.idle":"2025-11-17T22:38:32.250821Z","shell.execute_reply.started":"2025-11-17T22:38:32.234222Z","shell.execute_reply":"2025-11-17T22:38:32.250050Z"}},"outputs":[],"execution_count":148},{"id":"b8b7b9c7","cell_type":"code","source":"start_time = time.time()\n\noutput_dir = CONFIG[\"output_dir\"]\nos.makedirs(output_dir, exist_ok=True)\nlogger = setup_logging(output_dir)\nlog_config(logger, CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.251477Z","iopub.execute_input":"2025-11-17T22:38:32.251681Z","iopub.status.idle":"2025-11-17T22:38:32.269729Z","shell.execute_reply.started":"2025-11-17T22:38:32.251660Z","shell.execute_reply":"2025-11-17T22:38:32.269211Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:38:32,263 - INFO - Logger initialized.\nINFO:TABULAR_BOOSTING:Logger initialized.\n2025-11-17 22:38:32,264 - INFO - ========== CONFIG ==========\nINFO:TABULAR_BOOSTING:========== CONFIG ==========\n2025-11-17 22:38:32,266 - INFO - {\n  \"train_path\": \"/kaggle/input/petfinder-pawpularity-score/train.csv\",\n  \"test_path\": \"/kaggle/input/petfinder-pawpularity-score/test.csv\",\n  \"sep\": \",\",\n  \"id_column\": \"Id\",\n  \"target_column\": \"Pawpularity\",\n  \"datetime_columns\": [],\n  \"output_dir\": \"tabular_boosting_output\",\n  \"task_type\": \"regression\",\n  \"basic_drop_columns\": [],\n  \"basic_as_categorical\": [],\n  \"basic_max_cat_unique\": 64,\n  \"basic_datetime_expand\": false,\n  \"basic_datetime_features\": [\n    \"year\",\n    \"month\",\n    \"day\",\n    \"dow\",\n    \"hour\"\n  ],\n  \"cat_min_count\": 20,\n  \"cat_min_freq\": 0.0,\n  \"cat_max_rare_share\": 0.98,\n  \"cat_max_unique_after_group\": 500,\n  \"post_feature_drop_columns\": [],\n  \"address_column\": null,\n  \"address_city_index\": -1,\n  \"address_split_sep\": \",\",\n  \"agg_enable\": false,\n  \"agg_groupby_cols\": [],\n  \"agg_numeric_cols\": [],\n  \"agg_aggs\": [\n    \"mean\",\n    \"std\",\n    \"min\",\n    \"max\",\n    \"sum\",\n    \"median\",\n    \"nunique\",\n    \"count\"\n  ],\n  \"agg_prefix\": \"agg\",\n  \"geo_enable\": false,\n  \"geo_lat_from_col\": \"pickup_lat\",\n  \"geo_lon_from_col\": \"pickup_lon\",\n  \"geo_lat_to_col\": \"dropoff_lat\",\n  \"geo_lon_to_col\": \"dropoff_lon\",\n  \"geo_ref_lat\": \"category_geo_ref_lat\",\n  \"geo_ref_lon\": \"category_geo_ref_lon\",\n  \"geo_prefix\": \"geo\",\n  \"geo_extra_enable\": false,\n  \"geo_from_coord_col\": null,\n  \"geo_to_coord_col\": null,\n  \"geo_coord_string_sep\": \",\",\n  \"geo_reference\": {\n    \"enabled\": false,\n    \"category_column\": \"category\",\n    \"coordinates_column\": \"coordinates\",\n    \"lat_column\": null,\n    \"lon_column\": null,\n    \"output_column\": \"category_geo_ref\",\n    \"output_lat_column\": \"category_geo_ref_lat\",\n    \"output_lon_column\": \"category_geo_ref_lon\"\n  },\n  \"corr_enable\": false,\n  \"corr_pearson_min_abs\": 0.95,\n  \"corr_use_phik\": false,\n  \"corr_phik_min_abs\": 0.0,\n  \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n  \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n  \"image_ext\": \".jpg\",\n  \"file_names_column\": \"Id\",\n  \"batch_size\": 32,\n  \"text_enable\": false,\n  \"text_columns\": [],\n  \"text_external_tables\": [\n    {\n      \"name\": \"reviews\",\n      \"train_path\": \"reviews.tsv\",\n      \"test_path\": \"reviews.tsv\",\n      \"format\": \"tsv\",\n      \"id_column\": \"id\",\n      \"text_column\": \"text\",\n      \"output_column\": \"comments_text\"\n    }\n  ],\n  \"text_model_type\": \"tfidf_svd\",\n  \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n  \"text_max_length\": 256,\n  \"text_batch_size\": 32,\n  \"text_tfidf_max_features\": 50000,\n  \"text_svd_n_components\": 256,\n  \"cv_n_splits\": 5,\n  \"cv_random_state\": 42,\n  \"cv_shuffle\": true,\n  \"cv_stratified\": true,\n  \"use_catboost\": true,\n  \"use_lightgbm\": false,\n  \"use_xgboost\": false,\n  \"catboost_params\": {\n    \"iterations\": 3500,\n    \"learning_rate\": 0.05,\n    \"depth\": 6,\n    \"loss_function\": \"MultiClass\",\n    \"eval_metric\": \"AUC\",\n    \"random_seed\": 42,\n    \"verbose\": 100\n  },\n  \"lgb_params\": {\n    \"objective\": \"multiclass\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"lambda_l2\": 1.0,\n    \"num_threads\": 0,\n    \"verbose\": -1\n  },\n  \"xgb_params\": {\n    \"objective\": \"multi:softmax\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"max_depth\": 6,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"lambda\": 1.0,\n    \"alpha\": 0.0,\n    \"tree_method\": \"hist\"\n  },\n  \"blend_weights\": {\n    \"catboost\": 1,\n    \"lightgbm\": 0.35,\n    \"xgboost\": 0.15\n  },\n  \"compute_feature_importance\": true,\n  \"top_features_to_show\": 50,\n  \"early_stopping_rounds\": 100,\n  \"device\": \"cuda\",\n  \"seed\": 42,\n  \"verbose\": true\n}\nINFO:TABULAR_BOOSTING:{\n  \"train_path\": \"/kaggle/input/petfinder-pawpularity-score/train.csv\",\n  \"test_path\": \"/kaggle/input/petfinder-pawpularity-score/test.csv\",\n  \"sep\": \",\",\n  \"id_column\": \"Id\",\n  \"target_column\": \"Pawpularity\",\n  \"datetime_columns\": [],\n  \"output_dir\": \"tabular_boosting_output\",\n  \"task_type\": \"regression\",\n  \"basic_drop_columns\": [],\n  \"basic_as_categorical\": [],\n  \"basic_max_cat_unique\": 64,\n  \"basic_datetime_expand\": false,\n  \"basic_datetime_features\": [\n    \"year\",\n    \"month\",\n    \"day\",\n    \"dow\",\n    \"hour\"\n  ],\n  \"cat_min_count\": 20,\n  \"cat_min_freq\": 0.0,\n  \"cat_max_rare_share\": 0.98,\n  \"cat_max_unique_after_group\": 500,\n  \"post_feature_drop_columns\": [],\n  \"address_column\": null,\n  \"address_city_index\": -1,\n  \"address_split_sep\": \",\",\n  \"agg_enable\": false,\n  \"agg_groupby_cols\": [],\n  \"agg_numeric_cols\": [],\n  \"agg_aggs\": [\n    \"mean\",\n    \"std\",\n    \"min\",\n    \"max\",\n    \"sum\",\n    \"median\",\n    \"nunique\",\n    \"count\"\n  ],\n  \"agg_prefix\": \"agg\",\n  \"geo_enable\": false,\n  \"geo_lat_from_col\": \"pickup_lat\",\n  \"geo_lon_from_col\": \"pickup_lon\",\n  \"geo_lat_to_col\": \"dropoff_lat\",\n  \"geo_lon_to_col\": \"dropoff_lon\",\n  \"geo_ref_lat\": \"category_geo_ref_lat\",\n  \"geo_ref_lon\": \"category_geo_ref_lon\",\n  \"geo_prefix\": \"geo\",\n  \"geo_extra_enable\": false,\n  \"geo_from_coord_col\": null,\n  \"geo_to_coord_col\": null,\n  \"geo_coord_string_sep\": \",\",\n  \"geo_reference\": {\n    \"enabled\": false,\n    \"category_column\": \"category\",\n    \"coordinates_column\": \"coordinates\",\n    \"lat_column\": null,\n    \"lon_column\": null,\n    \"output_column\": \"category_geo_ref\",\n    \"output_lat_column\": \"category_geo_ref_lat\",\n    \"output_lon_column\": \"category_geo_ref_lon\"\n  },\n  \"corr_enable\": false,\n  \"corr_pearson_min_abs\": 0.95,\n  \"corr_use_phik\": false,\n  \"corr_phik_min_abs\": 0.0,\n  \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n  \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n  \"image_ext\": \".jpg\",\n  \"file_names_column\": \"Id\",\n  \"batch_size\": 32,\n  \"text_enable\": false,\n  \"text_columns\": [],\n  \"text_external_tables\": [\n    {\n      \"name\": \"reviews\",\n      \"train_path\": \"reviews.tsv\",\n      \"test_path\": \"reviews.tsv\",\n      \"format\": \"tsv\",\n      \"id_column\": \"id\",\n      \"text_column\": \"text\",\n      \"output_column\": \"comments_text\"\n    }\n  ],\n  \"text_model_type\": \"tfidf_svd\",\n  \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n  \"text_max_length\": 256,\n  \"text_batch_size\": 32,\n  \"text_tfidf_max_features\": 50000,\n  \"text_svd_n_components\": 256,\n  \"cv_n_splits\": 5,\n  \"cv_random_state\": 42,\n  \"cv_shuffle\": true,\n  \"cv_stratified\": true,\n  \"use_catboost\": true,\n  \"use_lightgbm\": false,\n  \"use_xgboost\": false,\n  \"catboost_params\": {\n    \"iterations\": 3500,\n    \"learning_rate\": 0.05,\n    \"depth\": 6,\n    \"loss_function\": \"MultiClass\",\n    \"eval_metric\": \"AUC\",\n    \"random_seed\": 42,\n    \"verbose\": 100\n  },\n  \"lgb_params\": {\n    \"objective\": \"multiclass\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"lambda_l2\": 1.0,\n    \"num_threads\": 0,\n    \"verbose\": -1\n  },\n  \"xgb_params\": {\n    \"objective\": \"multi:softmax\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"max_depth\": 6,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"lambda\": 1.0,\n    \"alpha\": 0.0,\n    \"tree_method\": \"hist\"\n  },\n  \"blend_weights\": {\n    \"catboost\": 1,\n    \"lightgbm\": 0.35,\n    \"xgboost\": 0.15\n  },\n  \"compute_feature_importance\": true,\n  \"top_features_to_show\": 50,\n  \"early_stopping_rounds\": 100,\n  \"device\": \"cuda\",\n  \"seed\": 42,\n  \"verbose\": true\n}\n","output_type":"stream"}],"execution_count":149},{"id":"8795137b","cell_type":"markdown","source":"### Utils","metadata":{}},{"id":"87f51611","cell_type":"code","source":"def _ensure_lat_lon_from_single_column(\n    df: pd.DataFrame,\n    coord_col: str,\n    lat_name: str,\n    lon_name: str,\n    sep: str = \",\"\n) -> pd.DataFrame:\n    \"\"\"\n    coord_col –º–æ–∂–µ—Ç –±—ã—Ç—å:\n      - —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ \"55.75,37.62\"\n      - —Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂ [55.75, 37.62]\n    —Å–æ–∑–¥–∞—ë–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ lat_name, lon_name\n    \"\"\"\n    if coord_col not in df.columns:\n        return df\n\n    def _parse_one(x):\n        if isinstance(x, (list, tuple)) and len(x) >= 2:\n            return x[0], x[1]\n        if isinstance(x, str):\n            parts = x.split(sep)\n            if len(parts) >= 2:\n                try:\n                    return float(parts[0]), float(parts[1])\n                except Exception:\n                    return np.nan, np.nan\n        return np.nan, np.nan\n\n    lat_list, lon_list = zip(*df[coord_col].map(_parse_one))\n    df = df.copy()\n    df[lat_name] = lat_list\n    df[lon_name] = lon_list\n    return df\n\n# NEW: –æ—á–∏—Å—Ç–∫–∞ –∏–º—ë–Ω —Ñ–∏—á–µ–π –¥–ª—è –±—É—Å—Ç–∏–Ω–≥–æ–≤ (XGBoost –æ—Å–æ–±–µ–Ω–Ω–æ —Å—Ç—Ä–æ–≥–∏–π)\ndef sanitize_feature_names(\n    df: pd.DataFrame,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Å–∏–º–≤–æ–ª–æ–≤ [, ], <, >,\n    —Å–ª–µ–¥–∏—Ç –∑–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å—é –∏–º—ë–Ω –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      - –Ω–æ–≤—ã–π DataFrame —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏\n      - mapping {—Å—Ç–∞—Ä–æ–µ_–∏–º—è -> –Ω–æ–≤–æ–µ_–∏–º—è}\n    \"\"\"\n    old_cols = list(df.columns)\n    new_cols: List[str] = []\n    mapping: Dict = {}\n    changed = False\n\n    df = df.copy()\n\n    for col in old_cols:\n        new = str(col)\n\n        # –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)\n        new = new.replace(\"[\", \"(\").replace(\"]\", \")\")\n        new = new.replace(\"<\", \"_lt_\").replace(\">\", \"_gt_\")\n\n        # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –æ—á–µ–Ω—å \"—ç–∫–∑–æ—Ç–∏–∫–∏\"\n        # (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É—Å—Ç—ã–µ –∏–º–µ–Ω–∞)\n        if new.strip() == \"\":\n            new = \"feature\"\n\n        base = new\n        k = 1\n        # –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å\n        while new in new_cols:\n            new = f\"{base}__{k}\"\n            k += 1\n\n        if new != col:\n            changed = True\n        mapping[col] = new\n        new_cols.append(new)\n\n    df.columns = new_cols\n\n    if changed:\n        logger.info(\"Sanitized feature names for boosting models.\")\n        # –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç—å –º–∞–ø–ø–∏–Ω–≥–∞\n        logger.info(\n            \"Example of feature name mapping: \" +\n            \", \".join(\n                f\"{k} -> {v}\" for k, v in list(mapping.items())[:10]\n            )\n        )\n\n    return df, mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.270532Z","iopub.execute_input":"2025-11-17T22:38:32.270775Z","iopub.status.idle":"2025-11-17T22:38:32.279948Z","shell.execute_reply.started":"2025-11-17T22:38:32.270755Z","shell.execute_reply":"2025-11-17T22:38:32.279254Z"}},"outputs":[],"execution_count":150},{"id":"fa0054b1","cell_type":"code","source":"class ImagesDataset:\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n    \n    def __getitem__(self, idx):\n        ### read image\n\n        file_path = self.file_paths[idx]\n        image = Image.open(file_path).convert(\"RGB\")\n        image = self.transform(image)\n        # DataLoader —Å–∞–º —Å–æ–∑–¥–∞—Å—Ç batch dimension, –ø–æ—ç—Ç–æ–º—É unsqueeze –Ω–µ –Ω—É–∂–µ–Ω\n        return image\n    \n    def __len__(self):\n        return len(self.file_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.280759Z","iopub.execute_input":"2025-11-17T22:38:32.281537Z","iopub.status.idle":"2025-11-17T22:38:32.292703Z","shell.execute_reply.started":"2025-11-17T22:38:32.281515Z","shell.execute_reply":"2025-11-17T22:38:32.292089Z"}},"outputs":[],"execution_count":151},{"id":"bb95ee92","cell_type":"code","source":"def detect_categorical_columns(\n    df: pd.DataFrame,\n    max_unique: int,\n    force_categorical: List[str]\n) -> List[str]:\n    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ max_unique –Ω–µ None\n    if max_unique is None:\n        max_unique = 64  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n    \n    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏–∑ force_categorical\n    cats = set(c for c in force_categorical if c is not None and isinstance(c, str))\n    \n    for col in df.columns:\n        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º None –∏–ª–∏ –Ω–µ-—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫\n        if col is None or not isinstance(col, str):\n            continue\n            \n        try:\n            if df[col].dtype == \"object\":\n                cats.add(col)\n            else:\n                try:\n                    nunique_val = df[col].nunique(dropna=True)\n                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ nunique_val –Ω–µ None –∏ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º\n                    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ max_unique - —ç—Ç–æ —á–∏—Å–ª–æ\n                    if (nunique_val is not None and \n                        isinstance(nunique_val, (int, np.integer)) and\n                        isinstance(max_unique, (int, np.integer, float))):\n                        if int(nunique_val) <= int(max_unique):\n                            cats.add(col)\n                except (TypeError, ValueError) as e:\n                    # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å nunique, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫—É\n                    continue\n        except Exception:\n            # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–ª–æ–Ω–∫–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—ë\n            continue\n    \n    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ –Ω–µ-—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π\n    # –¢–∞–∫–∂–µ —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Å—Ç—Ä–æ–∫–∏\n    cats_filtered = []\n    for c in cats:\n        if c is not None:\n            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–∫–∞\n            try:\n                c_str = str(c) if not isinstance(c, str) else c\n                if c_str and c_str.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è\n                    cats_filtered.append(c_str)\n            except Exception:\n                continue\n    \n    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏\n    try:\n        return sorted(cats_filtered)\n    except TypeError as e:\n        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –æ—à–∏–±–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏\n        return list(cats_filtered)\n\n\ndef process_categorical_features(\n    X: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    cat_cols: List[str],\n    config: Dict,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], List[str]]:\n    \"\"\"\n    –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Ñ–∏—á–∏:\n      1) –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –º–∞–ª–æ–π —á–∞—Å—Ç–æ—Ç–æ–π/–¥–æ–ª–µ–π —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ 'other'\n      2) –µ—Å–ª–∏ –¥–æ–ª—è —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫–∞ –∏–ª–∏\n         –¥–∞–∂–µ –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ‚Äî —Ñ–∏—á–∞ –≤—ã–∫–∏–¥—ã–≤–∞–µ—Ç—Å—è.\n\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      X_new, test_new, updated_cat_cols\n    \"\"\"\n    if not cat_cols:\n        return X, test, cat_cols\n\n    min_count = int(config.get(\"cat_min_count\", 0) or 0)\n    min_freq = float(config.get(\"cat_min_freq\", 0.0) or 0.0)\n    max_rare_share = float(config.get(\"cat_max_rare_share\", 1.0) or 1.0)\n    max_unique_after = int(config.get(\"cat_max_unique_after_group\", 10**9) or 10**9)\n\n    if min_count <= 0 and min_freq <= 0.0 and max_rare_share >= 1.0:\n        # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞\n        logger.info(\"Categorical processing: thresholds are trivial, skipping.\")\n        return X, test, cat_cols\n\n    logger.info(\n        \"Categorical processing: \"\n        f\"cat_min_count={min_count}, cat_min_freq={min_freq}, \"\n        f\"cat_max_rare_share={max_rare_share}, \"\n        f\"cat_max_unique_after_group={max_unique_after}\"\n    )\n\n    X_new = X.copy()\n    test_new = test.copy() if test is not None else None\n    to_drop = []\n\n    n = len(X_new)\n\n    for col in tqdm(cat_cols, desc=\"Processing categorical features\"):\n        if col not in X_new.columns:\n            continue\n\n        vc = X_new[col].value_counts(dropna=False)\n        total = vc.sum()\n\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º \"—Ä–µ–¥–∫–∏–µ\" –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n        rare_mask = np.zeros(len(vc), dtype=bool)\n        if min_count > 0:\n            rare_mask |= (vc.values < min_count)\n        if min_freq > 0.0:\n            rare_mask |= (vc.values / total < min_freq)\n\n        rare_cats = vc.index[rare_mask]\n        rare_share = vc[rare_cats].sum() / total if len(rare_cats) > 0 else 0.0\n\n        logger.info(\n            f\"[cat] {col}: unique={vc.size}, rare_cats={len(rare_cats)}, \"\n            f\"rare_share={rare_share:.4f}\"\n        )\n\n        # –µ—Å–ª–∏ —Ä–µ–¥–∫–∏—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî —Ñ–∏—á–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞, –≤—ã–∫–∏–¥—ã–≤–∞–µ–º\n        if rare_share >= max_rare_share:\n            logger.info(\n                f\"[cat] {col}: rare_share={rare_share:.4f} >= {max_rare_share}, \"\n                f\"dropping whole feature.\"\n            )\n            to_drop.append(col)\n            continue\n\n        if len(rare_cats) == 0:\n            # –Ω–µ—á–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å\n            continue\n\n        # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–¥–∫–∏–µ –≤ 'other'\n        other_label = \"__OTHER__\"\n        X_new[col] = X_new[col].where(~X_new[col].isin(rare_cats), other_label)\n        if test_new is not None:\n            test_new[col] = test_new[col].where(~test_new[col].isin(rare_cats), other_label)\n\n        # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n        new_unique = X_new[col].nunique(dropna=True)\n        if new_unique > max_unique_after:\n            logger.info(\n                f\"[cat] {col}: unique_after={new_unique} > \"\n                f\"cat_max_unique_after_group={max_unique_after}, dropping feature.\"\n            )\n            to_drop.append(col)\n\n    if to_drop:\n        logger.info(f\"Dropping categorical features: {to_drop}\")\n        X_new = X_new.drop(columns=[c for c in to_drop if c in X_new.columns])\n        if test_new is not None:\n            test_new = test_new.drop(columns=[c for c in to_drop if c in test_new.columns])\n        cat_cols = [c for c in cat_cols if c not in to_drop]\n\n    return X_new, test_new, cat_cols\n\n\ndef expand_datetime_columns(\n    df: pd.DataFrame,\n    datetime_cols: List[str],\n    features: List[str]\n) -> pd.DataFrame:\n    df = df.copy()\n    for col in datetime_cols:\n        if col not in df.columns:\n            continue\n        s = pd.to_datetime(df[col], errors=\"coerce\")\n\n        if \"year\" in features:\n            df[f\"{col}_year\"] = s.dt.year.astype(\"Int64\")\n        if \"month\" in features:\n            df[f\"{col}_month\"] = s.dt.month.astype(\"Int64\")\n        if \"day\" in features:\n            df[f\"{col}_day\"] = s.dt.day.astype(\"Int64\")\n        if \"dow\" in features:\n            df[f\"{col}_dow\"] = s.dt.dayofweek.astype(\"Int64\")\n        if \"hour\" in features:\n            df[f\"{col}_hour\"] = s.dt.hour.astype(\"Int64\")\n\n    return df\n\n\ndef haversine_distance(\n    lat1, lon1, lat2, lon2, radius: float = 6371.0\n) -> np.ndarray:\n    lat1 = np.radians(lat1.astype(float))\n    lon1 = np.radians(lon1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    lon2 = np.radians(lon2.astype(float))\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n\n    a = np.sin(dlat / 2.0) ** 2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return radius * c\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.293350Z","iopub.execute_input":"2025-11-17T22:38:32.293523Z","iopub.status.idle":"2025-11-17T22:38:32.312934Z","shell.execute_reply.started":"2025-11-17T22:38:32.293510Z","shell.execute_reply":"2025-11-17T22:38:32.312351Z"}},"outputs":[],"execution_count":152},{"id":"6493bc28","cell_type":"markdown","source":"### Data processing","metadata":{}},{"id":"504a78c8","cell_type":"code","source":"def load_data(config: Dict, logger: logging.Logger) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    train_path = config[\"train_path\"]\n    test_path = config[\"test_path\"]\n\n    logger.info(f\"üì• Loading train from {train_path}\")\n    train = pd.read_csv(train_path, sep=config[\"sep\"])\n\n    if test_path:\n        logger.info(f\"üì• Loading test from {test_path}\")\n        test = pd.read_csv(test_path, sep=config[\"sep\"])\n    else:\n        test = None\n\n    logger.info(f\"Train shape: {train.shape}\")\n    if test is not None:\n        logger.info(f\"Test  shape: {test.shape}\")\n\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.314918Z","iopub.execute_input":"2025-11-17T22:38:32.315326Z","iopub.status.idle":"2025-11-17T22:38:32.326262Z","shell.execute_reply.started":"2025-11-17T22:38:32.315310Z","shell.execute_reply":"2025-11-17T22:38:32.325543Z"}},"outputs":[],"execution_count":153},{"id":"99b6a7f4","cell_type":"code","source":"train_df, test_df = load_data(CONFIG, logger)\n\nid_col = CONFIG[\"id_column\"]\ntarget_col = CONFIG[\"target_column\"]\n\ntest_ids = None\nif test_df is not None and id_col in test_df.columns:\n    test_ids = test_df[id_col].copy()\n\ny = train_df[target_col]\nX = train_df.drop(columns=[target_col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.326818Z","iopub.execute_input":"2025-11-17T22:38:32.326972Z","iopub.status.idle":"2025-11-17T22:38:32.366418Z","shell.execute_reply.started":"2025-11-17T22:38:32.326959Z","shell.execute_reply":"2025-11-17T22:38:32.365747Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:38:32,336 - INFO -  Loading train from /kaggle/input/petfinder-pawpularity-score/train.csv\nINFO:TABULAR_BOOSTING:üì• Loading train from /kaggle/input/petfinder-pawpularity-score/train.csv\n2025-11-17 22:38:32,357 - INFO -  Loading test from /kaggle/input/petfinder-pawpularity-score/test.csv\nINFO:TABULAR_BOOSTING:üì• Loading test from /kaggle/input/petfinder-pawpularity-score/test.csv\n2025-11-17 22:38:32,359 - INFO - Train shape: (9912, 14)\nINFO:TABULAR_BOOSTING:Train shape: (9912, 14)\n2025-11-17 22:38:32,360 - INFO - Test  shape: (8, 13)\nINFO:TABULAR_BOOSTING:Test  shape: (8, 13)\n","output_type":"stream"}],"execution_count":154},{"id":"43910515","cell_type":"code","source":"### –î–æ–±–∞–≤—å —Å—é–¥–∞ —Å–æ–∑–¥–∞–Ω–∏–µ reference –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≥–µ–æ-—Ñ–∏—á–µ–π\n# –ù–∞ –æ—Å–Ω–æ–≤–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ\ndef fit_geo_reference(df: pd.DataFrame, config: Dict, logger: Optional[logging.Logger] = None) -> pd.DataFrame:\n    \"\"\"\n    –°—Ç—Ä–æ–∏—Ç DataFrame geo_ref_df, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—Ä–µ–¥–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã.\n    –ó–¥–µ—Å—å —Å–æ–∑–¥–∞—ë—Ç—Å—è reference-–∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –≥–µ–æ-—Ñ–∏—á–µ–π:\n      - <output_lat_column>, <output_lon_column> (float)\n      - <output_column> –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ \"lat,lon\".\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    cat_col = geo_cfg.get(\"category_column\", \"category\")\n    coord_col = geo_cfg.get(\"coordinates_column\", \"coordinates\")\n    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n\n    lat_col_cfg = geo_cfg.get(\"lat_column\")\n    lon_col_cfg = geo_cfg.get(\"lon_column\")\n\n    df_work = df.copy()\n\n    # 1) –ë–µ—Ä—ë–º lat/lon –ª–∏–±–æ –∏–∑ —è–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ª–∏–±–æ –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É \"lat,lon\"\n    if (\n        lat_col_cfg\n        and lon_col_cfg\n        and lat_col_cfg in df_work.columns\n        and lon_col_cfg in df_work.columns\n    ):\n        # –Ø–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —à–∏—Ä–æ—Ç—ã/–¥–æ–ª–≥–æ—Ç—ã\n        df_work[\"_lat\"] = pd.to_numeric(df_work[lat_col_cfg], errors=\"coerce\")\n        df_work[\"_lon\"] = pd.to_numeric(df_work[lon_col_cfg], errors=\"coerce\")\n    else:\n        # –ü–∞—Ä—Å–∏–º coordinates –∫–∞–∫ —Å—Ç—Ä–æ–∫—É \"lat,lon\" (–º–æ–∂–Ω–æ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏)\n        coords_parsed = (\n            df_work[coord_col]\n            .astype(str)\n            .str.extract(r\"([+-]?\\d+\\.?\\d*)\\s*,\\s*([+-]?\\d+\\.?\\d*)\")\n        )\n        df_work[\"_lat\"] = pd.to_numeric(coords_parsed[0], errors=\"coerce\")\n        df_work[\"_lon\"] = pd.to_numeric(coords_parsed[1], errors=\"coerce\")\n\n    # 2) –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ –≥—Ä—É–ø–ø–∞–º\n    geo_ref_df = (\n        df_work.groupby(cat_col)[[\"_lat\", \"_lon\"]]\n        .mean()\n        .rename(columns={\"_lat\": out_lat_col, \"_lon\": out_lon_col})\n        .reset_index()\n    )\n\n    # 3) –°—Ç—Ä–æ–∫–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ \"lat,lon\" (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏/—ç–∫—Å–ø–æ—Ä—Ç–∞)\n    geo_ref_df[out_col] = (\n        geo_ref_df[out_lat_col].round(6).astype(str)\n        + \",\" +\n        geo_ref_df[out_lon_col].round(6).astype(str)\n    )\n\n    if logger:\n        logger.info(\n            f\"üåç –û–±—É—á–∏–ª–∏ reference-–≥–µ–æ-—Ü–µ–Ω—Ç—Ä—ã –ø–æ {cat_col}, –≤—Å–µ–≥–æ –≥—Ä—É–ø–ø: {geo_ref_df.shape[0]}\"\n        )\n    return geo_ref_df\n\ndef add_geo_reference_column(\n    df: pd.DataFrame,\n    geo_ref_df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> pd.DataFrame:\n    \"\"\"\n    –î–æ–±–∞–≤–ª—è–µ—Ç reference-–≥–µ–æ-–∫–æ–ª–æ–Ω–∫—É(–∏) –∫ df, –∏—Å–ø–æ–ª—å–∑—É—è geo_ref_df, –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—É—é –Ω–∞ train.\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    cat_col = geo_cfg.get(\"category_column\", \"category\")\n    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n\n    df = df.merge(geo_ref_df[[cat_col, out_col, out_lat_col, out_lon_col]], how=\"left\", on=cat_col)\n    n_missing = df[out_col].isna().sum()\n    if logger:\n        logger.info(\n            f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã reference-–≥–µ–æ-–∫–æ–ª–æ–Ω–∫–∏ ({out_col}, {out_lat_col}, {out_lon_col}) \"\n            f\"–∫ df (geo reference –ø–æ {cat_col}). –ü—Ä–æ–ø—É—Å–∫–æ–≤: {n_missing}\"\n        )\n    return df\n\n\ndef maybe_apply_geo_reference(\n    df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None,\n    fit: bool = False,\n    geo_ref_df: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è train/test: –µ—Å–ª–∏ fit=True, –≤—ã—á–∏—Å–ª—è–µ—Ç geo reference –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ train,\n    –µ—Å–ª–∏ fit=False (test), –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç geo_ref_df –∫ df.\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      - –ø—Ä–∏ fit=True: (df_with_ref, geo_ref_df)\n      - –ø—Ä–∏ fit=False: df_with_ref\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    if not geo_cfg.get(\"enabled\", False):\n        if logger:\n            logger.info(\"üåé –ì–µ–æ-reference –≤—ã–∫–ª—é—á–µ–Ω (config['geo_reference']['enabled']=False)\")\n        return (df, None) if fit else df\n\n    if fit:\n        geo_ref_df = fit_geo_reference(df, config, logger)\n        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n        return df, geo_ref_df\n    else:\n        if geo_ref_df is None:\n            raise ValueError(\"geo_ref_df must be passed for applying on test set\")\n        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n        return df\n\n# --- –ü—Ä–∏–º–µ–Ω—è–µ–º reference-–≥–µ–æ-—Ñ–∏—á–∏ –∫ train –∏ test, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ ---\nif CONFIG.get(\"geo_reference\", {}).get(\"enabled\", False):\n    logger.info(\"üåé –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–µ–æ-reference features...\")\n    train_df, geo_ref_df = maybe_apply_geo_reference(train_df, CONFIG, logger, fit=True)\n    test_df = maybe_apply_geo_reference(test_df, CONFIG, logger, fit=False, geo_ref_df=geo_ref_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.367125Z","iopub.execute_input":"2025-11-17T22:38:32.367334Z","iopub.status.idle":"2025-11-17T22:38:32.379820Z","shell.execute_reply.started":"2025-11-17T22:38:32.367310Z","shell.execute_reply":"2025-11-17T22:38:32.379079Z"}},"outputs":[],"execution_count":155},{"id":"925eb654","cell_type":"code","source":"if CONFIG['task_type'] in ['multiclass', 'binary']:\n    y = y.astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.380535Z","iopub.execute_input":"2025-11-17T22:38:32.380844Z","iopub.status.idle":"2025-11-17T22:38:32.393506Z","shell.execute_reply.started":"2025-11-17T22:38:32.380822Z","shell.execute_reply":"2025-11-17T22:38:32.392852Z"}},"outputs":[],"execution_count":156},{"id":"8b12d371","cell_type":"markdown","source":"–£–î–ê–õ–ï–ù–ò–ï ID","metadata":{}},{"id":"3ec651d8","cell_type":"code","source":"# NOTE: ID column is intentionally NOT dropped here.\n# –û–Ω–æ –±—É–¥–µ—Ç —É–¥–∞–ª–µ–Ω–æ –ø–æ–∑–∂–µ, –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –±—É—Å—Ç–∏–Ω–≥–æ–≤,\n# —á—Ç–æ–±—ã —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å ID –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏—á (–∫–∞—Ä—Ç–∏–Ω–∫–∏/—Ç–µ–∫—Å—Ç—ã/–∞–≥—Ä–µ–≥–∞—Ü–∏–∏).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.394332Z","iopub.execute_input":"2025-11-17T22:38:32.394755Z","iopub.status.idle":"2025-11-17T22:38:32.403548Z","shell.execute_reply.started":"2025-11-17T22:38:32.394737Z","shell.execute_reply":"2025-11-17T22:38:32.402983Z"}},"outputs":[],"execution_count":157},{"id":"f17f2c77","cell_type":"markdown","source":"### Features generation","metadata":{}},{"id":"33a70100-a20c-44a8-a348-b157beabe4e8","cell_type":"code","source":"#!pip install open_clip_torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.404188Z","iopub.execute_input":"2025-11-17T22:38:32.404415Z","iopub.status.idle":"2025-11-17T22:38:32.414865Z","shell.execute_reply.started":"2025-11-17T22:38:32.404395Z","shell.execute_reply":"2025-11-17T22:38:32.414281Z"}},"outputs":[],"execution_count":158},{"id":"f24d9e6f-4279-44df-9781-2dbb26af4c27","cell_type":"code","source":"import open_clip\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.415545Z","iopub.execute_input":"2025-11-17T22:38:32.415786Z","iopub.status.idle":"2025-11-17T22:38:32.426148Z","shell.execute_reply.started":"2025-11-17T22:38:32.415766Z","shell.execute_reply":"2025-11-17T22:38:32.425488Z"}},"outputs":[],"execution_count":159},{"id":"44b85bc8","cell_type":"code","source":"def generate_basic_features(\n    df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> pd.DataFrame:\n    df = df.copy()\n\n    drop_cols = config[\"basic_drop_columns\"]\n    for c in drop_cols:\n        if c in df.columns:\n            if logger:\n                logger.info(f\"Dropping column: {c}\")\n            df = df.drop(columns=[c])\n\n    if config[\"basic_datetime_expand\"] and config[\"datetime_columns\"]:\n        if logger:\n            logger.info(f\"Expanding datetime columns: {config['datetime_columns']}\")\n        df = expand_datetime_columns(\n            df,\n            datetime_cols=config[\"datetime_columns\"],\n            features=config[\"basic_datetime_features\"]\n        )\n\n    return df\n\n\ndef generate_aggregate_features(\n    train: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    if not config[\"agg_enable\"]:\n        if logger:\n            logger.info(\"Aggregate features disabled in CONFIG.\")\n        return train, test\n\n    groupby_cols = config[\"agg_groupby_cols\"]\n    if not groupby_cols:\n        if logger:\n            logger.info(\"No agg_groupby_cols specified ‚Äî skipping aggregates.\")\n        return train, test\n\n    if logger:\n        logger.info(f\"Generating aggregate features by {groupby_cols}\")\n\n    all_df = train if test is None else pd.concat([train, test], axis=0, ignore_index=True)\n\n    if config[\"agg_numeric_cols\"]:\n        num_cols = [c for c in config[\"agg_numeric_cols\"] if c in all_df.columns]\n    else:\n        num_cols = [\n            c for c in all_df.columns\n            if pd.api.types.is_numeric_dtype(all_df[c])\n        ]\n        for c in [config[\"target_column\"], config[\"id_column\"]]:\n            if c in num_cols:\n                num_cols.remove(c)\n\n    aggs = config[\"agg_aggs\"]\n    prefix = config[\"agg_prefix\"]\n\n    if logger:\n        logger.info(f\"Aggregating numeric cols: {num_cols}\")\n        logger.info(f\"Using aggs: {aggs}, prefix: {prefix}\")\n\n    grouped = all_df.groupby(groupby_cols)[num_cols].agg(aggs)\n    grouped.columns = [\n        f\"{prefix}_\" + \"_\".join(map(str, col)).strip()\n        for col in grouped.columns.to_flat_index()\n    ]\n    grouped = grouped.reset_index()\n\n    if logger:\n        logger.info(f\"Aggregate frame shape: {grouped.shape}\")\n\n    train_merged = train.merge(grouped, on=groupby_cols, how=\"left\")\n    test_merged = None\n    if test is not None:\n        test_merged = test.merge(grouped, on=groupby_cols, how=\"left\")\n\n    return train_merged, test_merged\n\n\ndef generate_geo_features(\n    train: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    \"\"\"\n    –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏:\n      - geo_dist_from_to_km\n      - geo_dist_from_ref_km, geo_dist_to_ref_km (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω ref)\n      - geo_lat_from_abs / geo_lon_from_abs / geo_lat_to_abs / geo_lon_to_abs\n\n    –ï—Å–ª–∏ geo_extra_enable=True:\n      - geo_dlat / geo_dlon\n      - geo_manhattan_km\n      - geo_bearing_deg\n      - geo_mid_lat / geo_mid_lon\n    \"\"\"\n    if not config[\"geo_enable\"]:\n        if logger:\n            logger.info(\"Geo features disabled in CONFIG.\")\n        return train, test\n\n    lat_from_col = config[\"geo_lat_from_col\"]\n    lon_from_col = config[\"geo_lon_from_col\"]\n    lat_to_col = config[\"geo_lat_to_col\"]\n    lon_to_col = config[\"geo_lon_to_col\"]\n    prefix = config[\"geo_prefix\"]\n\n    from_coord_col = config.get(\"geo_from_coord_col\")\n    to_coord_col = config.get(\"geo_to_coord_col\")\n    coord_sep = config.get(\"geo_coord_string_sep\", \",\")\n\n    if from_coord_col is not None:\n        train = _ensure_lat_lon_from_single_column(\n            train, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n        )\n        if test is not None:\n            test = _ensure_lat_lon_from_single_column(\n                test, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n            )\n\n    if to_coord_col is not None:\n        train = _ensure_lat_lon_from_single_column(\n            train, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n        )\n        if test is not None:\n            test = _ensure_lat_lon_from_single_column(\n                test, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n            )\n\n\n    for col in [lat_from_col, lon_from_col, lat_to_col, lon_to_col]:\n        if col and col not in train.columns:\n            raise ValueError(f\"Column {col} not found in train for geo features\")\n\n    if logger:\n        logger.info(f\"Generating geo features with backend: {GEO_BACKEND}\")\n\n    def _distance_rowwise(\n        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n    ) -> np.ndarray:\n        if GEO_BACKEND == \"geopy\":\n            def _one(a, b, c, d):\n                if pd.isna(a) or pd.isna(b) or pd.isna(c) or pd.isna(d):\n                    return np.nan\n                return geodesic((a, b), (c, d)).km\n            return np.vectorize(_one)(lat1, lon1, lat2, lon2)\n        else:\n            return haversine_distance(lat1, lon1, lat2, lon2)\n\n    def _bearing(\n        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n    ) -> np.ndarray:\n        # initial bearing (degrees)\n        lat1_r = np.radians(lat1.astype(float))\n        lat2_r = np.radians(lat2.astype(float))\n        dlon_r = np.radians(lon2.astype(float) - lon1.astype(float))\n\n        x = np.sin(dlon_r) * np.cos(lat2_r)\n        y = np.cos(lat1_r) * np.sin(lat2_r) - np.sin(lat1_r) * np.cos(lat2_r) * np.cos(dlon_r)\n        brng = np.degrees(np.arctan2(x, y))\n        brng = (brng + 360) % 360\n        return brng\n\n    def _apply_geo(df: pd.DataFrame) -> pd.DataFrame:\n        df = df.copy()\n\n        df[f\"{prefix}_dist_from_to_km\"] = _distance_rowwise(\n            df[lat_from_col], df[lon_from_col],\n            df[lat_to_col], df[lon_to_col]\n        )\n\n        ref_lat = config[\"geo_ref_lat\"]\n        ref_lon = config[\"geo_ref_lon\"]\n        if ref_lat is not None and ref_lon is not None:\n            df[f\"{prefix}_dist_from_ref_km\"] = _distance_rowwise(\n                df[lat_from_col], df[lon_from_col],\n                pd.Series(ref_lat, index=df.index),\n                pd.Series(ref_lon, index=df.index),\n            )\n            df[f\"{prefix}_dist_to_ref_km\"] = _distance_rowwise(\n                df[lat_to_col], df[lon_to_col],\n                pd.Series(ref_lat, index=df.index),\n                pd.Series(ref_lon, index=df.index),\n            )\n\n        df[f\"{prefix}_lat_from_abs\"] = df[lat_from_col].abs()\n        df[f\"{prefix}_lon_from_abs\"] = df[lon_from_col].abs()\n        df[f\"{prefix}_lat_to_abs\"] = df[lat_to_col].abs()\n        df[f\"{prefix}_lon_to_abs\"] = df[lon_to_col].abs()\n\n        if config.get(\"geo_extra_enable\", False):\n            df[f\"{prefix}_dlat\"] = df[lat_to_col] - df[lat_from_col]\n            df[f\"{prefix}_dlon\"] = df[lon_to_col] - df[lon_from_col]\n\n            df[f\"{prefix}_manhattan_km\"] = (\n                haversine_distance(df[lat_from_col], df[lon_from_col], df[lat_to_col], df[lon_from_col]) +\n                haversine_distance(df[lat_to_col], df[lon_from_col], df[lat_to_col], df[lon_to_col])\n            )\n\n            df[f\"{prefix}_bearing_deg\"] = _bearing(\n                df[lat_from_col], df[lon_from_col],\n                df[lat_to_col], df[lon_to_col]\n            )\n\n            df[f\"{prefix}_mid_lat\"] = (df[lat_from_col] + df[lat_to_col]) / 2.0\n            df[f\"{prefix}_mid_lon\"] = (df[lon_from_col] + df[lon_to_col]) / 2.0\n\n        return df\n\n    train_geo = _apply_geo(train)\n    test_geo = _apply_geo(test) if test is not None else None\n    return train_geo, test_geo\n\n\ndef process_address_extract_city(\n    df: pd.DataFrame,\n    column: str,\n    city_index: int,\n    sep: str,\n    logger: logging.Logger\n) -> pd.DataFrame:\n    \"\"\"\n    –ó–¥–µ—Å—å –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞ '–≥–æ—Ä–æ–¥':\n      - address -> —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π (split)\n      - –∏–∑ —Å–ø–∏—Å–∫–∞ –±–µ—Ä—ë–º city_index (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é -1 = –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç)\n      - —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É address_city\n      - –æ–Ω–∞ –≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç\n        —Ç–∞–∫—É—é –∂–µ –æ–±—Ä–∞–±–æ—Ç–∫—É, –∫–∞–∫ –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏\n    \"\"\"\n    if column not in df.columns:\n        logger.warning(f\"Address column '{column}' not found, skipping city extraction.\")\n        return df\n\n    df = df.copy()\n\n    def _extract(addr):\n        if not isinstance(addr, str):\n            return None\n        parts = [p.strip() for p in addr.split(sep)]\n        if len(parts) == 0:\n            return None\n        try:\n            return parts[city_index]\n        except Exception:\n            # –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\n            return parts[-1]\n\n    df[\"address_city\"] = df[column].map(_extract)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.426790Z","iopub.execute_input":"2025-11-17T22:38:32.426986Z","iopub.status.idle":"2025-11-17T22:38:32.450411Z","shell.execute_reply.started":"2025-11-17T22:38:32.426963Z","shell.execute_reply":"2025-11-17T22:38:32.449789Z"}},"outputs":[],"execution_count":160},{"id":"0dd84223","cell_type":"code","source":"import os\n\ndef _normalize_image_name(name: str, default_ext: str | None = None) -> str:\n    \"\"\"\n    –ü—Ä–∏–≤–æ–¥–∏–º –∏–º—è —Ñ–∞–π–ª–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –≤–∏–¥—É:\n    - —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫\n    - –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º default_ext –∏–ª–∏ CONFIG[\"image_ext\"]\n    \"\"\"\n    name = str(name).strip()\n    base = os.path.basename(name)\n    root, ext = os.path.splitext(base)\n    if ext == \"\":\n        if default_ext is None:\n            default_ext = CONFIG.get(\"image_ext\", \"\")\n        return root + (default_ext or \"\")\n    return root + ext\n\ndef images_features(X, test_df):\n    \"\"\"\n    –î–æ–±–∞–≤–ª—è–µ—Ç –∫ —Ç–∞–±–ª–∏—á–Ω—ã–º —Ñ–∏—á–∞–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫, –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—ã–µ CLIP (OpenCLIP, —Ç–æ–ª—å–∫–æ image-encoder).\n    –û–∂–∏–¥–∞–µ—Ç—Å—è, —á—Ç–æ –≤ CONFIG –∑–∞–¥–∞–Ω—ã:\n      - 'file_names_column'  ‚Äî –∫–æ–ª–æ–Ω–∫–∞ —Å –∏–º–µ–Ω–∞–º–∏ —Ñ–∞–π–ª–æ–≤\n      - 'train_images_dir'   ‚Äî –ø–∞–ø–∫–∞ —Å train-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏\n      - 'test_images_dir'    ‚Äî –ø–∞–ø–∫–∞ —Å test-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏\n      - 'batch_size'         ‚Äî —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n    \"\"\"\n    file_name_column = CONFIG['file_names_column']\n\n    if file_name_column not in X.columns:\n        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} –Ω–µ—Ç –≤ train\")\n\n    if test_df is not None and file_name_column not in test_df.columns:\n        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} –Ω–µ—Ç –≤ test\")\n\n    # ----------------- CLIP BACKBONE (image encoder only) -----------------\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = \"mps\"\n    else:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    model, _, preprocess = open_clip.create_model_and_transforms(\n        \"ViT-B-32\",\n        pretrained=\"laion2b_s34b_b79k\"\n    )\n    model = model.to(device)\n    model.eval()\n    image_transform = preprocess\n\n    # ----------------- TRAIN IMAGES -----------------\n    train_file_paths = [\n        os.path.join(\n            CONFIG['train_images_dir'],\n            _normalize_image_name(file_name, CONFIG.get(\"image_ext\", \"\"))\n        )\n        for file_name in X[file_name_column]\n    ]\n    train_images_part = process_images(train_file_paths, model, image_transform, name=\"train_clip\")\n    train_images_part.index = X.index\n    X = pd.concat([X, train_images_part], axis=1)\n\n    # ----------------- TEST IMAGES ------------------\n    if test_df is not None:\n        test_file_paths = [\n            os.path.join(\n                CONFIG['test_images_dir'],\n                _normalize_image_name(file_name, CONFIG.get(\"image_ext\", \"\"))\n            )\n            for file_name in test_df[file_name_column]\n        ]\n        test_images_part = process_images(test_file_paths, model, image_transform, name=\"test_clip\")\n        test_images_part.index = test_df.index\n        test_df = pd.concat([test_df, test_images_part], axis=1)\n\n    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ —É—à–ª–∞ –≤ –±—É—Å—Ç–∏–Ω–≥–∏\n    for df in (X, test_df):\n        if df is not None and file_name_column in df.columns:\n            df.drop(columns=[file_name_column], inplace=True)\n\n    return X, test_df\n\ndef process_images(file_paths, model, image_transform, name):\n    \"\"\"\n    –°—á–∏—Ç–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –ø—É—Ç–µ–π –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –æ–±—ã—á–Ω—ã–µ CNN (model(x)), —Ç–∞–∫ –∏ CLIP-–º–æ–¥–µ–ª–∏ (model.encode_image(x)).\n    \"\"\"\n    img_dataset = ImagesDataset(file_paths, image_transform)\n    img_dataloader = DataLoader(img_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n\n    # –ü–æ–ª—É—á–∞–µ–º device –º–æ–¥–µ–ª–∏\n    device = next(model.parameters()).device\n\n    collect_embs = []\n    with torch.no_grad():\n        for images in tqdm(img_dataloader, total=len(img_dataloader), desc=f\"Images {name} process...\"):\n            images = images.to(device)\n\n            # –ï—Å–ª–∏ —ç—Ç–æ CLIP-–º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º encode_image, –∏–Ω–∞—á–µ –æ–±—ã—á–Ω—ã–π forward\n            if hasattr(model, \"encode_image\"):\n                embs = model.encode_image(images)\n            else:\n                embs = model(images)\n\n            # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—ë—Ç 4D-—Ç–µ–Ω–∑–æ—Ä (N, C, 1, 1)\n            if embs.ndim == 4:\n                embs = torch.flatten(embs, 1)\n\n            embs = embs.cpu().numpy()\n            # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º (num_samples x dim)\n            norms = np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12\n            embs = embs / norms\n            collect_embs.extend(embs.tolist())\n\n    if not collect_embs:\n        # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π, —á—Ç–æ–±—ã –Ω–µ —É–ø–∞—Å—Ç—å –Ω–∞ –ø—É—Å—Ç–æ–º —Å–ø–∏—Å–∫–µ\n        return pd.DataFrame()\n\n    n_feat = len(collect_embs[0])\n    return pd.DataFrame(collect_embs, columns=[f\"emb_{i}\" for i in range(n_feat)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.451275Z","iopub.execute_input":"2025-11-17T22:38:32.451502Z","iopub.status.idle":"2025-11-17T22:38:32.466727Z","shell.execute_reply.started":"2025-11-17T22:38:32.451487Z","shell.execute_reply":"2025-11-17T22:38:32.466063Z"}},"outputs":[],"execution_count":161},{"id":"f360d267","cell_type":"markdown","source":"–¢–ï–ö–°–¢–û–í–´–ï –§–ò–ß–ò","metadata":{}},{"id":"e964e493","cell_type":"code","source":"def get_text_device():\n    global TEXT_DEVICE\n    if TEXT_DEVICE is None:\n        TEXT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return TEXT_DEVICE\n\n\ndef init_bert_model():\n    \"\"\"\n    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BERT-–º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑.\n    \"\"\"\n    global TEXT_MODEL, TEXT_TOKENIZER, TEXT_DEVICE\n\n    if TEXT_MODEL is not None:\n        return\n\n    model_name = CONFIG.get(\n        \"text_bert_model_name\",\n        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n    )\n    TEXT_DEVICE = get_text_device()\n\n    TEXT_TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n    TEXT_MODEL = AutoModel.from_pretrained(model_name)\n    TEXT_MODEL.to(TEXT_DEVICE)\n    TEXT_MODEL.eval()\n    \ndef _detect_format_from_path(path: str) -> str:\n    ext = os.path.splitext(path)[1].lower()\n    if ext == \".parquet\":\n        return \"parquet\"\n    if ext == \".csv\":\n        return \"csv\"\n    if ext == \".tsv\":\n        return \"tsv\"\n    # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º csv\n    return \"csv\"\n\n\ndef load_text_table(path: str, fmt: str | None, id_column: str, text_column: str) -> pd.DataFrame:\n    \"\"\"\n    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–Ω–µ—à–Ω—é—é —Ç–∞–±–ª–∏—Ü—É —Å —Ç–µ–∫—Å—Ç–æ–º.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º parquet/csv/tsv. format=\"auto\" -> –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é.\n    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ª–±—Ü—ã [id_column, text_column].\n    \"\"\"\n    if fmt is None or fmt == \"auto\":\n        fmt = _detect_format_from_path(path)\n\n    if fmt == \"parquet\":\n        df = pd.read_parquet(path)\n    elif fmt == \"tsv\":\n        df = pd.read_csv(path, sep=\"\\t\")\n    else:  # \"csv\" –∏ –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é\n        df = pd.read_csv(path)\n\n    # –Ø–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å\n    missing = [c for c in (id_column, text_column) if c not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Columns {missing} not found in external text table {path}. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    return df[[id_column, text_column]].copy()\n\ndef attach_external_text_tables(X: pd.DataFrame, test_df: pd.DataFrame):\n    \"\"\"–ü–æ–¥—Ü–µ–ø–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (CONFIG['text_external_tables']) –∫ X –∏ test_df.\n\n    –í–∞–∂–Ω–æ: –µ—Å–ª–∏ –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ id,\n    –º—ã —Å–Ω–∞—á–∞–ª–∞ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –∏—Ö –≤ –û–î–ù–£ —Å—Ç—Ä–æ–∫—É (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª),\n    —á—Ç–æ–±—ã merge –ù–ï —Ä–∞–∑–¥—É–≤–∞–ª –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ.\n    \"\"\"\n    external_cfgs = CONFIG.get(\"text_external_tables\", []) or []\n    if not external_cfgs:\n        return X, test_df, []\n\n    main_id_col = CONFIG.get(\"id_column\", \"index\")\n    added_cols: list[str] = []\n\n    for cfg in external_cfgs:\n        name = cfg.get(\"name\", \"ext\")\n\n        train_path = cfg[\"train_path\"]\n        # –µ—Å–ª–∏ test_path –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª\n        test_path = cfg.get(\"test_path\", train_path)\n\n        fmt = cfg.get(\"format\", \"auto\")\n\n        # ID-–∫–æ–ª–æ–Ω–∫–∞ –≤ external-—Ç–∞–±–ª–∏—Ü–µ: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π\n        ext_id_col = cfg.get(\"id_column\", main_id_col)\n\n        text_col_ext = cfg[\"text_column\"]\n        output_col = cfg.get(\"output_column\", f\"text_ext_{name}\")\n\n        # --- TRAIN ---\n        ext_train = load_text_table(train_path, fmt, ext_id_col, text_col_ext)\n        ext_train = ext_train.rename(columns={text_col_ext: output_col})\n\n        # NEW: –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É id –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n        if ext_train[ext_id_col].duplicated().any():\n            ext_train = (\n                ext_train\n                .groupby(ext_id_col, as_index=False)[output_col]\n                .agg(lambda s: \" \".join(map(str, s)))\n            )\n\n        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ main_id_col –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö X\n        # –ï—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è merge\n        if main_id_col in X.columns:\n            # ID –∫–æ–ª–æ–Ω–∫–∞ –µ—Å—Ç—å –≤ X\n            if main_id_col == ext_id_col:\n                # –ò–º—è ID –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∏ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n                X = X.merge(\n                    ext_train[[ext_id_col, output_col]],\n                    on=ext_id_col,\n                    how=\"left\"\n                )\n            else:\n                # –ò–º—è ID –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∏ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–∞—Ö —Ä–∞–∑–Ω–æ–µ\n                X = X.merge(\n                    ext_train[[ext_id_col, output_col]],\n                    left_on=main_id_col,\n                    right_on=ext_id_col,\n                    how=\"left\"\n                ).drop(columns=[ext_id_col])\n        else:\n            # ID –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –≤ X, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å\n            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ext_id_col –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n            ext_train_indexed = ext_train.set_index(ext_id_col)[[output_col]]\n            X = X.merge(\n                ext_train_indexed,\n                left_index=True,\n                right_index=True,\n                how=\"left\"\n            )\n\n        # --- TEST ---\n        if test_df is not None:\n            ext_test = load_text_table(test_path, fmt, ext_id_col, text_col_ext)\n            ext_test = ext_test.rename(columns={text_col_ext: output_col})\n\n            # NEW: –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É id –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n            if ext_test[ext_id_col].duplicated().any():\n                ext_test = (\n                    ext_test\n                    .groupby(ext_id_col, as_index=False)[output_col]\n                    .agg(lambda s: \" \".join(map(str, s)))\n                )\n\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ main_id_col –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö test_df\n            if main_id_col in test_df.columns:\n                if main_id_col == ext_id_col:\n                    test_df = test_df.merge(\n                        ext_test[[ext_id_col, output_col]],\n                        on=ext_id_col,\n                        how=\"left\"\n                    )\n                else:\n                    test_df = test_df.merge(\n                        ext_test[[ext_id_col, output_col]],\n                        left_on=main_id_col,\n                        right_on=ext_id_col,\n                        how=\"left\"\n                    ).drop(columns=[ext_id_col])\n            else:\n                # ID –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –≤ test_df, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å\n                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ext_id_col –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n                ext_test_indexed = ext_test.set_index(ext_id_col)[[output_col]]\n                test_df = test_df.merge(\n                    ext_test_indexed,\n                    left_index=True,\n                    right_index=True,\n                    how=\"left\"\n                )\n\n        added_cols.append(output_col)\n\n    return X, test_df, added_cols\n\n\ndef build_bert_embeddings_for_field(field_name: str,\n                                    train_texts: list[str],\n                                    test_texts: list[str]) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    –°—Ç—Ä–æ–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ BERT –¥–ª—è –æ–¥–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è.\n    BERT-–º–æ–¥–µ–ª—å –æ–±—â–∞—è –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª–µ–π, –Ω–æ —Å—á–∏—Ç–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ.\n    \"\"\"\n    init_bert_model()\n\n    device = get_text_device()\n    max_len = CONFIG.get(\"text_max_length\", 256)\n    batch_size = CONFIG.get(\"text_batch_size\", 32)\n\n    def encode_list(texts: list[str], desc: str) -> np.ndarray:\n        all_embs = []\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"{desc} [{field_name}]\"):\n            batch = texts[i:i + batch_size]\n\n            enc = TEXT_TOKENIZER(\n                batch,\n                padding=True,\n                truncation=True,\n                max_length=max_len,\n                return_tensors=\"pt\"\n            )\n            enc = {k: v.to(device) for k, v in enc.items()}\n\n            with torch.no_grad():\n                outputs = TEXT_MODEL(**enc)\n                last_hidden = outputs.last_hidden_state  # [bs, seq_len, hidden]\n\n                mask = enc[\"attention_mask\"].unsqueeze(-1).expand(last_hidden.size())\n                masked = last_hidden * mask\n\n                summed = masked.sum(dim=1)\n                counts = mask.sum(dim=1).clamp(min=1)\n                mean_pooled = summed / counts  # [bs, hidden]\n\n                embs = mean_pooled.cpu().numpy()\n\n                # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n                norms = np.linalg.norm(embs, axis=1, keepdims=True)\n                norms = np.clip(norms, 1e-12, None)\n                embs = embs / norms\n\n                all_embs.append(embs)\n\n        return np.vstack(all_embs) if len(all_embs) > 0 else np.zeros((0, TEXT_MODEL.config.hidden_size))\n\n    train_embs = encode_list(train_texts, \"BERT text embeddings (train)\")\n    test_embs = encode_list(test_texts, \"BERT text embeddings (test)\")\n\n    return train_embs, test_embs\ndef build_tfidf_svd_embeddings_for_field(field_name: str,\n                                         train_texts: list[str],\n                                         test_texts: list[str],\n                                         logger: logging.Logger) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    TF-IDF (uni+bi-grams) -> TruncatedSVD –¥–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ -> L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.\n    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è —Å–≤–æ–π TF-IDF –∏ SVD, —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.\n    \"\"\"\n    max_features = CONFIG.get(\"text_tfidf_max_features\", 50000)\n    n_components = CONFIG.get(\"text_svd_n_components\", 256)\n    random_state = CONFIG.get(\"seed\", 42)\n\n    vect = TFIDF_VECTORIZERS.get(field_name)\n    svd = TFIDF_SVDS.get(field_name)\n\n    if vect is None or svd is None:\n        vect = TfidfVectorizer(\n            max_features=max_features,\n            ngram_range=(1, 2)\n        )\n        logger.info(f\"TF-IDF fitting for field '{field_name}'...\")\n        train_tfidf = vect.fit_transform(train_texts)\n\n        svd = TruncatedSVD(\n            n_components=n_components,\n            random_state=random_state\n        )\n        logger.info(f\"SVD fitting for field '{field_name}'...\")\n        train_embs = svd.fit_transform(train_tfidf)\n\n        TFIDF_VECTORIZERS[field_name] = vect\n        TFIDF_SVDS[field_name] = svd\n    else:\n        train_tfidf = vect.transform(train_texts)\n        train_embs = svd.transform(train_tfidf)\n\n    test_tfidf = vect.transform(test_texts)\n    test_embs = svd.transform(test_tfidf)\n\n    # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n    def l2_norm(x: np.ndarray) -> np.ndarray:\n        norms = np.linalg.norm(x, axis=1, keepdims=True)\n        norms = np.where(norms == 0, 1, norms)\n        return x / norms\n\n    train_embs = l2_norm(train_embs)\n    test_embs = l2_norm(test_embs)\n\n    return train_embs, test_embs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.467543Z","iopub.execute_input":"2025-11-17T22:38:32.467767Z","iopub.status.idle":"2025-11-17T22:38:32.490695Z","shell.execute_reply.started":"2025-11-17T22:38:32.467752Z","shell.execute_reply":"2025-11-17T22:38:32.490138Z"}},"outputs":[],"execution_count":162},{"id":"8e1e2baa","cell_type":"code","source":"def text_features(X: pd.DataFrame, test_df: pd.DataFrame, logger: logging.Logger):\n    \"\"\"\n    1) –ü—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (parquet/csv/tsv) –ø–æ id.\n    2) –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å—Ç—Ä–æ–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (BERT –∏–ª–∏ TF-IDF+SVD).\n    3) –ö–æ–Ω–∫–∞—Ç–∏–º –≤—Å—ë –∫ X –∏ test_df.\n    \"\"\"\n    if not CONFIG.get(\"text_enable\", False):\n        return X, test_df\n\n    logger.info(\"Text feature extraction starts...\")\n\n    # 1. –ü–æ–¥—Ü–µ–ø–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã\n    X, test_df, external_cols = attach_external_text_tables(X, test_df)\n\n    # 2. –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n    base_text_cols = CONFIG.get(\"text_columns\", []) or []\n    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ\n    base_text_cols = [c for c in base_text_cols if c in X.columns]\n\n    all_text_cols = base_text_cols + external_cols\n\n    if not all_text_cols:\n        logger.warning(\"No text columns found for text_features. Skipping.\")\n        return X, test_df\n\n    model_type = CONFIG.get(\"text_model_type\", \"bert\")\n\n    # 3. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å—Ç—Ä–æ–∏–º —Å–≤–æ–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–æ–Ω–∫–∞—Ç–∏–º\n    for col in tqdm(all_text_cols, desc=\"Building text embeddings\"):\n        logger.info(f\"Building text embeddings for column: {col}\")\n\n        # —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã (—Å—Ç—Ä–æ–∫–∞ -> str) - –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å—ã\n        train_texts = X[col].fillna(\"\").astype(str).tolist()\n        test_texts = test_df[col].fillna(\"\").astype(str).tolist()\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n        if len(train_texts) != len(X):\n            raise ValueError(f\"Train texts length {len(train_texts)} != X length {len(X)} for column {col}\")\n        if len(test_texts) != len(test_df):\n            raise ValueError(f\"Test texts length {len(test_texts)} != test_df length {len(test_df)} for column {col}\")\n\n        # –ø—Ä–æ–≤–µ—Ä–∫–∞, –∞ –Ω–µ –≤—Å–µ –ª–∏ –ø—É—Å—Ç—ã–µ\n        if all(t == \"\" for t in train_texts) and all(t == \"\" for t in test_texts):\n            logger.warning(f\"Column '{col}' has only empty texts. Skipping.\")\n            continue\n\n        field_name = col  # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –º–∞–ø–ø–∏—Ç—å/–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –µ—Å–ª–∏ —Ö–æ—á–µ—à—å\n\n        if model_type == \"bert\":\n            train_embs, test_embs = build_bert_embeddings_for_field(field_name, train_texts, test_texts)\n        elif model_type == \"tfidf_svd\":\n            train_embs, test_embs = build_tfidf_svd_embeddings_for_field(field_name, train_texts, test_texts, logger)\n        else:\n            raise ValueError(f\"Unknown text_model_type: {model_type}\")\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n        if train_embs.shape[0] != len(X):\n            raise ValueError(f\"Train embeddings shape {train_embs.shape[0]} != X length {len(X)} for column {col}\")\n        if test_embs.shape[0] != len(test_df):\n            raise ValueError(f\"Test embeddings shape {test_embs.shape[0]} != test_df length {len(test_df)} for column {col}\")\n\n        dim = train_embs.shape[1]\n        # –ø—Ä–µ—Ñ–∏–∫—Å —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–ª–æ–Ω–∫–∏, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫\n        prefix = f\"text_{field_name}_emb\"\n        cols = [f\"{prefix}_{i}\" for i in range(dim)]\n\n        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏\n        train_text_df = pd.DataFrame(train_embs, columns=cols, index=X.index)\n        test_text_df = pd.DataFrame(test_embs, columns=cols, index=test_df.index)\n\n        X = pd.concat([X, train_text_df], axis=1)\n        test_df = pd.concat([test_df, test_text_df], axis=1)\n\n        logger.info(f\"Text features for '{col}' added: {dim} dims ({prefix}_*)\")\n\n    return X, test_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.493007Z","iopub.execute_input":"2025-11-17T22:38:32.493262Z","iopub.status.idle":"2025-11-17T22:38:32.505803Z","shell.execute_reply.started":"2025-11-17T22:38:32.493237Z","shell.execute_reply":"2025-11-17T22:38:32.505085Z"}},"outputs":[],"execution_count":163},{"id":"517c555f","cell_type":"code","source":"def prepare_all_features(X, test_df, config, logger):\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã\n    original_X_size = len(X)\n    original_test_size = len(test_df) if test_df is not None else 0\n    logger.info(f\"üîç ORIGINAL SIZES: X={original_X_size}, test_df={original_test_size}\")\n    \n    # ----- Features -----  \n    logger.info(\"üîß Generating basic features...\")\n    logger.info(f\"X shape before basic: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before basic: {test_df.shape}\")\n    \n    X = generate_basic_features(X, config, logger)\n    if test_df is not None:\n        test_df = generate_basic_features(test_df, config, logger)\n    \n    logger.info(f\"X shape after basic: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after basic: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_basic_features: {original_test_size} -> {len(test_df)}\")\n\n    logger.info(\"üìä Generating aggregate features...\")\n    logger.info(f\"X shape before aggregate: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before aggregate: {test_df.shape}\")\n    \n    X, test_df = generate_aggregate_features(X, test_df, config, logger)\n    \n    logger.info(f\"X shape after aggregate: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after aggregate: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_aggregate_features: {original_test_size} -> {len(test_df)}\")\n\n    logger.info(\"üó∫  Generating geo features...\")\n    logger.info(f\"X shape before geo: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before geo: {test_df.shape}\")\n    \n    X, test_df = generate_geo_features(X, test_df, config, logger)\n    \n    logger.info(f\"X shape after geo: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after geo: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_geo_features: {original_test_size} -> {len(test_df)}\")\n    \n    if config['train_images_dir']:\n        logger.info(\"Images process starts...\")\n        X, test_df = images_features(X, test_df)\n    \n    if CONFIG.get(\"text_enable\", False):\n        logger.info(\"Text process starts...\")\n        logger.info(f\"X shape before text: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape before text: {test_df.shape}\")\n        \n        X, test_df = text_features(X, test_df, logger)\n        \n        logger.info(f\"X shape after text: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape after text: {test_df.shape}\")\n            if len(test_df) != original_test_size:\n                raise ValueError(f\"‚ùå test_df size changed in text_features: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Address ‚Üí city extraction -----\n    addr_col = config.get(\"address_column\")\n    if addr_col:\n        logger.info(f\"üèô  Extracting city from address column '{addr_col}' ...\")\n        logger.info(f\"X shape before address: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape before address: {test_df.shape}\")\n        \n        city_index = config.get(\"address_city_index\", -1)\n        sep = config.get(\"address_split_sep\", \",\")\n        X = process_address_extract_city(X, addr_col, city_index, sep, logger)\n        if test_df is not None:\n            test_df = process_address_extract_city(test_df, addr_col, city_index, sep, logger)\n        \n        logger.info(f\"X shape after address: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape after address: {test_df.shape}\")\n            if len(test_df) != original_test_size:\n                raise ValueError(f\"‚ùå test_df size changed in process_address_extract_city: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Categorical detection -----\n    logger.info(\"üîé Detecting categorical columns...\")\n    logger.info(f\"X shape before categorical detection: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before categorical detection: {test_df.shape}\")\n    \n    concat_df = pd.concat([X] if test_df is None else [X, test_df], ignore_index=False)\n    cat_cols = detect_categorical_columns(\n        concat_df,\n        max_unique=config[\"basic_max_cat_unique\"],\n        force_categorical=config[\"basic_as_categorical\"]\n    )\n    cat_cols = [c for c in cat_cols if c in X.columns]\n    logger.info(f\"Categorical columns: {cat_cols}\")\n\n    # ----- Categorical post-processing: rare ‚Üí 'other' -----\n    logger.info(\"üß© Processing categorical features (merge rare into 'other')...\")\n    logger.info(f\"X shape before categorical processing: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before categorical processing: {test_df.shape}\")\n    \n    X, test_df, cat_cols = process_categorical_features(X, test_df, cat_cols, config, logger)\n    logger.info(f\"Categorical columns after processing: {cat_cols}\")\n    \n    logger.info(f\"X shape after categorical processing: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after categorical processing: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in process_categorical_features: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Post-feature service columns drop -----\n    post_drop = config.get(\"post_feature_drop_columns\", [])\n    if post_drop:\n        logger.info(f\"üßπ Dropping post-feature service columns: {post_drop}\")\n        X = X.drop(columns=[c for c in post_drop if c in X.columns])\n        if test_df is not None:\n            test_df = test_df.drop(columns=[c for c in post_drop if c in test_df.columns])\n        # –æ–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\n        cat_cols = [c for c in cat_cols if c not in post_drop]\n\n    # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN: —Å–Ω–∞—á–∞–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –ø–æ—Ç–æ–º —á–∏—Å–ª–æ–≤—ã–µ\n    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n    for col_name in cat_cols:\n        if col_name in X.columns:\n            X[col_name] = X[col_name].fillna(\"__MISSING__\").astype(str)\n        if test_df is not None and col_name in test_df.columns:\n            test_df[col_name] = test_df[col_name].fillna(\"__MISSING__\").astype(str)\n    \n    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN\n    numeric_cols = [c for c in X.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(X[c])]\n    if numeric_cols:\n        X[numeric_cols] = X[numeric_cols].fillna(-99999999)\n        if test_df is not None:\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ test_df\n            numeric_cols_test = [c for c in numeric_cols if c in test_df.columns]\n            if numeric_cols_test:\n                test_df[numeric_cols_test] = test_df[numeric_cols_test].fillna(-99999999)\n    \n    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤\n    logger.info(f\"Final X shape: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"Final test_df shape: {test_df.shape}\")\n        \n        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä test_df –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã–ª –∏–∑–º–µ–Ω–∏—Ç—å—Å—è\n        if len(test_df) != original_test_size:\n            raise ValueError(\n                f\"‚ùå CRITICAL: test_df size changed from {original_test_size} to {len(test_df)} during prepare_all_features! \"\n                f\"This should not happen. Please check the code above.\"\n            )\n        \n        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–∫—Ä–æ–º–µ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö)\n        X_cols = set(X.columns)\n        test_cols = set(test_df.columns)\n        missing_in_test = X_cols - test_cols\n        extra_in_test = test_cols - X_cols\n        if missing_in_test:\n            logger.warning(f\"Columns in X but not in test_df: {list(missing_in_test)[:10]}\")\n        if extra_in_test:\n            logger.warning(f\"Columns in test_df but not in X: {list(extra_in_test)[:10]}\")\n    \n    return X, test_df, cat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.506668Z","iopub.execute_input":"2025-11-17T22:38:32.506835Z","iopub.status.idle":"2025-11-17T22:38:32.524801Z","shell.execute_reply.started":"2025-11-17T22:38:32.506823Z","shell.execute_reply":"2025-11-17T22:38:32.524274Z"}},"outputs":[],"execution_count":164},{"id":"defcb0fb","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ed9b13f","cell_type":"code","source":"X, test_df, cat_cols = prepare_all_features(X, test_df, CONFIG, logger)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:38:32.525512Z","iopub.execute_input":"2025-11-17T22:38:32.525737Z","iopub.status.idle":"2025-11-17T22:41:16.061950Z","shell.execute_reply.started":"2025-11-17T22:38:32.525712Z","shell.execute_reply":"2025-11-17T22:41:16.061434Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:38:32,535 - INFO -  ORIGINAL SIZES: X=9912, test_df=8\nINFO:TABULAR_BOOSTING:üîç ORIGINAL SIZES: X=9912, test_df=8\n2025-11-17 22:38:32,537 - INFO -  Generating basic features...\nINFO:TABULAR_BOOSTING:üîß Generating basic features...\n2025-11-17 22:38:32,538 - INFO - X shape before basic: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape before basic: (9912, 13)\n2025-11-17 22:38:32,540 - INFO - test_df shape before basic: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape before basic: (8, 13)\n2025-11-17 22:38:32,542 - INFO - X shape after basic: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape after basic: (9912, 13)\n2025-11-17 22:38:32,543 - INFO - test_df shape after basic: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape after basic: (8, 13)\n2025-11-17 22:38:32,545 - INFO -  Generating aggregate features...\nINFO:TABULAR_BOOSTING:üìä Generating aggregate features...\n2025-11-17 22:38:32,546 - INFO - X shape before aggregate: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape before aggregate: (9912, 13)\n2025-11-17 22:38:32,547 - INFO - test_df shape before aggregate: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape before aggregate: (8, 13)\n2025-11-17 22:38:32,549 - INFO - Aggregate features disabled in CONFIG.\nINFO:TABULAR_BOOSTING:Aggregate features disabled in CONFIG.\n2025-11-17 22:38:32,550 - INFO - X shape after aggregate: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape after aggregate: (9912, 13)\n2025-11-17 22:38:32,552 - INFO - test_df shape after aggregate: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape after aggregate: (8, 13)\n2025-11-17 22:38:32,553 - INFO -   Generating geo features...\nINFO:TABULAR_BOOSTING:üó∫  Generating geo features...\n2025-11-17 22:38:32,555 - INFO - X shape before geo: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape before geo: (9912, 13)\n2025-11-17 22:38:32,556 - INFO - test_df shape before geo: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape before geo: (8, 13)\n2025-11-17 22:38:32,557 - INFO - Geo features disabled in CONFIG.\nINFO:TABULAR_BOOSTING:Geo features disabled in CONFIG.\n2025-11-17 22:38:32,558 - INFO - X shape after geo: (9912, 13)\nINFO:TABULAR_BOOSTING:X shape after geo: (9912, 13)\n2025-11-17 22:38:32,559 - INFO - test_df shape after geo: (8, 13)\nINFO:TABULAR_BOOSTING:test_df shape after geo: (8, 13)\n2025-11-17 22:38:32,562 - INFO - Images process starts...\nINFO:TABULAR_BOOSTING:Images process starts...\nImages train_clip process...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [02:40<00:00,  1.94it/s]\nImages test_clip process...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 19.85it/s]\n2025-11-17 22:41:15,622 - INFO -  Detecting categorical columns...\nINFO:TABULAR_BOOSTING:üîé Detecting categorical columns...\n2025-11-17 22:41:15,623 - INFO - X shape before categorical detection: (9912, 524)\nINFO:TABULAR_BOOSTING:X shape before categorical detection: (9912, 524)\n2025-11-17 22:41:15,624 - INFO - test_df shape before categorical detection: (8, 524)\nINFO:TABULAR_BOOSTING:test_df shape before categorical detection: (8, 524)\n2025-11-17 22:41:15,842 - INFO - Categorical columns: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\nINFO:TABULAR_BOOSTING:Categorical columns: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\n2025-11-17 22:41:15,843 - INFO -  Processing categorical features (merge rare into 'other')...\nINFO:TABULAR_BOOSTING:üß© Processing categorical features (merge rare into 'other')...\n2025-11-17 22:41:15,845 - INFO - X shape before categorical processing: (9912, 524)\nINFO:TABULAR_BOOSTING:X shape before categorical processing: (9912, 524)\n2025-11-17 22:41:15,846 - INFO - test_df shape before categorical processing: (8, 524)\nINFO:TABULAR_BOOSTING:test_df shape before categorical processing: (8, 524)\n2025-11-17 22:41:15,847 - INFO - Categorical processing: cat_min_count=20, cat_min_freq=0.0, cat_max_rare_share=0.98, cat_max_unique_after_group=500\nINFO:TABULAR_BOOSTING:Categorical processing: cat_min_count=20, cat_min_freq=0.0, cat_max_rare_share=0.98, cat_max_unique_after_group=500\nProcessing categorical features:   0%|          | 0/12 [00:00<?, ?it/s]2025-11-17 22:41:15,859 - INFO - [cat] Accessory: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Accessory: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,861 - INFO - [cat] Action: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Action: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,863 - INFO - [cat] Blur: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Blur: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,865 - INFO - [cat] Collage: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Collage: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,866 - INFO - [cat] Eyes: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Eyes: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,868 - INFO - [cat] Face: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Face: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,869 - INFO - [cat] Group: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Group: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,871 - INFO - [cat] Human: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Human: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,873 - INFO - [cat] Info: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Info: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,874 - INFO - [cat] Near: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Near: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,877 - INFO - [cat] Occlusion: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Occlusion: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:41:15,878 - INFO - [cat] Subject Focus: unique=2, rare_cats=0, rare_share=0.0000\nINFO:TABULAR_BOOSTING:[cat] Subject Focus: unique=2, rare_cats=0, rare_share=0.0000\nProcessing categorical features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 549.18it/s]\n2025-11-17 22:41:15,881 - INFO - Categorical columns after processing: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\nINFO:TABULAR_BOOSTING:Categorical columns after processing: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\n2025-11-17 22:41:15,882 - INFO - X shape after categorical processing: (9912, 524)\nINFO:TABULAR_BOOSTING:X shape after categorical processing: (9912, 524)\n2025-11-17 22:41:15,883 - INFO - test_df shape after categorical processing: (8, 524)\nINFO:TABULAR_BOOSTING:test_df shape after categorical processing: (8, 524)\n2025-11-17 22:41:16,055 - INFO - Final X shape: (9912, 524)\nINFO:TABULAR_BOOSTING:Final X shape: (9912, 524)\n2025-11-17 22:41:16,057 - INFO - Final test_df shape: (8, 524)\nINFO:TABULAR_BOOSTING:Final test_df shape: (8, 524)\n","output_type":"stream"}],"execution_count":165},{"id":"60425ace","cell_type":"markdown","source":"### Features filter","metadata":{}},{"id":"78cfa405","cell_type":"code","source":"def filter_features_by_correlation(\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    1) Pearson –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n    2) phik (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω phik)\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç X —Å –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Å–ø–∏—Å–æ–∫ –∏—Ö –∏–º—ë–Ω.\n    \"\"\"\n    selected = list(X.columns)\n    task_type = config[\"task_type\"]\n\n    # --- Pearson ---\n    pearson_thr = float(config.get(\"corr_pearson_min_abs\", 0.0) or 0.0)\n    if pearson_thr > 0.0:\n        logger.info(f\"Applying Pearson filter with |corr| >= {pearson_thr}\")\n        corr_vals = {}\n        y_series = pd.Series(y)\n\n        for col in tqdm(selected, desc=\"Computing Pearson correlations\"):\n            if pd.api.types.is_numeric_dtype(X[col]):\n                try:\n                    c = X[col].corr(y_series)\n                except Exception:\n                    c = np.nan\n            else:\n                c = np.nan\n            corr_vals[col] = c\n\n        corr_series = pd.Series(corr_vals)\n        keep_mask = corr_series.abs() < pearson_thr\n        kept = corr_series.index[keep_mask].tolist()\n        logger.info(\n            f\"Pearson filter: kept {len(kept)} / {len(corr_series)} features \"\n            f\"(min abs corr {corr_series.abs().min():.6f}, max {corr_series.abs().max():.6f})\"\n        )\n        selected = kept\n\n    # --- phik ---\n    use_phik = bool(config.get(\"corr_use_phik\", False))\n    phik_thr = float(config.get(\"corr_phik_min_abs\", 0.0) or 0.0)\n\n    if use_phik and phik_thr > 0.0:\n        if not HAS_PHIK:\n            logger.warning(\"corr_use_phik=True, –Ω–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ phik –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º phik-—Ñ–∏–ª—å—Ç—Ä.\")\n        else:\n            logger.info(f\"Applying phik filter with |phik| >= {phik_thr}\")\n            df_phik = X[selected].copy()\n            tmp_target_col = \"__target_for_phik__\"\n            df_phik[tmp_target_col] = y.values\n\n            # —á–∏—Å–ª–æ–≤—ã–µ –¥–ª—è interval_cols\n            interval_cols = [\n                c for c in df_phik.columns\n                if pd.api.types.is_numeric_dtype(df_phik[c])\n            ]\n            try:\n                phik_matrix = df_phik.phik_matrix(interval_cols=interval_cols)\n                target_corr = phik_matrix[tmp_target_col].drop(\n                    labels=[tmp_target_col], errors=\"ignore\"\n                )\n                keep_mask = target_corr.abs() >= phik_thr\n                kept_phik = target_corr.index[keep_mask].tolist()\n                logger.info(\n                    f\"phik filter: kept {len(kept_phik)} / {len(target_corr)} features \"\n                    f\"(min abs phik {target_corr.abs().min():.6f}, \"\n                    f\"max {target_corr.abs().max():.6f})\"\n                )\n                # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ Pearson\n                selected = [c for c in selected if c in kept_phik]\n            except Exception as e:\n                logger.warning(f\"phik computation failed, skip phik filter. Error: {e}\")\n\n    logger.info(f\"Total features after correlation filtering: {len(selected)}\")\n    return X[selected], selected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:41:16.062752Z","iopub.execute_input":"2025-11-17T22:41:16.062970Z","iopub.status.idle":"2025-11-17T22:41:16.072703Z","shell.execute_reply.started":"2025-11-17T22:41:16.062954Z","shell.execute_reply":"2025-11-17T22:41:16.072076Z"}},"outputs":[],"execution_count":166},{"id":"b75dce25","cell_type":"code","source":"# NEW: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\nif CONFIG.get(\"corr_enable\", False):\n    logger.info(\"üìâ Applying correlation-based feature filtering...\")\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π\n    test_size_before_corr = len(test_df) if test_df is not None else 0\n    logger.info(f\"üîç test_df size BEFORE correlation filter: {test_size_before_corr}\")\n    \n    X, selected_cols = filter_features_by_correlation(X, y, CONFIG, logger)\n\n    if test_df is not None:\n        missing_in_test = [c for c in selected_cols if c not in test_df.columns]\n        if missing_in_test:\n            logger.warning(\n                f\"{len(missing_in_test)} features selected by correlation \"\n                f\"missing in test: {missing_in_test[:10]}...\"\n            )\n        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ—Å—Ç–µ\n        keep_for_test = [c for c in selected_cols if c in test_df.columns]\n        test_df = test_df[keep_for_test]\n        \n        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä –Ω–µ –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫\n        logger.info(f\"üîç test_df size AFTER correlation filter: {len(test_df)}\")\n        if len(test_df) != test_size_before_corr:\n            raise ValueError(\n                f\"‚ùå CRITICAL: test_df size changed from {test_size_before_corr} to {len(test_df)} \"\n                f\"during correlation filtering! This should not happen when filtering columns.\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:41:16.073485Z","iopub.execute_input":"2025-11-17T22:41:16.073726Z","iopub.status.idle":"2025-11-17T22:41:16.089213Z","shell.execute_reply.started":"2025-11-17T22:41:16.073710Z","shell.execute_reply":"2025-11-17T22:41:16.088522Z"}},"outputs":[],"execution_count":167},{"id":"1e440026","cell_type":"code","source":" # NEW: sanitize feature names –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (–æ—Å–æ–±–µ–Ω–Ω–æ XGBoost)\nlogger.info(\"üßæ Sanitizing feature names...\")\n# –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ sanitize\ntest_size_before_sanitize = len(test_df) if test_df is not None else 0\nlogger.info(f\"üîç test_df size BEFORE sanitize: {test_size_before_sanitize}\")\n\nX, feature_name_mapping = sanitize_feature_names(X, logger)\n\nif test_df is not None:\n    # —Å–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Ç–µ—Å—Ç–∞ –ø–æ —Ç–æ–º—É –∂–µ –º–∞–ø–ø–∏–Ω–≥—É\n    test_df = test_df.rename(columns=feature_name_mapping)\n    # –µ—Å–ª–∏ –≤ —Ç–µ—Å—Ç–µ –æ—Å—Ç–∞–ª–∏—Å—å –∫–∞–∫–∏–µ-—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∫–æ),\n    # –º–æ–∂–Ω–æ –µ—â—ë —Ä–∞–∑ –ø—Ä–æ–≥–Ω–∞—Ç—å sanitize, –Ω–æ –∏–º–µ–Ω–∞ —É–∂–µ –±—É–¥—É—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ\n    # test, _ = sanitize_feature_names(test, logger)\n    \n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä –Ω–µ –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏\n    logger.info(f\"üîç test_df size AFTER sanitize: {len(test_df)}\")\n    if len(test_df) != test_size_before_sanitize:\n        raise ValueError(\n            f\"‚ùå CRITICAL: test_df size changed from {test_size_before_sanitize} to {len(test_df)} \"\n            f\"during sanitize! This should not happen when renaming columns.\"\n        )\n\n# –æ–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ–¥ –Ω–æ–≤—ã–µ –∏–º–µ–Ω–∞\ncat_cols = [feature_name_mapping.get(c, c) for c in cat_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:41:16.089966Z","iopub.execute_input":"2025-11-17T22:41:16.090218Z","iopub.status.idle":"2025-11-17T22:41:16.149116Z","shell.execute_reply.started":"2025-11-17T22:41:16.090195Z","shell.execute_reply":"2025-11-17T22:41:16.148405Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:41:16,100 - INFO -  Sanitizing feature names...\nINFO:TABULAR_BOOSTING:üßæ Sanitizing feature names...\n2025-11-17 22:41:16,101 - INFO -  test_df size BEFORE sanitize: 8\nINFO:TABULAR_BOOSTING:üîç test_df size BEFORE sanitize: 8\n2025-11-17 22:41:16,145 - INFO -  test_df size AFTER sanitize: 8\nINFO:TABULAR_BOOSTING:üîç test_df size AFTER sanitize: 8\n","output_type":"stream"}],"execution_count":168},{"id":"d104dfaa","cell_type":"markdown","source":"### Cross-validation","metadata":{}},{"id":"476f2b4c","cell_type":"code","source":"def make_folds(\n    y: pd.Series,\n    config: Dict\n):\n    n_splits = config[\"cv_n_splits\"]\n    seed = config[\"cv_random_state\"]\n    shuffle = config[\"cv_shuffle\"]\n    stratified = config[\"cv_stratified\"]\n    task_type = config[\"task_type\"]\n\n    if stratified and task_type in [\"binary\", \"multiclass\"]:\n        splitter = StratifiedKFold(\n            n_splits=n_splits,\n            shuffle=shuffle,\n            random_state=seed\n        )\n    else:\n        splitter = KFold(\n            n_splits=n_splits,\n            shuffle=shuffle,\n            random_state=seed\n        )\n    return list(splitter.split(np.zeros(len(y)), y))\n\n\ndef build_catboost_model(\n    config: Dict\n):\n    params = config[\"catboost_params\"].copy()\n    \n    # –î–æ–±–∞–≤–ª—è–µ–º device –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (CatBoost –∏—Å–ø–æ–ª—å–∑—É–µ—Ç \"GPU\" –∏–ª–∏ \"CPU\")\n    device_str = config.get(\"device\", \"CPU\")\n    if device_str.upper() == \"CUDA\":\n        params[\"task_type\"] = \"GPU\"\n    elif device_str.upper() == \"GPU\":\n        params[\"task_type\"] = \"GPU\"\n    else:\n        params[\"task_type\"] = \"CPU\"\n    \n    if config[\"task_type\"] == \"regression\":\n        params[\"loss_function\"] = \"RMSE\"\n        params[\"eval_metric\"] = \"RMSE\"\n        model = CatBoostRegressor(**params)\n    else:\n        if config[\"task_type\"] == \"multiclass\":\n            params[\"loss_function\"] = \"MultiClass\"\n            params[\"eval_metric\"] = \"MultiClass\"\n        model = CatBoostClassifier(**params)\n    return model\n\n\ndef build_lgb_params(\n    config: Dict,\n    num_classes: Optional[int] = None\n):\n    params = config[\"lgb_params\"].copy()\n    task_type = config[\"task_type\"]\n\n    # ---------- Device (CPU / GPU) ----------\n    device_str = config.get(\"device\", \"CPU\").upper()\n    if device_str in [\"CUDA\", \"GPU\"]:\n        # –¥–ª—è LightGBM –ø—Ä–∞–≤–∏–ª—å–Ω–æ \"device_type\"\n        params[\"device_type\"] = \"gpu\"\n    else:\n        params[\"device_type\"] = \"cpu\"\n\n    # ---------- Objective + metric –ø–æ–¥ —Ç–∏–ø –∑–∞–¥–∞—á–∏ ----------\n    if task_type == \"regression\":\n        # –æ–±—ã—á–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n        params[\"objective\"] = \"regression\"\n        # –º–µ—Ç—Ä–∏–∫–∏ LightGBM –∑–∞–¥–∞—é—Ç—Å—è –∫–ª—é—á–æ–º \"metric\"\n        params[\"metric\"] = [\"mae\"]   # –∏–ª–∏ [\"rmse\"]\n    elif task_type == \"binary\":\n        params[\"objective\"] = \"binary\"\n        params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n    else:  # multiclass\n        if num_classes is None or num_classes <= 1:\n            raise ValueError(\"num_classes must be > 1 for multiclass\")\n        params[\"objective\"] = \"multiclass\"\n        params[\"num_class\"] = num_classes\n        params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n\n    return params\n\n\n\ndef build_xgb_params(\n    config: Dict,\n    num_classes: Optional[int] = None,\n):\n    params = config[\"xgb_params\"].copy()\n    task_type = config[\"task_type\"]\n\n    # ----- —á–∏—Å—Ç–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–π –º—É—Å–æ—Ä -----\n    # –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –±—ã–ª–æ —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ \"device\": 0 ‚Äî —É–±–∏—Ä–∞–µ–º\n    if \"device\" in params and not isinstance(params[\"device\"], str):\n        params.pop(\"device\")\n\n    # ----- CPU / GPU -----\n    device_str = config.get(\"device\", \"CPU\").upper()\n    if device_str in [\"CUDA\", \"GPU\"]:\n        # XGBoost 2.x —Ö–æ—á–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π device\n        params[\"device\"] = \"cuda\"      # –º–æ–∂–Ω–æ \"cuda:0\"\n        # –Ω–∞ GPU –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ tree_method=\"hist\" –∏–ª–∏ \"gpu_hist\"\n        # –æ—Å—Ç–∞–≤–∏–º \"gpu_hist\", –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ —É–∂–µ –∑–∞–¥–∞–Ω, –∏–Ω–∞—á–µ –ø–æ—Å—Ç–∞–≤–∏–º –º—ã\n        params.setdefault(\"tree_method\", \"gpu_hist\")\n    else:\n        params[\"device\"] = \"cpu\"\n        params.setdefault(\"tree_method\", \"hist\")\n\n    # –ø–æ—Ç–æ–∫–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å–µ —è–¥—Ä–∞\n    params.setdefault(\"nthread\", 0)\n\n    # ----- —Ç–∏–ø –∑–∞–¥–∞—á–∏ -----\n    if task_type == \"regression\":\n        params[\"objective\"] = \"reg:squarederror\"\n        params[\"eval_metric\"] = \"mae\"           # –º–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ \"rmse\"\n    elif task_type == \"binary\":\n        params[\"objective\"] = \"binary:logistic\"\n        params[\"eval_metric\"] = \"auc\"\n    else:\n        # multiclass\n        if num_classes is None:\n            raise ValueError(\"num_classes must be provided for multiclass\")\n        params[\"objective\"] = \"multi:softprob\"\n        params[\"num_class\"] = num_classes\n        params[\"eval_metric\"] = \"mlogloss\"\n\n    return params\n\n\n\n\ndef _evaluate_predictions(\n    y_true: pd.Series,\n    preds: np.ndarray,\n    task_type: str,\n    name_prefix: str,\n    logger: logging.Logger\n) -> Dict[str, float]:\n    metrics = {}\n    if preds is None:\n        return metrics\n\n    if task_type == \"regression\":\n        rmse = mean_squared_error(y_true, preds, squared=False)\n        mae = mean_absolute_error(y_true, preds)\n        metrics[f\"{name_prefix}_RMSE\"] = rmse\n        metrics[f\"{name_prefix}_MAE\"] = rmse\n        \n        logger.info(f\"{name_prefix} RMSE: {rmse:.6f} MAE: {mae:.6f}\")\n    elif task_type == \"binary\":\n        auc = roc_auc_score(y_true, preds)\n        logloss = log_loss(y_true, preds)\n\n        ## –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n        preds_label = (preds >= 0.5).astype(int)\n        acc = accuracy_score(y_true, preds_label)\n        cm = confusion_matrix(y_true, preds_label)\n        metrics[f\"{name_prefix}_AUC\"] = auc\n        metrics[f\"{name_prefix}_LogLoss\"] = logloss\n        metrics[f\"{name_prefix}_ACC\"] = acc\n        logger.info(f\"{name_prefix} AUC: {auc:.6f}, LogLoss: {logloss:.6f}, ACC: {acc:.6f}\")\n        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n    else:\n        # multiclass\n        # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (n_samples, n_classes) ‚Äî –±–µ—Ä—ë–º argmax\n        if preds.ndim == 2:\n            preds_label = np.argmax(preds, axis=1)\n        else:\n            preds_label = preds\n\n        acc = accuracy_score(y_true, preds_label)\n        cm = confusion_matrix(y_true, preds_label)\n        metrics[f\"{name_prefix}_ACC\"] = acc\n        logger.info(f\"{name_prefix} ACC: {acc:.6f}\")\n        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n\n    return metrics\n\n\ndef train_and_evaluate(\n    X: pd.DataFrame,\n    y: pd.Series,\n    cat_cols: List[str],\n    config: Dict,\n    logger: logging.Logger\n) -> Dict:\n    task_type = config[\"task_type\"]\n    folds = make_folds(y, config)\n    logger.info(f\"Using {len(folds)} folds.\")\n\n    cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n\n    # –°–ù–ê–ß–ê–õ–ê —Å—á–∏—Ç–∞–µ–º classes_\n    classes_ = np.unique(y) if task_type == \"multiclass\" else None\n\n    n = len(y)\n    if task_type == \"multiclass\":\n        n_classes = len(classes_)\n        oof_preds_cat = np.zeros((n, n_classes)) if config[\"use_catboost\"] else None\n        oof_preds_lgb = np.zeros((n, n_classes)) if config[\"use_lightgbm\"] else None\n        oof_preds_xgb = np.zeros((n, n_classes)) if config[\"use_xgboost\"] else None\n    else:\n        oof_preds_cat = np.zeros(n) if config[\"use_catboost\"] else None\n        oof_preds_lgb = np.zeros(n) if config[\"use_lightgbm\"] else None\n        oof_preds_xgb = np.zeros(n) if config[\"use_xgboost\"] else None\n\n    models_cat: List = []\n    models_lgb: List = []\n    models_xgb: List = []\n\n    feature_importances_lgb = []\n    feature_importances_xgb = []\n\n    for fold_idx, (tr_idx, val_idx) in enumerate(tqdm(folds, desc=\"Cross-validation folds\")):\n        logger.info(f\"========== Fold {fold_idx + 1}/{len(folds)} ==========\")\n\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n        if config[\"use_catboost\"]:\n            logger.info(\"Training CatBoost...\")\n            train_pool = Pool(\n                X_tr, y_tr,\n                cat_features=cat_features_idx\n            )\n            val_pool = Pool(\n                X_val, y_val,\n                cat_features=cat_features_idx\n            )\n            model_cat = build_catboost_model(config)\n            # Early stopping –¥–ª—è CatBoost\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_cat.fit(\n                train_pool, \n                eval_set=val_pool,\n                early_stopping_rounds=early_stopping_rounds,\n                use_best_model=True\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_cat.predict(val_pool)\n            else:\n                proba = model_cat.predict_proba(val_pool)\n                if task_type == \"binary\":\n                    preds_val = proba[:, 1]\n                else:\n                    oof_preds_cat[val_idx, :] = proba\n                    preds_val = proba  # –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≥–¥–µ-—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å\n\n            if task_type != \"multiclass\":\n                oof_preds_cat[val_idx] = preds_val\n            models_cat.append(model_cat)\n\n        if config[\"use_lightgbm\"]:\n            logger.info(\"Training LightGBM...\")\n\n            num_classes = len(classes_) if task_type == \"multiclass\" else None\n            lgb_params = build_lgb_params(config, num_classes=num_classes)\n\n            X_tr_lgb = X_tr.copy()\n            X_val_lgb = X_val.copy()\n            for c in cat_cols:\n                if c in X_tr_lgb.columns:\n                    X_tr_lgb[c] = X_tr_lgb[c].astype(\"category\")\n                    X_val_lgb[c] = X_val_lgb[c].astype(\"category\")\n\n            lgb_train = lgb.Dataset(X_tr_lgb, label=y_tr)\n            lgb_val = lgb.Dataset(X_val_lgb, label=y_val)\n            print(lgb_params)\n            # Early stopping –¥–ª—è LightGBM\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_lgb = lgb.train(\n                lgb_params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=[lgb_train, lgb_val],\n                valid_names=[\"train\", \"valid\"],\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=early_stopping_rounds),\n                    lgb.log_evaluation(100)\n                ]\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n            else:\n                proba = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n                if task_type == \"binary\":\n                    preds_val = proba\n                else:\n                    # multiclass: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n                    oof_preds_lgb[val_idx, :] = proba\n                    preds_val = proba\n\n            if task_type != \"multiclass\":\n                oof_preds_lgb[val_idx] = preds_val\n            models_lgb.append(model_lgb)\n            feature_importances_lgb.append(model_lgb.feature_importance(importance_type=\"gain\"))\n\n        if config[\"use_xgboost\"]:\n            logger.info(\"Training XGBoost...\")\n            num_classes = len(classes_) if task_type == \"multiclass\" else None\n            xgb_params = build_xgb_params(config, num_classes=num_classes)\n\n            X_tr_xgb = X_tr.copy()\n            X_val_xgb = X_val.copy()\n            for c in X_tr_xgb.columns:\n                if X_tr_xgb[c].dtype == \"object\":\n                    X_tr_xgb[c] = X_tr_xgb[c].astype(\"category\")\n                    X_val_xgb[c] = X_val_xgb[c].astype(\"category\")\n\n            dtrain = xgb.DMatrix(X_tr_xgb, label=y_tr, enable_categorical=True)\n            dvalid = xgb.DMatrix(X_val_xgb, label=y_val, enable_categorical=True)\n\n            evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n            print(dtrain, dvalid)\n            print(xgb_params)\n            # Early stopping –¥–ª—è XGBoost\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_xgb = xgb.train(\n                params=xgb_params,\n                dtrain=dtrain,\n                num_boost_round=1000,\n                evals=evals,\n                early_stopping_rounds=early_stopping_rounds,\n                verbose_eval=100\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_xgb.predict(\n                    dvalid,\n                    iteration_range=(0, model_xgb.best_iteration + 1)\n                )\n            else:\n                proba = model_xgb.predict(\n                    dvalid,\n                    iteration_range=(0, model_xgb.best_iteration + 1)\n                )\n                if task_type == \"binary\":\n                    preds_val = proba\n                else:\n                    # multiclass: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n                    oof_preds_xgb[val_idx, :] = proba\n                    preds_val = proba\n\n            if task_type != \"multiclass\":\n                oof_preds_xgb[val_idx] = preds_val\n            models_xgb.append(model_xgb)\n\n            fi_xgb = model_xgb.get_score(importance_type=\"gain\")\n            fi_vec = np.array([fi_xgb.get(f, 0.0) for f in X.columns])\n            feature_importances_xgb.append(fi_vec)\n\n        gc.collect()\n\n    metrics_all: Dict[str, float] = {}\n    metrics_all.update(_evaluate_predictions(y, oof_preds_cat, task_type, \"CatBoost\", logger))\n    metrics_all.update(_evaluate_predictions(y, oof_preds_lgb, task_type, \"LightGBM\", logger))\n    metrics_all.update(_evaluate_predictions(y, oof_preds_xgb, task_type, \"XGBoost\", logger))\n\n    logger.info(\"Evaluating blended OOF predictions...\")\n    blend_weights = config[\"blend_weights\"]\n    \n    blend_preds = None\n    if task_type == \"multiclass\":\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –ø–µ—Ä–≤–æ–π –Ω–µ-None –º–∞—Ç—Ä–∏—Ü–µ\n        num_classes = None\n        for arr in [oof_preds_cat, oof_preds_lgb, oof_preds_xgb]:\n            if arr is not None:\n                num_classes = arr.shape[1]\n                break\n        if num_classes is None:\n            logger.warning(\"No OOF predictions for multiclass blend.\")\n        else:\n            blend_num = np.zeros((n, num_classes), dtype=float)\n            blend_den = np.zeros(n, dtype=float)\n\n            if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n                w = blend_weights[\"catboost\"]\n                blend_num += w * oof_preds_cat\n                blend_den += w\n            if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n                w = blend_weights[\"lightgbm\"]\n                blend_num += w * oof_preds_lgb\n                blend_den += w\n            if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n                w = blend_weights[\"xgboost\"]\n                blend_num += w * oof_preds_xgb\n                blend_den += w\n\n            valid_mask = blend_den > 0\n            if valid_mask.any():\n                blend_preds = np.zeros_like(blend_num)\n                # –Ω–æ—Ä–º–∏—Ä—É–µ–º –ø–æ –≤–µ—Å–∞–º, broadcast –ø–æ axis=1\n                blend_preds[valid_mask] = (\n                    blend_num[valid_mask] /\n                    blend_den[valid_mask][:, None]\n                )\n                metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n            else:\n                logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n    else:\n        # binary / regression ‚Äî –∫–∞–∫ —Ä–∞–Ω—å—à–µ (1D)\n        blend_num = np.zeros(n, dtype=float)\n        blend_den = np.zeros(n, dtype=float)\n\n        if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n            w = blend_weights[\"catboost\"]\n            blend_num += w * oof_preds_cat\n            blend_den += w\n        if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n            w = blend_weights[\"lightgbm\"]\n            blend_num += w * oof_preds_lgb\n            blend_den += w\n        if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n            w = blend_weights[\"xgboost\"]\n            blend_num += w * oof_preds_xgb\n            blend_den += w\n\n        valid_mask = blend_den > 0\n        if valid_mask.any():\n            blend_preds = np.zeros_like(blend_num)\n            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n            metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n        else:\n            logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n    logger.info(\"========== OOF metrics ==========\")\n    logger.info(metrics_all)\n\n    results = {\n        \"oof_catboost\": oof_preds_cat,\n        \"oof_lightgbm\": oof_preds_lgb,\n        \"oof_xgboost\": oof_preds_xgb,\n        \"oof_blend\": blend_preds,\n        \"metrics\": metrics_all,\n        \"models_catboost\": models_cat,\n        \"models_lightgbm\": models_lgb,\n        \"models_xgboost\": models_xgb\n    }\n\n    if feature_importances_lgb:\n        fi_lgb = np.mean(feature_importances_lgb, axis=0)\n        results[\"feature_importance_lgb\"] = fi_lgb\n\n    if feature_importances_xgb:\n        fi_xgb_mean = np.mean(feature_importances_xgb, axis=0)\n        results[\"feature_importance_xgb\"] = fi_xgb_mean\n\n    return results\n\n\ndef ensemble_predict(\n    models: Dict[str, List],\n    X: pd.DataFrame,\n    cat_cols: List[str],\n    config: Dict\n) -> Dict[str, Optional[np.ndarray]]:\n    \"\"\"\n    –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª–µ–º:\n      - –ø–æ –∫–∞–∂–¥–æ–º—É –±—É—Å—Ç–∏–Ω–≥—É —É—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ñ–æ–ª–¥–∞–º\n      - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç blended –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≤–µ—Å–∞–º\n\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      {\n        \"catboost\": np.ndarray –∏–ª–∏ None,\n        \"lightgbm\": np.ndarray –∏–ª–∏ None,\n        \"xgboost\": np.ndarray –∏–ª–∏ None,\n        \"blend\": np.ndarray –∏–ª–∏ None,\n      }\n    \"\"\"\n    task_type = config[\"task_type\"]\n    blend_weights = config[\"blend_weights\"]\n\n    n = len(X)\n    preds_cat = None\n    preds_lgb = None\n    preds_xgb = None\n\n    X_lgb = X.copy()\n    for c in cat_cols:\n        if c in X_lgb.columns:\n            X_lgb[c] = X_lgb[c].astype(\"category\")\n\n    # CatBoost\n    if models.get(\"catboost\"):\n        cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n        test_pool = Pool(X, cat_features=cat_features_idx)\n        for m in models[\"catboost\"]:\n            if task_type == \"regression\":\n                proba = m.predict(test_pool)              # (n,)\n            else:\n                proba_full = m.predict_proba(test_pool)   # (n,2) –∏–ª–∏ (n,C)\n                if task_type == \"binary\":\n                    proba = proba_full[:, 1]              # (n,)\n                else:\n                    proba = proba_full                    # (n,C)\n            if preds_cat is None:\n                preds_cat = np.zeros_like(proba, dtype=float)\n            preds_cat += proba\n        preds_cat /= len(models[\"catboost\"])\n\n    if models.get(\"lightgbm\"):\n        for m in models[\"lightgbm\"]:\n            if task_type == \"regression\":\n                proba = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,)\n            else:\n                proba_full = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,) –∏–ª–∏ (n,C)\n                if task_type == \"binary\":\n                    proba = proba_full                                      # (n,)\n                else:\n                    proba = proba_full                                      # (n,C)\n            if preds_lgb is None:\n                preds_lgb = np.zeros_like(proba, dtype=float)\n            preds_lgb += proba\n        preds_lgb /= len(models[\"lightgbm\"])\n\n    # XGBoost\n    if models.get(\"xgboost\"):\n        X_xgb = X.copy()\n        for c in X_xgb.columns:\n            if X_xgb[c].dtype == \"object\":\n                X_xgb[c] = X_xgb[c].astype(\"category\")\n        dtest = xgb.DMatrix(X_xgb, enable_categorical=True)\n\n        for m in models[\"xgboost\"]:\n            if task_type == \"regression\":\n                proba = m.predict(\n                    dtest,\n                    iteration_range=(0, m.best_iteration + 1)\n                )  # shape: (n,)\n            else:\n                proba_full = m.predict(\n                    dtest,\n                    iteration_range=(0, m.best_iteration + 1)\n                )  # shape: (n,) for binary, (n, C) for multiclass\n                if task_type == \"binary\":\n                    proba = proba_full          # (n,)\n                else:\n                    proba = proba_full          # (n, C)\n\n            if preds_xgb is None:\n                preds_xgb = np.zeros_like(proba, dtype=float)\n            preds_xgb += proba\n\n        preds_xgb /= len(models[\"xgboost\"])\n\n    # Blending\n    blend_preds = None\n    if task_type == \"multiclass\":\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –ø–æ –ø–µ—Ä–≤–æ–π –Ω–µ-None –º–∞—Ç—Ä–∏—Ü–µ\n        base = None\n        for arr in [preds_cat, preds_lgb, preds_xgb]:\n            if arr is not None:\n                base = arr\n                break\n        if base is not None:\n            blend_num = np.zeros_like(base, dtype=float)  # (n,C)\n            blend_den = np.zeros(n, dtype=float)\n\n            if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n                w = blend_weights[\"catboost\"]\n                blend_num += w * preds_cat\n                blend_den += w\n            if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n                w = blend_weights[\"lightgbm\"]\n                blend_num += w * preds_lgb\n                blend_den += w\n            if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n                w = blend_weights[\"xgboost\"]\n                blend_num += w * preds_xgb\n                blend_den += w\n\n            valid_mask = blend_den > 0\n            if valid_mask.any():\n                blend_preds = np.zeros_like(blend_num)\n                blend_preds[valid_mask] = (\n                    blend_num[valid_mask] /\n                    blend_den[valid_mask][:, None]\n                )\n    else:\n        blend_num = np.zeros(n, dtype=float)\n        blend_den = np.zeros(n, dtype=float)\n        if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n            w = blend_weights[\"catboost\"]\n            blend_num += w * preds_cat\n            blend_den += w\n        if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n            w = blend_weights[\"lightgbm\"]\n            blend_num += w * preds_lgb\n            blend_den += w\n        if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n            w = blend_weights[\"xgboost\"]\n            blend_num += w * preds_xgb\n            blend_den += w\n\n        valid_mask = blend_den > 0\n        if valid_mask.any():\n            blend_preds = np.zeros_like(blend_num)\n            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n\n    return {\n        \"catboost\": preds_cat,\n        \"lightgbm\": preds_lgb,\n        \"xgboost\": preds_xgb,\n        \"blend\": blend_preds\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:41:16.149952Z","iopub.execute_input":"2025-11-17T22:41:16.150158Z","iopub.status.idle":"2025-11-17T22:41:16.196823Z","shell.execute_reply.started":"2025-11-17T22:41:16.150142Z","shell.execute_reply":"2025-11-17T22:41:16.196269Z"}},"outputs":[],"execution_count":169},{"id":"5f4c59c6","cell_type":"code","source":"# –£–¥–∞–ª—è–µ–º ID —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –±—É—Å—Ç–∏–Ω–≥–æ–≤,\n# —á—Ç–æ–±—ã ID –Ω–µ —É—Ç–µ–∫ –≤ –º–æ–¥–µ–ª–∏, –Ω–æ –æ—Å—Ç–∞–ª—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏—á (–∫–∞—Ä—Ç–∏–Ω–∫–∏/—Ç–µ–∫—Å—Ç—ã –∏ —Ç.–ø.)\nid_col = CONFIG[\"id_column\"]\n\nif id_col in X.columns:\n    logger.info(f\"Dropping ID column '{id_col}' before boosting\")\n    X = X.drop(columns=[id_col])\n\nif test_df is not None and id_col in test_df.columns:\n    test_df = test_df.drop(columns=[id_col])\n\n# –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∏—Å–∫–ª—é—á–∞–µ–º ID –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\nif \"cat_cols\" in globals():\n    cat_cols = [c for c in cat_cols if c != id_col]\n\nresults = train_and_evaluate(X, y, cat_cols, CONFIG, logger)\n\n# ----- Save OOF, metrics, FI -----\nlogger.info(\"üíæ Saving artifacts...\")\nif results[\"oof_catboost\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_catboost.npy\"), results[\"oof_catboost\"])\nif results[\"oof_lightgbm\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_lightgbm.npy\"), results[\"oof_lightgbm\"])\nif results[\"oof_xgboost\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_xgboost.npy\"), results[\"oof_xgboost\"])\nif results[\"oof_blend\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_blend.npy\"), results[\"oof_blend\"])\n\nwith open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(results[\"metrics\"], f, indent=2, ensure_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:41:16.198914Z","iopub.execute_input":"2025-11-17T22:41:16.199177Z","iopub.status.idle":"2025-11-17T22:42:43.108156Z","shell.execute_reply.started":"2025-11-17T22:41:16.199161Z","shell.execute_reply":"2025-11-17T22:42:43.107374Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:41:16,205 - INFO - Using 5 folds.\nINFO:TABULAR_BOOSTING:Using 5 folds.\nCross-validation folds:   0%|          | 0/5 [00:00<?, ?it/s]2025-11-17 22:41:16,208 - INFO - ========== Fold 1/5 ==========\nINFO:TABULAR_BOOSTING:========== Fold 1/5 ==========\n2025-11-17 22:41:16,222 - INFO - Training CatBoost...\nINFO:TABULAR_BOOSTING:Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.3509813\ttest: 20.8968497\tbest: 20.8968497 (0)\ttotal: 49ms\tremaining: 2m 51s\n100:\tlearn: 17.1722585\ttest: 18.6767809\tbest: 18.6767809 (100)\ttotal: 2.53s\tremaining: 1m 25s\n200:\tlearn: 16.1641711\ttest: 18.4464673\tbest: 18.4464673 (200)\ttotal: 4.84s\tremaining: 1m 19s\n300:\tlearn: 15.3545042\ttest: 18.3727145\tbest: 18.3571554 (267)\ttotal: 7.13s\tremaining: 1m 15s\n400:\tlearn: 14.6745026\ttest: 18.2805158\tbest: 18.2805158 (400)\ttotal: 9.4s\tremaining: 1m 12s\n500:\tlearn: 14.0644080\ttest: 18.2433360\tbest: 18.2392514 (477)\ttotal: 11.7s\tremaining: 1m 9s\n600:\tlearn: 13.5331456\ttest: 18.2240427\tbest: 18.2240427 (600)\ttotal: 14s\tremaining: 1m 7s\n700:\tlearn: 13.0542636\ttest: 18.2249049\tbest: 18.2116408 (622)\ttotal: 16.3s\tremaining: 1m 4s\nbestTest = 18.21164083\nbestIteration = 622\nShrink model to first 623 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  20%|‚ñà‚ñà        | 1/5 [00:17<01:11, 17.81s/it]2025-11-17 22:41:34,023 - INFO - ========== Fold 2/5 ==========\nINFO:TABULAR_BOOSTING:========== Fold 2/5 ==========\n2025-11-17 22:41:34,042 - INFO - Training CatBoost...\nINFO:TABULAR_BOOSTING:Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4647170\ttest: 20.4394947\tbest: 20.4394947 (0)\ttotal: 26.7ms\tremaining: 1m 33s\n100:\tlearn: 17.1269989\ttest: 18.5709513\tbest: 18.5709513 (100)\ttotal: 2.45s\tremaining: 1m 22s\n200:\tlearn: 16.1446972\ttest: 18.4335277\tbest: 18.4277459 (191)\ttotal: 4.73s\tremaining: 1m 17s\n300:\tlearn: 15.3075787\ttest: 18.3989811\tbest: 18.3869003 (275)\ttotal: 7.05s\tremaining: 1m 14s\n400:\tlearn: 14.6230574\ttest: 18.3657423\tbest: 18.3640716 (395)\ttotal: 9.3s\tremaining: 1m 11s\n500:\tlearn: 14.0326319\ttest: 18.3367599\tbest: 18.3360801 (498)\ttotal: 11.6s\tremaining: 1m 9s\n600:\tlearn: 13.5162672\ttest: 18.3127423\tbest: 18.3045903 (584)\ttotal: 13.8s\tremaining: 1m 6s\nbestTest = 18.30459026\nbestIteration = 584\nShrink model to first 585 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:34<00:51, 17.22s/it]2025-11-17 22:41:50,826 - INFO - ========== Fold 3/5 ==========\nINFO:TABULAR_BOOSTING:========== Fold 3/5 ==========\n2025-11-17 22:41:50,845 - INFO - Training CatBoost...\nINFO:TABULAR_BOOSTING:Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.5785771\ttest: 20.0000473\tbest: 20.0000473 (0)\ttotal: 26.8ms\tremaining: 1m 33s\n100:\tlearn: 17.3018962\ttest: 17.9568837\tbest: 17.9568837 (100)\ttotal: 2.42s\tremaining: 1m 21s\n200:\tlearn: 16.2963588\ttest: 17.7994578\tbest: 17.7994578 (200)\ttotal: 4.69s\tremaining: 1m 17s\n300:\tlearn: 15.4766334\ttest: 17.7283084\tbest: 17.7283084 (300)\ttotal: 6.94s\tremaining: 1m 13s\n400:\tlearn: 14.7961053\ttest: 17.7114247\tbest: 17.7065973 (397)\ttotal: 9.2s\tremaining: 1m 11s\nbestTest = 17.70659728\nbestIteration = 397\nShrink model to first 398 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:47<00:30, 15.03s/it]2025-11-17 22:42:03,242 - INFO - ========== Fold 4/5 ==========\nINFO:TABULAR_BOOSTING:========== Fold 4/5 ==========\n2025-11-17 22:42:03,262 - INFO - Training CatBoost...\nINFO:TABULAR_BOOSTING:Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4767392\ttest: 20.4635707\tbest: 20.4635707 (0)\ttotal: 25.1ms\tremaining: 1m 27s\n100:\tlearn: 17.2467077\ttest: 18.3303449\tbest: 18.3303449 (100)\ttotal: 2.42s\tremaining: 1m 21s\n200:\tlearn: 16.2350071\ttest: 18.1432834\tbest: 18.1432834 (200)\ttotal: 4.7s\tremaining: 1m 17s\n300:\tlearn: 15.4192109\ttest: 18.0421962\tbest: 18.0421962 (300)\ttotal: 6.96s\tremaining: 1m 14s\n400:\tlearn: 14.7411514\ttest: 17.9901886\tbest: 17.9901886 (400)\ttotal: 9.18s\tremaining: 1m 10s\n500:\tlearn: 14.1436286\ttest: 17.9704277\tbest: 17.9668801 (470)\ttotal: 11.4s\tremaining: 1m 8s\n600:\tlearn: 13.5918707\ttest: 17.9593210\tbest: 17.9522655 (562)\ttotal: 13.7s\tremaining: 1m 5s\n700:\tlearn: 13.1169694\ttest: 17.9453434\tbest: 17.9437891 (644)\ttotal: 15.9s\tremaining: 1m 3s\n800:\tlearn: 12.6550173\ttest: 17.9422469\tbest: 17.9376478 (740)\ttotal: 18.2s\tremaining: 1m 1s\nbestTest = 17.93764778\nbestIteration = 740\nShrink model to first 741 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:07<00:17, 17.05s/it]2025-11-17 22:42:23,398 - INFO - ========== Fold 5/5 ==========\nINFO:TABULAR_BOOSTING:========== Fold 5/5 ==========\n2025-11-17 22:42:23,414 - INFO - Training CatBoost...\nINFO:TABULAR_BOOSTING:Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4288712\ttest: 20.5685614\tbest: 20.5685614 (0)\ttotal: 26.8ms\tremaining: 1m 33s\n100:\tlearn: 17.1953137\ttest: 18.4641369\tbest: 18.4641369 (100)\ttotal: 2.38s\tremaining: 1m 20s\n200:\tlearn: 16.1935210\ttest: 18.3105309\tbest: 18.3105309 (200)\ttotal: 4.63s\tremaining: 1m 16s\n300:\tlearn: 15.4001359\ttest: 18.2430975\tbest: 18.2429264 (290)\ttotal: 6.89s\tremaining: 1m 13s\n400:\tlearn: 14.7449529\ttest: 18.2265263\tbest: 18.2175527 (381)\ttotal: 9.1s\tremaining: 1m 10s\n500:\tlearn: 14.1073176\ttest: 18.1987846\tbest: 18.1987846 (500)\ttotal: 11.3s\tremaining: 1m 7s\n600:\tlearn: 13.5640121\ttest: 18.1915819\tbest: 18.1825104 (520)\ttotal: 13.6s\tremaining: 1m 5s\n700:\tlearn: 13.0778895\ttest: 18.1746853\tbest: 18.1729450 (696)\ttotal: 15.8s\tremaining: 1m 2s\n800:\tlearn: 12.6253132\ttest: 18.1784890\tbest: 18.1682038 (730)\ttotal: 18s\tremaining: 1m\nbestTest = 18.16820375\nbestIteration = 730\nShrink model to first 731 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:26<00:00, 17.38s/it]\n2025-11-17 22:42:43,088 - INFO - CatBoost RMSE: 18.067071 MAE: 13.183403\nINFO:TABULAR_BOOSTING:CatBoost RMSE: 18.067071 MAE: 13.183403\n2025-11-17 22:42:43,089 - INFO - Evaluating blended OOF predictions...\nINFO:TABULAR_BOOSTING:Evaluating blended OOF predictions...\n2025-11-17 22:42:43,092 - INFO - Blend RMSE: 18.067071 MAE: 13.183403\nINFO:TABULAR_BOOSTING:Blend RMSE: 18.067071 MAE: 13.183403\n2025-11-17 22:42:43,093 - INFO - ========== OOF metrics ==========\nINFO:TABULAR_BOOSTING:========== OOF metrics ==========\n2025-11-17 22:42:43,095 - INFO - {'CatBoost_RMSE': 18.06707139565178, 'CatBoost_MAE': 18.06707139565178, 'Blend_RMSE': 18.06707139565178, 'Blend_MAE': 18.06707139565178}\nINFO:TABULAR_BOOSTING:{'CatBoost_RMSE': 18.06707139565178, 'CatBoost_MAE': 18.06707139565178, 'Blend_RMSE': 18.06707139565178, 'Blend_MAE': 18.06707139565178}\n2025-11-17 22:42:43,101 - INFO -  Saving artifacts...\nINFO:TABULAR_BOOSTING:üíæ Saving artifacts...\n","output_type":"stream"}],"execution_count":170},{"id":"6fe648f2","cell_type":"markdown","source":"### Importances","metadata":{}},{"id":"9194b81e","cell_type":"code","source":"if \"feature_importance_lgb\" in results:\n    fi = results[\"feature_importance_lgb\"]\n    fi_df = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_gain\": fi\n    }).sort_values(\"importance_gain\", ascending=False)\n    fi_df.to_csv(os.path.join(output_dir, \"feature_importance_lgb.csv\"), index=False)\n    logger.info(\"Top features (LGB, gain):\")\n    logger.info(fi_df.head(CONFIG[\"top_features_to_show\"]))\n\nif \"feature_importance_xgb\" in results:\n    fi_xgb = results[\"feature_importance_xgb\"]\n    fi_xgb_df = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_gain\": fi_xgb\n    }).sort_values(\"importance_gain\", ascending=False)\n    fi_xgb_df.to_csv(os.path.join(output_dir, \"feature_importance_xgb.csv\"), index=False)\n    logger.info(\"Top features (XGB, gain):\")\n    logger.info(fi_xgb_df.head(CONFIG[\"top_features_to_show\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.108946Z","iopub.execute_input":"2025-11-17T22:42:43.109225Z","iopub.status.idle":"2025-11-17T22:42:43.116726Z","shell.execute_reply.started":"2025-11-17T22:42:43.109205Z","shell.execute_reply":"2025-11-17T22:42:43.116005Z"}},"outputs":[],"execution_count":171},{"id":"9e844bbe","cell_type":"markdown","source":"### Submission","metadata":{}},{"id":"dfd7f5d0","cell_type":"code","source":"# ----- Test predictions (ensemble) -----\nif test_df is not None:\n    # NEW: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ—Å—Ç–∞ –ø–æ train,\n    # –º–∞–ø–ø–∏–º –≤—Å–µ unseen ‚Üí \"other\" (–µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å), –∏–Ω–∞—á–µ ‚Üí –º–æ–¥–∞ —Ç—Ä–µ–π–Ω–∞\n    if cat_cols:\n        logger.info(\"üß© Aligning categorical values in test with train for XGBoost...\")\n        for c in cat_cols:\n            if c in test_df.columns and c in X.columns:\n\n                train_vals = X[c].dropna()\n                if train_vals.empty:\n                    continue\n\n                known = set(train_vals.unique())\n                mask_new = ~test_df[c].isin(known)\n\n                if mask_new.any():\n                    # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π \"other\"\n                    known_lower = {str(v).lower() for v in known}\n                    if \"other\" in known_lower:\n                        # –Ω–∞—Ö–æ–¥–∏–º —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ \"other\" –≤ train\n                        other_value = next(v for v in known if str(v).lower() == \"other\")\n                        replacement = other_value\n                    else:\n                        # –µ—Å–ª–∏ \"other\" –Ω–µ—Ç ‚Äî fallback –Ω–∞ –º–æ–¥—É\n                        replacement = train_vals.mode(dropna=True).iloc[0]\n\n                    logger.info(\n                        f\"üîÑ Column '{c}': mapping {mask_new.sum()} unseen categories ‚Üí '{replacement}'\"\n                    )\n                    test_df.loc[mask_new, c] = replacement\n\n    logger.info(\"üì§ Making ensemble predictions for test...\")\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ test_df –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä\n    logger.info(f\"test_df shape before ensemble_predict: {test_df.shape}\")\n    logger.info(f\"test_ids length: {len(test_ids) if test_ids is not None else 'None'}\")\n    \n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä test_df –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å test_ids, —ç—Ç–æ –æ—à–∏–±–∫–∞\n    if test_ids is not None:\n        expected_test_size = len(test_ids)\n        actual_test_size = len(test_df)\n        if actual_test_size != expected_test_size:\n            error_msg = (\n                f\"‚ùå CRITICAL ERROR: test_df size ({actual_test_size}) != test_ids size ({expected_test_size})! \"\n                f\"This indicates a problem in data processing. \"\n                f\"test_df should have {expected_test_size} rows, but has {actual_test_size}. \"\n                f\"Please check your data processing pipeline, especially generate_aggregate_features.\"\n            )\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n    \n    pred_dict = ensemble_predict(\n        models={\n            \"catboost\": results[\"models_catboost\"],\n            \"lightgbm\": results[\"models_lightgbm\"],\n            \"xgboost\": results[\"models_xgboost\"],\n        },\n        X=test_df,\n        cat_cols=cat_cols,\n        config=CONFIG\n    )\n\n    if test_ids is None:\n        test_ids_series = pd.Series(np.arange(len(test_df)), name=id_col)\n    else:\n        test_ids_series = test_ids\n\n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º submission\n    logger.info(f\"Test DataFrame shape: {test_df.shape}\")\n    logger.info(f\"Test IDs length: {len(test_ids_series)}\")\n\n    for name, preds in pred_dict.items():\n        if preds is None:\n            continue\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n        if isinstance(preds, np.ndarray):\n            pred_len = preds.shape[0] if preds.ndim == 1 else preds.shape[0]\n            logger.info(f\"Predictions '{name}' shape: {preds.shape}, length: {pred_len}\")\n            \n            if pred_len != len(test_df):\n                raise ValueError(\n                    f\"Predictions length {pred_len} != test_df length {len(test_df)} for {name}. \"\n                    f\"Test_df shape: {test_df.shape}, predictions shape: {preds.shape}\"\n                )\n\n        if isinstance(preds, np.ndarray) and preds.ndim == 2:\n            n_classes = preds.shape[1]\n            data = {\n                id_col: test_ids_series.values,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .values –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n                \"prediction\": np.argmax(preds, axis=1),  # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å\n            }\n            for k in range(n_classes):\n                data[f\"proba_class_{k}\"] = preds[:, k]\n            out_df = pd.DataFrame(data)\n        else:\n            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç\n            preds_flat = preds.flatten() if isinstance(preds, np.ndarray) else preds\n            if len(preds_flat) != len(test_ids_series):\n                raise ValueError(\n                    f\"Predictions length {len(preds_flat)} != test_ids length {len(test_ids_series)} for {name}\"\n                )\n            out_df = pd.DataFrame({\n                id_col: test_ids_series.values,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .values\n                \"prediction\": preds_flat\n            })\n\n        out_df.to_csv(os.path.join(output_dir, f\"pred_{name}.csv\"), index=False)\n        logger.info(f\"Saved predictions: pred_{name}.csv\")\n\nelapsed = time.time() - start_time\nlogger.info(f\"‚è± Finished in {elapsed:.1f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.118361Z","iopub.execute_input":"2025-11-17T22:42:43.118755Z","iopub.status.idle":"2025-11-17T22:42:43.224020Z","shell.execute_reply.started":"2025-11-17T22:42:43.118733Z","shell.execute_reply":"2025-11-17T22:42:43.223440Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:42:43,135 - INFO -  Aligning categorical values in test with train for XGBoost...\nINFO:TABULAR_BOOSTING:üß© Aligning categorical values in test with train for XGBoost...\n2025-11-17 22:42:43,165 - INFO -  Making ensemble predictions for test...\nINFO:TABULAR_BOOSTING:üì§ Making ensemble predictions for test...\n2025-11-17 22:42:43,167 - INFO - test_df shape before ensemble_predict: (8, 524)\nINFO:TABULAR_BOOSTING:test_df shape before ensemble_predict: (8, 524)\n2025-11-17 22:42:43,169 - INFO - test_ids length: 8\nINFO:TABULAR_BOOSTING:test_ids length: 8\n2025-11-17 22:42:43,207 - INFO - Test DataFrame shape: (8, 524)\nINFO:TABULAR_BOOSTING:Test DataFrame shape: (8, 524)\n2025-11-17 22:42:43,209 - INFO - Test IDs length: 8\nINFO:TABULAR_BOOSTING:Test IDs length: 8\n2025-11-17 22:42:43,211 - INFO - Predictions 'catboost' shape: (8,), length: 8\nINFO:TABULAR_BOOSTING:Predictions 'catboost' shape: (8,), length: 8\n2025-11-17 22:42:43,214 - INFO - Saved predictions: pred_catboost.csv\nINFO:TABULAR_BOOSTING:Saved predictions: pred_catboost.csv\n2025-11-17 22:42:43,216 - INFO - Predictions 'blend' shape: (8,), length: 8\nINFO:TABULAR_BOOSTING:Predictions 'blend' shape: (8,), length: 8\n2025-11-17 22:42:43,219 - INFO - Saved predictions: pred_blend.csv\nINFO:TABULAR_BOOSTING:Saved predictions: pred_blend.csv\n2025-11-17 22:42:43,220 - INFO -  Finished in 251.0 seconds\nINFO:TABULAR_BOOSTING:‚è± Finished in 251.0 seconds\n","output_type":"stream"}],"execution_count":172},{"id":"146d23f0","cell_type":"code","source":"pred_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.224513Z","iopub.execute_input":"2025-11-17T22:42:43.224886Z","iopub.status.idle":"2025-11-17T22:42:43.231318Z","shell.execute_reply.started":"2025-11-17T22:42:43.224867Z","shell.execute_reply":"2025-11-17T22:42:43.230478Z"}},"outputs":[{"execution_count":173,"output_type":"execute_result","data":{"text/plain":"{'catboost': array([39.95485419, 43.05118036, 42.0404833 , 42.20302056, 41.35563384,\n        39.69841935, 40.951625  , 42.01597805]),\n 'lightgbm': None,\n 'xgboost': None,\n 'blend': array([39.95485419, 43.05118036, 42.0404833 , 42.20302056, 41.35563384,\n        39.69841935, 40.951625  , 42.01597805])}"},"metadata":{}}],"execution_count":173},{"id":"9783588e","cell_type":"code","source":"# ----- FINAL SUBMISSION -----\nlogger.info(\"üìù Preparing final submission file...\")\n\n# –ù–∞–π–¥—ë–º –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–π prediction\nfinal_preds = pred_dict.get(\"catboost\")\n\nif final_preds is None:\n    logger.warning(\"Blend predictions are None. Falling back to first available model.\")\n    for name in [\"catboost\", \"lightgbm\", \"xgboost\"]:\n        if pred_dict.get(name) is not None:\n            final_preds = pred_dict[name]\n            logger.info(f\"Using '{name}' predictions for submission.\")\n            break\n\nif final_preds is None:\n    raise ValueError(\"No predictions available to form submission.\")\n\n# –°–æ–∑–¥–∞—ë–º DataFrame —Å id\nsubmission = pd.DataFrame({id_col: test_ids_series})\n\n# –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª—è–µ–º target\ntask_type = CONFIG[\"task_type\"]\n\nif task_type == \"multiclass\":\n    # final_preds ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ (n, C)\n    submission[CONFIG[\"target_column\"]] = np.argmax(final_preds, axis=1)\n\nelif task_type == \"binary\":\n    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π prediction ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1\n    # –µ—Å–ª–∏ –Ω—É–∂–µ–Ω label, –∑–∞–ª–æ–∂–∏ –ø–æ—Ä–æ–≥ = 0.5\n    submission[CONFIG[\"target_column\"]] = (final_preds >= 0.5).astype(int)\n\nelse:  # regression\n    submission[CONFIG[\"target_column\"]] = final_preds\n\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º\nsub_path = os.path.join(output_dir, \"submission.csv\")\nif \"index\" in submission.columns:\n    submission.drop(columns=\"index\", inplace=True)\nsubmission.to_csv(sub_path, index=False)\n\nlogger.info(f\"‚úÖ Submission file saved: {sub_path}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.232423Z","iopub.execute_input":"2025-11-17T22:42:43.232629Z","iopub.status.idle":"2025-11-17T22:42:43.252580Z","shell.execute_reply.started":"2025-11-17T22:42:43.232613Z","shell.execute_reply":"2025-11-17T22:42:43.251868Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:42:43,242 - INFO -  Preparing final submission file...\nINFO:TABULAR_BOOSTING:üìù Preparing final submission file...\n2025-11-17 22:42:43,246 - INFO -  Submission file saved: tabular_boosting_output/submission.csv\nINFO:TABULAR_BOOSTING:‚úÖ Submission file saved: tabular_boosting_output/submission.csv\n","output_type":"stream"},{"name":"stdout","text":"                                 Id  Pawpularity\n0  4128bae22183829d2b5fea10effdb0c3    39.954854\n1  43a2262d7738e3d420d453815151079e    43.051180\n2  4e429cead1848a298432a0acad014c9d    42.040483\n3  80bc3ccafcc51b66303c2c263aa38486    42.203021\n4  8f49844c382931444e68dffbe20228f4    41.355634\n","output_type":"stream"}],"execution_count":174},{"id":"fcbc2f2a","cell_type":"code","source":"submission.head(35)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.253014Z","iopub.execute_input":"2025-11-17T22:42:43.253221Z","iopub.status.idle":"2025-11-17T22:42:43.265426Z","shell.execute_reply.started":"2025-11-17T22:42:43.253204Z","shell.execute_reply":"2025-11-17T22:42:43.264651Z"}},"outputs":[{"execution_count":175,"output_type":"execute_result","data":{"text/plain":"                                 Id  Pawpularity\n0  4128bae22183829d2b5fea10effdb0c3    39.954854\n1  43a2262d7738e3d420d453815151079e    43.051180\n2  4e429cead1848a298432a0acad014c9d    42.040483\n3  80bc3ccafcc51b66303c2c263aa38486    42.203021\n4  8f49844c382931444e68dffbe20228f4    41.355634\n5  b03f7041962238a7c9d6537e22f9b017    39.698419\n6  c978013571258ed6d4637f6e8cc9d6a3    40.951625\n7  e0de453c1bffc20c22b072b34b54e50f    42.015978","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Pawpularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4128bae22183829d2b5fea10effdb0c3</td>\n      <td>39.954854</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43a2262d7738e3d420d453815151079e</td>\n      <td>43.051180</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4e429cead1848a298432a0acad014c9d</td>\n      <td>42.040483</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n      <td>42.203021</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8f49844c382931444e68dffbe20228f4</td>\n      <td>41.355634</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>b03f7041962238a7c9d6537e22f9b017</td>\n      <td>39.698419</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>c978013571258ed6d4637f6e8cc9d6a3</td>\n      <td>40.951625</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>e0de453c1bffc20c22b072b34b54e50f</td>\n      <td>42.015978</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":175},{"id":"dff6fe35","cell_type":"code","source":"# ============================\n# Optuna: CPU-only hyperparameter tuning\n# ============================\nimport optuna\n\ndef _metric_for_optuna(y_true, y_pred, task_type: str):\n    \"\"\"\n    –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è Optuna: —á–µ–º –ë–û–õ–¨–®–ï, —Ç–µ–º –ª—É—á—à–µ.\n    - regression:  -RMSE\n    - binary/multiclass: accuracy\n    \"\"\"\n    import numpy as np\n    from sklearn.metrics import mean_squared_error, accuracy_score\n\n    if task_type == \"regression\":\n        rmse = mean_squared_error(y_true, y_pred, squared=False)\n        return -rmse\n    else:\n        if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:\n            y_hat = np.argmax(y_pred, axis=1)\n        else:\n            y_hat = y_pred\n        acc = accuracy_score(y_true, y_hat)\n        return acc\n\n\ndef tune_all_boostings_optuna(\n    X,\n    y,\n    cat_cols,\n    CONFIG,\n    logger,\n    timeout: int = 3600,\n    tune_cat: bool = True,\n    tune_lgb: bool = True,\n    tune_xgb: bool = True,\n):\n    \"\"\"\n    –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–æ–≤ CatBoost, LightGBM –∏ XGBoost –Ω–∞ CPU —Å –ø–æ–º–æ—â—å—é Optuna.\n\n    tune_cat / tune_lgb / tune_xgb ‚Äî —Ñ–ª–∞–≥–∏, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ç—é–Ω–∏—Ç—å.\n    –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã —Ç—é–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ CatBoost:\n        tune_cat=True, tune_lgb=False, tune_xgb=False\n    \"\"\"\n    import numpy as np\n    from sklearn.model_selection import StratifiedKFold, KFold\n    from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n    import lightgbm as lgb\n    import xgboost as xgb\n\n    task_type = CONFIG.get(\"task_type\", \"regression\")\n\n    # ====================\n    # CV-—Å–ø–ª–∏—Ç—Ç–µ—Ä\n    # ====================\n    n_splits = CONFIG.get(\"cv_n_splits\", 3)\n    cv_shuffle = CONFIG.get(\"cv_shuffle\", True)\n    cv_random_state = CONFIG.get(\"cv_random_state\", 42)\n    cv_stratified = CONFIG.get(\"cv_stratified\", True)\n\n    if task_type == \"regression\" or not cv_stratified:\n        kf = KFold(\n            n_splits=n_splits,\n            shuffle=cv_shuffle,\n            random_state=cv_random_state,\n        )\n    else:\n        kf = StratifiedKFold(\n            n_splits=n_splits,\n            shuffle=cv_shuffle,\n            random_state=cv_random_state,\n        )\n\n    # ======================================================\n    # 1) CatBoost (CPU, —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π loss/metric)\n    # ======================================================\n    if tune_cat and CONFIG.get(\"use_catboost\", False):\n        logger.info(\"üîç Optuna tuning for CatBoost (CPU)...\")\n\n        def objective_cat(trial):\n            base_params = CONFIG[\"catboost_params\"].copy()\n\n            # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ CPU (Optuna –Ω–µ —Ç—Ä–æ–≥–∞–µ—Ç —Ç–≤–æ—ë GPU-–æ–±—É—á–µ–Ω–∏–µ)\n            base_params[\"task_type\"] = \"CPU\"\n            base_params[\"thread_count\"] = 0\n            base_params[\"verbose\"] = False\n\n            # –≥–∏–ø–µ—Ä—ã –¥–ª—è —Ç—é–Ω–∏–Ω–≥–∞\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"depth\": trial.suggest_int(\"depth\", 4, 10),\n                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0, log=True),\n                \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n                \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n            }\n\n            # ----- loss_function / eval_metric –ø–æ–¥ task_type -----\n            if task_type == \"regression\":\n                params[\"loss_function\"] = \"MAE\"\n                params[\"eval_metric\"] = \"MAE\"\n                ModelCls = CatBoostRegressor\n            elif task_type == \"binary\":\n                params[\"loss_function\"] = \"Logloss\"\n                params[\"eval_metric\"] = \"AUC\"\n                ModelCls = CatBoostClassifier\n            elif task_type == \"multiclass\":\n                params[\"loss_function\"] = \"MultiClass\"\n                params[\"eval_metric\"] = \"MultiClass\"\n                ModelCls = CatBoostClassifier\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for CatBoost\")\n\n            oof_preds = None\n\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n                valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n\n                model = ModelCls(**params)\n\n                model.fit(\n                    train_pool,\n                    eval_set=valid_pool,\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose=False,\n                )\n\n                if task_type == \"regression\":\n                    preds = model.predict(X_va)\n                else:\n                    proba = model.predict_proba(X_va)\n                    if task_type == \"binary\":\n                        preds = proba[:, 1]\n                    else:\n                        preds = proba\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_cat = optuna.create_study(direction=\"maximize\")\n        study_cat.optimize(objective_cat, timeout=timeout)\n        logger.info(f\"CatBoost best value: {study_cat.best_value}\")\n        logger.info(f\"CatBoost best params: {study_cat.best_params}\")\n\n        CONFIG[\"catboost_params\"].update({\n            \"learning_rate\": study_cat.best_params[\"learning_rate\"],\n            \"depth\": study_cat.best_params[\"depth\"],\n            \"l2_leaf_reg\": study_cat.best_params[\"l2_leaf_reg\"],\n            \"bagging_temperature\": study_cat.best_params[\"bagging_temperature\"],\n            \"border_count\": study_cat.best_params[\"border_count\"],\n        })\n\n    # ======================================================\n    # 2) LightGBM (CPU, objective/metric –ø–æ task_type)\n    # ======================================================\n    if tune_lgb and CONFIG.get(\"use_lightgbm\", False):\n        logger.info(\"üîç Optuna tuning for LightGBM (CPU)...\")\n\n        def objective_lgb(trial):\n            base_params = CONFIG[\"lgb_params\"].copy()\n\n            base_params[\"device_type\"] = \"cpu\"\n            base_params[\"num_threads\"] = 0\n\n            if task_type == \"regression\":\n                base_params[\"objective\"] = \"regression\"\n                base_params[\"metric\"] = [\"mae\"]\n            elif task_type == \"binary\":\n                base_params[\"objective\"] = \"binary\"\n                base_params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n            elif task_type == \"multiclass\":\n                num_classes = len(np.unique(y))\n                base_params[\"objective\"] = \"multiclass\"\n                base_params[\"num_class\"] = num_classes\n                base_params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for LightGBM\")\n\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 10),\n                \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n            }\n\n            oof_preds = None\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                train_set = lgb.Dataset(X_tr, label=y_tr)\n                valid_set = lgb.Dataset(X_va, label=y_va)\n\n                model = lgb.train(\n                    params,\n                    train_set,\n                    num_boost_round=5000,\n                    valid_sets=[valid_set],\n                    valid_names=[\"valid\"],\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose_eval=False,\n                )\n\n                preds = model.predict(X_va, num_iteration=model.best_iteration)\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_lgb = optuna.create_study(direction=\"maximize\")\n        study_lgb.optimize(objective_lgb, timeout=timeout)\n        logger.info(f\"LightGBM best value: {study_lgb.best_value}\")\n        logger.info(f\"LightGBM best params: {study_lgb.best_params}\")\n\n        CONFIG[\"lgb_params\"].update({\n            \"learning_rate\": study_lgb.best_params[\"learning_rate\"],\n            \"num_leaves\": study_lgb.best_params[\"num_leaves\"],\n            \"feature_fraction\": study_lgb.best_params[\"feature_fraction\"],\n            \"bagging_fraction\": study_lgb.best_params[\"bagging_fraction\"],\n            \"bagging_freq\": study_lgb.best_params[\"bagging_freq\"],\n            \"min_data_in_leaf\": study_lgb.best_params[\"min_data_in_leaf\"],\n            \"lambda_l1\": study_lgb.best_params[\"lambda_l1\"],\n            \"lambda_l2\": study_lgb.best_params[\"lambda_l2\"],\n        })\n\n    # ======================================================\n    # 3) XGBoost (CPU, objective/metric –ø–æ task_type)\n    # ======================================================\n    if tune_xgb and CONFIG.get(\"use_xgboost\", False):\n        logger.info(\"üîç Optuna tuning for XGBoost (CPU)...\")\n\n        def objective_xgb(trial):\n            base_params = CONFIG[\"xgb_params\"].copy()\n\n            base_params[\"device\"] = \"cpu\"\n            base_params.setdefault(\"tree_method\", \"hist\")\n            base_params.setdefault(\"nthread\", 0)\n\n            if task_type == \"regression\":\n                base_params[\"objective\"] = \"reg:squarederror\"\n                base_params[\"eval_metric\"] = \"mae\"\n            elif task_type == \"binary\":\n                base_params[\"objective\"] = \"binary:logistic\"\n                base_params[\"eval_metric\"] = \"auc\"\n            elif task_type == \"multiclass\":\n                num_classes = len(np.unique(y))\n                base_params[\"objective\"] = \"multi:softprob\"\n                base_params[\"num_class\"] = num_classes\n                base_params[\"eval_metric\"] = \"mlogloss\"\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for XGBoost\")\n\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n                \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n                \"lambda\": trial.suggest_float(\"lambda\", 0.0, 10.0),\n                \"alpha\": trial.suggest_float(\"alpha\", 0.0, 10.0),\n            }\n\n            oof_preds = None\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                dtrain = xgb.DMatrix(X_tr, label=y_tr)\n                dvalid = xgb.DMatrix(X_va, label=y_va)\n\n                model = xgb.train(\n                    params=params,\n                    dtrain=dtrain,\n                    num_boost_round=4000,\n                    evals=[(dvalid, \"valid\")],\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose_eval=False,\n                )\n\n                preds = model.predict(dvalid, iteration_range=(0, model.best_iteration + 1))\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_xgb = optuna.create_study(direction=\"maximize\")\n        study_xgb.optimize(objective_xgb, timeout=timeout)\n        logger.info(f\"XGBoost best value: {study_xgb.best_value}\")\n        logger.info(f\"XGBoost best params: {study_xgb.best_params}\")\n\n        CONFIG[\"xgb_params\"].update({\n            \"learning_rate\": study_xgb.best_params[\"learning_rate\"],\n            \"max_depth\": study_xgb.best_params[\"max_depth\"],\n            \"min_child_weight\": study_xgb.best_params[\"min_child_weight\"],\n            \"subsample\": study_xgb.best_params[\"subsample\"],\n            \"colsample_bytree\": study_xgb.best_params[\"colsample_bytree\"],\n            \"lambda\": study_xgb.best_params[\"lambda\"],\n            \"alpha\": study_xgb.best_params[\"alpha\"],\n        })\n\n    logger.info(\"‚úÖ Optuna tuning finished. CONFIG –æ–±–Ω–æ–≤–ª—ë–Ω –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\")\n    return CONFIG\n\n# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ –¢–û–õ–¨–ö–û –¥–ª—è CatBoost:\n#CONFIG = tune_all_boostings_optuna(X, y, cat_cols, CONFIG, logger, timeout=3600,tune_cat=True, tune_lgb=False, tune_xgb=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:42:43.266116Z","iopub.execute_input":"2025-11-17T22:42:43.266316Z","iopub.status.idle":"2025-11-17T22:42:43.313225Z","shell.execute_reply.started":"2025-11-17T22:42:43.266299Z","shell.execute_reply":"2025-11-17T22:42:43.312502Z"}},"outputs":[],"execution_count":176},{"id":"08fa3d34","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}