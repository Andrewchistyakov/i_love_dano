{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":25383,"databundleVersionId":2684322,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"843c7fbe","cell_type":"code","source":"LOAD_LIBS = False\nif LOAD_LIBS:\n    !pip install -q -U catboost\n    !pip install -q -U lightgbm\n    !pip install -q -U xgboost\n    !pip install -q -U geopy\n    !pip install -q -U phik\n    \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:41.958243Z","iopub.execute_input":"2025-11-17T21:56:41.958901Z","iopub.status.idle":"2025-11-17T21:56:41.964516Z","shell.execute_reply.started":"2025-11-17T21:56:41.958875Z","shell.execute_reply":"2025-11-17T21:56:41.963633Z"}},"outputs":[],"execution_count":28},{"id":"f3d9bdca","cell_type":"code","source":"import os\nimport gc\nimport json\nimport time\nimport logging\nimport argparse\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Optional\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import (\n    roc_auc_score,\n    accuracy_score,\n    log_loss,\n    mean_squared_error,\n    mean_absolute_error,\n    confusion_matrix\n)\n\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n# –ì–µ–æ-–±—ç–∫–µ–Ω–¥: geopy (–µ—Å–ª–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω) –∏–ª–∏ haversine\ntry:\n    from geopy.distance import geodesic\n    GEO_BACKEND = \"geopy\"\nexcept ImportError:\n    GEO_BACKEND = \"haversine\"\n\n\ntry:\n    import phik\n    HAS_PHIK = True\nexcept ImportError:\n    HAS_PHIK = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:41.965672Z","iopub.execute_input":"2025-11-17T21:56:41.965843Z","iopub.status.idle":"2025-11-17T21:56:41.979196Z","shell.execute_reply.started":"2025-11-17T21:56:41.965824Z","shell.execute_reply":"2025-11-17T21:56:41.978384Z"}},"outputs":[],"execution_count":29},{"id":"6814b40a","cell_type":"code","source":"GEO_BACKEND, HAS_PHIK","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:41.980349Z","iopub.execute_input":"2025-11-17T21:56:41.980565Z","iopub.status.idle":"2025-11-17T21:56:41.992251Z","shell.execute_reply.started":"2025-11-17T21:56:41.980550Z","shell.execute_reply":"2025-11-17T21:56:41.991665Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('geopy', True)"},"metadata":{}}],"execution_count":30},{"id":"82061097","cell_type":"code","source":"# –¥–ª—è —Ç–µ–∫—Å—Ç–∞, –Ω–∏–∫–∞–∫ –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å\nTEXT_MODEL = None\nTEXT_TOKENIZER = None\nTEXT_DEVICE = None\n\n# TF-IDF –∏ SVD –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è (—á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏)\nTFIDF_VECTORIZERS = {}  # field_name -> vectorizer\nTFIDF_SVDS = {}         # field_name -> svd\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:41.992818Z","iopub.execute_input":"2025-11-17T21:56:41.992968Z","iopub.status.idle":"2025-11-17T21:56:42.003274Z","shell.execute_reply.started":"2025-11-17T21:56:41.992956Z","shell.execute_reply":"2025-11-17T21:56:42.002620Z"}},"outputs":[],"execution_count":31},{"id":"1ebf40cc-67c8-4f39-bcfb-a46cfdbf1c0a","cell_type":"code","source":"df = pd.read_csv('/kaggle/input/petfinder-pawpularity-score/train.csv')\ndf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.008375Z","iopub.execute_input":"2025-11-17T21:56:42.008542Z","iopub.status.idle":"2025-11-17T21:56:42.037086Z","shell.execute_reply.started":"2025-11-17T21:56:42.008529Z","shell.execute_reply":"2025-11-17T21:56:42.036546Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                    Id  Subject Focus  Eyes  Face  Near  \\\n0     0007de18844b0dbbb5e1f607da0606e0              0     1     1     1   \n1     0009c66b9439883ba2750fb825e1d7db              0     1     1     0   \n2     0013fd999caf9a3efe1352ca1b0d937e              0     1     1     1   \n3     0018df346ac9c1d8413cfcc888ca8246              0     1     1     1   \n4     001dc955e10590d3ca4673f034feeef2              0     0     0     1   \n...                                ...            ...   ...   ...   ...   \n9907  ffbfa0383c34dc513c95560d6e1fdb57              0     0     0     1   \n9908  ffcc8532d76436fc79e50eb2e5238e45              0     1     1     1   \n9909  ffdf2e8673a1da6fb80342fa3b119a20              0     1     1     1   \n9910  fff19e2ce11718548fa1c5d039a5192a              0     1     1     1   \n9911  fff8e47c766799c9e12f3cb3d66ad228              0     1     1     1   \n\n      Action  Accessory  Group  Collage  Human  Occlusion  Info  Blur  \\\n0          0          0      1        0      0          0     0     0   \n1          0          0      0        0      0          0     0     0   \n2          0          0      0        0      1          1     0     0   \n3          0          0      0        0      0          0     0     0   \n4          0          0      1        0      0          0     0     0   \n...      ...        ...    ...      ...    ...        ...   ...   ...   \n9907       0          0      0        0      0          0     0     1   \n9908       0          0      0        0      0          0     0     0   \n9909       0          0      0        0      1          1     0     0   \n9910       0          0      0        0      1          0     0     0   \n9911       0          0      0        0      0          0     0     0   \n\n      Pawpularity  \n0              63  \n1              42  \n2              28  \n3              15  \n4              72  \n...           ...  \n9907           15  \n9908           70  \n9909           20  \n9910           20  \n9911           30  \n\n[9912 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Subject Focus</th>\n      <th>Eyes</th>\n      <th>Face</th>\n      <th>Near</th>\n      <th>Action</th>\n      <th>Accessory</th>\n      <th>Group</th>\n      <th>Collage</th>\n      <th>Human</th>\n      <th>Occlusion</th>\n      <th>Info</th>\n      <th>Blur</th>\n      <th>Pawpularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0007de18844b0dbbb5e1f607da0606e0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0009c66b9439883ba2750fb825e1d7db</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>42</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0013fd999caf9a3efe1352ca1b0d937e</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0018df346ac9c1d8413cfcc888ca8246</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>001dc955e10590d3ca4673f034feeef2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>72</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9907</th>\n      <td>ffbfa0383c34dc513c95560d6e1fdb57</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>9908</th>\n      <td>ffcc8532d76436fc79e50eb2e5238e45</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>9909</th>\n      <td>ffdf2e8673a1da6fb80342fa3b119a20</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9910</th>\n      <td>fff19e2ce11718548fa1c5d039a5192a</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>9911</th>\n      <td>fff8e47c766799c9e12f3cb3d66ad228</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30</td>\n    </tr>\n  </tbody>\n</table>\n<p>9912 rows √ó 14 columns</p>\n</div>"},"metadata":{}}],"execution_count":32},{"id":"9f4654d2","cell_type":"code","source":"DATA_FOLDER_PATH = '.'\nCONFIG = {\n    # ---------- Paths ----------\n    \"train_path\": os.path.join(DATA_FOLDER_PATH, \"/kaggle/input/petfinder-pawpularity-score/train.csv\"),\n    \"test_path\": os.path.join(DATA_FOLDER_PATH, '/kaggle/input/petfinder-pawpularity-score/test.csv'),\n    \"sep\": \",\",\n    \"id_column\": \"Id\",\n    \"target_column\": \"Pawpularity\",\n    \"datetime_columns\": [],\n    \"output_dir\": \"tabular_boosting_output\",\n\n    # ---------- Task ----------\n    # \"binary\" / \"multiclass\" / \"regression\"\n    \"task_type\": \"regression\",\n\n    # ---------- Basic features ----------\n    \"basic_drop_columns\": [],\n    \"basic_as_categorical\": [],\n    \"basic_max_cat_unique\": 64,\n    \"basic_datetime_expand\": False,\n    \"basic_datetime_features\": [\"year\", \"month\", \"day\", \"dow\", \"hour\"],\n\n    # ---------- Categorical processing ----------\n    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π \"—Ä–∞–∑–º–µ—Ä\" –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–ø–æ —á–∏—Å–ª—É –æ–±—ä–µ–∫—Ç–æ–≤ –∏/–∏–ª–∏ –¥–æ–ª–µ),\n    # –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –º–µ–Ω—å—à–µ –ø–æ—Ä–æ–≥–∞ —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ 'other'\n    \"cat_min_count\": 20,          # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª-–≤–æ –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n    \"cat_min_freq\": 0.0,          # –º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–æ–ª—è (0..1); 0 = –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å\n    # –µ—Å–ª–∏ –¥–æ–ª—è \"—Ä–µ–¥–∫–∏—Ö\" –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –ø—Ä–∏–∑–Ω–∞–∫–µ >= —ç—Ç–æ–≥–æ –ø–æ—Ä–æ–≥–∞ ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ —É–¥–∞–ª—è–µ—Ç—Å—è\n    \"cat_max_rare_share\": 0.98,\n    # –µ—Å–ª–∏ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤ other —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –≤—Å—ë –µ—â—ë —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî –ø—Ä–∏–∑–Ω–∞–∫ —É–¥–∞–ª—è–µ—Ç—Å—è\n    \"cat_max_unique_after_group\": 500,\n\n    # ---------- Post-feature service columns ----------\n    # —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ñ–∏—á\n    # (–∏—Å—Ö–æ–¥–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã, raw id, —Å—ã—Ä—ã–µ –∞–¥—Ä–µ—Å–∞ –∏ —Ç.–ø.)\n    \"post_feature_drop_columns\": [], # –º–æ–∂–µ—Ç –±—ã—Ç—å —É–¥–∞–ª–∏—Ç—å category_geo_ref_lat, category_geo_ref_lon\n\n    # ---------- Address processing ----------\n    # –∫–æ–ª–æ–Ω–∫–∞-–∞–¥—Ä–µ—Å, –∏–∑ –∫–æ—Ç–æ—Ä–æ–π –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –≥–æ—Ä–æ–¥\n    \"address_column\": None,  \n\n    # –∏–Ω–¥–µ–∫—Å —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ split (–Ω–∞–ø—Ä–∏–º–µ—Ä [-1] = –ø–æ—Å–ª–µ–¥–Ω–∏–π)\n    # –º–æ–∂–Ω–æ —É–∫–∞–∑—ã–≤–∞—Ç—å –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π –∏–Ω–¥–µ–∫—Å\n    \"address_city_index\": -1,  \n\n    # —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å –≤ —Å—Ç—Ä–æ–∫–µ –∞–¥—Ä–µ—Å–∞\n    \"address_split_sep\": \",\",  \n\n    # ---------- Aggregates ----------\n    \"agg_enable\": False,\n    \"agg_groupby_cols\": [],\n    \"agg_numeric_cols\": [],\n    \"agg_aggs\": [\"mean\", \"std\", \"min\", \"max\", \"sum\", \"median\", \"nunique\", \"count\"],\n    \"agg_prefix\": \"agg\",\n\n    # ---------- Geo features ----------\n    \"geo_enable\": False, \n    \"geo_lat_from_col\": \"pickup_lat\",\n    \"geo_lon_from_col\": \"pickup_lon\",\n    \"geo_lat_to_col\": \"dropoff_lat\",\n    \"geo_lon_to_col\": \"dropoff_lon\",\n    \"geo_ref_lat\": 'category_geo_ref_lat',\n    \"geo_ref_lon\": 'category_geo_ref_lon',\n    \"geo_prefix\": \"geo\",\n    # –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∏—á–∏ (bearing, manhattan, dlat/dlon, midpoint)\n    \"geo_extra_enable\": False,\n\n    \"geo_from_coord_col\": None,\n    \"geo_to_coord_col\": None,\n    \"geo_coord_string_sep\": \",\",\n\n    # ---------- Geo reference config ----------\n    \"geo_reference\": {\n        \"enabled\": False,\n        \"category_column\": \"category\",\n        \"coordinates_column\": \"coordinates\",  # —Å—Ç—Ä–æ–∫–∞ \"lat,lon\", –µ—Å–ª–∏ lat/lon –Ω–µ –∑–∞–¥–∞–Ω—ã\n        \"lat_column\": None,                  # <- –µ—Å–ª–∏ –∑–∞–¥–∞—Ç—å, –±–µ—Ä—ë–º –æ—Ç—Å—é–¥–∞ —à–∏—Ä–æ—Ç—É\n        \"lon_column\": None,                  # <- –µ—Å–ª–∏ –∑–∞–¥–∞—Ç—å, –±–µ—Ä—ë–º –æ—Ç—Å—é–¥–∞ –¥–æ–ª–≥–æ—Ç—É\n        \"output_column\": \"category_geo_ref\",\n        \"output_lat_column\": \"category_geo_ref_lat\",\n        \"output_lon_column\": \"category_geo_ref_lon\",\n    },\n\n    # ---------- Correlation-based feature filtering ----------\n    \"corr_enable\": False,          # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\n    \"corr_pearson_min_abs\": 0.95,   # –ø–æ—Ä–æ–≥ |Pearson|; 0.1, 0.05 –∏ —Ç.–ø.\n    \"corr_use_phik\": False,        # —Å—á–∏—Ç–∞—Ç—å –ª–∏ phik\n    \"corr_phik_min_abs\": 0.0,      # –ø–æ—Ä–æ–≥ |phik|\n    \n    #----------images---------\n    # –ø–∞–ø–∫–∞ —Å train-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º image-—Ñ–∏—á–∏)\n    \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n    # –ø–∞–ø–∫–∞ —Å test-–∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏ (–µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º image-—Ñ–∏—á–∏)\n    \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n    # –ï—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ image_id —É–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç \".jpg\", –ø–æ—Å—Ç–∞–≤—å IMAGE_EXT = \"\"\n    # —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∫–∞—Ä—Ç–∏–Ω–æ–∫ (–µ—Å–ª–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ —Ç–æ–ª—å–∫–æ id –±–µ–∑ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è)\n    \"image_ext\":\".jpg\",\n    # –∫–æ–ª–æ–Ω–∫–∞ –≤ —Ç–∞–±–ª–∏—Ü–µ —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∏\n    \"file_names_column\": \"Id\",\n    # —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–∞—Ä—Ç–∏–Ω–æ–∫\n    \"batch_size\": 32,\n    \n    # ---------- Text features ----------\n    # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n    \"text_enable\": False,  # –≤–∫–ª—é—á–∏—Ç—å/–≤—ã–∫–ª—é—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∏—á–∏\n\n    # –ª—é–±—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π train/test —Ç–∞–±–ª–∏—Ü–µ\n    \"text_columns\": [],  # –Ω–∞–ø—Ä–∏–º–µ—Ä [\"title\", \"description\"]\n\n    # –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (id, text) –≤ parquet/csv/tsv\n    # –∫–∞–∂–¥–∞—è –∑–∞–ø–∏—Å—å:\n    # {\n    #   \"name\": \"comments\",        # –ª–æ–≥–∏—á–µ—Å–∫–æ–µ –∏–º—è (–¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏—è —Ñ–∏—á)\n    #   \"train_path\": \"train_comments.parquet\",\n    #   \"test_path\": \"test_comments.parquet\",\n    #   \"format\": \"parquet\",       # \"parquet\" / \"csv\" / \"tsv\" / \"auto\"\n    #   \"id_column\": \"index\",      # –∫–æ–ª–æ–Ω–∫–∞ id –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ\n    #   \"text_column\": \"text\",     # –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –≤ —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü–µ\n    #   \"output_column\": \"comments_text\"  # –∫–∞–∫ –Ω–∞–∑–≤–∞—Ç—å –∫–æ–ª–æ–Ω–∫—É –≤ –æ–±—â–µ–π —Ç–∞–±–ª–∏—Ü–µ (–æ–ø—Ü.)\n    # }\n    \"text_external_tables\": [{\n        \"name\": \"reviews\",\n        \"train_path\": \"reviews.tsv\",\n        \"test_path\": \"reviews.tsv\",\n        \"format\": \"tsv\",       # –∏–ª–∏ \"auto\"\n        # –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–º –æ–±—ä–µ–∫—Ç–∞\n    \"id_column\": \"id\",      # –≤ —ç—Ç–∏—Ö —Ñ–∞–π–ª–∞—Ö\n        \"text_column\": \"text\",     # –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º\n        \"output_column\": \"comments_text\"\n    }],\n\n    # —Ç–∏–ø —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n    # \"bert\"      ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä-—ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n    # \"tfidf_svd\" ‚Äî TF-IDF + TruncatedSVD\n    # —Ç–∏–ø —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏: \"bert\" –∏–ª–∏ \"tfidf_svd\"\n    \"text_model_type\": \"tfidf_svd\",\n\n    # –≤–∞—Ä–∏–∞–Ω—Ç—ã –º–æ–¥–µ–ª–µ–π (–∫–æ–º–º–µ–Ω—Ç–∞–º–∏, —á—Ç–æ–±—ã –º–æ–∂–Ω–æ –±—ã–ª–æ –±—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ–∫–ª—é—á–∞—Ç—å)\n    # \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"  # RU + EN (—É–Ω–∏–≤–µ—Ä—Å–∞–ª)\n    # \"DeepPavlov/rubert-base-cased-sentence\"                        # ru-only\n    # \"sentence-transformers/all-mpnet-base-v2\"                      # en-only\n    \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n\n    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ BERT\n    \"text_max_length\": 256,\n    \"text_batch_size\": 32,\n    # –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ TF-IDF + SVD\n    \"text_tfidf_max_features\": 50000,\n    \"text_svd_n_components\": 256,\n\n    # ---------- CV ----------\n    \"cv_n_splits\": 5,\n    \"cv_random_state\": 42,\n    \"cv_shuffle\": True,\n    \"cv_stratified\": True,\n\n    # ---------- Models ----------\n    \"use_catboost\": True,\n    \"use_lightgbm\": False,\n    \"use_xgboost\": False,\n\n    \"catboost_params\": {\n        \"iterations\": 3500,\n        \"learning_rate\": 0.05,\n        \"depth\": 6,\n        \"loss_function\": \"MultiClass\", # \"Logloss\"\n        \"eval_metric\": \"AUC\",\n        \"random_seed\": 42,\n        \"verbose\": 100\n    },\n\n\n    \"lgb_params\": {\n        \"objective\": \"multiclass\", # \"binary\"\n        \"eval_metric\": \"auc\", #[\"auc\", \"binary_logloss\"],\n        \"learning_rate\": 0.05,\n        \"num_leaves\": 31,\n        \"feature_fraction\": 0.9,\n        \"bagging_fraction\": 0.8,\n        \"bagging_freq\": 5,\n        \"lambda_l2\": 1.0,\n        \"num_threads\": 0,\n        \"verbose\": -1\n    },\n\n    \"xgb_params\": {\n        \"objective\": \"multi:softmax\", #\"binary:logistic\",\n        \"eval_metric\": \"auc\",\n        \"learning_rate\": 0.05,\n        \"max_depth\": 6,\n        \"subsample\": 0.8,\n        \"colsample_bytree\": 0.8,\n        \"lambda\": 1.0,\n        \"alpha\": 0.0,\n        \"tree_method\": \"hist\"\n    },\n\n    # ---------- Blending ----------\n    \"blend_weights\": {\n        \"catboost\": 1,\n        \"lightgbm\": 0.35,\n        \"xgboost\": 0.15\n    },\n\n    # ---------- Feature importance ----------\n    \"compute_feature_importance\": True,\n    \"top_features_to_show\": 50,\n\n    # ---------- Early stopping ----------\n    # –ø–∞—Ä–∞–º–µ—Ç—Ä —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–ª—è –±—É—Å—Ç–∏–Ω–≥–æ–≤\n    \"early_stopping_rounds\": 100,# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞—É–Ω–¥–æ–≤ –±–µ–∑ —É–ª—É—á—à–µ–Ω–∏—è –¥–ª—è —Ä–∞–Ω–Ω–µ–π –æ—Å—Ç–∞–Ω–æ–≤–∫–∏\n    \"device\": \"cuda\",\n\n    \"seed\": 42,\n    \"verbose\": True\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:08:29.067532Z","iopub.execute_input":"2025-11-17T22:08:29.067802Z","iopub.status.idle":"2025-11-17T22:08:29.079897Z","shell.execute_reply.started":"2025-11-17T22:08:29.067779Z","shell.execute_reply":"2025-11-17T22:08:29.079331Z"}},"outputs":[],"execution_count":64},{"id":"af431dbd","cell_type":"code","source":"def setup_logging(output_dir: str) -> logging.Logger:\n    os.makedirs(output_dir, exist_ok=True)\n    log_path = os.path.join(output_dir, \"training.log\")\n\n    logger = logging.getLogger(\"TABULAR_BOOSTING\")\n    logger.setLevel(logging.INFO)\n    logger.handlers = []\n\n    # –ò—Å–ø–æ–ª—å–∑—É–µ–º UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫—É –¥–ª—è —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —ç–º–æ–¥–∑–∏\n    fh = logging.FileHandler(log_path, encoding='utf-8')\n    fh.setLevel(logging.INFO)\n    \n    # –î–ª—è –∫–æ–Ω—Å–æ–ª–∏ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º UTF-8, –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ\n    import sys\n    if sys.stdout.encoding != 'utf-8':\n        # –ï—Å–ª–∏ –∫–æ–Ω—Å–æ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç UTF-8, —Å–æ–∑–¥–∞–µ–º –æ–±–µ—Ä—Ç–∫—É\n        class UTF8StreamHandler(logging.StreamHandler):\n            def emit(self, record):\n                try:\n                    msg = self.format(record)\n                    # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏ –¥–ª—è –∫–æ–Ω—Å–æ–ª–∏, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç UTF-8\n                    import re\n                    msg = re.sub(r'[^\\x00-\\x7F]+', '', msg)  # –£–¥–∞–ª—è–µ–º –Ω–µ-ASCII —Å–∏–º–≤–æ–ª—ã\n                    stream = self.stream\n                    stream.write(msg + self.terminator)\n                    self.flush()\n                except Exception:\n                    self.handleError(record)\n        ch = UTF8StreamHandler()\n    else:\n        ch = logging.StreamHandler()\n    ch.setLevel(logging.INFO)\n\n    fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n    fh.setFormatter(fmt)\n    ch.setFormatter(fmt)\n\n    logger.addHandler(fh)\n    logger.addHandler(ch)\n\n    logger.info(\"Logger initialized.\")\n    return logger\n\n\ndef log_config(logger: logging.Logger, config: Dict):\n    logger.info(\"========== CONFIG ==========\")\n    logger.info(json.dumps(config, indent=2, ensure_ascii=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.073515Z","iopub.execute_input":"2025-11-17T21:56:42.073726Z","iopub.status.idle":"2025-11-17T21:56:42.086358Z","shell.execute_reply.started":"2025-11-17T21:56:42.073710Z","shell.execute_reply":"2025-11-17T21:56:42.085704Z"}},"outputs":[],"execution_count":34},{"id":"b8b7b9c7","cell_type":"code","source":"start_time = time.time()\n\noutput_dir = CONFIG[\"output_dir\"]\nos.makedirs(output_dir, exist_ok=True)\nlogger = setup_logging(output_dir)\nlog_config(logger, CONFIG)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.086855Z","iopub.execute_input":"2025-11-17T21:56:42.087098Z","iopub.status.idle":"2025-11-17T21:56:42.104515Z","shell.execute_reply.started":"2025-11-17T21:56:42.087079Z","shell.execute_reply":"2025-11-17T21:56:42.103887Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 21:56:42,099 - INFO - Logger initialized.\n2025-11-17 21:56:42,100 - INFO - ========== CONFIG ==========\n2025-11-17 21:56:42,101 - INFO - {\n  \"train_path\": \"/kaggle/input/petfinder-pawpularity-score/train.csv\",\n  \"test_path\": \"/kaggle/input/petfinder-pawpularity-score/test.csv\",\n  \"sep\": \",\",\n  \"id_column\": \"Id\",\n  \"target_column\": \"Pawpularity\",\n  \"datetime_columns\": [],\n  \"output_dir\": \"tabular_boosting_output\",\n  \"task_type\": \"regression\",\n  \"basic_drop_columns\": [],\n  \"basic_as_categorical\": [],\n  \"basic_max_cat_unique\": 64,\n  \"basic_datetime_expand\": false,\n  \"basic_datetime_features\": [\n    \"year\",\n    \"month\",\n    \"day\",\n    \"dow\",\n    \"hour\"\n  ],\n  \"cat_min_count\": 20,\n  \"cat_min_freq\": 0.0,\n  \"cat_max_rare_share\": 0.98,\n  \"cat_max_unique_after_group\": 500,\n  \"post_feature_drop_columns\": [],\n  \"address_column\": null,\n  \"address_city_index\": -1,\n  \"address_split_sep\": \",\",\n  \"agg_enable\": false,\n  \"agg_groupby_cols\": [],\n  \"agg_numeric_cols\": [],\n  \"agg_aggs\": [\n    \"mean\",\n    \"std\",\n    \"min\",\n    \"max\",\n    \"sum\",\n    \"median\",\n    \"nunique\",\n    \"count\"\n  ],\n  \"agg_prefix\": \"agg\",\n  \"geo_enable\": false,\n  \"geo_lat_from_col\": \"pickup_lat\",\n  \"geo_lon_from_col\": \"pickup_lon\",\n  \"geo_lat_to_col\": \"dropoff_lat\",\n  \"geo_lon_to_col\": \"dropoff_lon\",\n  \"geo_ref_lat\": \"category_geo_ref_lat\",\n  \"geo_ref_lon\": \"category_geo_ref_lon\",\n  \"geo_prefix\": \"geo\",\n  \"geo_extra_enable\": false,\n  \"geo_from_coord_col\": null,\n  \"geo_to_coord_col\": null,\n  \"geo_coord_string_sep\": \",\",\n  \"geo_reference\": {\n    \"enabled\": false,\n    \"category_column\": \"category\",\n    \"coordinates_column\": \"coordinates\",\n    \"lat_column\": null,\n    \"lon_column\": null,\n    \"output_column\": \"category_geo_ref\",\n    \"output_lat_column\": \"category_geo_ref_lat\",\n    \"output_lon_column\": \"category_geo_ref_lon\"\n  },\n  \"corr_enable\": false,\n  \"corr_pearson_min_abs\": 0.95,\n  \"corr_use_phik\": false,\n  \"corr_phik_min_abs\": 0.0,\n  \"train_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/train\",\n  \"test_images_dir\": \"/kaggle/input/petfinder-pawpularity-score/test\",\n  \"image_ext\": \".jpg\",\n  \"file_names_column\": \"Id\",\n  \"batch_size\": 32,\n  \"text_enable\": false,\n  \"text_columns\": [],\n  \"text_external_tables\": [\n    {\n      \"name\": \"reviews\",\n      \"train_path\": \"reviews.tsv\",\n      \"test_path\": \"reviews.tsv\",\n      \"format\": \"tsv\",\n      \"id_column\": \"id\",\n      \"text_column\": \"text\",\n      \"output_column\": \"comments_text\"\n    }\n  ],\n  \"text_model_type\": \"tfidf_svd\",\n  \"text_bert_model_name\": \"DeepPavlov/rubert-base-cased-sentence\",\n  \"text_max_length\": 256,\n  \"text_batch_size\": 32,\n  \"text_tfidf_max_features\": 50000,\n  \"text_svd_n_components\": 256,\n  \"cv_n_splits\": 5,\n  \"cv_random_state\": 42,\n  \"cv_shuffle\": true,\n  \"cv_stratified\": true,\n  \"use_catboost\": true,\n  \"use_lightgbm\": false,\n  \"use_xgboost\": false,\n  \"catboost_params\": {\n    \"iterations\": 3500,\n    \"learning_rate\": 0.05,\n    \"depth\": 6,\n    \"loss_function\": \"MultiClass\",\n    \"eval_metric\": \"AUC\",\n    \"random_seed\": 42,\n    \"verbose\": 100\n  },\n  \"lgb_params\": {\n    \"objective\": \"multiclass\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 31,\n    \"feature_fraction\": 0.9,\n    \"bagging_fraction\": 0.8,\n    \"bagging_freq\": 5,\n    \"lambda_l2\": 1.0,\n    \"num_threads\": 0,\n    \"verbose\": -1\n  },\n  \"xgb_params\": {\n    \"objective\": \"multi:softmax\",\n    \"eval_metric\": \"auc\",\n    \"learning_rate\": 0.05,\n    \"max_depth\": 6,\n    \"subsample\": 0.8,\n    \"colsample_bytree\": 0.8,\n    \"lambda\": 1.0,\n    \"alpha\": 0.0,\n    \"tree_method\": \"hist\"\n  },\n  \"blend_weights\": {\n    \"catboost\": 1,\n    \"lightgbm\": 0.35,\n    \"xgboost\": 0.15\n  },\n  \"compute_feature_importance\": true,\n  \"top_features_to_show\": 50,\n  \"early_stopping_rounds\": 100,\n  \"seed\": 42,\n  \"verbose\": true\n}\n","output_type":"stream"}],"execution_count":35},{"id":"8795137b","cell_type":"markdown","source":"### Utils","metadata":{}},{"id":"87f51611","cell_type":"code","source":"def _ensure_lat_lon_from_single_column(\n    df: pd.DataFrame,\n    coord_col: str,\n    lat_name: str,\n    lon_name: str,\n    sep: str = \",\"\n) -> pd.DataFrame:\n    \"\"\"\n    coord_col –º–æ–∂–µ—Ç –±—ã—Ç—å:\n      - —Å—Ç—Ä–æ–∫–∞ –≤–∏–¥–∞ \"55.75,37.62\"\n      - —Å–ø–∏—Å–æ–∫/–∫–æ—Ä—Ç–µ–∂ [55.75, 37.62]\n    —Å–æ–∑–¥–∞—ë–º/–ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ lat_name, lon_name\n    \"\"\"\n    if coord_col not in df.columns:\n        return df\n\n    def _parse_one(x):\n        if isinstance(x, (list, tuple)) and len(x) >= 2:\n            return x[0], x[1]\n        if isinstance(x, str):\n            parts = x.split(sep)\n            if len(parts) >= 2:\n                try:\n                    return float(parts[0]), float(parts[1])\n                except Exception:\n                    return np.nan, np.nan\n        return np.nan, np.nan\n\n    lat_list, lon_list = zip(*df[coord_col].map(_parse_one))\n    df = df.copy()\n    df[lat_name] = lat_list\n    df[lon_name] = lon_list\n    return df\n\n# NEW: –æ—á–∏—Å—Ç–∫–∞ –∏–º—ë–Ω —Ñ–∏—á–µ–π –¥–ª—è –±—É—Å—Ç–∏–Ω–≥–æ–≤ (XGBoost –æ—Å–æ–±–µ–Ω–Ω–æ —Å—Ç—Ä–æ–≥–∏–π)\ndef sanitize_feature_names(\n    df: pd.DataFrame,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, Dict]:\n    \"\"\"\n    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ –≤ —Å—Ç—Ä–æ–∫–∏ –±–µ–∑ —Å–∏–º–≤–æ–ª–æ–≤ [, ], <, >,\n    —Å–ª–µ–¥–∏—Ç –∑–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å—é –∏–º—ë–Ω –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      - –Ω–æ–≤—ã–π DataFrame —Å –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏\n      - mapping {—Å—Ç–∞—Ä–æ–µ_–∏–º—è -> –Ω–æ–≤–æ–µ_–∏–º—è}\n    \"\"\"\n    old_cols = list(df.columns)\n    new_cols: List[str] = []\n    mapping: Dict = {}\n    changed = False\n\n    df = df.copy()\n\n    for col in old_cols:\n        new = str(col)\n\n        # –∑–∞–ø—Ä–µ—â—ë–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)\n        new = new.replace(\"[\", \"(\").replace(\"]\", \")\")\n        new = new.replace(\"<\", \"_lt_\").replace(\">\", \"_gt_\")\n\n        # –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∏–∑–±–∞–≤–∏–º—Å—è –æ—Ç –æ—á–µ–Ω—å \"—ç–∫–∑–æ—Ç–∏–∫–∏\"\n        # (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—É—Å—Ç—ã–µ –∏–º–µ–Ω–∞)\n        if new.strip() == \"\":\n            new = \"feature\"\n\n        base = new\n        k = 1\n        # –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å\n        while new in new_cols:\n            new = f\"{base}__{k}\"\n            k += 1\n\n        if new != col:\n            changed = True\n        mapping[col] = new\n        new_cols.append(new)\n\n    df.columns = new_cols\n\n    if changed:\n        logger.info(\"Sanitized feature names for boosting models.\")\n        # –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏ –º–æ–∂–Ω–æ –∑–∞–ª–æ–≥–∏—Ä–æ–≤–∞—Ç—å —á–∞—Å—Ç—å –º–∞–ø–ø–∏–Ω–≥–∞\n        logger.info(\n            \"Example of feature name mapping: \" +\n            \", \".join(\n                f\"{k} -> {v}\" for k, v in list(mapping.items())[:10]\n            )\n        )\n\n    return df, mapping","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.111808Z","iopub.execute_input":"2025-11-17T21:56:42.111997Z","iopub.status.idle":"2025-11-17T21:56:42.120431Z","shell.execute_reply.started":"2025-11-17T21:56:42.111984Z","shell.execute_reply":"2025-11-17T21:56:42.119721Z"}},"outputs":[],"execution_count":36},{"id":"fa0054b1","cell_type":"code","source":"class ImagesDataset:\n    def __init__(self, file_paths, transform=None):\n        self.file_paths = file_paths\n        self.transform = transform\n    \n    def __getitem__(self, idx):\n        ### read image\n\n        file_path = self.file_paths[idx]\n        image = Image.open(file_path).convert(\"RGB\")\n        image = self.transform(image)\n        # DataLoader —Å–∞–º —Å–æ–∑–¥–∞—Å—Ç batch dimension, –ø–æ—ç—Ç–æ–º—É unsqueeze –Ω–µ –Ω—É–∂–µ–Ω\n        return image\n    \n    def __len__(self):\n        return len(self.file_paths)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.121477Z","iopub.execute_input":"2025-11-17T21:56:42.121665Z","iopub.status.idle":"2025-11-17T21:56:42.136029Z","shell.execute_reply.started":"2025-11-17T21:56:42.121652Z","shell.execute_reply":"2025-11-17T21:56:42.135536Z"}},"outputs":[],"execution_count":37},{"id":"bb95ee92","cell_type":"code","source":"def detect_categorical_columns(\n    df: pd.DataFrame,\n    max_unique: int,\n    force_categorical: List[str]\n) -> List[str]:\n    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ max_unique –Ω–µ None\n    if max_unique is None:\n        max_unique = 64  # –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é\n    \n    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏–∑ force_categorical\n    cats = set(c for c in force_categorical if c is not None and isinstance(c, str))\n    \n    for col in df.columns:\n        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º None –∏–ª–∏ –Ω–µ-—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫\n        if col is None or not isinstance(col, str):\n            continue\n            \n        try:\n            if df[col].dtype == \"object\":\n                cats.add(col)\n            else:\n                try:\n                    nunique_val = df[col].nunique(dropna=True)\n                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ nunique_val –Ω–µ None –∏ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º\n                    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ max_unique - —ç—Ç–æ —á–∏—Å–ª–æ\n                    if (nunique_val is not None and \n                        isinstance(nunique_val, (int, np.integer)) and\n                        isinstance(max_unique, (int, np.integer, float))):\n                        if int(nunique_val) <= int(max_unique):\n                            cats.add(col)\n                except (TypeError, ValueError) as e:\n                    # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –≤—ã—á–∏—Å–ª–∏—Ç—å nunique, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ª–æ–Ω–∫—É\n                    continue\n        except Exception:\n            # –ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–æ–ª–æ–Ω–∫–æ–π, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—ë\n            continue\n    \n    # –§–∏–ª—å—Ç—Ä—É–µ–º None –∏ –Ω–µ-—Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–æ–π\n    # –¢–∞–∫–∂–µ —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è - —Å—Ç—Ä–æ–∫–∏\n    cats_filtered = []\n    for c in cats:\n        if c is not None:\n            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å—Ç—Ä–æ–∫—É, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–æ–∫–∞\n            try:\n                c_str = str(c) if not isinstance(c, str) else c\n                if c_str and c_str.strip():  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Ç—Ä–æ–∫–∞ –Ω–µ –ø—É—Å—Ç–∞—è\n                    cats_filtered.append(c_str)\n            except Exception:\n                continue\n    \n    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏\n    try:\n        return sorted(cats_filtered)\n    except TypeError as e:\n        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –æ—à–∏–±–∫–∞ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–µ–∑ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏\n        return list(cats_filtered)\n\n\ndef process_categorical_features(\n    X: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    cat_cols: List[str],\n    config: Dict,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame], List[str]]:\n    \"\"\"\n    –î–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π —Ñ–∏—á–∏:\n      1) –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å –º–∞–ª–æ–π —á–∞—Å—Ç–æ—Ç–æ–π/–¥–æ–ª–µ–π —Å–ª–∏–≤–∞—é—Ç—Å—è –≤ 'other'\n      2) –µ—Å–ª–∏ –¥–æ–ª—è —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å–ª–∏—à–∫–æ–º –≤–µ–ª–∏–∫–∞ –∏–ª–∏\n         –¥–∞–∂–µ –ø–æ—Å–ª–µ —Å–ª–∏—è–Ω–∏—è —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö ‚Äî —Ñ–∏—á–∞ –≤—ã–∫–∏–¥—ã–≤–∞–µ—Ç—Å—è.\n\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      X_new, test_new, updated_cat_cols\n    \"\"\"\n    if not cat_cols:\n        return X, test, cat_cols\n\n    min_count = int(config.get(\"cat_min_count\", 0) or 0)\n    min_freq = float(config.get(\"cat_min_freq\", 0.0) or 0.0)\n    max_rare_share = float(config.get(\"cat_max_rare_share\", 1.0) or 1.0)\n    max_unique_after = int(config.get(\"cat_max_unique_after_group\", 10**9) or 10**9)\n\n    if min_count <= 0 and min_freq <= 0.0 and max_rare_share >= 1.0:\n        # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞\n        logger.info(\"Categorical processing: thresholds are trivial, skipping.\")\n        return X, test, cat_cols\n\n    logger.info(\n        \"Categorical processing: \"\n        f\"cat_min_count={min_count}, cat_min_freq={min_freq}, \"\n        f\"cat_max_rare_share={max_rare_share}, \"\n        f\"cat_max_unique_after_group={max_unique_after}\"\n    )\n\n    X_new = X.copy()\n    test_new = test.copy() if test is not None else None\n    to_drop = []\n\n    n = len(X_new)\n\n    for col in tqdm(cat_cols, desc=\"Processing categorical features\"):\n        if col not in X_new.columns:\n            continue\n\n        vc = X_new[col].value_counts(dropna=False)\n        total = vc.sum()\n\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º \"—Ä–µ–¥–∫–∏–µ\" –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\n        rare_mask = np.zeros(len(vc), dtype=bool)\n        if min_count > 0:\n            rare_mask |= (vc.values < min_count)\n        if min_freq > 0.0:\n            rare_mask |= (vc.values / total < min_freq)\n\n        rare_cats = vc.index[rare_mask]\n        rare_share = vc[rare_cats].sum() / total if len(rare_cats) > 0 else 0.0\n\n        logger.info(\n            f\"[cat] {col}: unique={vc.size}, rare_cats={len(rare_cats)}, \"\n            f\"rare_share={rare_share:.4f}\"\n        )\n\n        # –µ—Å–ª–∏ —Ä–µ–¥–∫–∏—Ö —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ ‚Äî —Ñ–∏—á–∞ –±–µ—Å–ø–æ–ª–µ–∑–Ω–∞, –≤—ã–∫–∏–¥—ã–≤–∞–µ–º\n        if rare_share >= max_rare_share:\n            logger.info(\n                f\"[cat] {col}: rare_share={rare_share:.4f} >= {max_rare_share}, \"\n                f\"dropping whole feature.\"\n            )\n            to_drop.append(col)\n            continue\n\n        if len(rare_cats) == 0:\n            # –Ω–µ—á–µ–≥–æ –æ–±—ä–µ–¥–∏–Ω—è—Ç—å\n            continue\n\n        # –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–¥–∫–∏–µ –≤ 'other'\n        other_label = \"__OTHER__\"\n        X_new[col] = X_new[col].where(~X_new[col].isin(rare_cats), other_label)\n        if test_new is not None:\n            test_new[col] = test_new[col].where(~test_new[col].isin(rare_cats), other_label)\n\n        # –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è\n        new_unique = X_new[col].nunique(dropna=True)\n        if new_unique > max_unique_after:\n            logger.info(\n                f\"[cat] {col}: unique_after={new_unique} > \"\n                f\"cat_max_unique_after_group={max_unique_after}, dropping feature.\"\n            )\n            to_drop.append(col)\n\n    if to_drop:\n        logger.info(f\"Dropping categorical features: {to_drop}\")\n        X_new = X_new.drop(columns=[c for c in to_drop if c in X_new.columns])\n        if test_new is not None:\n            test_new = test_new.drop(columns=[c for c in to_drop if c in test_new.columns])\n        cat_cols = [c for c in cat_cols if c not in to_drop]\n\n    return X_new, test_new, cat_cols\n\n\ndef expand_datetime_columns(\n    df: pd.DataFrame,\n    datetime_cols: List[str],\n    features: List[str]\n) -> pd.DataFrame:\n    df = df.copy()\n    for col in datetime_cols:\n        if col not in df.columns:\n            continue\n        s = pd.to_datetime(df[col], errors=\"coerce\")\n\n        if \"year\" in features:\n            df[f\"{col}_year\"] = s.dt.year.astype(\"Int64\")\n        if \"month\" in features:\n            df[f\"{col}_month\"] = s.dt.month.astype(\"Int64\")\n        if \"day\" in features:\n            df[f\"{col}_day\"] = s.dt.day.astype(\"Int64\")\n        if \"dow\" in features:\n            df[f\"{col}_dow\"] = s.dt.dayofweek.astype(\"Int64\")\n        if \"hour\" in features:\n            df[f\"{col}_hour\"] = s.dt.hour.astype(\"Int64\")\n\n    return df\n\n\ndef haversine_distance(\n    lat1, lon1, lat2, lon2, radius: float = 6371.0\n) -> np.ndarray:\n    lat1 = np.radians(lat1.astype(float))\n    lon1 = np.radians(lon1.astype(float))\n    lat2 = np.radians(lat2.astype(float))\n    lon2 = np.radians(lon2.astype(float))\n\n    dlat = lat2 - lat1\n    dlon = lon2 - lon1\n\n    a = np.sin(dlat / 2.0) ** 2 + \\\n        np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2.0) ** 2\n    c = 2 * np.arcsin(np.sqrt(a))\n    return radius * c\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.137009Z","iopub.execute_input":"2025-11-17T21:56:42.137273Z","iopub.status.idle":"2025-11-17T21:56:42.155676Z","shell.execute_reply.started":"2025-11-17T21:56:42.137258Z","shell.execute_reply":"2025-11-17T21:56:42.155089Z"}},"outputs":[],"execution_count":38},{"id":"6493bc28","cell_type":"markdown","source":"### Data processing","metadata":{}},{"id":"504a78c8","cell_type":"code","source":"def load_data(config: Dict, logger: logging.Logger) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    train_path = config[\"train_path\"]\n    test_path = config[\"test_path\"]\n\n    logger.info(f\"üì• Loading train from {train_path}\")\n    train = pd.read_csv(train_path, sep=config[\"sep\"])\n\n    if test_path:\n        logger.info(f\"üì• Loading test from {test_path}\")\n        test = pd.read_csv(test_path, sep=config[\"sep\"])\n    else:\n        test = None\n\n    logger.info(f\"Train shape: {train.shape}\")\n    if test is not None:\n        logger.info(f\"Test  shape: {test.shape}\")\n\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.156244Z","iopub.execute_input":"2025-11-17T21:56:42.156432Z","iopub.status.idle":"2025-11-17T21:56:42.170675Z","shell.execute_reply.started":"2025-11-17T21:56:42.156411Z","shell.execute_reply":"2025-11-17T21:56:42.170078Z"}},"outputs":[],"execution_count":39},{"id":"99b6a7f4","cell_type":"code","source":"train_df, test_df = load_data(CONFIG, logger)\n\nid_col = CONFIG[\"id_column\"]\ntarget_col = CONFIG[\"target_column\"]\n\ntest_ids = None\nif test_df is not None and id_col in test_df.columns:\n    test_ids = test_df[id_col].copy()\n\ny = train_df[target_col]\nX = train_df.drop(columns=[target_col])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.172220Z","iopub.execute_input":"2025-11-17T21:56:42.172413Z","iopub.status.idle":"2025-11-17T21:56:42.201458Z","shell.execute_reply.started":"2025-11-17T21:56:42.172399Z","shell.execute_reply":"2025-11-17T21:56:42.200951Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 21:56:42,181 - INFO -  Loading train from /kaggle/input/petfinder-pawpularity-score/train.csv\n2025-11-17 21:56:42,194 - INFO -  Loading test from /kaggle/input/petfinder-pawpularity-score/test.csv\n2025-11-17 21:56:42,196 - INFO - Train shape: (9912, 14)\n2025-11-17 21:56:42,197 - INFO - Test  shape: (8, 13)\n","output_type":"stream"}],"execution_count":40},{"id":"43910515","cell_type":"code","source":"### –î–æ–±–∞–≤—å —Å—é–¥–∞ —Å–æ–∑–¥–∞–Ω–∏–µ reference –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≥–µ–æ-—Ñ–∏—á–µ–π\n# –ù–∞ –æ—Å–Ω–æ–≤–µ —É—Å—Ä–µ–¥–Ω–µ–Ω–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç –ø—Ä–∏ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–µ\ndef fit_geo_reference(df: pd.DataFrame, config: Dict, logger: Optional[logging.Logger] = None) -> pd.DataFrame:\n    \"\"\"\n    –°—Ç—Ä–æ–∏—Ç DataFrame geo_ref_df, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å—Ä–µ–¥–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã.\n    –ó–¥–µ—Å—å —Å–æ–∑–¥–∞—ë—Ç—Å—è reference-–∫–æ–ª–æ–Ω–∫–∞ –¥–ª—è –≥–µ–æ-—Ñ–∏—á–µ–π:\n      - <output_lat_column>, <output_lon_column> (float)\n      - <output_column> –∫–∞–∫ —Å—Ç—Ä–æ–∫–∞ \"lat,lon\".\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    cat_col = geo_cfg.get(\"category_column\", \"category\")\n    coord_col = geo_cfg.get(\"coordinates_column\", \"coordinates\")\n    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n\n    lat_col_cfg = geo_cfg.get(\"lat_column\")\n    lon_col_cfg = geo_cfg.get(\"lon_column\")\n\n    df_work = df.copy()\n\n    # 1) –ë–µ—Ä—ë–º lat/lon –ª–∏–±–æ –∏–∑ —è–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫, –ª–∏–±–æ –ø–∞—Ä—Å–∏–º —Å—Ç—Ä–æ–∫—É \"lat,lon\"\n    if (\n        lat_col_cfg\n        and lon_col_cfg\n        and lat_col_cfg in df_work.columns\n        and lon_col_cfg in df_work.columns\n    ):\n        # –Ø–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —à–∏—Ä–æ—Ç—ã/–¥–æ–ª–≥–æ—Ç—ã\n        df_work[\"_lat\"] = pd.to_numeric(df_work[lat_col_cfg], errors=\"coerce\")\n        df_work[\"_lon\"] = pd.to_numeric(df_work[lon_col_cfg], errors=\"coerce\")\n    else:\n        # –ü–∞—Ä—Å–∏–º coordinates –∫–∞–∫ —Å—Ç—Ä–æ–∫—É \"lat,lon\" (–º–æ–∂–Ω–æ —Å –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏)\n        coords_parsed = (\n            df_work[coord_col]\n            .astype(str)\n            .str.extract(r\"([+-]?\\d+\\.?\\d*)\\s*,\\s*([+-]?\\d+\\.?\\d*)\")\n        )\n        df_work[\"_lat\"] = pd.to_numeric(coords_parsed[0], errors=\"coerce\")\n        df_work[\"_lon\"] = pd.to_numeric(coords_parsed[1], errors=\"coerce\")\n\n    # 2) –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –ø–æ –≥—Ä—É–ø–ø–∞–º\n    geo_ref_df = (\n        df_work.groupby(cat_col)[[\"_lat\", \"_lon\"]]\n        .mean()\n        .rename(columns={\"_lat\": out_lat_col, \"_lon\": out_lon_col})\n        .reset_index()\n    )\n\n    # 3) –°—Ç—Ä–æ–∫–æ–≤–∞—è –∫–æ–ª–æ–Ω–∫–∞ \"lat,lon\" (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏/—ç–∫—Å–ø–æ—Ä—Ç–∞)\n    geo_ref_df[out_col] = (\n        geo_ref_df[out_lat_col].round(6).astype(str)\n        + \",\" +\n        geo_ref_df[out_lon_col].round(6).astype(str)\n    )\n\n    if logger:\n        logger.info(\n            f\"üåç –û–±—É—á–∏–ª–∏ reference-–≥–µ–æ-—Ü–µ–Ω—Ç—Ä—ã –ø–æ {cat_col}, –≤—Å–µ–≥–æ –≥—Ä—É–ø–ø: {geo_ref_df.shape[0]}\"\n        )\n    return geo_ref_df\n\ndef add_geo_reference_column(\n    df: pd.DataFrame,\n    geo_ref_df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> pd.DataFrame:\n    \"\"\"\n    –î–æ–±–∞–≤–ª—è–µ—Ç reference-–≥–µ–æ-–∫–æ–ª–æ–Ω–∫—É(–∏) –∫ df, –∏—Å–ø–æ–ª—å–∑—É—è geo_ref_df, –ø–æ—Å—á–∏—Ç–∞–Ω–Ω—É—é –Ω–∞ train.\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    cat_col = geo_cfg.get(\"category_column\", \"category\")\n    out_col = geo_cfg.get(\"output_column\", f\"{cat_col}_geo_ref\")\n    out_lat_col = geo_cfg.get(\"output_lat_column\", f\"{cat_col}_geo_ref_lat\")\n    out_lon_col = geo_cfg.get(\"output_lon_column\", f\"{cat_col}_geo_ref_lon\")\n\n    df = df.merge(geo_ref_df[[cat_col, out_col, out_lat_col, out_lon_col]], how=\"left\", on=cat_col)\n    n_missing = df[out_col].isna().sum()\n    if logger:\n        logger.info(\n            f\"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã reference-–≥–µ–æ-–∫–æ–ª–æ–Ω–∫–∏ ({out_col}, {out_lat_col}, {out_lon_col}) \"\n            f\"–∫ df (geo reference –ø–æ {cat_col}). –ü—Ä–æ–ø—É—Å–∫–æ–≤: {n_missing}\"\n        )\n    return df\n\n\ndef maybe_apply_geo_reference(\n    df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None,\n    fit: bool = False,\n    geo_ref_df: Optional[pd.DataFrame] = None\n):\n    \"\"\"\n    –û–±–µ—Ä—Ç–∫–∞ –¥–ª—è train/test: –µ—Å–ª–∏ fit=True, –≤—ã—á–∏—Å–ª—è–µ—Ç geo reference –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≤ train,\n    –µ—Å–ª–∏ fit=False (test), –ø—Ä–æ—Å—Ç–æ –ø—Ä–∏–º–µ–Ω—è–µ—Ç geo_ref_df –∫ df.\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      - –ø—Ä–∏ fit=True: (df_with_ref, geo_ref_df)\n      - –ø—Ä–∏ fit=False: df_with_ref\n    \"\"\"\n    geo_cfg = config.get(\"geo_reference\", {})\n    if not geo_cfg.get(\"enabled\", False):\n        if logger:\n            logger.info(\"üåé –ì–µ–æ-reference –≤—ã–∫–ª—é—á–µ–Ω (config['geo_reference']['enabled']=False)\")\n        return (df, None) if fit else df\n\n    if fit:\n        geo_ref_df = fit_geo_reference(df, config, logger)\n        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n        return df, geo_ref_df\n    else:\n        if geo_ref_df is None:\n            raise ValueError(\"geo_ref_df must be passed for applying on test set\")\n        df = add_geo_reference_column(df, geo_ref_df, config, logger)\n        return df\n\n# --- –ü—Ä–∏–º–µ–Ω—è–µ–º reference-–≥–µ–æ-—Ñ–∏—á–∏ –∫ train –∏ test, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ ---\nif CONFIG.get(\"geo_reference\", {}).get(\"enabled\", False):\n    logger.info(\"üåé –ü—Ä–∏–º–µ–Ω—è–µ–º –≥–µ–æ-reference features...\")\n    train_df, geo_ref_df = maybe_apply_geo_reference(train_df, CONFIG, logger, fit=True)\n    test_df = maybe_apply_geo_reference(test_df, CONFIG, logger, fit=False, geo_ref_df=geo_ref_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.202094Z","iopub.execute_input":"2025-11-17T21:56:42.202278Z","iopub.status.idle":"2025-11-17T21:56:42.214941Z","shell.execute_reply.started":"2025-11-17T21:56:42.202258Z","shell.execute_reply":"2025-11-17T21:56:42.214279Z"}},"outputs":[],"execution_count":41},{"id":"925eb654","cell_type":"code","source":"if CONFIG['task_type'] in ['multiclass', 'binary']:\n    y = y.astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.215699Z","iopub.execute_input":"2025-11-17T21:56:42.215959Z","iopub.status.idle":"2025-11-17T21:56:42.230345Z","shell.execute_reply.started":"2025-11-17T21:56:42.215937Z","shell.execute_reply":"2025-11-17T21:56:42.229692Z"}},"outputs":[],"execution_count":42},{"id":"8b12d371","cell_type":"markdown","source":"–£–î–ê–õ–ï–ù–ò–ï ID","metadata":{}},{"id":"3ec651d8","cell_type":"code","source":"# ID column will be dropped later before boosting models (see training cell).","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.231123Z","iopub.execute_input":"2025-11-17T21:56:42.231382Z","iopub.status.idle":"2025-11-17T21:56:42.241123Z","shell.execute_reply.started":"2025-11-17T21:56:42.231361Z","shell.execute_reply":"2025-11-17T21:56:42.240408Z"}},"outputs":[],"execution_count":43},{"id":"f17f2c77","cell_type":"markdown","source":"### Features generation","metadata":{}},{"id":"44b85bc8","cell_type":"code","source":"def generate_basic_features(\n    df: pd.DataFrame,\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> pd.DataFrame:\n    df = df.copy()\n\n    drop_cols = config[\"basic_drop_columns\"]\n    for c in drop_cols:\n        if c in df.columns:\n            if logger:\n                logger.info(f\"Dropping column: {c}\")\n            df = df.drop(columns=[c])\n\n    if config[\"basic_datetime_expand\"] and config[\"datetime_columns\"]:\n        if logger:\n            logger.info(f\"Expanding datetime columns: {config['datetime_columns']}\")\n        df = expand_datetime_columns(\n            df,\n            datetime_cols=config[\"datetime_columns\"],\n            features=config[\"basic_datetime_features\"]\n        )\n\n    return df\n\n\ndef generate_aggregate_features(\n    train: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    if not config[\"agg_enable\"]:\n        if logger:\n            logger.info(\"Aggregate features disabled in CONFIG.\")\n        return train, test\n\n    groupby_cols = config[\"agg_groupby_cols\"]\n    if not groupby_cols:\n        if logger:\n            logger.info(\"No agg_groupby_cols specified ‚Äî skipping aggregates.\")\n        return train, test\n\n    if logger:\n        logger.info(f\"Generating aggregate features by {groupby_cols}\")\n\n    all_df = train if test is None else pd.concat([train, test], axis=0, ignore_index=True)\n\n    if config[\"agg_numeric_cols\"]:\n        num_cols = [c for c in config[\"agg_numeric_cols\"] if c in all_df.columns]\n    else:\n        num_cols = [\n            c for c in all_df.columns\n            if pd.api.types.is_numeric_dtype(all_df[c])\n        ]\n        for c in [config[\"target_column\"], config[\"id_column\"]]:\n            if c in num_cols:\n                num_cols.remove(c)\n\n    aggs = config[\"agg_aggs\"]\n    prefix = config[\"agg_prefix\"]\n\n    if logger:\n        logger.info(f\"Aggregating numeric cols: {num_cols}\")\n        logger.info(f\"Using aggs: {aggs}, prefix: {prefix}\")\n\n    grouped = all_df.groupby(groupby_cols)[num_cols].agg(aggs)\n    grouped.columns = [\n        f\"{prefix}_\" + \"_\".join(map(str, col)).strip()\n        for col in grouped.columns.to_flat_index()\n    ]\n    grouped = grouped.reset_index()\n\n    if logger:\n        logger.info(f\"Aggregate frame shape: {grouped.shape}\")\n\n    train_merged = train.merge(grouped, on=groupby_cols, how=\"left\")\n    test_merged = None\n    if test is not None:\n        test_merged = test.merge(grouped, on=groupby_cols, how=\"left\")\n\n    return train_merged, test_merged\n\n\ndef generate_geo_features(\n    train: pd.DataFrame,\n    test: Optional[pd.DataFrame],\n    config: Dict,\n    logger: Optional[logging.Logger] = None\n) -> Tuple[pd.DataFrame, Optional[pd.DataFrame]]:\n    \"\"\"\n    –ë–∞–∑–æ–≤—ã–µ —Ñ–∏—á–∏:\n      - geo_dist_from_to_km\n      - geo_dist_from_ref_km, geo_dist_to_ref_km (–µ—Å–ª–∏ –∑–∞–¥–∞–Ω ref)\n      - geo_lat_from_abs / geo_lon_from_abs / geo_lat_to_abs / geo_lon_to_abs\n\n    –ï—Å–ª–∏ geo_extra_enable=True:\n      - geo_dlat / geo_dlon\n      - geo_manhattan_km\n      - geo_bearing_deg\n      - geo_mid_lat / geo_mid_lon\n    \"\"\"\n    if not config[\"geo_enable\"]:\n        if logger:\n            logger.info(\"Geo features disabled in CONFIG.\")\n        return train, test\n\n    lat_from_col = config[\"geo_lat_from_col\"]\n    lon_from_col = config[\"geo_lon_from_col\"]\n    lat_to_col = config[\"geo_lat_to_col\"]\n    lon_to_col = config[\"geo_lon_to_col\"]\n    prefix = config[\"geo_prefix\"]\n\n    from_coord_col = config.get(\"geo_from_coord_col\")\n    to_coord_col = config.get(\"geo_to_coord_col\")\n    coord_sep = config.get(\"geo_coord_string_sep\", \",\")\n\n    if from_coord_col is not None:\n        train = _ensure_lat_lon_from_single_column(\n            train, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n        )\n        if test is not None:\n            test = _ensure_lat_lon_from_single_column(\n                test, from_coord_col, lat_from_col, lon_from_col, sep=coord_sep\n            )\n\n    if to_coord_col is not None:\n        train = _ensure_lat_lon_from_single_column(\n            train, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n        )\n        if test is not None:\n            test = _ensure_lat_lon_from_single_column(\n                test, to_coord_col, lat_to_col, lon_to_col, sep=coord_sep\n            )\n\n\n    for col in [lat_from_col, lon_from_col, lat_to_col, lon_to_col]:\n        if col and col not in train.columns:\n            raise ValueError(f\"Column {col} not found in train for geo features\")\n\n    if logger:\n        logger.info(f\"Generating geo features with backend: {GEO_BACKEND}\")\n\n    def _distance_rowwise(\n        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n    ) -> np.ndarray:\n        if GEO_BACKEND == \"geopy\":\n            def _one(a, b, c, d):\n                if pd.isna(a) or pd.isna(b) or pd.isna(c) or pd.isna(d):\n                    return np.nan\n                return geodesic((a, b), (c, d)).km\n            return np.vectorize(_one)(lat1, lon1, lat2, lon2)\n        else:\n            return haversine_distance(lat1, lon1, lat2, lon2)\n\n    def _bearing(\n        lat1: pd.Series, lon1: pd.Series, lat2: pd.Series, lon2: pd.Series\n    ) -> np.ndarray:\n        # initial bearing (degrees)\n        lat1_r = np.radians(lat1.astype(float))\n        lat2_r = np.radians(lat2.astype(float))\n        dlon_r = np.radians(lon2.astype(float) - lon1.astype(float))\n\n        x = np.sin(dlon_r) * np.cos(lat2_r)\n        y = np.cos(lat1_r) * np.sin(lat2_r) - np.sin(lat1_r) * np.cos(lat2_r) * np.cos(dlon_r)\n        brng = np.degrees(np.arctan2(x, y))\n        brng = (brng + 360) % 360\n        return brng\n\n    def _apply_geo(df: pd.DataFrame) -> pd.DataFrame:\n        df = df.copy()\n\n        df[f\"{prefix}_dist_from_to_km\"] = _distance_rowwise(\n            df[lat_from_col], df[lon_from_col],\n            df[lat_to_col], df[lon_to_col]\n        )\n\n        ref_lat = config[\"geo_ref_lat\"]\n        ref_lon = config[\"geo_ref_lon\"]\n        if ref_lat is not None and ref_lon is not None:\n            df[f\"{prefix}_dist_from_ref_km\"] = _distance_rowwise(\n                df[lat_from_col], df[lon_from_col],\n                pd.Series(ref_lat, index=df.index),\n                pd.Series(ref_lon, index=df.index),\n            )\n            df[f\"{prefix}_dist_to_ref_km\"] = _distance_rowwise(\n                df[lat_to_col], df[lon_to_col],\n                pd.Series(ref_lat, index=df.index),\n                pd.Series(ref_lon, index=df.index),\n            )\n\n        df[f\"{prefix}_lat_from_abs\"] = df[lat_from_col].abs()\n        df[f\"{prefix}_lon_from_abs\"] = df[lon_from_col].abs()\n        df[f\"{prefix}_lat_to_abs\"] = df[lat_to_col].abs()\n        df[f\"{prefix}_lon_to_abs\"] = df[lon_to_col].abs()\n\n        if config.get(\"geo_extra_enable\", False):\n            df[f\"{prefix}_dlat\"] = df[lat_to_col] - df[lat_from_col]\n            df[f\"{prefix}_dlon\"] = df[lon_to_col] - df[lon_from_col]\n\n            df[f\"{prefix}_manhattan_km\"] = (\n                haversine_distance(df[lat_from_col], df[lon_from_col], df[lat_to_col], df[lon_from_col]) +\n                haversine_distance(df[lat_to_col], df[lon_from_col], df[lat_to_col], df[lon_to_col])\n            )\n\n            df[f\"{prefix}_bearing_deg\"] = _bearing(\n                df[lat_from_col], df[lon_from_col],\n                df[lat_to_col], df[lon_to_col]\n            )\n\n            df[f\"{prefix}_mid_lat\"] = (df[lat_from_col] + df[lat_to_col]) / 2.0\n            df[f\"{prefix}_mid_lon\"] = (df[lon_from_col] + df[lon_to_col]) / 2.0\n\n        return df\n\n    train_geo = _apply_geo(train)\n    test_geo = _apply_geo(test) if test is not None else None\n    return train_geo, test_geo\n\n\ndef process_address_extract_city(\n    df: pd.DataFrame,\n    column: str,\n    city_index: int,\n    sep: str,\n    logger: logging.Logger\n) -> pd.DataFrame:\n    \"\"\"\n    –ó–¥–µ—Å—å –∏–∑–≤–ª–µ–∫–∞–µ—Ç—Å—è –∫–æ–ª–æ–Ω–∫–∞ '–≥–æ—Ä–æ–¥':\n      - address -> —Å–ø–∏—Å–æ–∫ —á–∞—Å—Ç–µ–π (split)\n      - –∏–∑ —Å–ø–∏—Å–∫–∞ –±–µ—Ä—ë–º city_index (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é -1 = –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç)\n      - —Å–æ–∑–¥–∞—ë–º –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É address_city\n      - –æ–Ω–∞ –≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç\n        —Ç–∞–∫—É—é –∂–µ –æ–±—Ä–∞–±–æ—Ç–∫—É, –∫–∞–∫ –∏ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Ñ–∏—á–∏\n    \"\"\"\n    if column not in df.columns:\n        logger.warning(f\"Address column '{column}' not found, skipping city extraction.\")\n        return df\n\n    df = df.copy()\n\n    def _extract(addr):\n        if not isinstance(addr, str):\n            return None\n        parts = [p.strip() for p in addr.split(sep)]\n        if len(parts) == 0:\n            return None\n        try:\n            return parts[city_index]\n        except Exception:\n            # –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞\n            return parts[-1]\n\n    df[\"address_city\"] = df[column].map(_extract)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.241911Z","iopub.execute_input":"2025-11-17T21:56:42.242158Z","iopub.status.idle":"2025-11-17T21:56:42.265798Z","shell.execute_reply.started":"2025-11-17T21:56:42.242138Z","shell.execute_reply":"2025-11-17T21:56:42.265010Z"}},"outputs":[],"execution_count":44},{"id":"0dd84223","cell_type":"code","source":"import os\n\ndef _normalize_image_name(name: str, default_ext=\".jpg\") -> str:\n    \"\"\"\n    –ü—Ä–∏–≤–æ–¥–∏–º –∏–º—è —Ñ–∞–π–ª–∞ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–º—É –≤–∏–¥—É:\n    - —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã/–ø–µ—Ä–µ–≤–æ–¥—ã —Å—Ç—Ä–æ–∫\n    - –µ—Å–ª–∏ –Ω–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è, –¥–æ–±–∞–≤–ª—è–µ–º default_ext\n    \"\"\"\n    name = str(name).strip()\n    base = os.path.basename(name)\n    root, ext = os.path.splitext(base)\n    if ext == \"\":\n        ext = default_ext\n    return root + ext\n\n\ndef images_features(X, test_df):\n    \"\"\"\n    –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ä—Ç–∏–Ω–æ–∫: –¥–æ–±–∞–≤–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ConvNeXt-Tiny –ø–æ file_names_column.\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–µ X –∏ test_df.\n    \"\"\"\n    file_name_column = CONFIG['file_names_column']\n\n    if file_name_column not in X.columns:\n        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} –Ω–µ—Ç –≤ train\")\n\n    if test_df is not None and file_name_column not in test_df.columns:\n        raise KeyError(f\"CONFIG['file_names_column']={file_name_column!r} –Ω–µ—Ç –≤ test\")\n\n    # ---- ConvNeXt-Tiny backbone ----\n    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n        device = \"mps\"\n    else:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    weights = models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1\n    backbone_tiny = models.convnext_tiny(weights=weights)\n    backbone_tiny.classifier = nn.Identity()\n    backbone_tiny.eval().to(device)\n\n    image_transform = weights.transforms()\n\n    # ---- Train images ----\n    train_file_paths = [\n        os.path.join(\n            CONFIG['train_images_dir'],\n            _normalize_image_name(file_name)\n        )\n        for file_name in X[file_name_column]\n    ]\n\n    # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –º–æ–∂–Ω–æ —Å—Ä–∞–∑—É –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –∏ —É–ø–∞—Å—Ç—å —Å –ø–æ–Ω—è—Ç–Ω—ã–º —Å–æ–æ–±—â–µ–Ω–∏–µ–º)\n    # missing_train = [p for p in train_file_paths if not os.path.exists(p)]\n    # if missing_train:\n    #     raise FileNotFoundError(\n    #         f\"–ù–µ –Ω–∞–π–¥–µ–Ω—ã {len(missing_train)} train-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–µ—Ä–≤—ã–µ 5: {missing_train[:5]}\"\n    #     )\n\n    train_images_part = process_images(train_file_paths, backbone_tiny, image_transform, name=\"train\")\n    train_images_part.index = X.index\n    X = pd.concat([X, train_images_part], axis=1)\n\n    # ---- Test images ----\n    if test_df is not None:\n        test_file_paths = [\n            os.path.join(\n                CONFIG['test_images_dir'],\n                _normalize_image_name(file_name)\n            )\n            for file_name in test_df[file_name_column]\n        ]\n\n        # missing_test = [p for p in test_file_paths if not os.path.exists(p)]\n        # if missing_test:\n        #     raise FileNotFoundError(\n        #         f\"–ù–µ –Ω–∞–π–¥–µ–Ω—ã {len(missing_test)} test-–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –ø–µ—Ä–≤—ã–µ 5: {missing_test[:5]}\"\n        #     )\n\n        test_images_part = process_images(test_file_paths, backbone_tiny, image_transform, name=\"test\")\n        test_images_part.index = test_df.index\n        test_df = pd.concat([test_df, test_images_part], axis=1)\n\n    # –≤—ã–∫–∏–¥—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞, —á—Ç–æ–±—ã –æ–Ω–∞ –Ω–µ —É—à–ª–∞ –≤ –±—É—Å—Ç–∏–Ω–≥–∏\n    for df in (X, test_df):\n        if df is not None and file_name_column in df.columns:\n            df.drop(columns=[file_name_column], inplace=True)\n\n    return X, test_df\n\n\n\ndef process_images(file_paths, model, image_transform, name):\n    \"\"\"–°—á–∏—Ç–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–∏—Å–∫–∞ –ø—É—Ç–µ–π –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º –æ–¥–Ω–æ–π –º–æ–¥–µ–ª—å—é.\"\"\"\n    img_dataset = ImagesDataset(file_paths, image_transform)\n    img_dataloader = DataLoader(img_dataset, batch_size=CONFIG['batch_size'], shuffle=False)\n\n    # –ü–æ–ª—É—á–∞–µ–º device –º–æ–¥–µ–ª–∏\n    device = next(model.parameters()).device\n\n    collect_embs = []\n    with torch.no_grad():\n        for images in tqdm(img_dataloader, total=len(img_dataloader), desc=f\"Images {name} process...\"):\n            images = images.to(device)  # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –±–∞—Ç—á –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏\n            embs = model(images)\n            # –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—ë—Ç 4D-—Ç–µ–Ω–∑–æ—Ä (N, C, 1, 1)\n            if embs.ndim == 4:\n                embs = torch.flatten(embs, 1)\n            embs = embs.cpu().numpy()\n            # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ —Å—Ç—Ä–æ–∫–∞–º\n            embs /= np.linalg.norm(embs, axis=1, keepdims=True) + 1e-12\n            collect_embs.extend(embs.tolist())\n\n    n_feat = len(collect_embs[0]) if collect_embs else 0\n    return pd.DataFrame(collect_embs, columns=[f\"emb_{i}\" for i in range(n_feat)])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:16.877336Z","iopub.execute_input":"2025-11-17T22:01:16.877660Z","iopub.status.idle":"2025-11-17T22:01:16.890151Z","shell.execute_reply.started":"2025-11-17T22:01:16.877638Z","shell.execute_reply":"2025-11-17T22:01:16.889372Z"}},"outputs":[],"execution_count":50},{"id":"f360d267","cell_type":"markdown","source":"–¢–ï–ö–°–¢–û–í–´–ï –§–ò–ß–ò","metadata":{}},{"id":"e964e493","cell_type":"code","source":"def get_text_device():\n    global TEXT_DEVICE\n    if TEXT_DEVICE is None:\n        TEXT_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    return TEXT_DEVICE\n\n\ndef init_bert_model():\n    \"\"\"\n    –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º BERT-–º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –æ–¥–∏–Ω —Ä–∞–∑.\n    \"\"\"\n    global TEXT_MODEL, TEXT_TOKENIZER, TEXT_DEVICE\n\n    if TEXT_MODEL is not None:\n        return\n\n    model_name = CONFIG.get(\n        \"text_bert_model_name\",\n        \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n    )\n    TEXT_DEVICE = get_text_device()\n\n    TEXT_TOKENIZER = AutoTokenizer.from_pretrained(model_name)\n    TEXT_MODEL = AutoModel.from_pretrained(model_name)\n    TEXT_MODEL.to(TEXT_DEVICE)\n    TEXT_MODEL.eval()\n    \ndef _detect_format_from_path(path: str) -> str:\n    ext = os.path.splitext(path)[1].lower()\n    if ext == \".parquet\":\n        return \"parquet\"\n    if ext == \".csv\":\n        return \"csv\"\n    if ext == \".tsv\":\n        return \"tsv\"\n    # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º csv\n    return \"csv\"\n\n\ndef load_text_table(path: str, fmt: str | None, id_column: str, text_column: str) -> pd.DataFrame:\n    \"\"\"\n    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–Ω–µ—à–Ω—é—é —Ç–∞–±–ª–∏—Ü—É —Å —Ç–µ–∫—Å—Ç–æ–º.\n    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º parquet/csv/tsv. format=\"auto\" -> –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é.\n    –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–æ–ª–±—Ü—ã [id_column, text_column].\n    \"\"\"\n    if fmt is None or fmt == \"auto\":\n        fmt = _detect_format_from_path(path)\n\n    if fmt == \"parquet\":\n        df = pd.read_parquet(path)\n    elif fmt == \"tsv\":\n        df = pd.read_csv(path, sep=\"\\t\")\n    else:  # \"csv\" –∏ –≤—Å—ë –æ—Å—Ç–∞–ª—å–Ω–æ–µ –ø–æ-—É–º–æ–ª—á–∞–Ω–∏—é\n        df = pd.read_csv(path)\n\n    # –Ø–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å\n    missing = [c for c in (id_column, text_column) if c not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"Columns {missing} not found in external text table {path}. \"\n            f\"Available columns: {list(df.columns)}\"\n        )\n\n    return df[[id_column, text_column]].copy()\n\ndef attach_external_text_tables(X: pd.DataFrame, test_df: pd.DataFrame):\n    \"\"\"–ü–æ–¥—Ü–µ–ø–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (CONFIG['text_external_tables']) –∫ X –∏ test_df.\n\n    –í–∞–∂–Ω–æ: –µ—Å–ª–∏ –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –Ω–∞ –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ id,\n    –º—ã —Å–Ω–∞—á–∞–ª–∞ –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –∏—Ö –≤ –û–î–ù–£ —Å—Ç—Ä–æ–∫—É (–∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç—ã —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª),\n    —á—Ç–æ–±—ã merge –ù–ï —Ä–∞–∑–¥—É–≤–∞–ª –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ.\n    \"\"\"\n    external_cfgs = CONFIG.get(\"text_external_tables\", []) or []\n    if not external_cfgs:\n        return X, test_df, []\n\n    main_id_col = CONFIG.get(\"id_column\", \"index\")\n    added_cols: list[str] = []\n\n    for cfg in external_cfgs:\n        name = cfg.get(\"name\", \"ext\")\n\n        train_path = cfg[\"train_path\"]\n        # –µ—Å–ª–∏ test_path –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ñ–∞–π–ª\n        test_path = cfg.get(\"test_path\", train_path)\n\n        fmt = cfg.get(\"format\", \"auto\")\n\n        # ID-–∫–æ–ª–æ–Ω–∫–∞ –≤ external-—Ç–∞–±–ª–∏—Ü–µ: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–∞–∫–∞—è –∂–µ, –∫–∞–∫ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π\n        ext_id_col = cfg.get(\"id_column\", main_id_col)\n\n        text_col_ext = cfg[\"text_column\"]\n        output_col = cfg.get(\"output_column\", f\"text_ext_{name}\")\n\n        # --- TRAIN ---\n        ext_train = load_text_table(train_path, fmt, ext_id_col, text_col_ext)\n        ext_train = ext_train.rename(columns={text_col_ext: output_col})\n\n        # NEW: –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É id –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n        if ext_train[ext_id_col].duplicated().any():\n            ext_train = (\n                ext_train\n                .groupby(ext_id_col, as_index=False)[output_col]\n                .agg(lambda s: \" \".join(map(str, s)))\n            )\n\n        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ main_id_col –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö X\n        # –ï—Å–ª–∏ –Ω–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è merge\n        if main_id_col in X.columns:\n            # ID –∫–æ–ª–æ–Ω–∫–∞ –µ—Å—Ç—å –≤ X\n            if main_id_col == ext_id_col:\n                # –ò–º—è ID –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∏ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n                X = X.merge(\n                    ext_train[[ext_id_col, output_col]],\n                    on=ext_id_col,\n                    how=\"left\"\n                )\n            else:\n                # –ò–º—è ID –≤ –æ—Å–Ω–æ–≤–Ω–æ–π –∏ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–∞—Ö —Ä–∞–∑–Ω–æ–µ\n                X = X.merge(\n                    ext_train[[ext_id_col, output_col]],\n                    left_on=main_id_col,\n                    right_on=ext_id_col,\n                    how=\"left\"\n                ).drop(columns=[ext_id_col])\n        else:\n            # ID –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –≤ X, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å\n            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ext_id_col –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n            ext_train_indexed = ext_train.set_index(ext_id_col)[[output_col]]\n            X = X.merge(\n                ext_train_indexed,\n                left_index=True,\n                right_index=True,\n                how=\"left\"\n            )\n\n        # --- TEST ---\n        if test_df is not None:\n            ext_test = load_text_table(test_path, fmt, ext_id_col, text_col_ext)\n            ext_test = ext_test.rename(columns={text_col_ext: output_col})\n\n            # NEW: –∞–≥—Ä–µ–≥–∏—Ä—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤ –ø–æ –æ–¥–Ω–æ–º—É id –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É\n            if ext_test[ext_id_col].duplicated().any():\n                ext_test = (\n                    ext_test\n                    .groupby(ext_id_col, as_index=False)[output_col]\n                    .agg(lambda s: \" \".join(map(str, s)))\n                )\n\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ main_id_col –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö test_df\n            if main_id_col in test_df.columns:\n                if main_id_col == ext_id_col:\n                    test_df = test_df.merge(\n                        ext_test[[ext_id_col, output_col]],\n                        on=ext_id_col,\n                        how=\"left\"\n                    )\n                else:\n                    test_df = test_df.merge(\n                        ext_test[[ext_id_col, output_col]],\n                        left_on=main_id_col,\n                        right_on=ext_id_col,\n                        how=\"left\"\n                    ).drop(columns=[ext_id_col])\n            else:\n                # ID –∫–æ–ª–æ–Ω–∫–∏ –Ω–µ—Ç –≤ test_df, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω–¥–µ–∫—Å\n                # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º ext_id_col –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –≤–æ –≤–Ω–µ—à–Ω–µ–π —Ç–∞–±–ª–∏—Ü–µ\n                ext_test_indexed = ext_test.set_index(ext_id_col)[[output_col]]\n                test_df = test_df.merge(\n                    ext_test_indexed,\n                    left_index=True,\n                    right_index=True,\n                    how=\"left\"\n                )\n\n        added_cols.append(output_col)\n\n    return X, test_df, added_cols\n\n\ndef build_bert_embeddings_for_field(field_name: str,\n                                    train_texts: list[str],\n                                    test_texts: list[str]) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    –°—Ç—Ä–æ–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ BERT –¥–ª—è –æ–¥–Ω–æ–≥–æ –ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è.\n    BERT-–º–æ–¥–µ–ª—å –æ–±—â–∞—è –¥–ª—è –≤—Å–µ—Ö –ø–æ–ª–µ–π, –Ω–æ —Å—á–∏—Ç–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ.\n    \"\"\"\n    init_bert_model()\n\n    device = get_text_device()\n    max_len = CONFIG.get(\"text_max_length\", 256)\n    batch_size = CONFIG.get(\"text_batch_size\", 32)\n\n    def encode_list(texts: list[str], desc: str) -> np.ndarray:\n        all_embs = []\n        for i in tqdm(range(0, len(texts), batch_size), desc=f\"{desc} [{field_name}]\"):\n            batch = texts[i:i + batch_size]\n\n            enc = TEXT_TOKENIZER(\n                batch,\n                padding=True,\n                truncation=True,\n                max_length=max_len,\n                return_tensors=\"pt\"\n            )\n            enc = {k: v.to(device) for k, v in enc.items()}\n\n            with torch.no_grad():\n                outputs = TEXT_MODEL(**enc)\n                last_hidden = outputs.last_hidden_state  # [bs, seq_len, hidden]\n\n                mask = enc[\"attention_mask\"].unsqueeze(-1).expand(last_hidden.size())\n                masked = last_hidden * mask\n\n                summed = masked.sum(dim=1)\n                counts = mask.sum(dim=1).clamp(min=1)\n                mean_pooled = summed / counts  # [bs, hidden]\n\n                embs = mean_pooled.cpu().numpy()\n\n                # L2 –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n                norms = np.linalg.norm(embs, axis=1, keepdims=True)\n                norms = np.clip(norms, 1e-12, None)\n                embs = embs / norms\n\n                all_embs.append(embs)\n\n        return np.vstack(all_embs) if len(all_embs) > 0 else np.zeros((0, TEXT_MODEL.config.hidden_size))\n\n    train_embs = encode_list(train_texts, \"BERT text embeddings (train)\")\n    test_embs = encode_list(test_texts, \"BERT text embeddings (test)\")\n\n    return train_embs, test_embs\ndef build_tfidf_svd_embeddings_for_field(field_name: str,\n                                         train_texts: list[str],\n                                         test_texts: list[str],\n                                         logger: logging.Logger) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    TF-IDF (uni+bi-grams) -> TruncatedSVD –¥–æ –∫–æ–º–ø–∞–∫—Ç–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ -> L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è.\n    –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—è —Å–≤–æ–π TF-IDF –∏ SVD, —á—Ç–æ–±—ã –Ω–µ –º–µ—à–∞—Ç—å —Ä–∞–∑–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.\n    \"\"\"\n    max_features = CONFIG.get(\"text_tfidf_max_features\", 50000)\n    n_components = CONFIG.get(\"text_svd_n_components\", 256)\n    random_state = CONFIG.get(\"seed\", 42)\n\n    vect = TFIDF_VECTORIZERS.get(field_name)\n    svd = TFIDF_SVDS.get(field_name)\n\n    if vect is None or svd is None:\n        vect = TfidfVectorizer(\n            max_features=max_features,\n            ngram_range=(1, 2)\n        )\n        logger.info(f\"TF-IDF fitting for field '{field_name}'...\")\n        train_tfidf = vect.fit_transform(train_texts)\n\n        svd = TruncatedSVD(\n            n_components=n_components,\n            random_state=random_state\n        )\n        logger.info(f\"SVD fitting for field '{field_name}'...\")\n        train_embs = svd.fit_transform(train_tfidf)\n\n        TFIDF_VECTORIZERS[field_name] = vect\n        TFIDF_SVDS[field_name] = svd\n    else:\n        train_tfidf = vect.transform(train_texts)\n        train_embs = svd.transform(train_tfidf)\n\n    test_tfidf = vect.transform(test_texts)\n    test_embs = svd.transform(test_tfidf)\n\n    # L2-–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è\n    def l2_norm(x: np.ndarray) -> np.ndarray:\n        norms = np.linalg.norm(x, axis=1, keepdims=True)\n        norms = np.where(norms == 0, 1, norms)\n        return x / norms\n\n    train_embs = l2_norm(train_embs)\n    test_embs = l2_norm(test_embs)\n\n    return train_embs, test_embs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:18.921090Z","iopub.execute_input":"2025-11-17T22:01:18.921355Z","iopub.status.idle":"2025-11-17T22:01:18.945408Z","shell.execute_reply.started":"2025-11-17T22:01:18.921336Z","shell.execute_reply":"2025-11-17T22:01:18.944710Z"}},"outputs":[],"execution_count":51},{"id":"8e1e2baa","cell_type":"code","source":"def text_features(X: pd.DataFrame, test_df: pd.DataFrame, logger: logging.Logger):\n    \"\"\"\n    1) –ü—Ä–∏–∫–ª–µ–∏–≤–∞–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (parquet/csv/tsv) –ø–æ id.\n    2) –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å—Ç—Ä–æ–∏–º –æ—Ç–¥–µ–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ (BERT –∏–ª–∏ TF-IDF+SVD).\n    3) –ö–æ–Ω–∫–∞—Ç–∏–º –≤—Å—ë –∫ X –∏ test_df.\n    \"\"\"\n    if not CONFIG.get(\"text_enable\", False):\n        return X, test_df\n\n    logger.info(\"Text feature extraction starts...\")\n\n    # 1. –ü–æ–¥—Ü–µ–ø–ª—è–µ–º –≤–Ω–µ—à–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã\n    X, test_df, external_cols = attach_external_text_tables(X, test_df)\n\n    # 2. –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n    base_text_cols = CONFIG.get(\"text_columns\", []) or []\n    # —Ñ–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –≤ —Ç–∞–±–ª–∏—Ü–µ\n    base_text_cols = [c for c in base_text_cols if c in X.columns]\n\n    all_text_cols = base_text_cols + external_cols\n\n    if not all_text_cols:\n        logger.warning(\"No text columns found for text_features. Skipping.\")\n        return X, test_df\n\n    model_type = CONFIG.get(\"text_model_type\", \"bert\")\n\n    # 3. –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ —Å—Ç—Ä–æ–∏–º —Å–≤–æ–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏ –∫–æ–Ω–∫–∞—Ç–∏–º\n    for col in tqdm(all_text_cols, desc=\"Building text embeddings\"):\n        logger.info(f\"Building text embeddings for column: {col}\")\n\n        # —Å–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç—ã (—Å—Ç—Ä–æ–∫–∞ -> str) - –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —á–µ—Ä–µ–∑ –∏–Ω–¥–µ–∫—Å—ã\n        train_texts = X[col].fillna(\"\").astype(str).tolist()\n        test_texts = test_df[col].fillna(\"\").astype(str).tolist()\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n        if len(train_texts) != len(X):\n            raise ValueError(f\"Train texts length {len(train_texts)} != X length {len(X)} for column {col}\")\n        if len(test_texts) != len(test_df):\n            raise ValueError(f\"Test texts length {len(test_texts)} != test_df length {len(test_df)} for column {col}\")\n\n        # –ø—Ä–æ–≤–µ—Ä–∫–∞, –∞ –Ω–µ –≤—Å–µ –ª–∏ –ø—É—Å—Ç—ã–µ\n        if all(t == \"\" for t in train_texts) and all(t == \"\" for t in test_texts):\n            logger.warning(f\"Column '{col}' has only empty texts. Skipping.\")\n            continue\n\n        field_name = col  # –º–æ–∂–Ω–æ –ø–æ—Ç–æ–º –º–∞–ø–ø–∏—Ç—å/–ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –µ—Å–ª–∏ —Ö–æ—á–µ—à—å\n\n        if model_type == \"bert\":\n            train_embs, test_embs = build_bert_embeddings_for_field(field_name, train_texts, test_texts)\n        elif model_type == \"tfidf_svd\":\n            train_embs, test_embs = build_tfidf_svd_embeddings_for_field(field_name, train_texts, test_texts, logger)\n        else:\n            raise ValueError(f\"Unknown text_model_type: {model_type}\")\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n        if train_embs.shape[0] != len(X):\n            raise ValueError(f\"Train embeddings shape {train_embs.shape[0]} != X length {len(X)} for column {col}\")\n        if test_embs.shape[0] != len(test_df):\n            raise ValueError(f\"Test embeddings shape {test_embs.shape[0]} != test_df length {len(test_df)} for column {col}\")\n\n        dim = train_embs.shape[1]\n        # –ø—Ä–µ—Ñ–∏–∫—Å —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º –∫–æ–ª–æ–Ω–∫–∏, —á—Ç–æ–±—ã –ø–æ–Ω–∏–º–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫\n        prefix = f\"text_{field_name}_emb\"\n        cols = [f\"{prefix}_{i}\" for i in range(dim)]\n\n        # –°–æ–∑–¥–∞–µ–º DataFrame —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏\n        train_text_df = pd.DataFrame(train_embs, columns=cols, index=X.index)\n        test_text_df = pd.DataFrame(test_embs, columns=cols, index=test_df.index)\n\n        X = pd.concat([X, train_text_df], axis=1)\n        test_df = pd.concat([test_df, test_text_df], axis=1)\n\n        logger.info(f\"Text features for '{col}' added: {dim} dims ({prefix}_*)\")\n\n    return X, test_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:19.019558Z","iopub.execute_input":"2025-11-17T22:01:19.019949Z","iopub.status.idle":"2025-11-17T22:01:19.028933Z","shell.execute_reply.started":"2025-11-17T22:01:19.019932Z","shell.execute_reply":"2025-11-17T22:01:19.028205Z"}},"outputs":[],"execution_count":52},{"id":"517c555f","cell_type":"code","source":"def prepare_all_features(X, test_df, config, logger):\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ —Ä–∞–∑–º–µ—Ä—ã\n    original_X_size = len(X)\n    original_test_size = len(test_df) if test_df is not None else 0\n    logger.info(f\"üîç ORIGINAL SIZES: X={original_X_size}, test_df={original_test_size}\")\n    \n    # ----- Features -----  \n    logger.info(\"üîß Generating basic features...\")\n    logger.info(f\"X shape before basic: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before basic: {test_df.shape}\")\n    \n    X = generate_basic_features(X, config, logger)\n    if test_df is not None:\n        test_df = generate_basic_features(test_df, config, logger)\n    \n    logger.info(f\"X shape after basic: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after basic: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_basic_features: {original_test_size} -> {len(test_df)}\")\n\n    logger.info(\"üìä Generating aggregate features...\")\n    logger.info(f\"X shape before aggregate: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before aggregate: {test_df.shape}\")\n    \n    X, test_df = generate_aggregate_features(X, test_df, config, logger)\n    \n    logger.info(f\"X shape after aggregate: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after aggregate: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_aggregate_features: {original_test_size} -> {len(test_df)}\")\n\n    logger.info(\"üó∫  Generating geo features...\")\n    logger.info(f\"X shape before geo: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before geo: {test_df.shape}\")\n    \n    X, test_df = generate_geo_features(X, test_df, config, logger)\n    \n    logger.info(f\"X shape after geo: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after geo: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in generate_geo_features: {original_test_size} -> {len(test_df)}\")\n    \n    if config['train_images_dir']:\n        logger.info(\"Images process starts...\")\n        X, test_df = images_features(X, test_df)\n    \n    if CONFIG.get(\"text_enable\", False):\n        logger.info(\"Text process starts...\")\n        logger.info(f\"X shape before text: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape before text: {test_df.shape}\")\n        \n        X, test_df = text_features(X, test_df, logger)\n        \n        logger.info(f\"X shape after text: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape after text: {test_df.shape}\")\n            if len(test_df) != original_test_size:\n                raise ValueError(f\"‚ùå test_df size changed in text_features: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Address ‚Üí city extraction -----\n    addr_col = config.get(\"address_column\")\n    if addr_col:\n        logger.info(f\"üèô  Extracting city from address column '{addr_col}' ...\")\n        logger.info(f\"X shape before address: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape before address: {test_df.shape}\")\n        \n        city_index = config.get(\"address_city_index\", -1)\n        sep = config.get(\"address_split_sep\", \",\")\n        X = process_address_extract_city(X, addr_col, city_index, sep, logger)\n        if test_df is not None:\n            test_df = process_address_extract_city(test_df, addr_col, city_index, sep, logger)\n        \n        logger.info(f\"X shape after address: {X.shape}\")\n        if test_df is not None:\n            logger.info(f\"test_df shape after address: {test_df.shape}\")\n            if len(test_df) != original_test_size:\n                raise ValueError(f\"‚ùå test_df size changed in process_address_extract_city: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Categorical detection -----\n    logger.info(\"üîé Detecting categorical columns...\")\n    logger.info(f\"X shape before categorical detection: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before categorical detection: {test_df.shape}\")\n    \n    concat_df = pd.concat([X] if test_df is None else [X, test_df], ignore_index=False)\n    cat_cols = detect_categorical_columns(\n        concat_df,\n        max_unique=config[\"basic_max_cat_unique\"],\n        force_categorical=config[\"basic_as_categorical\"]\n    )\n    cat_cols = [c for c in cat_cols if c in X.columns]\n    logger.info(f\"Categorical columns: {cat_cols}\")\n\n    # ----- Categorical post-processing: rare ‚Üí 'other' -----\n    logger.info(\"üß© Processing categorical features (merge rare into 'other')...\")\n    logger.info(f\"X shape before categorical processing: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape before categorical processing: {test_df.shape}\")\n    \n    X, test_df, cat_cols = process_categorical_features(X, test_df, cat_cols, config, logger)\n    logger.info(f\"Categorical columns after processing: {cat_cols}\")\n    \n    logger.info(f\"X shape after categorical processing: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"test_df shape after categorical processing: {test_df.shape}\")\n        if len(test_df) != original_test_size:\n            raise ValueError(f\"‚ùå test_df size changed in process_categorical_features: {original_test_size} -> {len(test_df)}\")\n\n    # ----- Post-feature service columns drop -----\n    post_drop = config.get(\"post_feature_drop_columns\", [])\n    if post_drop:\n        logger.info(f\"üßπ Dropping post-feature service columns: {post_drop}\")\n        X = X.drop(columns=[c for c in post_drop if c in X.columns])\n        if test_df is not None:\n            test_df = test_df.drop(columns=[c for c in post_drop if c in test_df.columns])\n        # –æ–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\n        cat_cols = [c for c in cat_cols if c not in post_drop]\n\n    # –û–±—Ä–∞–±–æ—Ç–∫–∞ NaN: —Å–Ω–∞—á–∞–ª–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ, –ø–æ—Ç–æ–º —á–∏—Å–ª–æ–≤—ã–µ\n    # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n    for col_name in cat_cols:\n        if col_name in X.columns:\n            X[col_name] = X[col_name].fillna(\"__MISSING__\").astype(str)\n        if test_df is not None and col_name in test_df.columns:\n            test_df[col_name] = test_df[col_name].fillna(\"__MISSING__\").astype(str)\n    \n    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∑–∞–ø–æ–ª–Ω—è–µ–º NaN\n    numeric_cols = [c for c in X.columns if c not in cat_cols and pd.api.types.is_numeric_dtype(X[c])]\n    if numeric_cols:\n        X[numeric_cols] = X[numeric_cols].fillna(-99999999)\n        if test_df is not None:\n            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ –µ—Å—Ç—å –≤ test_df\n            numeric_cols_test = [c for c in numeric_cols if c in test_df.columns]\n            if numeric_cols_test:\n                test_df[numeric_cols_test] = test_df[numeric_cols_test].fillna(-99999999)\n    \n    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤\n    logger.info(f\"Final X shape: {X.shape}\")\n    if test_df is not None:\n        logger.info(f\"Final test_df shape: {test_df.shape}\")\n        \n        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä test_df –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã–ª –∏–∑–º–µ–Ω–∏—Ç—å—Å—è\n        if len(test_df) != original_test_size:\n            raise ValueError(\n                f\"‚ùå CRITICAL: test_df size changed from {original_test_size} to {len(test_df)} during prepare_all_features! \"\n                f\"This should not happen. Please check the code above.\"\n            )\n        \n        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç (–∫—Ä–æ–º–µ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∞–∑–ª–∏—á–∏–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö)\n        X_cols = set(X.columns)\n        test_cols = set(test_df.columns)\n        missing_in_test = X_cols - test_cols\n        extra_in_test = test_cols - X_cols\n        if missing_in_test:\n            logger.warning(f\"Columns in X but not in test_df: {list(missing_in_test)[:10]}\")\n        if extra_in_test:\n            logger.warning(f\"Columns in test_df but not in X: {list(extra_in_test)[:10]}\")\n    \n    return X, test_df, cat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:19.198272Z","iopub.execute_input":"2025-11-17T22:01:19.198474Z","iopub.status.idle":"2025-11-17T22:01:19.215796Z","shell.execute_reply.started":"2025-11-17T22:01:19.198458Z","shell.execute_reply":"2025-11-17T22:01:19.215046Z"}},"outputs":[],"execution_count":53},{"id":"defcb0fb","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5ed9b13f","cell_type":"code","source":"X, test_df, cat_cols = prepare_all_features(X, test_df, CONFIG, logger)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:01:19.359399Z","iopub.execute_input":"2025-11-17T22:01:19.359792Z","iopub.status.idle":"2025-11-17T22:04:59.579863Z","shell.execute_reply.started":"2025-11-17T22:01:19.359774Z","shell.execute_reply":"2025-11-17T22:04:59.579319Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:01:19,360 - INFO -  ORIGINAL SIZES: X=9912, test_df=8\n2025-11-17 22:01:19,361 - INFO -  Generating basic features...\n2025-11-17 22:01:19,362 - INFO - X shape before basic: (9912, 13)\n2025-11-17 22:01:19,363 - INFO - test_df shape before basic: (8, 13)\n2025-11-17 22:01:19,364 - INFO - X shape after basic: (9912, 13)\n2025-11-17 22:01:19,365 - INFO - test_df shape after basic: (8, 13)\n2025-11-17 22:01:19,366 - INFO -  Generating aggregate features...\n2025-11-17 22:01:19,367 - INFO - X shape before aggregate: (9912, 13)\n2025-11-17 22:01:19,367 - INFO - test_df shape before aggregate: (8, 13)\n2025-11-17 22:01:19,368 - INFO - Aggregate features disabled in CONFIG.\n2025-11-17 22:01:19,369 - INFO - X shape after aggregate: (9912, 13)\n2025-11-17 22:01:19,370 - INFO - test_df shape after aggregate: (8, 13)\n2025-11-17 22:01:19,371 - INFO -   Generating geo features...\n2025-11-17 22:01:19,372 - INFO - X shape before geo: (9912, 13)\n2025-11-17 22:01:19,373 - INFO - test_df shape before geo: (8, 13)\n2025-11-17 22:01:19,374 - INFO - Geo features disabled in CONFIG.\n2025-11-17 22:01:19,374 - INFO - X shape after geo: (9912, 13)\n2025-11-17 22:01:19,375 - INFO - test_df shape after geo: (8, 13)\n2025-11-17 22:01:19,376 - INFO - Images process starts...\nImages train process...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [03:37<00:00,  1.43it/s]\nImages test process...: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  8.75it/s]\n2025-11-17 22:04:58,916 - INFO -  Detecting categorical columns...\n2025-11-17 22:04:58,917 - INFO - X shape before categorical detection: (9912, 780)\n2025-11-17 22:04:58,918 - INFO - test_df shape before categorical detection: (8, 780)\n2025-11-17 22:04:59,253 - INFO - Categorical columns: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\n2025-11-17 22:04:59,253 - INFO -  Processing categorical features (merge rare into 'other')...\n2025-11-17 22:04:59,254 - INFO - X shape before categorical processing: (9912, 780)\n2025-11-17 22:04:59,254 - INFO - test_df shape before categorical processing: (8, 780)\n2025-11-17 22:04:59,255 - INFO - Categorical processing: cat_min_count=20, cat_min_freq=0.0, cat_max_rare_share=0.98, cat_max_unique_after_group=500\nProcessing categorical features:   0%|          | 0/12 [00:00<?, ?it/s]2025-11-17 22:04:59,282 - INFO - [cat] Accessory: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,284 - INFO - [cat] Action: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,285 - INFO - [cat] Blur: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,286 - INFO - [cat] Collage: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,287 - INFO - [cat] Eyes: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,288 - INFO - [cat] Face: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,289 - INFO - [cat] Group: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,291 - INFO - [cat] Human: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,292 - INFO - [cat] Info: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,293 - INFO - [cat] Near: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,294 - INFO - [cat] Occlusion: unique=2, rare_cats=0, rare_share=0.0000\n2025-11-17 22:04:59,295 - INFO - [cat] Subject Focus: unique=2, rare_cats=0, rare_share=0.0000\nProcessing categorical features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 691.24it/s]\n2025-11-17 22:04:59,298 - INFO - Categorical columns after processing: ['Accessory', 'Action', 'Blur', 'Collage', 'Eyes', 'Face', 'Group', 'Human', 'Info', 'Near', 'Occlusion', 'Subject Focus']\n2025-11-17 22:04:59,298 - INFO - X shape after categorical processing: (9912, 780)\n2025-11-17 22:04:59,299 - INFO - test_df shape after categorical processing: (8, 780)\n2025-11-17 22:04:59,574 - INFO - Final X shape: (9912, 780)\n2025-11-17 22:04:59,574 - INFO - Final test_df shape: (8, 780)\n","output_type":"stream"}],"execution_count":54},{"id":"60425ace","cell_type":"markdown","source":"### Features filter","metadata":{}},{"id":"78cfa405","cell_type":"code","source":"def filter_features_by_correlation(\n    X: pd.DataFrame,\n    y: pd.Series,\n    config: Dict,\n    logger: logging.Logger\n) -> Tuple[pd.DataFrame, List[str]]:\n    \"\"\"\n    1) Pearson –ø–æ —á–∏—Å–ª–æ–≤—ã–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º\n    2) phik (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω phik)\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç X —Å –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Å–ø–∏—Å–æ–∫ –∏—Ö –∏–º—ë–Ω.\n    \"\"\"\n    selected = list(X.columns)\n    task_type = config[\"task_type\"]\n\n    # --- Pearson ---\n    pearson_thr = float(config.get(\"corr_pearson_min_abs\", 0.0) or 0.0)\n    if pearson_thr > 0.0:\n        logger.info(f\"Applying Pearson filter with |corr| >= {pearson_thr}\")\n        corr_vals = {}\n        y_series = pd.Series(y)\n\n        for col in tqdm(selected, desc=\"Computing Pearson correlations\"):\n            if pd.api.types.is_numeric_dtype(X[col]):\n                try:\n                    c = X[col].corr(y_series)\n                except Exception:\n                    c = np.nan\n            else:\n                c = np.nan\n            corr_vals[col] = c\n\n        corr_series = pd.Series(corr_vals)\n        keep_mask = corr_series.abs() < pearson_thr\n        kept = corr_series.index[keep_mask].tolist()\n        logger.info(\n            f\"Pearson filter: kept {len(kept)} / {len(corr_series)} features \"\n            f\"(min abs corr {corr_series.abs().min():.6f}, max {corr_series.abs().max():.6f})\"\n        )\n        selected = kept\n\n    # --- phik ---\n    use_phik = bool(config.get(\"corr_use_phik\", False))\n    phik_thr = float(config.get(\"corr_phik_min_abs\", 0.0) or 0.0)\n\n    if use_phik and phik_thr > 0.0:\n        if not HAS_PHIK:\n            logger.warning(\"corr_use_phik=True, –Ω–æ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ phik –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º phik-—Ñ–∏–ª—å—Ç—Ä.\")\n        else:\n            logger.info(f\"Applying phik filter with |phik| >= {phik_thr}\")\n            df_phik = X[selected].copy()\n            tmp_target_col = \"__target_for_phik__\"\n            df_phik[tmp_target_col] = y.values\n\n            # —á–∏—Å–ª–æ–≤—ã–µ –¥–ª—è interval_cols\n            interval_cols = [\n                c for c in df_phik.columns\n                if pd.api.types.is_numeric_dtype(df_phik[c])\n            ]\n            try:\n                phik_matrix = df_phik.phik_matrix(interval_cols=interval_cols)\n                target_corr = phik_matrix[tmp_target_col].drop(\n                    labels=[tmp_target_col], errors=\"ignore\"\n                )\n                keep_mask = target_corr.abs() >= phik_thr\n                kept_phik = target_corr.index[keep_mask].tolist()\n                logger.info(\n                    f\"phik filter: kept {len(kept_phik)} / {len(target_corr)} features \"\n                    f\"(min abs phik {target_corr.abs().min():.6f}, \"\n                    f\"max {target_corr.abs().max():.6f})\"\n                )\n                # –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —É–∂–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–æ Pearson\n                selected = [c for c in selected if c in kept_phik]\n            except Exception as e:\n                logger.warning(f\"phik computation failed, skip phik filter. Error: {e}\")\n\n    logger.info(f\"Total features after correlation filtering: {len(selected)}\")\n    return X[selected], selected","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:08:48.511853Z","iopub.execute_input":"2025-11-17T22:08:48.512430Z","iopub.status.idle":"2025-11-17T22:08:48.523285Z","shell.execute_reply.started":"2025-11-17T22:08:48.512404Z","shell.execute_reply":"2025-11-17T22:08:48.522528Z"}},"outputs":[],"execution_count":65},{"id":"b75dce25","cell_type":"code","source":"# NEW: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏\nif CONFIG.get(\"corr_enable\", False):\n    logger.info(\"üìâ Applying correlation-based feature filtering...\")\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π\n    test_size_before_corr = len(test_df) if test_df is not None else 0\n    logger.info(f\"üîç test_df size BEFORE correlation filter: {test_size_before_corr}\")\n    \n    X, selected_cols = filter_features_by_correlation(X, y, CONFIG, logger)\n\n    if test_df is not None:\n        missing_in_test = [c for c in selected_cols if c not in test_df.columns]\n        if missing_in_test:\n            logger.warning(\n                f\"{len(missing_in_test)} features selected by correlation \"\n                f\"missing in test: {missing_in_test[:10]}...\"\n            )\n        # –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ —Ç–µ—Å—Ç–µ\n        keep_for_test = [c for c in selected_cols if c in test_df.columns]\n        test_df = test_df[keep_for_test]\n        \n        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä –Ω–µ –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø—Ä–∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫\n        logger.info(f\"üîç test_df size AFTER correlation filter: {len(test_df)}\")\n        if len(test_df) != test_size_before_corr:\n            raise ValueError(\n                f\"‚ùå CRITICAL: test_df size changed from {test_size_before_corr} to {len(test_df)} \"\n                f\"during correlation filtering! This should not happen when filtering columns.\"\n            )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:08:48.603872Z","iopub.execute_input":"2025-11-17T22:08:48.604068Z","iopub.status.idle":"2025-11-17T22:08:48.609366Z","shell.execute_reply.started":"2025-11-17T22:08:48.604033Z","shell.execute_reply":"2025-11-17T22:08:48.608779Z"}},"outputs":[],"execution_count":66},{"id":"1e440026","cell_type":"code","source":" # NEW: sanitize feature names –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π (–æ—Å–æ–±–µ–Ω–Ω–æ XGBoost)\nlogger.info(\"üßæ Sanitizing feature names...\")\n# –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ sanitize\ntest_size_before_sanitize = len(test_df) if test_df is not None else 0\nlogger.info(f\"üîç test_df size BEFORE sanitize: {test_size_before_sanitize}\")\n\nX, feature_name_mapping = sanitize_feature_names(X, logger)\n\nif test_df is not None:\n    # —Å–Ω–∞—á–∞–ª–∞ –ø–µ—Ä–µ–∏–º–µ–Ω—É–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Ç–µ—Å—Ç–∞ –ø–æ —Ç–æ–º—É –∂–µ –º–∞–ø–ø–∏–Ω–≥—É\n    test_df = test_df.rename(columns=feature_name_mapping)\n    # –µ—Å–ª–∏ –≤ —Ç–µ—Å—Ç–µ –æ—Å—Ç–∞–ª–∏—Å—å –∫–∞–∫–∏–µ-—Ç–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä–µ–¥–∫–æ),\n    # –º–æ–∂–Ω–æ –µ—â—ë —Ä–∞–∑ –ø—Ä–æ–≥–Ω–∞—Ç—å sanitize, –Ω–æ –∏–º–µ–Ω–∞ —É–∂–µ –±—É–¥—É—Ç –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ\n    # test, _ = sanitize_feature_names(test, logger)\n    \n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —Ä–∞–∑–º–µ—Ä –Ω–µ –¥–æ–ª–∂–µ–Ω –∏–∑–º–µ–Ω–∏—Ç—å—Å—è –ø—Ä–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏\n    logger.info(f\"üîç test_df size AFTER sanitize: {len(test_df)}\")\n    if len(test_df) != test_size_before_sanitize:\n        raise ValueError(\n            f\"‚ùå CRITICAL: test_df size changed from {test_size_before_sanitize} to {len(test_df)} \"\n            f\"during sanitize! This should not happen when renaming columns.\"\n        )\n\n# –æ–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ–¥ –Ω–æ–≤—ã–µ –∏–º–µ–Ω–∞\ncat_cols = [feature_name_mapping.get(c, c) for c in cat_cols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:08:48.728321Z","iopub.execute_input":"2025-11-17T22:08:48.728491Z","iopub.status.idle":"2025-11-17T22:08:48.763691Z","shell.execute_reply.started":"2025-11-17T22:08:48.728478Z","shell.execute_reply":"2025-11-17T22:08:48.762968Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:08:48,730 - INFO -  Sanitizing feature names...\n2025-11-17 22:08:48,732 - INFO -  test_df size BEFORE sanitize: 8\n2025-11-17 22:08:48,760 - INFO -  test_df size AFTER sanitize: 8\n","output_type":"stream"}],"execution_count":67},{"id":"d104dfaa","cell_type":"markdown","source":"### Cross-validation","metadata":{}},{"id":"476f2b4c","cell_type":"code","source":"def make_folds(\n    y: pd.Series,\n    config: Dict\n):\n    n_splits = config[\"cv_n_splits\"]\n    seed = config[\"cv_random_state\"]\n    shuffle = config[\"cv_shuffle\"]\n    stratified = config[\"cv_stratified\"]\n    task_type = config[\"task_type\"]\n\n    if stratified and task_type in [\"binary\", \"multiclass\"]:\n        splitter = StratifiedKFold(\n            n_splits=n_splits,\n            shuffle=shuffle,\n            random_state=seed\n        )\n    else:\n        splitter = KFold(\n            n_splits=n_splits,\n            shuffle=shuffle,\n            random_state=seed\n        )\n    return list(splitter.split(np.zeros(len(y)), y))\n\n\ndef build_catboost_model(\n    config: Dict\n):\n    params = config[\"catboost_params\"].copy()\n    \n    # –î–æ–±–∞–≤–ª—è–µ–º device –≤ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (CatBoost –∏—Å–ø–æ–ª—å–∑—É–µ—Ç \"GPU\" –∏–ª–∏ \"CPU\")\n    device_str = config.get(\"device\", \"CPU\")\n    if device_str.upper() == \"CUDA\":\n        params[\"task_type\"] = \"GPU\"\n    elif device_str.upper() == \"GPU\":\n        params[\"task_type\"] = \"GPU\"\n    else:\n        params[\"task_type\"] = \"CPU\"\n    \n    if config[\"task_type\"] == \"regression\":\n        params[\"loss_function\"] = \"RMSE\"\n        params[\"eval_metric\"] = \"RMSE\"\n        model = CatBoostRegressor(**params)\n    else:\n        if config[\"task_type\"] == \"multiclass\":\n            params[\"loss_function\"] = \"MultiClass\"\n            params[\"eval_metric\"] = \"MultiClass\"\n        model = CatBoostClassifier(**params)\n    return model\n\n\ndef build_lgb_params(\n    config: Dict,\n    num_classes: Optional[int] = None\n):\n    params = config[\"lgb_params\"].copy()\n    task_type = config[\"task_type\"]\n\n    # ---------- Device (CPU / GPU) ----------\n    device_str = config.get(\"device\", \"CPU\").upper()\n    if device_str in [\"CUDA\", \"GPU\"]:\n        # –¥–ª—è LightGBM –ø—Ä–∞–≤–∏–ª—å–Ω–æ \"device_type\"\n        params[\"device_type\"] = \"gpu\"\n    else:\n        params[\"device_type\"] = \"cpu\"\n\n    # ---------- Objective + metric –ø–æ–¥ —Ç–∏–ø –∑–∞–¥–∞—á–∏ ----------\n    if task_type == \"regression\":\n        # –æ–±—ã—á–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è\n        params[\"objective\"] = \"regression\"\n        # –º–µ—Ç—Ä–∏–∫–∏ LightGBM –∑–∞–¥–∞—é—Ç—Å—è –∫–ª—é—á–æ–º \"metric\"\n        params[\"metric\"] = [\"mae\"]   # –∏–ª–∏ [\"rmse\"]\n    elif task_type == \"binary\":\n        params[\"objective\"] = \"binary\"\n        params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n    else:  # multiclass\n        if num_classes is None or num_classes <= 1:\n            raise ValueError(\"num_classes must be > 1 for multiclass\")\n        params[\"objective\"] = \"multiclass\"\n        params[\"num_class\"] = num_classes\n        params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n\n    return params\n\n\n\ndef build_xgb_params(\n    config: Dict,\n    num_classes: Optional[int] = None,\n):\n    params = config[\"xgb_params\"].copy()\n    task_type = config[\"task_type\"]\n\n    # ----- —á–∏—Å—Ç–∏–º –≤–æ–∑–º–æ–∂–Ω—ã–π –º—É—Å–æ—Ä -----\n    # –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ –±—ã–ª–æ —á—Ç–æ-—Ç–æ –≤—Ä–æ–¥–µ \"device\": 0 ‚Äî —É–±–∏—Ä–∞–µ–º\n    if \"device\" in params and not isinstance(params[\"device\"], str):\n        params.pop(\"device\")\n\n    # ----- CPU / GPU -----\n    device_str = config.get(\"device\", \"CPU\").upper()\n    if device_str in [\"CUDA\", \"GPU\"]:\n        # XGBoost 2.x —Ö–æ—á–µ—Ç —Å—Ç—Ä–æ–∫–æ–≤—ã–π device\n        params[\"device\"] = \"cuda\"      # –º–æ–∂–Ω–æ \"cuda:0\"\n        # –Ω–∞ GPU –æ–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ tree_method=\"hist\" –∏–ª–∏ \"gpu_hist\"\n        # –æ—Å—Ç–∞–≤–∏–º \"gpu_hist\", –µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ —É–∂–µ –∑–∞–¥–∞–Ω, –∏–Ω–∞—á–µ –ø–æ—Å—Ç–∞–≤–∏–º –º—ã\n        params.setdefault(\"tree_method\", \"gpu_hist\")\n    else:\n        params[\"device\"] = \"cpu\"\n        params.setdefault(\"tree_method\", \"hist\")\n\n    # –ø–æ—Ç–æ–∫–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å–µ —è–¥—Ä–∞\n    params.setdefault(\"nthread\", 0)\n\n    # ----- —Ç–∏–ø –∑–∞–¥–∞—á–∏ -----\n    if task_type == \"regression\":\n        params[\"objective\"] = \"reg:squarederror\"\n        params[\"eval_metric\"] = \"mae\"           # –º–æ–∂–Ω–æ –ø–æ–º–µ–Ω—è—Ç—å –Ω–∞ \"rmse\"\n    elif task_type == \"binary\":\n        params[\"objective\"] = \"binary:logistic\"\n        params[\"eval_metric\"] = \"auc\"\n    else:\n        # multiclass\n        if num_classes is None:\n            raise ValueError(\"num_classes must be provided for multiclass\")\n        params[\"objective\"] = \"multi:softprob\"\n        params[\"num_class\"] = num_classes\n        params[\"eval_metric\"] = \"mlogloss\"\n\n    return params\n\n\n\n\ndef _evaluate_predictions(\n    y_true: pd.Series,\n    preds: np.ndarray,\n    task_type: str,\n    name_prefix: str,\n    logger: logging.Logger\n) -> Dict[str, float]:\n    metrics = {}\n    if preds is None:\n        return metrics\n\n    if task_type == \"regression\":\n        rmse = mean_squared_error(y_true, preds, squared=False)\n        mae = mean_absolute_error(y_true, preds)\n        metrics[f\"{name_prefix}_RMSE\"] = rmse\n        metrics[f\"{name_prefix}_MAE\"] = rmse\n        \n        logger.info(f\"{name_prefix} RMSE: {rmse:.6f} MAE: {mae:.6f}\")\n    elif task_type == \"binary\":\n        auc = roc_auc_score(y_true, preds)\n        logloss = log_loss(y_true, preds)\n\n        ## –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –ø–æ–¥–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞\n        preds_label = (preds >= 0.5).astype(int)\n        acc = accuracy_score(y_true, preds_label)\n        cm = confusion_matrix(y_true, preds_label)\n        metrics[f\"{name_prefix}_AUC\"] = auc\n        metrics[f\"{name_prefix}_LogLoss\"] = logloss\n        metrics[f\"{name_prefix}_ACC\"] = acc\n        logger.info(f\"{name_prefix} AUC: {auc:.6f}, LogLoss: {logloss:.6f}, ACC: {acc:.6f}\")\n        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n    else:\n        # multiclass\n        # –µ—Å–ª–∏ –ø—Ä–∏—à–ª–∏ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (n_samples, n_classes) ‚Äî –±–µ—Ä—ë–º argmax\n        if preds.ndim == 2:\n            preds_label = np.argmax(preds, axis=1)\n        else:\n            preds_label = preds\n\n        acc = accuracy_score(y_true, preds_label)\n        cm = confusion_matrix(y_true, preds_label)\n        metrics[f\"{name_prefix}_ACC\"] = acc\n        logger.info(f\"{name_prefix} ACC: {acc:.6f}\")\n        logger.info(f\"{name_prefix} confusion matrix:\\n{cm}\")\n\n    return metrics\n\n\ndef train_and_evaluate(\n    X: pd.DataFrame,\n    y: pd.Series,\n    cat_cols: List[str],\n    config: Dict,\n    logger: logging.Logger\n) -> Dict:\n    task_type = config[\"task_type\"]\n    folds = make_folds(y, config)\n    logger.info(f\"Using {len(folds)} folds.\")\n\n    cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n\n    # –°–ù–ê–ß–ê–õ–ê —Å—á–∏—Ç–∞–µ–º classes_\n    classes_ = np.unique(y) if task_type == \"multiclass\" else None\n\n    n = len(y)\n    if task_type == \"multiclass\":\n        n_classes = len(classes_)\n        oof_preds_cat = np.zeros((n, n_classes)) if config[\"use_catboost\"] else None\n        oof_preds_lgb = np.zeros((n, n_classes)) if config[\"use_lightgbm\"] else None\n        oof_preds_xgb = np.zeros((n, n_classes)) if config[\"use_xgboost\"] else None\n    else:\n        oof_preds_cat = np.zeros(n) if config[\"use_catboost\"] else None\n        oof_preds_lgb = np.zeros(n) if config[\"use_lightgbm\"] else None\n        oof_preds_xgb = np.zeros(n) if config[\"use_xgboost\"] else None\n\n    models_cat: List = []\n    models_lgb: List = []\n    models_xgb: List = []\n\n    feature_importances_lgb = []\n    feature_importances_xgb = []\n\n    for fold_idx, (tr_idx, val_idx) in enumerate(tqdm(folds, desc=\"Cross-validation folds\")):\n        logger.info(f\"========== Fold {fold_idx + 1}/{len(folds)} ==========\")\n\n        X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n\n        if config[\"use_catboost\"]:\n            logger.info(\"Training CatBoost...\")\n            train_pool = Pool(\n                X_tr, y_tr,\n                cat_features=cat_features_idx\n            )\n            val_pool = Pool(\n                X_val, y_val,\n                cat_features=cat_features_idx\n            )\n            model_cat = build_catboost_model(config)\n            # Early stopping –¥–ª—è CatBoost\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_cat.fit(\n                train_pool, \n                eval_set=val_pool,\n                early_stopping_rounds=early_stopping_rounds,\n                use_best_model=True\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_cat.predict(val_pool)\n            else:\n                proba = model_cat.predict_proba(val_pool)\n                if task_type == \"binary\":\n                    preds_val = proba[:, 1]\n                else:\n                    oof_preds_cat[val_idx, :] = proba\n                    preds_val = proba  # –¥–ª—è –µ–¥–∏–Ω–æ–æ–±—Ä–∞–∑–∏—è, –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –≥–¥–µ-—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å\n\n            if task_type != \"multiclass\":\n                oof_preds_cat[val_idx] = preds_val\n            models_cat.append(model_cat)\n\n        if config[\"use_lightgbm\"]:\n            logger.info(\"Training LightGBM...\")\n\n            num_classes = len(classes_) if task_type == \"multiclass\" else None\n            lgb_params = build_lgb_params(config, num_classes=num_classes)\n\n            X_tr_lgb = X_tr.copy()\n            X_val_lgb = X_val.copy()\n            for c in cat_cols:\n                if c in X_tr_lgb.columns:\n                    X_tr_lgb[c] = X_tr_lgb[c].astype(\"category\")\n                    X_val_lgb[c] = X_val_lgb[c].astype(\"category\")\n\n            lgb_train = lgb.Dataset(X_tr_lgb, label=y_tr)\n            lgb_val = lgb.Dataset(X_val_lgb, label=y_val)\n            print(lgb_params)\n            # Early stopping –¥–ª—è LightGBM\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_lgb = lgb.train(\n                lgb_params,\n                lgb_train,\n                num_boost_round=1000,\n                valid_sets=[lgb_train, lgb_val],\n                valid_names=[\"train\", \"valid\"],\n                callbacks=[\n                    lgb.early_stopping(stopping_rounds=early_stopping_rounds),\n                    lgb.log_evaluation(100)\n                ]\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n            else:\n                proba = model_lgb.predict(X_val_lgb, num_iteration=model_lgb.best_iteration)\n                if task_type == \"binary\":\n                    preds_val = proba\n                else:\n                    # multiclass: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n                    oof_preds_lgb[val_idx, :] = proba\n                    preds_val = proba\n\n            if task_type != \"multiclass\":\n                oof_preds_lgb[val_idx] = preds_val\n            models_lgb.append(model_lgb)\n            feature_importances_lgb.append(model_lgb.feature_importance(importance_type=\"gain\"))\n\n        if config[\"use_xgboost\"]:\n            logger.info(\"Training XGBoost...\")\n            num_classes = len(classes_) if task_type == \"multiclass\" else None\n            xgb_params = build_xgb_params(config, num_classes=num_classes)\n\n            X_tr_xgb = X_tr.copy()\n            X_val_xgb = X_val.copy()\n            for c in X_tr_xgb.columns:\n                if X_tr_xgb[c].dtype == \"object\":\n                    X_tr_xgb[c] = X_tr_xgb[c].astype(\"category\")\n                    X_val_xgb[c] = X_val_xgb[c].astype(\"category\")\n\n            dtrain = xgb.DMatrix(X_tr_xgb, label=y_tr, enable_categorical=True)\n            dvalid = xgb.DMatrix(X_val_xgb, label=y_val, enable_categorical=True)\n\n            evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n            print(dtrain, dvalid)\n            print(xgb_params)\n            # Early stopping –¥–ª—è XGBoost\n            early_stopping_rounds = config.get(\"early_stopping_rounds\", 100)\n            model_xgb = xgb.train(\n                params=xgb_params,\n                dtrain=dtrain,\n                num_boost_round=1000,\n                evals=evals,\n                early_stopping_rounds=early_stopping_rounds,\n                verbose_eval=100\n            )\n\n            if task_type == \"regression\":\n                preds_val = model_xgb.predict(\n                    dvalid,\n                    iteration_range=(0, model_xgb.best_iteration + 1)\n                )\n            else:\n                proba = model_xgb.predict(\n                    dvalid,\n                    iteration_range=(0, model_xgb.best_iteration + 1)\n                )\n                if task_type == \"binary\":\n                    preds_val = proba\n                else:\n                    # multiclass: —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n                    oof_preds_xgb[val_idx, :] = proba\n                    preds_val = proba\n\n            if task_type != \"multiclass\":\n                oof_preds_xgb[val_idx] = preds_val\n            models_xgb.append(model_xgb)\n\n            fi_xgb = model_xgb.get_score(importance_type=\"gain\")\n            fi_vec = np.array([fi_xgb.get(f, 0.0) for f in X.columns])\n            feature_importances_xgb.append(fi_vec)\n\n        gc.collect()\n\n    metrics_all: Dict[str, float] = {}\n    metrics_all.update(_evaluate_predictions(y, oof_preds_cat, task_type, \"CatBoost\", logger))\n    metrics_all.update(_evaluate_predictions(y, oof_preds_lgb, task_type, \"LightGBM\", logger))\n    metrics_all.update(_evaluate_predictions(y, oof_preds_xgb, task_type, \"XGBoost\", logger))\n\n    logger.info(\"Evaluating blended OOF predictions...\")\n    blend_weights = config[\"blend_weights\"]\n    \n    blend_preds = None\n    if task_type == \"multiclass\":\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —á–∏—Å–ª–æ –∫–ª–∞—Å—Å–æ–≤ –ø–æ –ø–µ—Ä–≤–æ–π –Ω–µ-None –º–∞—Ç—Ä–∏—Ü–µ\n        num_classes = None\n        for arr in [oof_preds_cat, oof_preds_lgb, oof_preds_xgb]:\n            if arr is not None:\n                num_classes = arr.shape[1]\n                break\n        if num_classes is None:\n            logger.warning(\"No OOF predictions for multiclass blend.\")\n        else:\n            blend_num = np.zeros((n, num_classes), dtype=float)\n            blend_den = np.zeros(n, dtype=float)\n\n            if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n                w = blend_weights[\"catboost\"]\n                blend_num += w * oof_preds_cat\n                blend_den += w\n            if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n                w = blend_weights[\"lightgbm\"]\n                blend_num += w * oof_preds_lgb\n                blend_den += w\n            if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n                w = blend_weights[\"xgboost\"]\n                blend_num += w * oof_preds_xgb\n                blend_den += w\n\n            valid_mask = blend_den > 0\n            if valid_mask.any():\n                blend_preds = np.zeros_like(blend_num)\n                # –Ω–æ—Ä–º–∏—Ä—É–µ–º –ø–æ –≤–µ—Å–∞–º, broadcast –ø–æ axis=1\n                blend_preds[valid_mask] = (\n                    blend_num[valid_mask] /\n                    blend_den[valid_mask][:, None]\n                )\n                metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n            else:\n                logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n    else:\n        # binary / regression ‚Äî –∫–∞–∫ —Ä–∞–Ω—å—à–µ (1D)\n        blend_num = np.zeros(n, dtype=float)\n        blend_den = np.zeros(n, dtype=float)\n\n        if oof_preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n            w = blend_weights[\"catboost\"]\n            blend_num += w * oof_preds_cat\n            blend_den += w\n        if oof_preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n            w = blend_weights[\"lightgbm\"]\n            blend_num += w * oof_preds_lgb\n            blend_den += w\n        if oof_preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n            w = blend_weights[\"xgboost\"]\n            blend_num += w * oof_preds_xgb\n            blend_den += w\n\n        valid_mask = blend_den > 0\n        if valid_mask.any():\n            blend_preds = np.zeros_like(blend_num)\n            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n            metrics_all.update(_evaluate_predictions(y, blend_preds, task_type, \"Blend\", logger))\n        else:\n            logger.warning(\"No models contributions to blend; blend_den is zero everywhere.\")\n    logger.info(\"========== OOF metrics ==========\")\n    logger.info(metrics_all)\n\n    results = {\n        \"oof_catboost\": oof_preds_cat,\n        \"oof_lightgbm\": oof_preds_lgb,\n        \"oof_xgboost\": oof_preds_xgb,\n        \"oof_blend\": blend_preds,\n        \"metrics\": metrics_all,\n        \"models_catboost\": models_cat,\n        \"models_lightgbm\": models_lgb,\n        \"models_xgboost\": models_xgb\n    }\n\n    if feature_importances_lgb:\n        fi_lgb = np.mean(feature_importances_lgb, axis=0)\n        results[\"feature_importance_lgb\"] = fi_lgb\n\n    if feature_importances_xgb:\n        fi_xgb_mean = np.mean(feature_importances_xgb, axis=0)\n        results[\"feature_importance_xgb\"] = fi_xgb_mean\n\n    return results\n\n\ndef ensemble_predict(\n    models: Dict[str, List],\n    X: pd.DataFrame,\n    cat_cols: List[str],\n    config: Dict\n) -> Dict[str, Optional[np.ndarray]]:\n    \"\"\"\n    –î–µ–ª–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∞–Ω—Å–∞–º–±–ª–µ–º:\n      - –ø–æ –∫–∞–∂–¥–æ–º—É –±—É—Å—Ç–∏–Ω–≥—É —É—Å—Ä–µ–¥–Ω—è–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ —Ñ–æ–ª–¥–∞–º\n      - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç blended –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –ø–æ –≤–µ—Å–∞–º\n\n    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n      {\n        \"catboost\": np.ndarray –∏–ª–∏ None,\n        \"lightgbm\": np.ndarray –∏–ª–∏ None,\n        \"xgboost\": np.ndarray –∏–ª–∏ None,\n        \"blend\": np.ndarray –∏–ª–∏ None,\n      }\n    \"\"\"\n    task_type = config[\"task_type\"]\n    blend_weights = config[\"blend_weights\"]\n\n    n = len(X)\n    preds_cat = None\n    preds_lgb = None\n    preds_xgb = None\n\n    X_lgb = X.copy()\n    for c in cat_cols:\n        if c in X_lgb.columns:\n            X_lgb[c] = X_lgb[c].astype(\"category\")\n\n    # CatBoost\n    if models.get(\"catboost\"):\n        cat_features_idx = [X.columns.get_loc(c) for c in cat_cols if c in X.columns]\n        test_pool = Pool(X, cat_features=cat_features_idx)\n        for m in models[\"catboost\"]:\n            if task_type == \"regression\":\n                proba = m.predict(test_pool)              # (n,)\n            else:\n                proba_full = m.predict_proba(test_pool)   # (n,2) –∏–ª–∏ (n,C)\n                if task_type == \"binary\":\n                    proba = proba_full[:, 1]              # (n,)\n                else:\n                    proba = proba_full                    # (n,C)\n            if preds_cat is None:\n                preds_cat = np.zeros_like(proba, dtype=float)\n            preds_cat += proba\n        preds_cat /= len(models[\"catboost\"])\n\n    if models.get(\"lightgbm\"):\n        for m in models[\"lightgbm\"]:\n            if task_type == \"regression\":\n                proba = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,)\n            else:\n                proba_full = m.predict(X_lgb, num_iteration=m.best_iteration)  # (n,) –∏–ª–∏ (n,C)\n                if task_type == \"binary\":\n                    proba = proba_full                                      # (n,)\n                else:\n                    proba = proba_full                                      # (n,C)\n            if preds_lgb is None:\n                preds_lgb = np.zeros_like(proba, dtype=float)\n            preds_lgb += proba\n        preds_lgb /= len(models[\"lightgbm\"])\n\n    # XGBoost\n    if models.get(\"xgboost\"):\n        X_xgb = X.copy()\n        for c in X_xgb.columns:\n            if X_xgb[c].dtype == \"object\":\n                X_xgb[c] = X_xgb[c].astype(\"category\")\n        dtest = xgb.DMatrix(X_xgb, enable_categorical=True)\n\n        for m in models[\"xgboost\"]:\n            if task_type == \"regression\":\n                proba = m.predict(\n                    dtest,\n                    iteration_range=(0, m.best_iteration + 1)\n                )  # shape: (n,)\n            else:\n                proba_full = m.predict(\n                    dtest,\n                    iteration_range=(0, m.best_iteration + 1)\n                )  # shape: (n,) for binary, (n, C) for multiclass\n                if task_type == \"binary\":\n                    proba = proba_full          # (n,)\n                else:\n                    proba = proba_full          # (n, C)\n\n            if preds_xgb is None:\n                preds_xgb = np.zeros_like(proba, dtype=float)\n            preds_xgb += proba\n\n        preds_xgb /= len(models[\"xgboost\"])\n\n    # Blending\n    blend_preds = None\n    if task_type == \"multiclass\":\n        # –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–æ—Ä–º—É –ø–æ –ø–µ—Ä–≤–æ–π –Ω–µ-None –º–∞—Ç—Ä–∏—Ü–µ\n        base = None\n        for arr in [preds_cat, preds_lgb, preds_xgb]:\n            if arr is not None:\n                base = arr\n                break\n        if base is not None:\n            blend_num = np.zeros_like(base, dtype=float)  # (n,C)\n            blend_den = np.zeros(n, dtype=float)\n\n            if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n                w = blend_weights[\"catboost\"]\n                blend_num += w * preds_cat\n                blend_den += w\n            if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n                w = blend_weights[\"lightgbm\"]\n                blend_num += w * preds_lgb\n                blend_den += w\n            if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n                w = blend_weights[\"xgboost\"]\n                blend_num += w * preds_xgb\n                blend_den += w\n\n            valid_mask = blend_den > 0\n            if valid_mask.any():\n                blend_preds = np.zeros_like(blend_num)\n                blend_preds[valid_mask] = (\n                    blend_num[valid_mask] /\n                    blend_den[valid_mask][:, None]\n                )\n    else:\n        blend_num = np.zeros(n, dtype=float)\n        blend_den = np.zeros(n, dtype=float)\n        if preds_cat is not None and blend_weights.get(\"catboost\", 0) != 0:\n            w = blend_weights[\"catboost\"]\n            blend_num += w * preds_cat\n            blend_den += w\n        if preds_lgb is not None and blend_weights.get(\"lightgbm\", 0) != 0:\n            w = blend_weights[\"lightgbm\"]\n            blend_num += w * preds_lgb\n            blend_den += w\n        if preds_xgb is not None and blend_weights.get(\"xgboost\", 0) != 0:\n            w = blend_weights[\"xgboost\"]\n            blend_num += w * preds_xgb\n            blend_den += w\n\n        valid_mask = blend_den > 0\n        if valid_mask.any():\n            blend_preds = np.zeros_like(blend_num)\n            blend_preds[valid_mask] = blend_num[valid_mask] / blend_den[valid_mask]\n\n    return {\n        \"catboost\": preds_cat,\n        \"lightgbm\": preds_lgb,\n        \"xgboost\": preds_xgb,\n        \"blend\": blend_preds\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:10:51.789333Z","iopub.execute_input":"2025-11-17T22:10:51.790213Z","iopub.status.idle":"2025-11-17T22:10:51.838476Z","shell.execute_reply.started":"2025-11-17T22:10:51.790186Z","shell.execute_reply":"2025-11-17T22:10:51.837875Z"}},"outputs":[],"execution_count":70},{"id":"5f4c59c6","cell_type":"code","source":"# –£–¥–∞–ª—è–µ–º ID —Ç–æ–ª—å–∫–æ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º –±—É—Å—Ç–∏–Ω–≥–æ–≤, —á—Ç–æ–±—ã –Ω–µ –ª–æ–º–∞—Ç—å —Ñ–∏—á–µ–≥–µ–Ω (–∫–∞—Ä—Ç–∏–Ω–∫–∏/—Ç–µ–∫—Å—Ç—ã)\nif id_col in X.columns:\n    logger.info(f\"Dropping ID column '{id_col}' before boosting\")\n    X = X.drop(columns=[id_col])\n\nif test_df is not None and id_col in test_df.columns:\n    test_df = test_df.drop(columns=[id_col])\n\n# –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —É–±–µ—Ä—ë–º ID –∏–∑ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö\ncat_cols = [c for c in cat_cols if c != id_col]\n\nresults = train_and_evaluate(X, y, cat_cols, CONFIG, logger)\n\n# ----- Save OOF, metrics, FI -----\nlogger.info(\"üíæ Saving artifacts...\")\nif results[\"oof_catboost\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_catboost.npy\"), results[\"oof_catboost\"])\nif results[\"oof_lightgbm\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_lightgbm.npy\"), results[\"oof_lightgbm\"])\nif results[\"oof_xgboost\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_xgboost.npy\"), results[\"oof_xgboost\"])\nif results[\"oof_blend\"] is not None:\n    np.save(os.path.join(output_dir, \"oof_blend.npy\"), results[\"oof_blend\"])\n\nwith open(os.path.join(output_dir, \"metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n    json.dump(results[\"metrics\"], f, indent=2, ensure_ascii=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:10:51.886448Z","iopub.execute_input":"2025-11-17T22:10:51.886652Z","iopub.status.idle":"2025-11-17T22:12:37.916896Z","shell.execute_reply.started":"2025-11-17T22:10:51.886637Z","shell.execute_reply":"2025-11-17T22:12:37.916236Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:10:51,894 - INFO - Using 5 folds.\nCross-validation folds:   0%|          | 0/5 [00:00<?, ?it/s]2025-11-17 22:10:51,896 - INFO - ========== Fold 1/5 ==========\n2025-11-17 22:10:51,923 - INFO - Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.3666210\ttest: 20.9160448\tbest: 20.9160448 (0)\ttotal: 64.7ms\tremaining: 3m 46s\n100:\tlearn: 17.3011458\ttest: 18.7793576\tbest: 18.7793576 (100)\ttotal: 3.25s\tremaining: 1m 49s\n200:\tlearn: 16.2372460\ttest: 18.5426210\tbest: 18.5426210 (200)\ttotal: 6.23s\tremaining: 1m 42s\n300:\tlearn: 15.3424025\ttest: 18.4469244\tbest: 18.4464357 (298)\ttotal: 9.22s\tremaining: 1m 38s\n400:\tlearn: 14.6272440\ttest: 18.4206252\tbest: 18.4206252 (400)\ttotal: 12.1s\tremaining: 1m 33s\n500:\tlearn: 13.9874252\ttest: 18.3938498\tbest: 18.3913437 (487)\ttotal: 15.1s\tremaining: 1m 30s\n600:\tlearn: 13.4406143\ttest: 18.3790735\tbest: 18.3790735 (600)\ttotal: 18s\tremaining: 1m 26s\n700:\tlearn: 12.9281463\ttest: 18.3845997\tbest: 18.3690670 (641)\ttotal: 20.9s\tremaining: 1m 23s\nbestTest = 18.36906703\nbestIteration = 641\nShrink model to first 642 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  20%|‚ñà‚ñà        | 1/5 [00:23<01:33, 23.36s/it]2025-11-17 22:11:15,253 - INFO - ========== Fold 2/5 ==========\n2025-11-17 22:11:15,287 - INFO - Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4771735\ttest: 20.4679574\tbest: 20.4679574 (0)\ttotal: 34.2ms\tremaining: 1m 59s\n100:\tlearn: 17.2707978\ttest: 18.8053503\tbest: 18.8053503 (100)\ttotal: 3.15s\tremaining: 1m 46s\n200:\tlearn: 16.2560179\ttest: 18.6607415\tbest: 18.6607415 (200)\ttotal: 6.18s\tremaining: 1m 41s\n300:\tlearn: 15.4198290\ttest: 18.5819456\tbest: 18.5791341 (299)\ttotal: 9.14s\tremaining: 1m 37s\n400:\tlearn: 14.6948809\ttest: 18.5692405\tbest: 18.5517974 (355)\ttotal: 12.1s\tremaining: 1m 33s\nbestTest = 18.55179744\nbestIteration = 355\nShrink model to first 356 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:38<00:55, 18.42s/it]2025-11-17 22:11:30,213 - INFO - ========== Fold 3/5 ==========\n2025-11-17 22:11:30,247 - INFO - Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.5934220\ttest: 19.9850853\tbest: 19.9850853 (0)\ttotal: 34.2ms\tremaining: 1m 59s\n100:\tlearn: 17.5121431\ttest: 18.0485596\tbest: 18.0485596 (100)\ttotal: 3.18s\tremaining: 1m 47s\n200:\tlearn: 16.4274246\ttest: 17.8457838\tbest: 17.8433504 (195)\ttotal: 6.2s\tremaining: 1m 41s\n300:\tlearn: 15.5787569\ttest: 17.7853156\tbest: 17.7841321 (298)\ttotal: 9.16s\tremaining: 1m 37s\n400:\tlearn: 14.8573726\ttest: 17.7848289\tbest: 17.7823642 (351)\ttotal: 12.1s\tremaining: 1m 33s\nbestTest = 17.78236415\nbestIteration = 351\nShrink model to first 352 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:53<00:33, 16.74s/it]2025-11-17 22:11:44,953 - INFO - ========== Fold 4/5 ==========\n2025-11-17 22:11:44,986 - INFO - Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4814290\ttest: 20.4794928\tbest: 20.4794928 (0)\ttotal: 33.7ms\tremaining: 1m 57s\n100:\tlearn: 17.3670812\ttest: 18.4959828\tbest: 18.4959828 (100)\ttotal: 3.16s\tremaining: 1m 46s\n200:\tlearn: 16.3382341\ttest: 18.2725686\tbest: 18.2725686 (200)\ttotal: 6.18s\tremaining: 1m 41s\n300:\tlearn: 15.4613907\ttest: 18.1577069\tbest: 18.1504219 (294)\ttotal: 9.17s\tremaining: 1m 37s\n400:\tlearn: 14.7657572\ttest: 18.1268845\tbest: 18.1200187 (391)\ttotal: 12.1s\tremaining: 1m 33s\n500:\tlearn: 14.1550152\ttest: 18.0919089\tbest: 18.0899802 (498)\ttotal: 15s\tremaining: 1m 29s\n600:\tlearn: 13.5887124\ttest: 18.0670456\tbest: 18.0670456 (600)\ttotal: 17.9s\tremaining: 1m 26s\n700:\tlearn: 13.0764979\ttest: 18.0484731\tbest: 18.0484731 (700)\ttotal: 20.9s\tremaining: 1m 23s\n800:\tlearn: 12.5762761\ttest: 18.0480503\tbest: 18.0413817 (778)\ttotal: 23.8s\tremaining: 1m 20s\n900:\tlearn: 12.1561807\ttest: 18.0392885\tbest: 18.0355351 (859)\ttotal: 26.7s\tremaining: 1m 16s\nbestTest = 18.03553506\nbestIteration = 859\nShrink model to first 860 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [01:22<00:21, 21.80s/it]2025-11-17 22:12:14,520 - INFO - ========== Fold 5/5 ==========\n2025-11-17 22:12:14,555 - INFO - Training CatBoost...\n","output_type":"stream"},{"name":"stdout","text":"0:\tlearn: 20.4578034\ttest: 20.6183195\tbest: 20.6183195 (0)\ttotal: 32.5ms\tremaining: 1m 53s\n100:\tlearn: 17.2872166\ttest: 18.6907085\tbest: 18.6907085 (100)\ttotal: 3.16s\tremaining: 1m 46s\n200:\tlearn: 16.2297245\ttest: 18.4599624\tbest: 18.4597720 (199)\ttotal: 6.18s\tremaining: 1m 41s\n300:\tlearn: 15.4018088\ttest: 18.4129828\tbest: 18.4066905 (291)\ttotal: 9.12s\tremaining: 1m 36s\n400:\tlearn: 14.6776583\ttest: 18.3843449\tbest: 18.3825224 (398)\ttotal: 12s\tremaining: 1m 33s\n500:\tlearn: 14.0256234\ttest: 18.3670857\tbest: 18.3644415 (491)\ttotal: 15s\tremaining: 1m 29s\n600:\tlearn: 13.4662105\ttest: 18.3469676\tbest: 18.3467124 (598)\ttotal: 17.9s\tremaining: 1m 26s\n700:\tlearn: 12.9803039\ttest: 18.3455642\tbest: 18.3358293 (647)\ttotal: 20.8s\tremaining: 1m 23s\nbestTest = 18.3358293\nbestIteration = 647\nShrink model to first 648 iterations.\n","output_type":"stream"},{"name":"stderr","text":"Cross-validation folds: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [01:45<00:00, 21.20s/it]\n2025-11-17 22:12:37,899 - INFO - CatBoost RMSE: 18.217006 MAE: 13.372917\n2025-11-17 22:12:37,899 - INFO - Evaluating blended OOF predictions...\n2025-11-17 22:12:37,904 - INFO - Blend RMSE: 18.217006 MAE: 13.372917\n2025-11-17 22:12:37,905 - INFO - ========== OOF metrics ==========\n2025-11-17 22:12:37,906 - INFO - {'CatBoost_RMSE': 18.217006007819673, 'CatBoost_MAE': 18.217006007819673, 'Blend_RMSE': 18.217006007819673, 'Blend_MAE': 18.217006007819673}\n2025-11-17 22:12:37,912 - INFO -  Saving artifacts...\n","output_type":"stream"}],"execution_count":71},{"id":"6fe648f2","cell_type":"markdown","source":"### Importances","metadata":{}},{"id":"9194b81e","cell_type":"code","source":"if \"feature_importance_lgb\" in results:\n    fi = results[\"feature_importance_lgb\"]\n    fi_df = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_gain\": fi\n    }).sort_values(\"importance_gain\", ascending=False)\n    fi_df.to_csv(os.path.join(output_dir, \"feature_importance_lgb.csv\"), index=False)\n    logger.info(\"Top features (LGB, gain):\")\n    logger.info(fi_df.head(CONFIG[\"top_features_to_show\"]))\n\nif \"feature_importance_xgb\" in results:\n    fi_xgb = results[\"feature_importance_xgb\"]\n    fi_xgb_df = pd.DataFrame({\n        \"feature\": X.columns,\n        \"importance_gain\": fi_xgb\n    }).sort_values(\"importance_gain\", ascending=False)\n    fi_xgb_df.to_csv(os.path.join(output_dir, \"feature_importance_xgb.csv\"), index=False)\n    logger.info(\"Top features (XGB, gain):\")\n    logger.info(fi_xgb_df.head(CONFIG[\"top_features_to_show\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:12:37.918733Z","iopub.execute_input":"2025-11-17T22:12:37.918973Z","iopub.status.idle":"2025-11-17T22:12:37.925750Z","shell.execute_reply.started":"2025-11-17T22:12:37.918952Z","shell.execute_reply":"2025-11-17T22:12:37.924886Z"}},"outputs":[],"execution_count":72},{"id":"9e844bbe","cell_type":"markdown","source":"### Submission","metadata":{}},{"id":"dfd7f5d0","cell_type":"code","source":"# ----- Test predictions (ensemble) -----\nif test_df is not None:\n    # NEW: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è —Ç–µ—Å—Ç–∞ –ø–æ train,\n    # –º–∞–ø–ø–∏–º –≤—Å–µ unseen ‚Üí \"other\" (–µ—Å–ª–∏ –æ–Ω–∞ –µ—Å—Ç—å), –∏–Ω–∞—á–µ ‚Üí –º–æ–¥–∞ —Ç—Ä–µ–π–Ω–∞\n    if cat_cols:\n        logger.info(\"üß© Aligning categorical values in test with train for XGBoost...\")\n        for c in cat_cols:\n            if c in test_df.columns and c in X.columns:\n\n                train_vals = X[c].dropna()\n                if train_vals.empty:\n                    continue\n\n                known = set(train_vals.unique())\n                mask_new = ~test_df[c].isin(known)\n\n                if mask_new.any():\n                    # –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π \"other\"\n                    known_lower = {str(v).lower() for v in known}\n                    if \"other\" in known_lower:\n                        # –Ω–∞—Ö–æ–¥–∏–º —Ç–æ—á–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ \"other\" –≤ train\n                        other_value = next(v for v in known if str(v).lower() == \"other\")\n                        replacement = other_value\n                    else:\n                        # –µ—Å–ª–∏ \"other\" –Ω–µ—Ç ‚Äî fallback –Ω–∞ –º–æ–¥—É\n                        replacement = train_vals.mode(dropna=True).iloc[0]\n\n                    logger.info(\n                        f\"üîÑ Column '{c}': mapping {mask_new.sum()} unseen categories ‚Üí '{replacement}'\"\n                    )\n                    test_df.loc[mask_new, c] = replacement\n\n    logger.info(\"üì§ Making ensemble predictions for test...\")\n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: —É–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ test_df –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä\n    logger.info(f\"test_df shape before ensemble_predict: {test_df.shape}\")\n    logger.info(f\"test_ids length: {len(test_ids) if test_ids is not None else 'None'}\")\n    \n    # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–í–ï–†–ö–ê: –µ—Å–ª–∏ —Ä–∞–∑–º–µ—Ä test_df –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å test_ids, —ç—Ç–æ –æ—à–∏–±–∫–∞\n    if test_ids is not None:\n        expected_test_size = len(test_ids)\n        actual_test_size = len(test_df)\n        if actual_test_size != expected_test_size:\n            error_msg = (\n                f\"‚ùå CRITICAL ERROR: test_df size ({actual_test_size}) != test_ids size ({expected_test_size})! \"\n                f\"This indicates a problem in data processing. \"\n                f\"test_df should have {expected_test_size} rows, but has {actual_test_size}. \"\n                f\"Please check your data processing pipeline, especially generate_aggregate_features.\"\n            )\n            logger.error(error_msg)\n            raise ValueError(error_msg)\n    \n    pred_dict = ensemble_predict(\n        models={\n            \"catboost\": results[\"models_catboost\"],\n            \"lightgbm\": results[\"models_lightgbm\"],\n            \"xgboost\": results[\"models_xgboost\"],\n        },\n        X=test_df,\n        cat_cols=cat_cols,\n        config=CONFIG\n    )\n\n    if test_ids is None:\n        test_ids_series = pd.Series(np.arange(len(test_df)), name=id_col)\n    else:\n        test_ids_series = test_ids\n\n    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø–µ—Ä–µ–¥ —Å–æ–∑–¥–∞–Ω–∏–µ–º submission\n    logger.info(f\"Test DataFrame shape: {test_df.shape}\")\n    logger.info(f\"Test IDs length: {len(test_ids_series)}\")\n\n    for name, preds in pred_dict.items():\n        if preds is None:\n            continue\n\n        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n        if isinstance(preds, np.ndarray):\n            pred_len = preds.shape[0] if preds.ndim == 1 else preds.shape[0]\n            logger.info(f\"Predictions '{name}' shape: {preds.shape}, length: {pred_len}\")\n            \n            if pred_len != len(test_df):\n                raise ValueError(\n                    f\"Predictions length {pred_len} != test_df length {len(test_df)} for {name}. \"\n                    f\"Test_df shape: {test_df.shape}, predictions shape: {preds.shape}\"\n                )\n\n        if isinstance(preds, np.ndarray) and preds.ndim == 2:\n            n_classes = preds.shape[1]\n            data = {\n                id_col: test_ids_series.values,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .values –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞\n                \"prediction\": np.argmax(preds, axis=1),  # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å\n            }\n            for k in range(n_classes):\n                data[f\"proba_class_{k}\"] = preds[:, k]\n            out_df = pd.DataFrame(data)\n        else:\n            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç\n            preds_flat = preds.flatten() if isinstance(preds, np.ndarray) else preds\n            if len(preds_flat) != len(test_ids_series):\n                raise ValueError(\n                    f\"Predictions length {len(preds_flat)} != test_ids length {len(test_ids_series)} for {name}\"\n                )\n            out_df = pd.DataFrame({\n                id_col: test_ids_series.values,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º .values\n                \"prediction\": preds_flat\n            })\n\n        out_df.to_csv(os.path.join(output_dir, f\"pred_{name}.csv\"), index=False)\n        logger.info(f\"Saved predictions: pred_{name}.csv\")\n\nelapsed = time.time() - start_time\nlogger.info(f\"‚è± Finished in {elapsed:.1f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:12:37.926633Z","iopub.execute_input":"2025-11-17T22:12:37.927393Z","iopub.status.idle":"2025-11-17T22:12:38.037170Z","shell.execute_reply.started":"2025-11-17T22:12:37.927371Z","shell.execute_reply":"2025-11-17T22:12:38.036599Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:12:37,943 - INFO -  Aligning categorical values in test with train for XGBoost...\n2025-11-17 22:12:37,972 - INFO -  Making ensemble predictions for test...\n2025-11-17 22:12:37,973 - INFO - test_df shape before ensemble_predict: (8, 780)\n2025-11-17 22:12:37,974 - INFO - test_ids length: 8\n2025-11-17 22:12:38,022 - INFO - Test DataFrame shape: (8, 780)\n2025-11-17 22:12:38,022 - INFO - Test IDs length: 8\n2025-11-17 22:12:38,023 - INFO - Predictions 'catboost' shape: (8,), length: 8\n2025-11-17 22:12:38,031 - INFO - Saved predictions: pred_catboost.csv\n2025-11-17 22:12:38,031 - INFO - Predictions 'blend' shape: (8,), length: 8\n2025-11-17 22:12:38,034 - INFO - Saved predictions: pred_blend.csv\n2025-11-17 22:12:38,034 - INFO -  Finished in 955.9 seconds\n","output_type":"stream"}],"execution_count":73},{"id":"146d23f0","cell_type":"code","source":"pred_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:12:38.038539Z","iopub.execute_input":"2025-11-17T22:12:38.038778Z","iopub.status.idle":"2025-11-17T22:12:38.045497Z","shell.execute_reply.started":"2025-11-17T22:12:38.038760Z","shell.execute_reply":"2025-11-17T22:12:38.044747Z"}},"outputs":[{"execution_count":74,"output_type":"execute_result","data":{"text/plain":"{'catboost': array([43.39089807, 42.86031178, 41.95564802, 42.5169854 , 43.4911699 ,\n        44.34301199, 42.32943855, 43.66510427]),\n 'lightgbm': None,\n 'xgboost': None,\n 'blend': array([43.39089807, 42.86031178, 41.95564802, 42.5169854 , 43.4911699 ,\n        44.34301199, 42.32943855, 43.66510427])}"},"metadata":{}}],"execution_count":74},{"id":"9783588e","cell_type":"code","source":"# ----- FINAL SUBMISSION -----\nlogger.info(\"üìù Preparing final submission file...\")\n\n# –ù–∞–π–¥—ë–º –∏—Ç–æ–≥–æ–≤—ã–π –∞–Ω—Å–∞–º–±–ª–µ–≤—ã–π prediction\nfinal_preds = pred_dict.get(\"catboost\")\n\nif final_preds is None:\n    logger.warning(\"Blend predictions are None. Falling back to first available model.\")\n    for name in [\"catboost\", \"lightgbm\", \"xgboost\"]:\n        if pred_dict.get(name) is not None:\n            final_preds = pred_dict[name]\n            logger.info(f\"Using '{name}' predictions for submission.\")\n            break\n\nif final_preds is None:\n    raise ValueError(\"No predictions available to form submission.\")\n\n# –°–æ–∑–¥–∞—ë–º DataFrame —Å id\nsubmission = pd.DataFrame({id_col: test_ids_series})\n\n# –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–ª—è–µ–º target\ntask_type = CONFIG[\"task_type\"]\n\nif task_type == \"multiclass\":\n    # final_preds ‚Äî –º–∞—Ç—Ä–∏—Ü–∞ (n, C)\n    submission[CONFIG[\"target_column\"]] = np.argmax(final_preds, axis=1)\n\nelif task_type == \"binary\":\n    # —Ñ–∏–Ω–∞–ª—å–Ω—ã–π prediction ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1\n    # –µ—Å–ª–∏ –Ω—É–∂–µ–Ω label, –∑–∞–ª–æ–∂–∏ –ø–æ—Ä–æ–≥ = 0.5\n    submission[CONFIG[\"target_column\"]] = (final_preds >= 0.5).astype(int)\n\nelse:  # regression\n    submission[CONFIG[\"target_column\"]] = final_preds\n\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º\nsub_path = os.path.join(output_dir, \"submission.csv\")\nif \"index\" in submission.columns:\n    submission.drop(columns=\"index\", inplace=True)\nsubmission.to_csv(sub_path, index=False)\n\nlogger.info(f\"‚úÖ Submission file saved: {sub_path}\")\nprint(submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:12:38.046249Z","iopub.execute_input":"2025-11-17T22:12:38.046491Z","iopub.status.idle":"2025-11-17T22:12:38.063662Z","shell.execute_reply.started":"2025-11-17T22:12:38.046474Z","shell.execute_reply":"2025-11-17T22:12:38.062937Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 22:12:38,054 - INFO -  Preparing final submission file...\n2025-11-17 22:12:38,058 - INFO -  Submission file saved: tabular_boosting_output/submission.csv\n","output_type":"stream"},{"name":"stdout","text":"                                 Id  Pawpularity\n0  4128bae22183829d2b5fea10effdb0c3    43.390898\n1  43a2262d7738e3d420d453815151079e    42.860312\n2  4e429cead1848a298432a0acad014c9d    41.955648\n3  80bc3ccafcc51b66303c2c263aa38486    42.516985\n4  8f49844c382931444e68dffbe20228f4    43.491170\n","output_type":"stream"}],"execution_count":75},{"id":"fcbc2f2a","cell_type":"code","source":"submission.head(35)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T22:12:38.064496Z","iopub.execute_input":"2025-11-17T22:12:38.064712Z","iopub.status.idle":"2025-11-17T22:12:38.078557Z","shell.execute_reply.started":"2025-11-17T22:12:38.064694Z","shell.execute_reply":"2025-11-17T22:12:38.077775Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"                                 Id  Pawpularity\n0  4128bae22183829d2b5fea10effdb0c3    43.390898\n1  43a2262d7738e3d420d453815151079e    42.860312\n2  4e429cead1848a298432a0acad014c9d    41.955648\n3  80bc3ccafcc51b66303c2c263aa38486    42.516985\n4  8f49844c382931444e68dffbe20228f4    43.491170\n5  b03f7041962238a7c9d6537e22f9b017    44.343012\n6  c978013571258ed6d4637f6e8cc9d6a3    42.329439\n7  e0de453c1bffc20c22b072b34b54e50f    43.665104","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Id</th>\n      <th>Pawpularity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4128bae22183829d2b5fea10effdb0c3</td>\n      <td>43.390898</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>43a2262d7738e3d420d453815151079e</td>\n      <td>42.860312</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4e429cead1848a298432a0acad014c9d</td>\n      <td>41.955648</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80bc3ccafcc51b66303c2c263aa38486</td>\n      <td>42.516985</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8f49844c382931444e68dffbe20228f4</td>\n      <td>43.491170</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>b03f7041962238a7c9d6537e22f9b017</td>\n      <td>44.343012</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>c978013571258ed6d4637f6e8cc9d6a3</td>\n      <td>42.329439</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>e0de453c1bffc20c22b072b34b54e50f</td>\n      <td>43.665104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":76},{"id":"dff6fe35","cell_type":"code","source":"# ============================\n# Optuna: CPU-only hyperparameter tuning\n# ============================\nimport optuna\n\ndef _metric_for_optuna(y_true, y_pred, task_type: str):\n    \"\"\"\n    –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –º–µ—Ç—Ä–∏–∫–∞ –¥–ª—è Optuna: —á–µ–º –ë–û–õ–¨–®–ï, —Ç–µ–º –ª—É—á—à–µ.\n    - regression:  -RMSE\n    - binary/multiclass: accuracy\n    \"\"\"\n    import numpy as np\n    from sklearn.metrics import mean_squared_error, accuracy_score\n\n    if task_type == \"regression\":\n        rmse = mean_squared_error(y_true, y_pred, squared=False)\n        return -rmse\n    else:\n        if isinstance(y_pred, np.ndarray) and y_pred.ndim == 2:\n            y_hat = np.argmax(y_pred, axis=1)\n        else:\n            y_hat = y_pred\n        acc = accuracy_score(y_true, y_hat)\n        return acc\n\n\ndef tune_all_boostings_optuna(\n    X,\n    y,\n    cat_cols,\n    CONFIG,\n    logger,\n    timeout: int = 3600,\n    tune_cat: bool = True,\n    tune_lgb: bool = True,\n    tune_xgb: bool = True,\n):\n    \"\"\"\n    –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–æ–≤ CatBoost, LightGBM –∏ XGBoost –Ω–∞ CPU —Å –ø–æ–º–æ—â—å—é Optuna.\n\n    tune_cat / tune_lgb / tune_xgb ‚Äî —Ñ–ª–∞–≥–∏, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ —Ç—é–Ω–∏—Ç—å.\n    –ù–∞–ø—Ä–∏–º–µ—Ä, —á—Ç–æ–±—ã —Ç—é–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ CatBoost:\n        tune_cat=True, tune_lgb=False, tune_xgb=False\n    \"\"\"\n    import numpy as np\n    from sklearn.model_selection import StratifiedKFold, KFold\n    from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n    import lightgbm as lgb\n    import xgboost as xgb\n\n    task_type = CONFIG.get(\"task_type\", \"regression\")\n\n    # ====================\n    # CV-—Å–ø–ª–∏—Ç—Ç–µ—Ä\n    # ====================\n    n_splits = CONFIG.get(\"cv_n_splits\", 3)\n    cv_shuffle = CONFIG.get(\"cv_shuffle\", True)\n    cv_random_state = CONFIG.get(\"cv_random_state\", 42)\n    cv_stratified = CONFIG.get(\"cv_stratified\", True)\n\n    if task_type == \"regression\" or not cv_stratified:\n        kf = KFold(\n            n_splits=n_splits,\n            shuffle=cv_shuffle,\n            random_state=cv_random_state,\n        )\n    else:\n        kf = StratifiedKFold(\n            n_splits=n_splits,\n            shuffle=cv_shuffle,\n            random_state=cv_random_state,\n        )\n\n    # ======================================================\n    # 1) CatBoost (CPU, —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–æ–π loss/metric)\n    # ======================================================\n    if tune_cat and CONFIG.get(\"use_catboost\", False):\n        logger.info(\"üîç Optuna tuning for CatBoost (CPU)...\")\n\n        def objective_cat(trial):\n            base_params = CONFIG[\"catboost_params\"].copy()\n\n            # –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –Ω–∞ CPU (Optuna –Ω–µ —Ç—Ä–æ–≥–∞–µ—Ç —Ç–≤–æ—ë GPU-–æ–±—É—á–µ–Ω–∏–µ)\n            base_params[\"task_type\"] = \"CPU\"\n            base_params[\"thread_count\"] = 0\n            base_params[\"verbose\"] = False\n\n            # –≥–∏–ø–µ—Ä—ã –¥–ª—è —Ç—é–Ω–∏–Ω–≥–∞\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"depth\": trial.suggest_int(\"depth\", 4, 10),\n                \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-2, 10.0, log=True),\n                \"bagging_temperature\": trial.suggest_float(\"bagging_temperature\", 0.0, 10.0),\n                \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n            }\n\n            # ----- loss_function / eval_metric –ø–æ–¥ task_type -----\n            if task_type == \"regression\":\n                params[\"loss_function\"] = \"MAE\"\n                params[\"eval_metric\"] = \"MAE\"\n                ModelCls = CatBoostRegressor\n            elif task_type == \"binary\":\n                params[\"loss_function\"] = \"Logloss\"\n                params[\"eval_metric\"] = \"AUC\"\n                ModelCls = CatBoostClassifier\n            elif task_type == \"multiclass\":\n                params[\"loss_function\"] = \"MultiClass\"\n                params[\"eval_metric\"] = \"MultiClass\"\n                ModelCls = CatBoostClassifier\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for CatBoost\")\n\n            oof_preds = None\n\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                train_pool = Pool(X_tr, y_tr, cat_features=cat_cols)\n                valid_pool = Pool(X_va, y_va, cat_features=cat_cols)\n\n                model = ModelCls(**params)\n\n                model.fit(\n                    train_pool,\n                    eval_set=valid_pool,\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose=False,\n                )\n\n                if task_type == \"regression\":\n                    preds = model.predict(X_va)\n                else:\n                    proba = model.predict_proba(X_va)\n                    if task_type == \"binary\":\n                        preds = proba[:, 1]\n                    else:\n                        preds = proba\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_cat = optuna.create_study(direction=\"maximize\")\n        study_cat.optimize(objective_cat, timeout=timeout)\n        logger.info(f\"CatBoost best value: {study_cat.best_value}\")\n        logger.info(f\"CatBoost best params: {study_cat.best_params}\")\n\n        CONFIG[\"catboost_params\"].update({\n            \"learning_rate\": study_cat.best_params[\"learning_rate\"],\n            \"depth\": study_cat.best_params[\"depth\"],\n            \"l2_leaf_reg\": study_cat.best_params[\"l2_leaf_reg\"],\n            \"bagging_temperature\": study_cat.best_params[\"bagging_temperature\"],\n            \"border_count\": study_cat.best_params[\"border_count\"],\n        })\n\n    # ======================================================\n    # 2) LightGBM (CPU, objective/metric –ø–æ task_type)\n    # ======================================================\n    if tune_lgb and CONFIG.get(\"use_lightgbm\", False):\n        logger.info(\"üîç Optuna tuning for LightGBM (CPU)...\")\n\n        def objective_lgb(trial):\n            base_params = CONFIG[\"lgb_params\"].copy()\n\n            base_params[\"device_type\"] = \"cpu\"\n            base_params[\"num_threads\"] = 0\n\n            if task_type == \"regression\":\n                base_params[\"objective\"] = \"regression\"\n                base_params[\"metric\"] = [\"mae\"]\n            elif task_type == \"binary\":\n                base_params[\"objective\"] = \"binary\"\n                base_params[\"metric\"] = [\"auc\", \"binary_logloss\"]\n            elif task_type == \"multiclass\":\n                num_classes = len(np.unique(y))\n                base_params[\"objective\"] = \"multiclass\"\n                base_params[\"num_class\"] = num_classes\n                base_params[\"metric\"] = [\"multi_logloss\", \"multi_error\"]\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for LightGBM\")\n\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"num_leaves\": trial.suggest_int(\"num_leaves\", 31, 255),\n                \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n                \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n                \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 0, 10),\n                \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n                \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 10.0),\n                \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 10.0),\n            }\n\n            oof_preds = None\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                train_set = lgb.Dataset(X_tr, label=y_tr)\n                valid_set = lgb.Dataset(X_va, label=y_va)\n\n                model = lgb.train(\n                    params,\n                    train_set,\n                    num_boost_round=5000,\n                    valid_sets=[valid_set],\n                    valid_names=[\"valid\"],\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose_eval=False,\n                )\n\n                preds = model.predict(X_va, num_iteration=model.best_iteration)\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_lgb = optuna.create_study(direction=\"maximize\")\n        study_lgb.optimize(objective_lgb, timeout=timeout)\n        logger.info(f\"LightGBM best value: {study_lgb.best_value}\")\n        logger.info(f\"LightGBM best params: {study_lgb.best_params}\")\n\n        CONFIG[\"lgb_params\"].update({\n            \"learning_rate\": study_lgb.best_params[\"learning_rate\"],\n            \"num_leaves\": study_lgb.best_params[\"num_leaves\"],\n            \"feature_fraction\": study_lgb.best_params[\"feature_fraction\"],\n            \"bagging_fraction\": study_lgb.best_params[\"bagging_fraction\"],\n            \"bagging_freq\": study_lgb.best_params[\"bagging_freq\"],\n            \"min_data_in_leaf\": study_lgb.best_params[\"min_data_in_leaf\"],\n            \"lambda_l1\": study_lgb.best_params[\"lambda_l1\"],\n            \"lambda_l2\": study_lgb.best_params[\"lambda_l2\"],\n        })\n\n    # ======================================================\n    # 3) XGBoost (CPU, objective/metric –ø–æ task_type)\n    # ======================================================\n    if tune_xgb and CONFIG.get(\"use_xgboost\", False):\n        logger.info(\"üîç Optuna tuning for XGBoost (CPU)...\")\n\n        def objective_xgb(trial):\n            base_params = CONFIG[\"xgb_params\"].copy()\n\n            base_params[\"device\"] = \"cpu\"\n            base_params.setdefault(\"tree_method\", \"hist\")\n            base_params.setdefault(\"nthread\", 0)\n\n            if task_type == \"regression\":\n                base_params[\"objective\"] = \"reg:squarederror\"\n                base_params[\"eval_metric\"] = \"mae\"\n            elif task_type == \"binary\":\n                base_params[\"objective\"] = \"binary:logistic\"\n                base_params[\"eval_metric\"] = \"auc\"\n            elif task_type == \"multiclass\":\n                num_classes = len(np.unique(y))\n                base_params[\"objective\"] = \"multi:softprob\"\n                base_params[\"num_class\"] = num_classes\n                base_params[\"eval_metric\"] = \"mlogloss\"\n            else:\n                raise ValueError(f\"Unsupported task_type '{task_type}' for XGBoost\")\n\n            params = {\n                **base_params,\n                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n                \"min_child_weight\": trial.suggest_float(\"min_child_weight\", 1e-2, 10.0, log=True),\n                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n                \"lambda\": trial.suggest_float(\"lambda\", 0.0, 10.0),\n                \"alpha\": trial.suggest_float(\"alpha\", 0.0, 10.0),\n            }\n\n            oof_preds = None\n            for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y)):\n                X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n                y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n\n                dtrain = xgb.DMatrix(X_tr, label=y_tr)\n                dvalid = xgb.DMatrix(X_va, label=y_va)\n\n                model = xgb.train(\n                    params=params,\n                    dtrain=dtrain,\n                    num_boost_round=4000,\n                    evals=[(dvalid, \"valid\")],\n                    early_stopping_rounds=CONFIG.get(\"early_stopping_rounds\", 100),\n                    verbose_eval=False,\n                )\n\n                preds = model.predict(dvalid, iteration_range=(0, model.best_iteration + 1))\n\n                if oof_preds is None:\n                    if isinstance(preds, np.ndarray) and preds.ndim == 2:\n                        oof_preds = np.zeros((len(y), preds.shape[1]), dtype=float)\n                    else:\n                        oof_preds = np.zeros(len(y), dtype=float)\n                oof_preds[va_idx] = preds\n\n            score = _metric_for_optuna(y.values, oof_preds, task_type)\n            return score\n\n        study_xgb = optuna.create_study(direction=\"maximize\")\n        study_xgb.optimize(objective_xgb, timeout=timeout)\n        logger.info(f\"XGBoost best value: {study_xgb.best_value}\")\n        logger.info(f\"XGBoost best params: {study_xgb.best_params}\")\n\n        CONFIG[\"xgb_params\"].update({\n            \"learning_rate\": study_xgb.best_params[\"learning_rate\"],\n            \"max_depth\": study_xgb.best_params[\"max_depth\"],\n            \"min_child_weight\": study_xgb.best_params[\"min_child_weight\"],\n            \"subsample\": study_xgb.best_params[\"subsample\"],\n            \"colsample_bytree\": study_xgb.best_params[\"colsample_bytree\"],\n            \"lambda\": study_xgb.best_params[\"lambda\"],\n            \"alpha\": study_xgb.best_params[\"alpha\"],\n        })\n\n    logger.info(\"‚úÖ Optuna tuning finished. CONFIG –æ–±–Ω–æ–≤–ª—ë–Ω –ª—É—á—à–∏–º–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.\")\n    return CONFIG\n\n# –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞ –¢–û–õ–¨–ö–û –¥–ª—è CatBoost:\n#CONFIG = tune_all_boostings_optuna(X, y, cat_cols, CONFIG, logger, timeout=3600,tune_cat=True, tune_lgb=False, tune_xgb=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:56:42.978153Z","iopub.status.idle":"2025-11-17T21:56:42.978429Z","shell.execute_reply.started":"2025-11-17T21:56:42.978266Z","shell.execute_reply":"2025-11-17T21:56:42.978279Z"}},"outputs":[],"execution_count":null},{"id":"08fa3d34","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}