{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# начало работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pip] install: pypdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pip] install: sentence-transformers\n",
      "[env] setup complete:\n",
      "  - python: 3.12.11\n",
      "  - cuda_available: True\n",
      "  - PyPDF2: 3.0.1\n",
      "  - pandas: 2.3.3\n",
      "  - numpy: 2.3.5\n",
      "  - pyarrow: 22.0.0\n",
      "  - faiss: 1.12.0\n",
      "  - transformers: 4.57.1\n",
      "  - sentence_transformers: 5.1.2\n",
      "  - accelerate: 1.11.0\n",
      "  - tqdm: 4.67.1\n",
      "  - langchain: 1.0.5\n",
      "  - langchain_community: 0.4.1\n",
      "  - rank_bm25: unknown\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# === Block 0: env setup for RAG on vast.ai ===================================\n",
    "# Ставит / проверяет:\n",
    "# - PyPDF2 / pypdf (чтение PDF)\n",
    "# - pandas, numpy, pyarrow (parquet)\n",
    "# - faiss-gpu (если CUDA), fallback → faiss-cpu\n",
    "# - transformers, sentence-transformers, accelerate, tqdm\n",
    "# - langchain, langchain-community (для PyPDFLoader и цепочек)\n",
    "# - rank-bm25 (BM25 / \"b25\")\n",
    "\n",
    "import os, sys, importlib, subprocess\n",
    "\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "def _pip_install(spec: str):\n",
    "    print(f\"[pip] install: {spec}\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", spec])\n",
    "\n",
    "def _mod_ok(name: str) -> bool:\n",
    "    try:\n",
    "        importlib.import_module(name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _ver(name: str) -> str:\n",
    "    try:\n",
    "        m = importlib.import_module(name)\n",
    "        return getattr(m, \"__version__\", \"unknown\")\n",
    "    except Exception:\n",
    "        return \"not_imported\"\n",
    "\n",
    "# --- CUDA / faiss-gpu vs faiss-cpu ---\n",
    "cuda_available = False\n",
    "try:\n",
    "    import torch\n",
    "    cuda_available = bool(getattr(torch, \"cuda\", None) and torch.cuda.is_available())\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- базовые пакеты ---\n",
    "to_install = []\n",
    "\n",
    "# PDF: pypdf (нужен LangChain'у) + опционально PyPDF2\n",
    "if not _mod_ok(\"pypdf\"):\n",
    "    to_install.append(\"pypdf\")\n",
    "\n",
    "# PyPDF2 нам не обязателен, но если хочешь, можно оставить:\n",
    "if not _mod_ok(\"PyPDF2\"):\n",
    "    to_install.append(\"PyPDF2\")\n",
    "    \n",
    "# таблички\n",
    "for pkg in [\"pandas\", \"numpy\", \"pyarrow\"]:\n",
    "    if not _mod_ok(pkg):\n",
    "        to_install.append(pkg)\n",
    "\n",
    "# transformers / sentence-transformers / accelerate / tqdm\n",
    "for pkg in [\"transformers\", \"sentence-transformers\", \"accelerate\", \"tqdm\"]:\n",
    "    if not _mod_ok(pkg):\n",
    "        to_install.append(pkg)\n",
    "\n",
    "# langchain + langchain-community\n",
    "if not _mod_ok(\"langchain\"):\n",
    "    to_install.append(\"langchain>=0.2.0\")\n",
    "if not _mod_ok(\"langchain_community\"):\n",
    "    to_install.append(\"langchain-community>=0.2.0\")\n",
    "\n",
    "# BM25\n",
    "if not _mod_ok(\"rank_bm25\"):\n",
    "    to_install.append(\"rank-bm25\")\n",
    "\n",
    "# faiss: сначала пробуем импорт, потом ставим\n",
    "faiss_ok = _mod_ok(\"faiss\")\n",
    "if not faiss_ok:\n",
    "    if cuda_available:\n",
    "        to_install.append(\"faiss-gpu\")\n",
    "    else:\n",
    "        to_install.append(\"faiss-cpu\")\n",
    "\n",
    "# --- установка ---\n",
    "for spec in to_install:\n",
    "    try:\n",
    "        _pip_install(spec)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        if spec == \"faiss-gpu\":\n",
    "            print(\"[pip] faiss-gpu не установился, пробуем faiss-cpu…\")\n",
    "            _pip_install(\"faiss-cpu\")\n",
    "        else:\n",
    "            print(f\"[warn] не удалось установить {spec}: {e}\")\n",
    "\n",
    "# финальная проверка faiss\n",
    "if not _mod_ok(\"faiss\"):\n",
    "    try:\n",
    "        _pip_install(\"faiss-cpu\")\n",
    "    except Exception as e:\n",
    "        print(\"[warn] faiss так и не установился, индекс FAISS работать не будет:\", e)\n",
    "\n",
    "# --- summary ---\n",
    "summary = {\n",
    "    \"python\": sys.version.split()[0],\n",
    "    \"cuda_available\": cuda_available,\n",
    "    \"PyPDF2\": _ver(\"PyPDF2\") if _mod_ok(\"PyPDF2\") else _ver(\"pypdf\"),\n",
    "    \"pandas\": _ver(\"pandas\"),\n",
    "    \"numpy\": _ver(\"numpy\"),\n",
    "    \"pyarrow\": _ver(\"pyarrow\"),\n",
    "    \"faiss\": _ver(\"faiss\"),\n",
    "    \"transformers\": _ver(\"transformers\"),\n",
    "    \"sentence_transformers\": _ver(\"sentence_transformers\"),\n",
    "    \"accelerate\": _ver(\"accelerate\"),\n",
    "    \"tqdm\": _ver(\"tqdm\"),\n",
    "    \"langchain\": _ver(\"langchain\"),\n",
    "    \"langchain_community\": _ver(\"langchain_community\"),\n",
    "    \"rank_bm25\": _ver(\"rank_bm25\"),\n",
    "}\n",
    "print(\"[env] setup complete:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"  - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-80GB\n",
      "BASE_DIR: /workspace\n",
      "DATA_DIR: /workspace/data\n",
      "DOCS_DIR: /workspace/data/docs\n",
      "OUTPUT_DIR: /workspace/outputs\n",
      "SEED: 42\n"
     ]
    }
   ],
   "source": [
    "# === Block 0. Базовые импорты, конфиг, сиды ===\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- базовые пути (можно править под конкретное соревнование) ---\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATA_DIR = BASE_DIR / \"data\"          # сюда обычно кладём исходные данные\n",
    "DOCS_DIR = DATA_DIR / \"docs\"          # сюда — документы для RAG (pdf/txt/html и т.д.)\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"     # сюда — артефакты, индекс, сабмиты\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- глобальный конфиг (можно потом расширять) ---\n",
    "SEED = 42\n",
    "\n",
    "# размер чанка и overlap будут важны для сплиттера,\n",
    "# здесь просто задаём дефолт, позже ещё будет отдельный блок про чанкинг\n",
    "CHUNK_SIZE = 512\n",
    "CHUNK_OVERLAP = 64\n",
    "\n",
    "# максимальная длина контекста, который будем скармливать модели\n",
    "MAX_CONTEXT_CHARS = 4000\n",
    "\n",
    "# заглушки для имён моделей (позже конкретизируем в отдельном блоке)\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "GENERATION_MODEL_NAME = \"gpt2\"  # пример; позже поменяем на нужную локальную модель\n",
    "\n",
    "# --- функция для фиксации сида ---\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# --- информация о девайсе ---\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"BASE_DIR:\", BASE_DIR)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DOCS_DIR:\", DOCS_DIR)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
    "print(\"SEED:\", SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# подгруз данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Скопировал book.pdf → /workspace/data/docs/book.pdf\n",
      "Скопировал queries.json → /workspace/data/queries.json\n"
     ]
    }
   ],
   "source": [
    "# === Block 0.5. Копируем данные соревнования в нашу структуру ===\n",
    "\n",
    "# создаём папки, если их ещё нет\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "src_dir = Path(\".\")\n",
    "\n",
    "# копируем pdf в docs\n",
    "src_book = src_dir / \"book.pdf\"\n",
    "dst_book = DOCS_DIR / \"book.pdf\"\n",
    "\n",
    "if src_book.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(src_book, dst_book)\n",
    "    print(\"Скопировал book.pdf →\", dst_book)\n",
    "else:\n",
    "    print(\"НЕ НАШЁЛ файл:\", src_book)\n",
    "\n",
    "# копируем queries.json в корень data\n",
    "src_queries = src_dir / \"queries.json\"\n",
    "dst_queries = DATA_DIR / \"queries.json\"\n",
    "\n",
    "if src_queries.exists():\n",
    "    import shutil\n",
    "    shutil.copy2(src_queries, dst_queries)\n",
    "    print(\"Скопировал queries.json →\", dst_queries)\n",
    "else:\n",
    "    print(\"НЕ НАШЁЛ файл:\", src_queries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Найдено документов: 1\n",
      "   doc_id                           path   ext\n",
      "0       0  /workspace/data/docs/book.pdf  .pdf\n"
     ]
    }
   ],
   "source": [
    "# === Block 1. Поиск документов для RAG в DOCS_DIR ===\n",
    "\n",
    "# какие типы файлов считаем документами\n",
    "SUPPORTED_EXTENSIONS = [\".pdf\", \".txt\", \".md\", \".html\", \".htm\"]\n",
    "\n",
    "def list_documents(docs_dir: Path, exts=None) -> pd.DataFrame:\n",
    "    # если не передали свой список расширений — используем дефолтный\n",
    "    if exts is None:\n",
    "        exts = SUPPORTED_EXTENSIONS\n",
    "\n",
    "    docs = []\n",
    "\n",
    "    # если папки с документами ещё нет — создаём пустую и сразу возвращаем пустой DataFrame\n",
    "    if not docs_dir.exists():\n",
    "        print(\"DOCS_DIR не существует, создаю пустую папку:\", docs_dir)\n",
    "        docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "        return pd.DataFrame(columns=[\"doc_id\", \"path\", \"ext\"])\n",
    "\n",
    "    # рекурсивно обходим DOCS_DIR и собираем все подходящие файлы\n",
    "    for path in sorted(docs_dir.rglob(\"*\")):\n",
    "        if path.is_file():\n",
    "            ext = path.suffix.lower()\n",
    "            if ext in exts:\n",
    "                docs.append(\n",
    "                    {\n",
    "                        \"doc_id\": len(docs),   # простой числовой id документа\n",
    "                        \"path\": path,          # полный путь к файлу\n",
    "                        \"ext\": ext,            # расширение файла\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "\n",
    "docs_df = list_documents(DOCS_DIR)\n",
    "\n",
    "print(\"Найдено документов:\", len(docs_df))\n",
    "print(docs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: uv in /usr/local/lib/python3.12/dist-packages (0.8.17)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: No virtual environment found; run `\u001b[32muv venv\u001b[39m` to create an environment, or pass `\u001b[32m--system\u001b[39m` to install into a non-virtual environment\n"
     ]
    }
   ],
   "source": [
    "!uv pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-core<2.0.0,>=1.0.1 (from langchain-community)\n",
      "  Downloading langchain_core-1.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
      "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting SQLAlchemy<3.0.0,>=1.4.0 (from langchain-community)\n",
      "  Downloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain-community)\n",
      "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain-community)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.43-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.3.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain-community)\n",
      "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting langchain-text-splitters<2.0.0,>=1.0.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting pydantic<3.0.0,>=2.7.4 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core<2.0.0,>=1.0.1->langchain-community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (25.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain-community) (4.15.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
      "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting zstandard>=0.23.0 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
      "Collecting greenlet>=1 (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community)\n",
      "  Downloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.10.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain-community) (3.0.0)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
      "  Downloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
      "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-1.0.5-py3-none-any.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.4.43-py3-none-any.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.2/410.2 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.9/51.9 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sqlalchemy-2.0.44-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (607 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m607.6/607.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
      "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.1/256.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.11.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.3/136.3 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m463.4/463.4 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.41.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.25.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: zstandard, typing-inspection, tenacity, python-dotenv, pydantic-core, propcache, orjson, mypy-extensions, multidict, marshmallow, jsonpatch, httpx-sse, greenlet, frozenlist, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests-toolbelt, pydantic, aiosignal, pydantic-settings, langsmith, dataclasses-json, aiohttp, langchain-core, langchain-text-splitters, langchain-classic, langchain-community\n",
      "Successfully installed SQLAlchemy-2.0.44 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 annotated-types-0.7.0 dataclasses-json-0.6.7 frozenlist-1.8.0 greenlet-3.2.4 httpx-sse-0.4.3 jsonpatch-1.33 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.0.5 langchain-text-splitters-1.0.0 langsmith-0.4.43 marshmallow-3.26.1 multidict-6.7.0 mypy-extensions-1.1.0 orjson-3.11.4 propcache-0.4.1 pydantic-2.12.4 pydantic-core-2.41.5 pydantic-settings-2.12.0 python-dotenv-1.2.1 requests-toolbelt-1.0.0 tenacity-9.1.2 typing-inspect-0.9.0 typing-inspection-0.4.2 yarl-1.22.0 zstandard-0.25.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Downloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf\n",
      "Successfully installed pypdf-6.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю текст документов...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:11<00:00, 11.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено документов: 1\n",
      "   doc_id   ext  n_chars\n",
      "0       0  .pdf  2256059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Block 2. Загрузка текста документов (pdf / txt / md / html) ===\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader  # для pdf через langchain\n",
    "\n",
    "\n",
    "def load_pdf(path: Path) -> str:\n",
    "    # читаем PDF постранично и склеиваем в один текст\n",
    "    loader = PyPDFLoader(str(path))\n",
    "    pages = loader.load()  # список объектов Document с .page_content\n",
    "    texts = [p.page_content for p in pages]\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "\n",
    "def load_text_file(path: Path, encoding: str = \"utf-8\") -> str:\n",
    "    # простой читатель для txt/md/html\n",
    "    # (для html сейчас не чистим теги, просто читаем как есть — этого уже достаточно для baseline)\n",
    "    with open(path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def load_single_document(row: pd.Series) -> Dict[str, Any]:\n",
    "    doc_id = int(row[\"doc_id\"])\n",
    "    path = Path(row[\"path\"])\n",
    "    ext = str(row[\"ext\"]).lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        text = load_pdf(path)\n",
    "    else:\n",
    "        text = load_text_file(path)\n",
    "\n",
    "    return {\n",
    "        \"doc_id\": doc_id,\n",
    "        \"path\": str(path),\n",
    "        \"ext\": ext,\n",
    "        \"text\": text,\n",
    "        \"n_chars\": len(text),\n",
    "    }\n",
    "\n",
    "\n",
    "raw_docs: List[Dict[str, Any]] = []\n",
    "\n",
    "if len(docs_df) == 0:\n",
    "    print(\"В DOCS_DIR пока нет документов. Block 2 ничего не загрузил.\")\n",
    "else:\n",
    "    print(\"Загружаю текст документов...\")\n",
    "    for _, row in tqdm(docs_df.iterrows(), total=len(docs_df)):\n",
    "        doc_info = load_single_document(row)\n",
    "        raw_docs.append(doc_info)\n",
    "\n",
    "    print(f\"Загружено документов: {len(raw_docs)}\")\n",
    "\n",
    "# переводим в DataFrame для удобства\n",
    "raw_docs_df = pd.DataFrame(raw_docs)\n",
    "print(raw_docs_df[[\"doc_id\", \"ext\", \"n_chars\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# чанкинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEURISTIC_CHUNK_SIZE (target for 2–3 sentences): 300\n",
      "CHUNK_SIZE (config): 512\n",
      "CHUNK_SIZE_EFFECTIVE (used): 300\n",
      "CHUNK_OVERLAP (config): 64\n",
      "EFFECTIVE_OVERLAP (used, >= 25%): 75\n",
      "Делаю чанкинг документов...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a7eb0ceee3143ff88da78a20b912acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего чанков: 10007\n",
      "Размер chunks_df: (10007, 7)\n",
      "   chunk_id  doc_id  chunk_idx_in_doc  n_chars\n",
      "0         0       0                 0      175\n",
      "1         1       0                 1      210\n",
      "2         2       0                 2      272\n",
      "3         3       0                 3      260\n",
      "4         4       0                 4      197\n"
     ]
    }
   ],
   "source": [
    "# === Block 3. Чанкинг текста документов (семантический/структурный) ===\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- эвристика под 2–3 предложения ---\n",
    "# 1 предложение ~ 13 слов, длина слова ~ 8 символов + пробел ≈ 9\n",
    "# 1 предложение ~ 13 * 9 ≈ 117 символов\n",
    "# хотим 2–3 предложения → возьмём 2.5 → 117 * 2.5 ≈ 292 ≈ 300 символов\n",
    "HEURISTIC_CHUNK_SIZE = 300  # целевой размер чанка (символы)\n",
    "\n",
    "# Если CHUNK_SIZE/CHUNK_OVERLAP были заданы раньше — аккуратно их учитываем\n",
    "try:\n",
    "    cfg_chunk_size = int(CHUNK_SIZE)\n",
    "except NameError:\n",
    "    cfg_chunk_size = None\n",
    "\n",
    "try:\n",
    "    cfg_chunk_overlap = int(CHUNK_OVERLAP)\n",
    "except NameError:\n",
    "    cfg_chunk_overlap = 0\n",
    "\n",
    "# Финальный размер чанка:\n",
    "# - если раньше заданный CHUNK_SIZE меньше эвристики → используем его\n",
    "# - иначе берём эвристическое 300\n",
    "CHUNK_SIZE_EFFECTIVE = cfg_chunk_size if (cfg_chunk_size and cfg_chunk_size <= HEURISTIC_CHUNK_SIZE) else HEURISTIC_CHUNK_SIZE\n",
    "\n",
    "# Минимальная доля пересечения между чанками (по символам) — > 20% как ты просил\n",
    "MIN_OVERLAP_FRACTION = 0.25\n",
    "\n",
    "min_overlap = int(CHUNK_SIZE_EFFECTIVE * MIN_OVERLAP_FRACTION)\n",
    "EFFECTIVE_OVERLAP = max(cfg_chunk_overlap, min_overlap)\n",
    "\n",
    "print(\"HEURISTIC_CHUNK_SIZE (target for 2–3 sentences):\", HEURISTIC_CHUNK_SIZE)\n",
    "print(\"CHUNK_SIZE (config):\", cfg_chunk_size)\n",
    "print(\"CHUNK_SIZE_EFFECTIVE (used):\", CHUNK_SIZE_EFFECTIVE)\n",
    "print(\"CHUNK_OVERLAP (config):\", cfg_chunk_overlap)\n",
    "print(\"EFFECTIVE_OVERLAP (used, >= 25%):\", EFFECTIVE_OVERLAP)\n",
    "\n",
    "# Сепараторы подобраны так, чтобы сперва резать по \"крупной структуре\":\n",
    "#   - пустые строки / параграфы\n",
    "#   - одиночные переводы строки\n",
    "#   - предложения \". \"\n",
    "#   - пробелы\n",
    "#   - в крайнем случае — посимвольно\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE_EFFECTIVE,\n",
    "    chunk_overlap=EFFECTIVE_OVERLAP,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    length_function=len,  # считаем длину в символах\n",
    ")\n",
    "\n",
    "def split_doc_to_chunks(doc_row: pd.Series) -> List[Dict[str, Any]]:\n",
    "    doc_id = int(doc_row[\"doc_id\"])\n",
    "    text = str(doc_row[\"text\"])\n",
    "    path = str(doc_row[\"path\"])\n",
    "    ext = str(doc_row[\"ext\"])\n",
    "\n",
    "    if not text.strip():\n",
    "        return []\n",
    "\n",
    "    # создаём список \"документов\" langchain с общими метаданными\n",
    "    lc_docs = text_splitter.create_documents(\n",
    "        texts=[text],\n",
    "        metadatas=[{\n",
    "            \"doc_id\": doc_id,\n",
    "            \"path\": path,\n",
    "            \"ext\": ext,\n",
    "        }],\n",
    "    )\n",
    "\n",
    "    chunks: List[Dict[str, Any]] = []\n",
    "    for local_idx, d in enumerate(lc_docs):\n",
    "        chunk_text = d.page_content\n",
    "        meta = d.metadata or {}\n",
    "\n",
    "        # Дополнительно можно сделать мягкий cut, если вдруг чанк вышел сильно длиннее\n",
    "        # чем 3 предложения — но пока оставим только ограничение по символам.\n",
    "        chunks.append(\n",
    "            {\n",
    "                \"chunk_id\": None,  # заполним позже глобальными id\n",
    "                \"doc_id\": meta.get(\"doc_id\", doc_id),\n",
    "                \"chunk_idx_in_doc\": local_idx,\n",
    "                \"path\": meta.get(\"path\", path),\n",
    "                \"ext\": meta.get(\"ext\", ext),\n",
    "                \"text\": chunk_text,\n",
    "                \"n_chars\": len(chunk_text),\n",
    "            }\n",
    "        )\n",
    "    return chunks\n",
    "\n",
    "\n",
    "all_chunks: List[Dict[str, Any]] = []\n",
    "\n",
    "if raw_docs_df is None or len(raw_docs_df) == 0:\n",
    "    print(\"raw_docs_df пуст — сначала нужно загрузить документы (Block 2).\")\n",
    "else:\n",
    "    print(\"Делаю чанкинг документов...\")\n",
    "    for _, row in tqdm(raw_docs_df.iterrows(), total=len(raw_docs_df)):\n",
    "        doc_chunks = split_doc_to_chunks(row)\n",
    "        all_chunks.extend(doc_chunks)\n",
    "\n",
    "    # присваиваем глобальные chunk_id\n",
    "    for idx, ch in enumerate(all_chunks):\n",
    "        ch[\"chunk_id\"] = idx\n",
    "\n",
    "    print(f\"Всего чанков: {len(all_chunks)}\")\n",
    "\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "\n",
    "print(\"Размер chunks_df:\", chunks_df.shape)\n",
    "print(chunks_df[[\"chunk_id\", \"doc_id\", \"chunk_idx_in_doc\", \"n_chars\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# эмбединги чанков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T23:04:34.788556Z",
     "iopub.status.busy": "2025-11-14T23:04:34.787895Z",
     "iopub.status.idle": "2025-11-14T23:04:35.053302Z",
     "shell.execute_reply": "2025-11-14T23:04:35.052532Z",
     "shell.execute_reply.started": "2025-11-14T23:04:34.788527Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 107ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install faiss-cpu\n",
    "\n",
    "'''по факту нужен gpu но на кагле он сдохнет с зависимостями и не может установиться'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю embedding-модель: intfloat/multilingual-e5-large\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc77c2c8804ad78c2728b085e75089",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a230d29edec494c8bbc17bfa1008cc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e941e3aebaf641c8bbc9274d6e9a1f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994cee9439f04227a9f918db8385617b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544aad8efdb84c1b8889a47f27b59202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e14db446e0e479797774741f031f080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb748a77d1354afab4d77a48899842e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1879373ac6e2404cb2ee017388c9bafc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe607ab0a89c4851bcb2535857a45f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51291bb15ed40939dbd61c221bf3077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBED_DIM: 1024\n",
      "embed_texts() готова к использованию\n"
     ]
    }
   ],
   "source": [
    "# === Block 4A. Инициализация эмбеддинг-модели и helper-функции ==============\n",
    "# Здесь выбираем модель эмбеддингов.\n",
    "# По умолчанию: intfloat/multilingual-e5-large (сильная мультиязычная модель).\n",
    "#\n",
    "# Можно переопределить через ENV:\n",
    "#   os.environ[\"RAG_EMBED_MODEL\"] = \"intfloat/multilingual-e5-base\"\n",
    "# или просто поменять EMBEDDING_MODEL_NAME ниже.\n",
    "\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss  # сам индекс будем делать в следующем блоке\n",
    "\n",
    "# --- выбор модели ---\n",
    "EMBEDDING_MODEL_NAME = os.getenv(\n",
    "    \"RAG_EMBED_MODEL\",\n",
    "    \"intfloat/multilingual-e5-large\",  # дефолт: мощная мультиязычная e5\n",
    ").strip()\n",
    "\n",
    "print(\"Загружаю embedding-модель:\", EMBEDDING_MODEL_NAME)\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=str(DEVICE))\n",
    "\n",
    "# Проверим размерность эмбеддингов на одном примере\n",
    "test_emb = embed_model.encode([\"test\"], convert_to_numpy=True, show_progress_bar=False)\n",
    "EMBED_DIM = int(test_emb.shape[1])\n",
    "print(\"EMBED_DIM:\", EMBED_DIM)\n",
    "\n",
    "\n",
    "def embed_texts(texts, batch_size: int = 64):\n",
    "    \"\"\"\n",
    "    texts — список строк\n",
    "    возвращает numpy-массив размера [len(texts), EMBED_DIM] (float32, L2-нормированный)\n",
    "    \"\"\"\n",
    "    emb = embed_model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        normalize_embeddings=True,  # сразу L2-нормализация → удобно для cosine/IP в FAISS\n",
    "    )\n",
    "    # FAISS любит float32\n",
    "    return emb.astype(\"float32\")\n",
    "\n",
    "\n",
    "print(\"embed_texts() готова к использованию\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Embedding models cheat sheet ===\n",
    "# Английский (рекомендуется для текущего конкурса):\n",
    "#   1) \"BAAI/bge-large-en-v1.5\"   # максимум качества, 1024-dim\n",
    "#   2) \"BAAI/bge-base-en-v1.5\"    # чуть легче, 768-dim\n",
    "#   3) \"BAAI/bge-small-en-v1.5\"   # быстрый, 384-dim, всё ещё лучше MiniLM\n",
    "#   4) \"thenlper/gte-large\"\n",
    "#   5) \"thenlper/gte-base\"\n",
    "#   6) \"jinaai/jina-embeddings-v2-base-en\"\n",
    "#   7) \"intfloat/e5-large-v2\"\n",
    "#   8) \"intfloat/e5-base-v2\"\n",
    "#\n",
    "# Мультиязычные / RU+EN:\n",
    "#   9)  \"BAAI/bge-m3\"\n",
    "#   10) \"intfloat/multilingual-e5-large\"\n",
    "#   11) \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "\n",
    "# EMBEDDING_MODEL_NAME = \"BAAI/bge-base-en-v1.5\"  # ← МОЖНО МЕНЯТЬ ЗДЕСЬ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Считаю эмбеддинги для чанков...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69980c31530e403092c208f31e220a6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1bb04d1353427b93fcd147ec97971c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ec21af05edd410894c179dbe2ae907e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c4af4bd1d94cd8ad7d0399247669ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308b0d8c0baf471393b5e27b3762d143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da298c26b4e490a9c2efa0c2afe0c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff5353a632d481fb31a2eb74118a022",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b73cd7e2644563a187a9e303a7c461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52a56c45a864671ab87a99403e667c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee6ba469049412d878da43b62ccf49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cd0e6f666ea48fbac74a68a2f5e8b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf464949e7f4ab085efb2c493aa0a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51865c2b771546bda96bcffa1084add3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206cc5132cf44cf3bc9f5a33f024996a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8d388338dc54f40b5035e7eebb563f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "529afa4d4b6348749dee03bf3ae9afc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbe249951544749968914d160a5d622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43e4c452b0634af0997cc05c544ab419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2a952499764a6490ef4b65077e7a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af87a2e9e174928967c531455bad4fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69357a2b5c464a74bd2824103311ba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08ef37c8e3e4abea9ada764c915fdbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6fd766f8c7b4f8abc6b97a0c41957d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59c741dcdcad431b88530546fde13daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d5969446743f89efd9a9eb7d76698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9013116fe8341838d2c84c4e2b4cbf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678476a223984c0c85d683aeb7726d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b93b0376994a0ba3f963022859ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9441f2993904c9ab64c2e862e1eff26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce5fb1f40ee4f9dab5f8c52ee54f2b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae61562cf75149cba55cfbf5cdf1f464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d80e99e808464d891774139c3b545b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44e255daefc4b188b5de57c26c96f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05098381f61d4963bb95ec8f7f77e9ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6f6ca1a75546aaa0456f48eac700a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb6639c880944b808292b82c0d61e311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b02b2e799378415a94c0376114b6bf48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bb1c49798a46869e60255ef3929fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09399783985c470ba459250b68285fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90dfcf4625e34a74a7a1beab8ad11032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a4f3d130234f54bdfbb98d1b672edf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Форма embeddings_np: (10007, 1024)\n",
      "FAISS-индекс построен.\n",
      "Количество векторов в индексе: 10007\n"
     ]
    }
   ],
   "source": [
    "# === Block 4B. Эмбеддинги чанков + построение FAISS-индекса ==================\n",
    "# Требует:\n",
    "#   - chunks_df (из Block 3)\n",
    "#   - embed_texts() и EMBED_DIM (из Block 4A)\n",
    "#\n",
    "# Результат:\n",
    "#   - embeddings_np: np.ndarray [num_chunks, EMBED_DIM]\n",
    "#   - faiss_index:   FAISS IndexFlatIP с L2-нормированными эмбеддингами\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if \"chunks_df\" not in globals() or chunks_df is None or len(chunks_df) == 0:\n",
    "    print(\"chunks_df пуст — сначала нужно выполнить блоки с загрузкой и чанкингом (2, 3).\")\n",
    "else:\n",
    "    print(\"Считаю эмбеддинги для чанков...\")\n",
    "\n",
    "    texts = chunks_df[\"text\"].astype(str).tolist()\n",
    "    # батчами на всякий случай\n",
    "    batch_size = 256\n",
    "    embs_list = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i : i + batch_size]\n",
    "        embs = embed_texts(batch)  # уже float32 и L2-normalized\n",
    "        embs_list.append(embs)\n",
    "    embeddings_np = np.vstack(embs_list).astype(\"float32\")\n",
    "\n",
    "    print(\"Форма embeddings_np:\", embeddings_np.shape)\n",
    "\n",
    "    # FAISS: IndexFlatIP по L2-нормированным векторам = косинусная близость\n",
    "    index_flat = faiss.IndexFlatIP(EMBED_DIM)\n",
    "    index_flat.add(embeddings_np)\n",
    "\n",
    "    faiss_index = index_flat  # <- ВАЖНО: глобальное имя, которое ищет Block 5B\n",
    "\n",
    "    print(\"FAISS-индекс построен.\")\n",
    "    print(\"Количество векторов в индексе:\", faiss_index.ntotal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAISS + BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T23:07:25.339526Z",
     "iopub.status.busy": "2025-11-14T23:07:25.338727Z",
     "iopub.status.idle": "2025-11-14T23:07:25.738373Z",
     "shell.execute_reply": "2025-11-14T23:07:25.737603Z",
     "shell.execute_reply.started": "2025-11-14T23:07:25.339503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n",
      "\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m                                               \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!uv pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting docopt>=0.6 (from pymorphy2)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=079539095ff15ad0e10b7f586a1a376c3a9ec040448202bb22784ab2c876946d\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pip] installing pymorphy2 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] pymorphy2 version: 0.9.1\n",
      "[pip] installing nltk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] nltk version: 3.9.2\n",
      "[nltk] скачиваю wordnet + omw-1.4 ...\n",
      "[ok] wordnet скачан\n"
     ]
    }
   ],
   "source": [
    "# === Setup: корректная установка pymorphy2 + nltk для BM25-лемматизации ======\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def pip_install(pkg: str):\n",
    "    print(f\"[pip] installing {pkg} ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", pkg])\n",
    "\n",
    "# --- pymorphy2 ---\n",
    "try:\n",
    "    importlib.import_module(\"pymorphy2\")\n",
    "    print(\"[ok] pymorphy2 уже установлен\")\n",
    "except ImportError:\n",
    "    pip_install(\"pymorphy2\")\n",
    "    import pymorphy2  # проверяем импорт\n",
    "    print(\"[ok] pymorphy2 version:\", pymorphy2.__version__)\n",
    "\n",
    "# --- nltk ---\n",
    "try:\n",
    "    importlib.import_module(\"nltk\")\n",
    "    print(\"[ok] nltk уже установлен\")\n",
    "except ImportError:\n",
    "    pip_install(\"nltk\")\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(\"[ok] nltk version:\", nltk.__version__)\n",
    "\n",
    "# --- wordnet для лемматизации английского ---\n",
    "try:\n",
    "    _ = wordnet.synsets(\"test\")\n",
    "    print(\"[ok] wordnet уже доступен\")\n",
    "except LookupError:\n",
    "    print(\"[nltk] скачиваю wordnet + omw-1.4 ...\")\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "    nltk.download(\"omw-1.4\", quiet=True)\n",
    "    print(\"[ok] wordnet скачан\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] nltk уже установлен\n",
      "[ok] nltk version: 3.9.2\n",
      "[ok] wordnet уже доступен\n"
     ]
    }
   ],
   "source": [
    "# === Setup: установка nltk + wordnet для EN-лемматизации =====================\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib\n",
    "\n",
    "def pip_install(pkg: str):\n",
    "    print(f\"[pip] installing {pkg} ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-qU\", pkg])\n",
    "\n",
    "# --- nltk ---\n",
    "try:\n",
    "    importlib.import_module(\"nltk\")\n",
    "    print(\"[ok] nltk уже установлен\")\n",
    "except ImportError:\n",
    "    pip_install(\"nltk\")\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "print(\"[ok] nltk version:\", nltk.__version__)\n",
    "\n",
    "# --- wordnet для лемматизации английского ---\n",
    "try:\n",
    "    _ = wordnet.synsets(\"test\")\n",
    "    print(\"[ok] wordnet уже доступен\")\n",
    "except LookupError:\n",
    "    print(\"[nltk] скачиваю wordnet + omw-1.4 ...\")\n",
    "    nltk.download(\"wordnet\", quiet=True)\n",
    "    nltk.download(\"omw-1.4\", quiet=True)\n",
    "    print(\"[ok] wordnet скачан\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[bm25] pymorphy2 установлен, но не совместим с текущим Python; RU-лемма отключена: module 'inspect' has no attribute 'getargspec'\n",
      "[bm25] NLTK WordNetLemmatizer готов, будет лемматизация английского текста\n",
      "Готовлю корпус для BM25 по чанкам...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be91584caa54a58a6bb60a082381d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10007 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Строю BM25Okapi...\n",
      "BM25-индекс готов\n",
      "Количество документов в BM25: 10007\n"
     ]
    }
   ],
   "source": [
    "# === Block 5A. Построение BM25-индекса по чанкам (безопасная RU+EN лемма) ====\n",
    "import re\n",
    "from typing import List\n",
    "from rank_bm25 import BM25Okapi\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- попытка подключить лемматизатор для русского ---\n",
    "_morph_ru = None\n",
    "try:\n",
    "    import pymorphy2\n",
    "    try:\n",
    "        _morph_ru = pymorphy2.MorphAnalyzer()\n",
    "        print(\"[bm25] pymorphy2 найден и инициализирован, будет лемматизация русского текста\")\n",
    "    except Exception as e:\n",
    "        print(\"[bm25] pymorphy2 установлен, но не совместим с текущим Python; RU-лемма отключена:\", e)\n",
    "        _morph_ru = None\n",
    "except ImportError:\n",
    "    print(\"[bm25] pymorphy2 не найден, русская лемматизация отключена\")\n",
    "\n",
    "# --- попытка подключить лемматизатор для английского через NLTK ---\n",
    "_morph_en = None\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "\n",
    "    _morph_en = WordNetLemmatizer()\n",
    "    try:\n",
    "        _ = wordnet.synsets(\"test\")\n",
    "    except LookupError:\n",
    "        # если вдруг setup-ячейка не была запущена\n",
    "        print(\"[bm25] wordnet не найден, пробую скачать...\")\n",
    "        nltk.download(\"wordnet\", quiet=True)\n",
    "        nltk.download(\"omw-1.4\", quiet=True)\n",
    "    print(\"[bm25] NLTK WordNetLemmatizer готов, будет лемматизация английского текста\")\n",
    "except Exception as e:\n",
    "    print(\"[bm25] NLTK/WordNet недоступны, английская лемматизация отключена:\", e)\n",
    "    _morph_en = None\n",
    "\n",
    "LAT_RE   = re.compile(r\"[a-zA-Z]\")\n",
    "CYR_RE   = re.compile(r\"[а-яА-ЯёЁ]\")\n",
    "SPLIT_RE = re.compile(r\"[^a-zA-Z0-9а-яА-ЯёЁ]+\")\n",
    "\n",
    "\n",
    "def _lemmatize_token(token: str) -> str:\n",
    "    \"\"\"\n",
    "    Лемматизация токена:\n",
    "      - русские слова → pymorphy2 (если получилось инициализировать)\n",
    "      - английские слова → NLTK WordNetLemmatizer (если доступен)\n",
    "      - остальное → просто lowercase\n",
    "    \"\"\"\n",
    "    t = token.lower()\n",
    "    if not t:\n",
    "        return t\n",
    "\n",
    "    # русские токены → pymorphy2\n",
    "    if CYR_RE.search(t) and _morph_ru is not None:\n",
    "        try:\n",
    "            return _morph_ru.parse(t)[0].normal_form\n",
    "        except Exception:\n",
    "            return t\n",
    "\n",
    "    # английские токены → WordNetLemmatizer\n",
    "    if LAT_RE.search(t) and _morph_en is not None:\n",
    "        try:\n",
    "            return _morph_en.lemmatize(t)\n",
    "        except Exception:\n",
    "            return t\n",
    "\n",
    "    # всё остальное — просто lowercase\n",
    "    return t\n",
    "\n",
    "\n",
    "def bm25_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Токенайзер для BM25:\n",
    "      - lower()\n",
    "      - сплит по небуквенным символам\n",
    "      - фильтрация пустых\n",
    "      - лемматизация RU/EN, если реально работает\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    raw_tokens = SPLIT_RE.split(text)\n",
    "    raw_tokens = [t for t in raw_tokens if t]\n",
    "\n",
    "    tokens = [_lemmatize_token(t) for t in raw_tokens]\n",
    "    # при желании можно выкинуть совсем короткие токены:\n",
    "    # tokens = [t for t in tokens if len(t) > 1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "if \"chunks_df\" not in globals() or chunks_df is None or len(chunks_df) == 0:\n",
    "    print(\"chunks_df пуст — нужно выполнить блоки с загрузкой и чанкингом (2, 3).\")\n",
    "else:\n",
    "    print(\"Готовлю корпус для BM25 по чанкам...\")\n",
    "\n",
    "    bm25_texts = chunks_df[\"text\"].astype(str).tolist()\n",
    "    bm25_corpus_tokens = [bm25_tokenize(t) for t in tqdm(bm25_texts)]\n",
    "\n",
    "    # соответствие: позиция в BM25 ↔ chunk_id\n",
    "    bm25_chunk_ids = chunks_df[\"chunk_id\"].tolist()\n",
    "\n",
    "    print(\"Строю BM25Okapi...\")\n",
    "    bm25 = BM25Okapi(bm25_corpus_tokens)\n",
    "\n",
    "    print(\"BM25-индекс готов\")\n",
    "    print(\"Количество документов в BM25:\", len(bm25_corpus_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hybrid_search_one(), hybrid_search_batch() и hybrid_search_chunks() готовы к использованию\n"
     ]
    }
   ],
   "source": [
    "# === Block 5B. Гибридный поиск по чанкам: FAISS + BM25 =======================\n",
    "# Идея:\n",
    "#   - отдельно берём top-K по FAISS (dense)\n",
    "#   - отдельно берём top-K по BM25 (sparse)\n",
    "#   - объединяем кандидатов по chunk_id\n",
    "#   - нормируем скоры и считаем гибрид:\n",
    "#         hybrid = W_DENSE * dense_norm + W_SPARSE * bm25_norm\n",
    "#   - возвращаем top-K по hybrid\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- находим FAISS-индекс, построенный в Block 4B ---\n",
    "if \"faiss_index\" in globals():\n",
    "    _faiss_index = faiss_index\n",
    "elif \"index\" in globals():\n",
    "    _faiss_index = index\n",
    "else:\n",
    "    _faiss_index = None\n",
    "    print(\"[warn] FAISS индекс не найден (ni 'faiss_index', ni 'index'). Dense-поиск работать не будет.\")\n",
    "\n",
    "# --- конфиги ---\n",
    "TOP_K_DENSE  = int(os.getenv(\"RAG_TOPK_DENSE\",  \"40\"))  # сколько брать из FAISS\n",
    "TOP_K_SPARSE = int(os.getenv(\"RAG_TOPK_SPARSE\", \"40\"))  # сколько брать из BM25\n",
    "TOP_K_MERGED = int(os.getenv(\"RAG_TOPK_MERGED\", \"50\"))  # сколько оставить после слияния\n",
    "\n",
    "W_DENSE  = float(os.getenv(\"RAG_W_DENSE\",  \"0.6\"))\n",
    "W_SPARSE = float(os.getenv(\"RAG_W_SPARSE\", \"0.4\"))\n",
    "\n",
    "\n",
    "def _minmax_normalize(arr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Простая нормализация [min,max] -> [0,1]. Если все значения одинаковые или массив пустой — нули.\"\"\"\n",
    "    if arr is None or len(arr) == 0:\n",
    "        return np.zeros_like(arr)\n",
    "    a_min = float(np.min(arr))\n",
    "    a_max = float(np.max(arr))\n",
    "    if not np.isfinite(a_min) or not np.isfinite(a_max) or a_max <= a_min:\n",
    "        return np.zeros_like(arr)\n",
    "    return (arr - a_min) / (a_max - a_min)\n",
    "\n",
    "\n",
    "def dense_search(query: str):\n",
    "    \"\"\"Top-K кандидатов по FAISS (dense). Возвращает список (chunk_id, raw_score, norm_score).\"\"\"\n",
    "    if _faiss_index is None:\n",
    "        return []\n",
    "\n",
    "    q_text = query  # при желании сюда можно добавить e5-префикс \"query: \"\n",
    "\n",
    "    q_emb = embed_texts([q_text], batch_size=1)  # shape (1, EMBED_DIM), уже float32 и L2-normalized\n",
    "    D, I = _faiss_index.search(q_emb, TOP_K_DENSE)  # D: similarity, I: indices (совпадают с chunk_id)\n",
    "\n",
    "    scores = D[0]\n",
    "    ids = I[0]\n",
    "\n",
    "    scores_norm = _minmax_normalize(scores)\n",
    "\n",
    "    out = []\n",
    "    for cid, s, sn in zip(ids, scores, scores_norm):\n",
    "        cid = int(cid)\n",
    "        if cid < 0:\n",
    "            continue\n",
    "        out.append((cid, float(s), float(sn)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def sparse_search(query: str):\n",
    "    \"\"\"Top-K кандидатов по BM25. Возвращает список (chunk_id, raw_score, norm_score).\"\"\"\n",
    "    if \"bm25\" not in globals():\n",
    "        return []\n",
    "\n",
    "    tokens = bm25_tokenize(query)\n",
    "    scores = bm25.get_scores(tokens)  # shape: (N_docs,)\n",
    "    if scores.size == 0:\n",
    "        return []\n",
    "\n",
    "    top_k = min(TOP_K_SPARSE, scores.shape[0])\n",
    "    top_idx = np.argsort(scores)[-top_k:][::-1]  # от больших к меньшим\n",
    "    top_scores = scores[top_idx]\n",
    "    scores_norm = _minmax_normalize(top_scores)\n",
    "\n",
    "    out = []\n",
    "    for i, s, sn in zip(top_idx, top_scores, scores_norm):\n",
    "        # i — это позиция документа в bm25_corpus_tokens -> переводим в chunk_id\n",
    "        cid = int(bm25_chunk_ids[int(i)])\n",
    "        out.append((cid, float(s), float(sn)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def hybrid_search_one(qid: int, query: str):\n",
    "    \"\"\"\n",
    "    Гибридный поиск для одного запроса.\n",
    "    Возвращает список dict'ов по кандидатным чанкам.\n",
    "    \"\"\"\n",
    "    dense = dense_search(query)\n",
    "    sparse = sparse_search(query)\n",
    "\n",
    "    dense_dict = {cid: (s_raw, s_norm) for cid, s_raw, s_norm in dense}\n",
    "    sparse_dict = {cid: (s_raw, s_norm) for cid, s_raw, s_norm in sparse}\n",
    "\n",
    "    candidates = set(dense_dict.keys()) | set(sparse_dict.keys())\n",
    "\n",
    "    rows = []\n",
    "    for cid in candidates:\n",
    "        d_raw, d_norm = dense_dict.get(cid, (0.0, 0.0))\n",
    "        s_raw, s_norm = sparse_dict.get(cid, (0.0, 0.0))\n",
    "\n",
    "        hybrid = W_DENSE * d_norm + W_SPARSE * s_norm\n",
    "\n",
    "        rows.append({\n",
    "            \"query_id\": int(qid),\n",
    "            \"query\": str(query),\n",
    "            \"chunk_id\": int(cid),\n",
    "            \"dense_score\": float(d_raw),\n",
    "            \"dense_norm\": float(d_norm),\n",
    "            \"bm25_score\": float(s_raw),\n",
    "            \"bm25_norm\": float(s_norm),\n",
    "            \"hybrid_score\": float(hybrid),\n",
    "        })\n",
    "\n",
    "    rows_sorted = sorted(rows, key=lambda r: r[\"hybrid_score\"], reverse=True)[:TOP_K_MERGED]\n",
    "    return rows_sorted\n",
    "\n",
    "\n",
    "def hybrid_search_batch(queries, query_ids=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Запуск гибридного поиска по батчу запросов.\n",
    "    queries: список строк\n",
    "    query_ids: список id (если None, берём 0..len-1)\n",
    "    \"\"\"\n",
    "    if query_ids is None:\n",
    "        query_ids = list(range(len(queries)))\n",
    "\n",
    "    all_rows = []\n",
    "    for qid, q in zip(query_ids, queries):\n",
    "        all_rows.extend(hybrid_search_one(int(qid), str(q)))\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    print(\"[hybrid] queries:\", len(queries), \"| rows:\", len(df))\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- совместимый wrapper для старого API: hybrid_search_chunks --------------\n",
    "\n",
    "# --- совместимый wrapper для старого API: hybrid_search_chunks --------------\n",
    "\n",
    "def hybrid_search_chunks(\n",
    "    query: str,\n",
    "    top_k: int | None = None,\n",
    "    top_k_faiss: int | None = None,\n",
    "    top_k_bm25: int | None = None,\n",
    "    top_k_merged: int | None = None,\n",
    "    **kwargs,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Backwards-compatible wrapper, чтобы старый код (retrieve_relevant_chunks и тесты)\n",
    "    могли вызывать hybrid_search_chunks() с аргументами:\n",
    "        - top_k_faiss\n",
    "        - top_k_bm25\n",
    "        - top_k_merged\n",
    "        - top_k\n",
    "    Сейчас:\n",
    "      - для простоты мы используем нашу hybrid_search_batch() как есть,\n",
    "        а затем (если указано) обрезаем по top_k или top_k_merged.\n",
    "      - top_k_faiss и top_k_bm25 здесь игнорируем (но принимаем, чтобы не падать).\n",
    "    \"\"\"\n",
    "    df = hybrid_search_batch([query], query_ids=[0])\n",
    "\n",
    "    # если передали top_k_merged — считаем его основным лимитом\n",
    "    if top_k_merged is not None and \"hybrid_score\" in df.columns:\n",
    "        df = df.sort_values(\"hybrid_score\", ascending=False).head(int(top_k_merged))\n",
    "    # иначе используем top_k, если он задан\n",
    "    elif top_k is not None and \"hybrid_score\" in df.columns:\n",
    "        df = df.sort_values(\"hybrid_score\", ascending=False).head(int(top_k))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "print(\"hybrid_search_one(), hybrid_search_batch() и hybrid_search_chunks() готовы к использованию\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# груз ллм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T23:08:10.615039Z",
     "iopub.status.busy": "2025-11-14T23:08:10.614403Z",
     "iopub.status.idle": "2025-11-14T23:08:26.233284Z",
     "shell.execute_reply": "2025-11-14T23:08:26.232033Z",
     "shell.execute_reply.started": "2025-11-14T23:08:10.615013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_MODEL_ID: Qwen/Qwen2.5-1.5B-Instruct\n",
      "LOCAL_MODEL_DIR: /kaggle/working/models/Qwen_Qwen2.5-1.5B-Instruct\n",
      "Загружаю tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256e2020b4b14dd3a8cd8484e494c381",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db35e38d9b34eafbdf9d980278ade64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f826fdbf78bb4b4ab93b9604e699782a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e06703c11449f8872987f714f00eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю модель (это может занять время при первом запуске)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd07b5fe7464318845140ac6a8b888e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d7a33200fa467fbaa14d6115407e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cd25af46dba4605b2cbe2cf9105a6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/2352180011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m gen_pipe = pipeline(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         self.check_model_type(\n\u001b[1;32m    118\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;34m\"The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;34m\"discard the `device` argument when creating your pipeline object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."
     ]
    }
   ],
   "source": [
    "'''# === Block 6A. Загрузка LLM с Hugging Face (Qwen2.5-1.5B-Instruct по умолчанию) ===\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Hugging Face ID модели по умолчанию\n",
    "DEFAULT_HF_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "# можно переопределить через переменную окружения, если захочешь\n",
    "HF_MODEL_ID = os.environ.get(\"RAG_LLM_ID\", DEFAULT_HF_MODEL_ID)\n",
    "\n",
    "# папка для локального кеша моделей (внутри Permanent)\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# отдельная подпапка под конкретную модель\n",
    "LOCAL_MODEL_DIR = MODELS_DIR / HF_MODEL_ID.replace(\"/\", \"_\")\n",
    "LOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"HF_MODEL_ID:\", HF_MODEL_ID)\n",
    "print(\"LOCAL_MODEL_DIR:\", LOCAL_MODEL_DIR)\n",
    "\n",
    "# при первом вызове from_pretrained модель скачается в LOCAL_MODEL_DIR,\n",
    "# дальше будет использовать локальный кеш, даже если интернет отвалится\n",
    "print(\"Загружаю tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    cache_dir=str(LOCAL_MODEL_DIR),\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Загружаю модель (это может занять время при первом запуске)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    cache_dir=str(LOCAL_MODEL_DIR),\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,   # на A100 это ок\n",
    "    device_map=\"auto\"           # автоматически раскидывает слои по доступным устройствам\n",
    ")\n",
    "\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"LLM загружена и готова к генерации.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF_MODEL_ID: Qwen/Qwen2.5-1.5B-Instruct\n",
      "LOCAL_MODEL_DIR: /workspace/models/Qwen_Qwen2.5-1.5B-Instruct\n",
      "Загружаю tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037b371f1322428a960d9a51c857efab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a279d95c49c54f9dafa376992e863a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b19d29f75c4efeacdefcf0f667c4b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da154b857694188a69e4f55fafe61dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружаю модель (это может занять время при первом запуске)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d6b6e9eebd4a3cb81d62d3283f7ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742ae0e3eaf14716b7159a91d466b35e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26972e2c3a614817ba063876144802e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM загружена и готова к генерации.\n"
     ]
    }
   ],
   "source": [
    "# === Block 6A. Загрузка LLM с Hugging Face (фиксанутая версия для Kaggle) ===\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "DEFAULT_HF_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "HF_MODEL_ID = os.environ.get(\"RAG_LLM_ID\", DEFAULT_HF_MODEL_ID)\n",
    "\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOCAL_MODEL_DIR = MODELS_DIR / HF_MODEL_ID.replace(\"/\", \"_\")\n",
    "LOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"HF_MODEL_ID:\", HF_MODEL_ID)\n",
    "print(\"LOCAL_MODEL_DIR:\", LOCAL_MODEL_DIR)\n",
    "\n",
    "print(\"Загружаю tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    cache_dir=str(LOCAL_MODEL_DIR),\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"Загружаю модель (это может занять время при первом запуске)...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    cache_dir=str(LOCAL_MODEL_DIR),\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",   # accelerate сам раскинет по девайсу\n",
    ")\n",
    "\n",
    "# ВАЖНО: НЕ передаём device=..., иначе будет тот самый ValueError\n",
    "gen_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"LLM загружена и готова к генерации.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HF_MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID, cache_dir=str(LOCAL_MODEL_DIR), trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    cache_dir=str(LOCAL_MODEL_DIR),\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HF_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T02:45:19.199445Z",
     "iopub.status.busy": "2025-11-15T02:45:19.198667Z",
     "iopub.status.idle": "2025-11-15T02:45:19.202733Z",
     "shell.execute_reply": "2025-11-15T02:45:19.202043Z",
     "shell.execute_reply.started": "2025-11-15T02:45:19.199420Z"
    }
   },
   "outputs": [],
   "source": [
    "''' HF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" '''\n",
    "\n",
    "'''1. LLaMA-3.1-8B-Instruct — лучший вариант\n",
    "\n",
    "HF ID: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "VRAM: 12–16 GB (fp16), 8–10 GB (fp8/GPTQ)\n",
    "Почему выбрать:\n",
    "\n",
    "стабильная, ровная, очень сильная на английском;\n",
    "\n",
    "работает лучше Qwen-2.5-7B в reasoning и factual QA;\n",
    "\n",
    "идеально подходит для RAG, умеет следовать структуре.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HF_MODEL_ID = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "'''\n",
    "\n",
    "'''2. Qwen2.5-14B-Instruct — сильнее 7B почти во всём\n",
    "\n",
    "HF ID: Qwen/Qwen2.5-14B-Instruct\n",
    "VRAM: 28–32 GB (fp16), ~18 GB (AWQ / GPTQ)\n",
    "Плюсы:\n",
    "\n",
    "мощная reasoning-модель;\n",
    "\n",
    "отличное качество ответов + хорошая устойчивость к галлюцинациям;\n",
    "\n",
    "один из лучших вариантов под англоязычный RAG.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HF_MODEL_ID = \"mistralai/Mistral-Nemo-12B-Instruct\"\n",
    "'''\n",
    "\n",
    "'''3. Mistral-NeMo-12B-Instruct — очень хороший баланс\n",
    "\n",
    "HF ID: mistralai/Mistral-Nemo-12B-Instruct\n",
    "VRAM: 14–18 GB (fp16)\n",
    "Плюсы:\n",
    "\n",
    "сильный reasoning;\n",
    "\n",
    "хорошая работа с длинными документами;\n",
    "\n",
    "быстрый инференс.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''HF_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-GPTQ\"\n",
    "'''\n",
    "\n",
    "'''4. DeepSeek-R1-Distill-LLaMA-70B (GPTQ) — почти GPT-4 уровень\n",
    "\n",
    "HF ID:\n",
    "deepseek-ai/DeepSeek-R1-Distill-Llama-70B-GPTQ\n",
    "или\n",
    "unsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit\n",
    "\n",
    "VRAM: 18–28 GB (4-bit)\n",
    "Плюсы:\n",
    "\n",
    "супер-reasoning, очень аккуратные аргументы;\n",
    "\n",
    "минимальные галлюцинации;\n",
    "\n",
    "идеальна для RAG c длинными документами.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T02:49:13.653728Z",
     "iopub.status.busy": "2025-11-15T02:49:13.653159Z",
     "iopub.status.idle": "2025-11-15T02:49:13.658482Z",
     "shell.execute_reply": "2025-11-15T02:49:13.657665Z",
     "shell.execute_reply.started": "2025-11-15T02:49:13.653695Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Для GPTQ/AWQ:'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Параметры загрузки (копировать в Block 6A)'''\n",
    "\n",
    "'''from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "HF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"   # ← меняешь ID здесь\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "'''\n",
    "\n",
    "'''Для GPTQ/AWQ:'''\n",
    "'''model = AutoModelForCausalLM.from_pretrained(\n",
    "    HF_MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "'''\n",
    "\n",
    "'''Что делать, если не знаешь, какую модель выбрать:\n",
    "\n",
    "Сначала пробуй LLaMA-3.1-8B-Instruct\n",
    "— лучший «универсал» под RAG.\n",
    "\n",
    "Если остаётся VRAM → переходи на Qwen2.5-14B (GPTQ).\n",
    "\n",
    "Если есть реальный запас VRAM → DeepSeek-R1-70B (GPTQ).\n",
    "\n",
    "Если интернет медленный: бери Qwen2.5-7B-Instruct (самый лёгкий скачиваемый сильный вариант).'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "функция генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Функция generate_answer() определена.\n"
     ]
    }
   ],
   "source": [
    "# === Block 6B. Функция generate_answer() поверх gen_pipe ===\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "def generate_answer(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    do_sample: bool = True,\n",
    "    stop_token: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Обёртка над gen_pipe.\n",
    "    На вход: готовый prompt (обычно system+context+question).\n",
    "    На выход: одна строка с ответом модели.\n",
    "    \"\"\"\n",
    "    # gen_pipe возвращает список объектов вида:\n",
    "    # [{\"generated_text\": \"...\"}]\n",
    "    outputs = gen_pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=do_sample,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    full_text = outputs[0][\"generated_text\"]\n",
    "\n",
    "    # Многие модели возвращают \"prompt + continuation\".\n",
    "    # Простейший способ отделить — отрезать префикс prompt, если он совпадает.\n",
    "    if full_text.startswith(prompt):\n",
    "        answer = full_text[len(prompt):]\n",
    "    else:\n",
    "        answer = full_text\n",
    "\n",
    "    # Если задан stop_token — обрезаем по нему (например, \"\\n\\nUser:\" или т.п.)\n",
    "    if stop_token is not None and stop_token in answer:\n",
    "        answer = answer.split(stop_token, 1)[0]\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "print(\"Функция generate_answer() определена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# вытаскивание релевантных чанков и сбор промта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieve_relevant_chunks() перезаписана и использует hybrid_search_chunks + chunks_df.\n"
     ]
    }
   ],
   "source": [
    "# === Block 7A. retrieve_relevant_chunks: от гибридного поиска к контексту ====\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Настройки по умолчанию (можно переопределить через ENV)\n",
    "CTX_TOPK_MERGED = int(os.getenv(\"RAG_CTX_TOPK_MERGED\", \"50\"))   # сколько кандидатов брать из hybrid_search\n",
    "CTX_MAX_CHUNKS  = int(os.getenv(\"RAG_CTX_MAX_CHUNKS\",  \"10\"))   # сколько чанков максимум в контексте\n",
    "CTX_MAX_CHARS   = int(os.getenv(\"RAG_CTX_MAX_CHARS\",  \"8000\"))  # максимум символов в общем контексте\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(\n",
    "    query: str,\n",
    "    ctx_topk_merged: int | None = None,\n",
    "    ctx_max_chunks: int | None = None,\n",
    "    ctx_max_chars: int | None = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Высокоуровневый ретривер:\n",
    "    1) hybrid_search_chunks(query, ...) → кандидаты (FAISS + BM25 гибрид)\n",
    "    2) join с chunks_df по chunk_id → добавляем текст, doc_id и пр.\n",
    "    3) сортируем по hybrid_score и набираем контекст до лимитов:\n",
    "       - не больше ctx_max_chunks чанков\n",
    "       - не больше ctx_max_chars символов суммарно\n",
    "    Возвращает:\n",
    "      {\n",
    "        \"context_text\": <str>,\n",
    "        \"chunks\": [\n",
    "            {\n",
    "                \"chunk_id\": int,\n",
    "                \"doc_id\": int | None,\n",
    "                \"score_hybrid\": float,\n",
    "                \"score_dense\": float | None,\n",
    "                \"score_bm25\": float | None,\n",
    "            },\n",
    "            ...\n",
    "        ],\n",
    "        \"df\": <DataFrame с подробностями>  # опционально, для дебага\n",
    "      }\n",
    "    \"\"\"\n",
    "    if \"chunks_df\" not in globals() or chunks_df is None or len(chunks_df) == 0:\n",
    "        raise RuntimeError(\"chunks_df is empty. Run Blocks 2–3 (loading + chunking) first.\")\n",
    "\n",
    "    # применяем дефолты, если не заданы\n",
    "    if ctx_topk_merged is None:\n",
    "        ctx_topk_merged = CTX_TOPK_MERGED\n",
    "    if ctx_max_chunks is None:\n",
    "        ctx_max_chunks = CTX_MAX_CHUNKS\n",
    "    if ctx_max_chars is None:\n",
    "        ctx_max_chars = CTX_MAX_CHARS\n",
    "\n",
    "    # 1) гибридный поиск кандидатов (FAISS + BM25)\n",
    "    # wrapper hybrid_search_chunks уже совместим с legacy-аргументами\n",
    "    hybrid_df = hybrid_search_chunks(\n",
    "        query,\n",
    "        top_k_merged=ctx_topk_merged,\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    if hybrid_df is None or len(hybrid_df) == 0:\n",
    "        # ничего не нашли\n",
    "        return {\n",
    "            \"context_text\": \"\",\n",
    "            \"chunks\": [],\n",
    "            \"df\": hybrid_df,\n",
    "        }\n",
    "\n",
    "    # 2) join с chunks_df по chunk_id\n",
    "    # гарантируем, что chunk_id в обоих фреймах числовой\n",
    "    hdf = hybrid_df.copy()\n",
    "    hdf[\"chunk_id\"] = hdf[\"chunk_id\"].astype(int)\n",
    "\n",
    "    cdf = chunks_df.copy()\n",
    "    if \"chunk_id\" not in cdf.columns:\n",
    "        raise RuntimeError(\"chunks_df does not contain 'chunk_id' column.\")\n",
    "    # убедимся, что есть doc_id; если нет — создадим заглушку\n",
    "    if \"doc_id\" not in cdf.columns:\n",
    "        cdf[\"doc_id\"] = -1\n",
    "\n",
    "    cdf[\"chunk_id\"] = cdf[\"chunk_id\"].astype(int)\n",
    "\n",
    "    merged = hdf.merge(\n",
    "        cdf[[\"chunk_id\", \"doc_id\", \"text\"]],\n",
    "        on=\"chunk_id\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # 3) сортировка и набор контекста\n",
    "    if \"hybrid_score\" in merged.columns:\n",
    "        merged = merged.sort_values(\"hybrid_score\", ascending=False)\n",
    "    elif \"ce_score\" in merged.columns:\n",
    "        merged = merged.sort_values(\"ce_score\", ascending=False)\n",
    "\n",
    "    context_parts = []\n",
    "    used_chars = 0\n",
    "    selected_rows = []\n",
    "\n",
    "    for _, row in merged.iterrows():\n",
    "        text = str(row.get(\"text\", \"\")).strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        n_chars = len(text)\n",
    "        if used_chars + n_chars > ctx_max_chars and used_chars > 0:\n",
    "            # если уже что-то набрали и следующий чанк сильно вылезает за предел —\n",
    "            # выходим (чтобы не делать слишком длинный контекст)\n",
    "            break\n",
    "\n",
    "        context_parts.append(text)\n",
    "        used_chars += n_chars\n",
    "        selected_rows.append(row)\n",
    "\n",
    "        if len(selected_rows) >= ctx_max_chunks:\n",
    "            break\n",
    "\n",
    "    # если ничего не удалось добавить (например, все тексты пустые)\n",
    "    if not context_parts:\n",
    "        return {\n",
    "            \"context_text\": \"\",\n",
    "            \"chunks\": [],\n",
    "            \"df\": merged,\n",
    "        }\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "\n",
    "    # 4) собираем компактный список чанков с нужными полями\n",
    "    chunks_meta = []\n",
    "    for row in selected_rows:\n",
    "        cid = int(row.get(\"chunk_id\"))\n",
    "        did_raw = row.get(\"doc_id\")\n",
    "        try:\n",
    "            did = int(did_raw) if pd.notna(did_raw) else None\n",
    "        except Exception:\n",
    "            did = None\n",
    "\n",
    "        chunks_meta.append(\n",
    "            {\n",
    "                \"chunk_id\": cid,\n",
    "                \"doc_id\": did,\n",
    "                \"score_hybrid\": float(row.get(\"hybrid_score\", 0.0)),\n",
    "                \"score_dense\": float(row.get(\"dense_score\", 0.0))\n",
    "                if \"dense_score\" in row\n",
    "                else None,\n",
    "                \"score_bm25\": float(row.get(\"bm25_score\", 0.0))\n",
    "                if \"bm25_score\" in row\n",
    "                else None,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"context_text\": context_text,\n",
    "        \"chunks\": chunks_meta,\n",
    "        \"df\": merged,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"retrieve_relevant_chunks() перезаписана и использует hybrid_search_chunks + chunks_df.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''instruction = (\n",
    "    \"You are an expert in document analysis and RAG.\\n\"\n",
    "    \"Your task is to answer questions strictly based on the provided context.\\n\"\n",
    "    \"Answer in English, clearly and in a structured way.\\n\"\n",
    "    \"Do not invent facts that are not present in the context.\\n\"\n",
    "    \"If the information is insufficient, say so explicitly.\\n\\n\"\n",
    "    \"Answer format:\\n\"\n",
    "    \"1) SHORT ANSWER — 1–3 sentences, the main idea.\\n\"\n",
    "    \"2) REASONING — detailed explanation referring to key parts of the context (paraphrased).\\n\"\n",
    "    \"3) MISSING INFORMATION — what exactly cannot be answered from this context.\\n\"\n",
    ")'''\n",
    "\n",
    "'''\"Answer strictly in the following structure (no extra sections):\\n\"\n",
    "\"SHORT ANSWER:\\n\"\n",
    "\"- ...\\n\\n\"\n",
    "\"REASONING:\\n\"\n",
    "\"- ...\\n\\n\"\n",
    "\"MISSING INFORMATION:\\n\"\n",
    "\"- ...\\n\\n\"\n",
    "\"=== MODEL ANSWER ===\\n\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''в answer_question и rag_answer_with_context_and_refs вернуть:'''\n",
    "'''return answer.strip() вместо normalize_rag_answer(...).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def build_rag_prompt(\\n    query: str,\\n    context_text: str,\\n    instruction: str = None,\\n):\\n    \"\"\"\\n    Собирает финальный текстовый prompt для LLM.\\n    Ролевая инструкция + жёсткая структура ответа.\\n    Всё служебное — на английском, ответы тоже на английском.\\n    \"\"\"\\n    if instruction is None:\\n        instruction = (\\n        \"You are an expert in document analysis and RAG.\\n\"\\n        \"Your task is to answer questions strictly based on the provided context.\\n\"\\n        \"Answer in English, clearly and in a structured way.\\n\"\\n        \"Do not invent facts that are not present in the context.\\n\"\\n        \"If the information is insufficient, say so explicitly.\\n\\n\"\\n        \"Answer format:\\n\"\\n        \"1) SHORT ANSWER — 1–3 sentences, the main idea.\\n\"\\n        \"2) REASONING — detailed explanation referring to key parts of the context (paraphrased).\\n\"\\n        \"3) MISSING INFORMATION — what exactly cannot be answered from this context.\\n\"\\n)\\n\\n    prompt = (\\n        f\"{instruction}\\n\"\\n        \"=== CONTEXT ===\\n\"\\n        f\"{context_text}\\n\"\\n        \"=== QUESTION ===\\n\"\\n        f\"{query}\\n\"\\n        \"=== OUTPUT FORMAT INSTRUCTIONS ===\\n\"\\n        \"Respond strictly using the following structure (no extra sections):\\n\"\\n        \"SHORT ANSWER:\\n\"\\n        \"- ...\\n\\n\"\\n        \"REASONING:\\n\"\\n        \"- ...\\n\\n\"\\n        \"MISSING INFORMATION:\\n\"\\n        \"- ...\\n\\n\"\\n        \"=== MODEL ANSWER ===\\n\"\\n    )\\n    return prompt\\n\\n\\n\\ndef answer_question(\\n    query: str,\\n    max_new_tokens: int = 256,\\n    temperature: float = 0.2,\\n    top_p: float = 0.9,\\n    debug: bool = False,\\n):\\n    \"\"\"\\n    Высокоуровневая функция:\\n    1) ищет релевантные чанки (retrieve_relevant_chunks)\\n    2) собирает промпт (build_rag_prompt)\\n    3) генерирует ответ (generate_answer)\\n    \"\"\"\\n    retrieval = retrieve_relevant_chunks(query)\\n    context_text = retrieval[\"context_text\"]\\n    selected_chunks = retrieval[\"chunks\"]\\n\\n    if debug:\\n        print(\"=== DEBUG: выбрано чанков ===\", len(selected_chunks))\\n        for ch in selected_chunks:\\n            print(f\"- chunk_id={ch[\\'chunk_id\\']}, doc_id={ch[\\'doc_id\\']}, score={ch[\\'score_hybrid\\']:.4f}\")\\n\\n    if not context_text.strip():\\n        return (\\n            \"КРАТКИЙ ОТВЕТ:\\n\"\\n            \"- Я не нашёл релевантной информации в документах.\\n\\n\"\\n            \"ОБОСНОВАНИЕ:\\n\"\\n            \"- Гибридный поиск не вернул подходящих фрагментов, поэтому я не могу ответить на вопрос на основе контекста.\\n\\n\"\\n            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\\n            \"- Не хватает любых фрагментов документов, относящихся к заданному вопросу.\"\\n        )\\n\\n    prompt = build_rag_prompt(query, context_text)\\n\\n    answer = generate_answer(\\n    prompt,\\n    max_new_tokens=max_new_tokens,\\n    temperature=temperature,\\n    top_p=top_p,\\n    do_sample=True,\\n    )\\n\\n    return answer.strip()\\n\\n\\nprint(\"Обновлённые build_rag_prompt() и answer_question() определены.\")'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Block 7B (новая версия). Сбор промпта и answer_question() с чёткой структурой ===\n",
    "\n",
    "'''def build_rag_prompt(\n",
    "    query: str,\n",
    "    context_text: str,\n",
    "    instruction: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Собирает финальный текстовый prompt для LLM.\n",
    "    Ролевая инструкция + жёсткая структура ответа.\n",
    "    \"\"\"\n",
    "    if instruction is None:\n",
    "        instruction = (\n",
    "            \"Ты опытный эксперт по анализу документов и задачам RAG.\\n\"\n",
    "            \".\\n\"\n",
    "            \"Твоя задача — отвечать на вопросы строго на основе предоставленного контекста.\\n\"\n",
    "            \"Отвечай на русском языке, чётко и структурированно.\\n\"\n",
    "            \"Нельзя придумывать факты, которых нет в контексте.\\n\"\n",
    "            \"Если информации недостаточно, честно сообщи об этом.\\n\\n\"\n",
    "            \"Формат ответа:\\n\"\n",
    "            \"1) КРАТКИЙ ОТВЕТ — 1–3 предложения, самая суть.\\n\"\n",
    "            \"2) ОБОСНОВАНИЕ — подробное пояснение со ссылкой на ключевые фрагменты контекста (пересказ, а не дословная цитата).\\n\"\n",
    "            \"3) НЕДОСТАЮЩИЕ ДАННЫЕ — что именно невозможно ответить по этому контексту (если всё покрыто, напиши, что таких нет).\\n\"\n",
    "        )\n",
    "\n",
    "    prompt = (\n",
    "        f\"{instruction}\\n\"\n",
    "        \"=== КОНТЕКСТ ===\\n\"\n",
    "        f\"{context_text}\\n\"\n",
    "        \"=== ВОПРОС ===\\n\"\n",
    "        f\"{query}\\n\"\n",
    "        \"=== ИНСТРУКЦИЯ ПО ФОРМАТУ ВЫВОДА ===\\n\"\n",
    "        \"Ответ строго в следующей структуре (без лишних разделов):\\n\"\n",
    "        \"КРАТКИЙ ОТВЕТ:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"ОБОСНОВАНИЕ:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"=== ОТВЕТ МОДЕЛИ ===\\n\"\n",
    "    )\n",
    "    return prompt'''\n",
    "\"рабочая ячейка\"\n",
    "'''def build_rag_prompt(\n",
    "    query: str,\n",
    "    context_text: str,\n",
    "    instruction: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Собирает финальный текстовый prompt для LLM.\n",
    "    Ролевая инструкция + жёсткая структура ответа.\n",
    "    Всё служебное — на английском, ответы тоже на английском.\n",
    "    \"\"\"\n",
    "    if instruction is None:\n",
    "        instruction = (\n",
    "        \"You are an expert in document analysis and RAG.\\n\"\n",
    "        \"Your task is to answer questions strictly based on the provided context.\\n\"\n",
    "        \"Answer in English, clearly and in a structured way.\\n\"\n",
    "        \"Do not invent facts that are not present in the context.\\n\"\n",
    "        \"If the information is insufficient, say so explicitly.\\n\\n\"\n",
    "        \"Answer format:\\n\"\n",
    "        \"1) SHORT ANSWER — 1–3 sentences, the main idea.\\n\"\n",
    "        \"2) REASONING — detailed explanation referring to key parts of the context (paraphrased).\\n\"\n",
    "        \"3) MISSING INFORMATION — what exactly cannot be answered from this context.\\n\"\n",
    ")\n",
    "\n",
    "    prompt = (\n",
    "        f\"{instruction}\\n\"\n",
    "        \"=== CONTEXT ===\\n\"\n",
    "        f\"{context_text}\\n\"\n",
    "        \"=== QUESTION ===\\n\"\n",
    "        f\"{query}\\n\"\n",
    "        \"=== OUTPUT FORMAT INSTRUCTIONS ===\\n\"\n",
    "        \"Respond strictly using the following structure (no extra sections):\\n\"\n",
    "        \"SHORT ANSWER:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"REASONING:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"MISSING INFORMATION:\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"=== MODEL ANSWER ===\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "\n",
    "def answer_question(\n",
    "    query: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Высокоуровневая функция:\n",
    "    1) ищет релевантные чанки (retrieve_relevant_chunks)\n",
    "    2) собирает промпт (build_rag_prompt)\n",
    "    3) генерирует ответ (generate_answer)\n",
    "    \"\"\"\n",
    "    retrieval = retrieve_relevant_chunks(query)\n",
    "    context_text = retrieval[\"context_text\"]\n",
    "    selected_chunks = retrieval[\"chunks\"]\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== DEBUG: выбрано чанков ===\", len(selected_chunks))\n",
    "        for ch in selected_chunks:\n",
    "            print(f\"- chunk_id={ch['chunk_id']}, doc_id={ch['doc_id']}, score={ch['score_hybrid']:.4f}\")\n",
    "\n",
    "    if not context_text.strip():\n",
    "        return (\n",
    "            \"КРАТКИЙ ОТВЕТ:\\n\"\n",
    "            \"- Я не нашёл релевантной информации в документах.\\n\\n\"\n",
    "            \"ОБОСНОВАНИЕ:\\n\"\n",
    "            \"- Гибридный поиск не вернул подходящих фрагментов, поэтому я не могу ответить на вопрос на основе контекста.\\n\\n\"\n",
    "            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n",
    "            \"- Не хватает любых фрагментов документов, относящихся к заданному вопросу.\"\n",
    "        )\n",
    "\n",
    "    prompt = build_rag_prompt(query, context_text)\n",
    "\n",
    "    answer = generate_answer(\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    do_sample=True,\n",
    "    )\n",
    "\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "print(\"Обновлённые build_rag_prompt() и answer_question() определены.\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build_llm_messages() и answer_question() (seminar9-style + pipeline autodetect) определены.\n"
     ]
    }
   ],
   "source": [
    "# === Block 7B. build_llm_messages + answer_question (seminar9-style, fixed) ==\n",
    "\n",
    "SYSTEM_PROMPT_EN = (\n",
    "    \"You are an expert in document analysis and retrieval-augmented generation (RAG). \"\n",
    "    \"You receive a CONTEXT (fragments from documents) and a QUESTION. \"\n",
    "    \"Your task is to answer the question as accurately and honestly as possible, \"\n",
    "    \"STRICTLY based on the provided context.\\n\"\n",
    "    \"Do not add information that is not supported by the context. \"\n",
    "    \"If the context is not sufficient, say so explicitly.\\n\"\n",
    "    \"Answer in English.\"\n",
    ")\n",
    "\n",
    "def build_llm_messages(query: str, context_text: str):\n",
    "    \"\"\"\n",
    "    Собираем messages в том же стиле, что и в seminar9:\n",
    "    - system-prompt\n",
    "    - user-сообщение = контекст + вопрос\n",
    "    \"\"\"\n",
    "    user_prompt = f\"\"\"Context:\n",
    "{context_text}\n",
    "\n",
    "Question:\n",
    "{query}\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_EN},\n",
    "        {\"role\": \"user\",   \"content\": user_prompt},\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "\n",
    "def _get_generation_pipeline():\n",
    "    \"\"\"\n",
    "    Находим, как именно называется пайплайн генерации в этом ноутбуке.\n",
    "    Поддерживаем несколько вариантов имени.\n",
    "    \"\"\"\n",
    "    if \"generation_pipeline\" in globals():\n",
    "        return generation_pipeline\n",
    "    if \"gen_pipe\" in globals():\n",
    "        return gen_pipe\n",
    "    if \"gen_pipeline\" in globals():\n",
    "        return gen_pipeline\n",
    "    raise RuntimeError(\n",
    "        \"No generation pipeline found. Expected one of: \"\n",
    "        \"'generation_pipeline', 'gen_pipe', 'gen_pipeline'. \"\n",
    "        \"Make sure Block 6A (LLM loading) has been run.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def answer_question(\n",
    "    query: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    High-level:\n",
    "    1) retrieve_relevant_chunks(query) → контекст\n",
    "    2) build_llm_messages(query, context)\n",
    "    3) прогон через LLM-пайплайн (как в seminar9)\n",
    "    \"\"\"\n",
    "    retrieval = retrieve_relevant_chunks(query)\n",
    "    context_text = retrieval[\"context_text\"]\n",
    "    selected_chunks = retrieval[\"chunks\"]\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== DEBUG: selected chunks ===\", len(selected_chunks))\n",
    "        for ch in selected_chunks:\n",
    "            cid   = ch.get(\"chunk_id\", \"NA\")\n",
    "            did   = ch.get(\"doc_id\", \"NA\")\n",
    "            score = (\n",
    "                ch.get(\"score_ce\")\n",
    "                or ch.get(\"score_hybrid\")\n",
    "                or ch.get(\"score\")\n",
    "                or 0.0\n",
    "            )\n",
    "            try:\n",
    "                score_f = float(score)\n",
    "            except Exception:\n",
    "                score_f = 0.0\n",
    "            print(f\"- chunk_id={cid}, doc_id={did}, score={score_f:.4f}\")\n",
    "\n",
    "    if not str(context_text).strip():\n",
    "        return (\n",
    "            \"I could not find any relevant information in the documents to answer this question. \"\n",
    "            \"The retrieval step returned no chunks clearly related to the query.\"\n",
    "        )\n",
    "\n",
    "    messages = build_llm_messages(query, context_text)\n",
    "\n",
    "    pipe = _get_generation_pipeline()\n",
    "    output = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    # формат как в seminar9: берём последнюю реплику\n",
    "    answer = output[0][\"generated_text\"][-1][\"content\"]\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "print(\"build_llm_messages() и answer_question() (seminar9-style + pipeline autodetect) определены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Функция normalize_rag_answer() определена.\n"
     ]
    }
   ],
   "source": [
    "# === Block 7C. Постобработка ответа: normalize_rag_answer() ===\n",
    "\n",
    "import re\n",
    "\n",
    "def _extract_section(raw_text: str, current_name_re: str, next_names_re: str):\n",
    "    \"\"\"\n",
    "    Вырезает кусок текста между заголовком current_name_re и\n",
    "    следующим заголовком из next_names_re (или до конца текста).\n",
    "    \"\"\"\n",
    "    pattern = rf\"(?is){current_name_re}\\s*:?\\s*(.*?)(?:(?:{next_names_re})\\s*:|\\Z)\"\n",
    "    m = re.search(pattern, raw_text, flags=re.IGNORECASE | re.DOTALL)\n",
    "    if not m:\n",
    "        return \"\"\n",
    "    content = m.group(1).strip()\n",
    "    return content\n",
    "\n",
    "\n",
    "def normalize_rag_answer(raw_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Приводит ответ к виду:\n",
    "\n",
    "    КРАТКИЙ ОТВЕТ:\n",
    "    - ...\n",
    "\n",
    "    ОБОСНОВАНИЕ:\n",
    "    - ...\n",
    "\n",
    "    НЕДОСТАЮЩИЕ ДАННЫЕ:\n",
    "    - ...\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_text, str):\n",
    "        raw_text = str(raw_text or \"\")\n",
    "\n",
    "    text = raw_text.strip()\n",
    "    if not text:\n",
    "        return (\n",
    "            \"КРАТКИЙ ОТВЕТ:\\n\"\n",
    "            \"- Модель не смогла сгенерировать ответ.\\n\\n\"\n",
    "            \"ОБОСНОВАНИЕ:\\n\"\n",
    "            \"- Ответ модели пуст.\\n\\n\"\n",
    "            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n",
    "            \"- Не хватает любой информации для ответа.\"\n",
    "        )\n",
    "\n",
    "    # Регэкспы для заголовков (в нижнем регистре, но матчим с IGNORECASE)\n",
    "    name_short = r\"краткий\\s+ответ\"\n",
    "    name_reason = r\"обоснование\"\n",
    "    name_missing = r\"недостающие\\s+данные\"\n",
    "\n",
    "    # Попробуем достать три секции из сырого текста\n",
    "    short_raw = _extract_section(\n",
    "        text,\n",
    "        current_name_re=name_short,\n",
    "        next_names_re=f\"{name_reason}|{name_missing}\",\n",
    "    )\n",
    "    reason_raw = _extract_section(\n",
    "        text,\n",
    "        current_name_re=name_reason,\n",
    "        next_names_re=name_missing,\n",
    "    )\n",
    "    missing_raw = _extract_section(\n",
    "        text,\n",
    "        current_name_re=name_missing,\n",
    "        next_names_re=r\"$^\",  # \"ничего\", чтобы матчить до конца\n",
    "    )\n",
    "\n",
    "    # Если вообще не нашли \"краткий ответ\", считаем, что весь текст — это ОБОСНОВАНИЕ\n",
    "    if not short_raw and not reason_raw and not missing_raw:\n",
    "        reason_raw = text\n",
    "\n",
    "    def _normalize_section_body(content: str, default_msg: str) -> str:\n",
    "        content = content.strip()\n",
    "        if not content:\n",
    "            return f\"- {default_msg}\"\n",
    "        # Если нет ни одной строки, начинающейся с \"-\", добавим буллет\n",
    "        lines = [ln.rstrip() for ln in content.splitlines() if ln.strip()]\n",
    "        if not lines:\n",
    "            return f\"- {default_msg}\"\n",
    "        if not any(ln.lstrip().startswith(\"-\") for ln in lines):\n",
    "            # склеим в одну строку с одним буллетом\n",
    "            return \"- \" + \" \".join(lines)\n",
    "        # иначе вернём как есть (но без лишних пустых строк)\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    short_norm = _normalize_section_body(\n",
    "        short_raw, \"Модель не указала краткий ответ.\"\n",
    "    )\n",
    "    reason_norm = _normalize_section_body(\n",
    "        reason_raw, \"Модель не привела обоснование.\"\n",
    "    )\n",
    "    missing_norm = _normalize_section_body(\n",
    "        missing_raw, \"Модель не указала недостающие данные (считаем, что их нет или они не выделены).\"\n",
    "    )\n",
    "\n",
    "    normalized = (\n",
    "        \"КРАТКИЙ ОТВЕТ:\\n\"\n",
    "        f\"{short_norm}\\n\\n\"\n",
    "        \"ОБОСНОВАНИЕ:\\n\"\n",
    "        f\"{reason_norm}\\n\\n\"\n",
    "        \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n",
    "        f\"{missing_norm}\"\n",
    "    )\n",
    "\n",
    "    return normalized.strip()\n",
    "\n",
    "print(\"Функция normalize_rag_answer() определена.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# мини чек как дела"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[ТЕСТ 1] ВОПРОС:\n",
      "What is the general purpose of this document?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2321b65d984d76b404e318f45c39e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n",
      "=== DEBUG: selected chunks === 10\n",
      "- chunk_id=7151, doc_id=0, score=0.8296\n",
      "- chunk_id=5904, doc_id=0, score=0.6000\n",
      "- chunk_id=2930, doc_id=0, score=0.4922\n",
      "- chunk_id=7112, doc_id=0, score=0.4597\n",
      "- chunk_id=7121, doc_id=0, score=0.3938\n",
      "- chunk_id=2103, doc_id=0, score=0.3727\n",
      "- chunk_id=1471, doc_id=0, score=0.3496\n",
      "- chunk_id=978, doc_id=0, score=0.3114\n",
      "- chunk_id=2681, doc_id=0, score=0.2451\n",
      "- chunk_id=4158, doc_id=0, score=0.2374\n",
      "\n",
      "[ОТВЕТ]:\n",
      "The general purpose of this document appears to be providing educational content related to various aspects of psychology and mental health. It covers topics such as intelligence tests, the Diagnostic and Statistical Manual of Mental Disorders (DSM-5), and psychological disorders, among others. The learning objectives listed at the beginning suggest that the document aims to educate readers about different concepts within psychology, including the development of intelligence tests, the history of IQ tests, and the purposes and benefits of intelligence testing. Additionally, it provides information on the Diagnostic and Statistical Manual of Mental Disorders (DSM-5), which is a classification system used in psychiatry to diagnose mental disorders. The document also touches upon various psychological disorders and their diagnostic criteria. Overall, the document seems to serve as an educational resource covering a range of topics related to psychology and mental health.\n",
      "================================================================================\n",
      "[ТЕСТ 2] ВОПРОС:\n",
      "What are the main conclusions presented in the text?\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4f643a4a31486daa6764a431f6315f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n",
      "=== DEBUG: selected chunks === 10\n",
      "- chunk_id=2847, doc_id=0, score=0.6000\n",
      "- chunk_id=756, doc_id=0, score=0.4310\n",
      "- chunk_id=2684, doc_id=0, score=0.4000\n",
      "- chunk_id=2, doc_id=0, score=0.3306\n",
      "- chunk_id=2249, doc_id=0, score=0.3080\n",
      "- chunk_id=572, doc_id=0, score=0.2408\n",
      "- chunk_id=33, doc_id=0, score=0.2358\n",
      "- chunk_id=4146, doc_id=0, score=0.2161\n",
      "- chunk_id=885, doc_id=0, score=0.2129\n",
      "- chunk_id=2103, doc_id=0, score=0.2062\n",
      "\n",
      "[ОТВЕТ]:\n",
      "The text does not present specific conclusions but rather discusses various aspects related to problem-solving, learning, and developmental stages. It mentions key terms like \"correlational,\" \"stages of development,\" and \"death and dying.\" The text also includes questions for critical thinking and personal application, such as defining learning, comparing classical and operant conditioning, and discussing color perception theories. However, it does not conclude with a single definitive statement or set of conclusions.\n",
      "================================================================================\n",
      "Тесты Block 8 выполнены.\n"
     ]
    }
   ],
   "source": [
    "# === Block 8. Ручная проверка пайплайна RAG на 1–2 запросах ===\n",
    "\n",
    "# ВНИМАНИЕ:\n",
    "# перед этим блоком должны быть выполнены:\n",
    "# - Block 0 (импорты, пути, конфиг)\n",
    "# - Block 1 (поиск документов)\n",
    "# - Block 2 (загрузка текста)\n",
    "# - Block 3A, 3B (чанкинг)\n",
    "# - Block 4A, 4B (эмбеддинги + FAISS)\n",
    "# - Block 5A, 5B (BM25 + hybrid search)\n",
    "# - Block 6A, 6B (LLM + generate_answer)\n",
    "# - Block 7A, 7B (retrieve_relevant_chunks + answer_question)\n",
    "\n",
    "if \"answer_question\" not in globals():\n",
    "    print(\"Функция answer_question не найдена. Проверь, что все предыдущие блоки выполнены.\")\n",
    "else:\n",
    "    # пример тестового запроса — подставь сюда что-то из своих документов\n",
    "    test_queries = [\n",
    "        \"What is the general purpose of this document?\",\n",
    "        \"What are the main conclusions presented in the text?\",\n",
    "    ]\n",
    "\n",
    "    for i, q in enumerate(test_queries, start=1):\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"[ТЕСТ {i}] ВОПРОС:\")\n",
    "        print(q)\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        try:\n",
    "            # debug=True, чтобы увидеть, какие чанки выбираются\n",
    "            answer = answer_question(\n",
    "                q,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.2,\n",
    "                top_p=0.9,\n",
    "                debug=True,\n",
    "            )\n",
    "            print(\"\\n[ОТВЕТ]:\")\n",
    "            print(answer)\n",
    "        except Exception as e:\n",
    "            print(\"Ошибка при вызове answer_question:\", repr(e))\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Тесты Block 8 выполнены.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "[DEBUG] ВОПРОС:\n",
      "What is the general purpose of this document?\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e26771073841b6aad0700a98f73108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n",
      "[DEBUG] Количество выбранных чанков: 10\n",
      "--------------------------------------------------------------------------------\n",
      "[DEBUG] Список чанков (chunk_id, doc_id, score):\n",
      "- chunk_id=7151, doc_id=0, score_hybrid=0.8296\n",
      "- chunk_id=5904, doc_id=0, score_hybrid=0.6000\n",
      "- chunk_id=2930, doc_id=0, score_hybrid=0.4922\n",
      "- chunk_id=7112, doc_id=0, score_hybrid=0.4597\n",
      "- chunk_id=7121, doc_id=0, score_hybrid=0.3938\n",
      "- chunk_id=2103, doc_id=0, score_hybrid=0.3727\n",
      "- chunk_id=1471, doc_id=0, score_hybrid=0.3496\n",
      "- chunk_id=978, doc_id=0, score_hybrid=0.3114\n",
      "- chunk_id=2681, doc_id=0, score_hybrid=0.2451\n",
      "- chunk_id=4158, doc_id=0, score_hybrid=0.2374\n",
      "================================================================================\n",
      "[DEBUG] КОНТЕКСТ, ПЕРЕДАВАЕМЫЙ В МОДЕЛЬ:\n",
      "is used for clinical purposes, this tool is also used to examine the general health of populations and to monitor\n",
      "the prevalence of diseases and other health problems internationally (WHO, 2013). The ICD is in its 10th\n",
      "544 15 • Psychological Disorders\n",
      "Access for free at openstax.org\n",
      "\n",
      "lists the tasks, knowledge, skills, abilities, work context, work activities, education requirements, interests,\n",
      "personality requirements, and work styles that are deemed necessary for success in that position. You can also\n",
      "see data on average earnings and projected job growth in that industry.\n",
      "\n",
      "LEARNING OBJECTIVES\n",
      "By the end of this section, you will be able to:\n",
      "• Explain how intelligence tests are developed\n",
      "• Describe the history of the use of IQ tests\n",
      "• Describe the purposes and benefits of intelligence testing\n",
      "\n",
      "LEARNING OBJECTIVES\n",
      "By the end of this section, you will be able to:\n",
      "• Explain why classification systems are necessary in the study of psychopathology\n",
      "• Describe the basic features of the Diagnostic and Statistical Manual of Mental Disorders, Fifth Edition (DSM-5)\n",
      "\n",
      "disorder is described in detail, including an overview of the disorder (diagnostic features), specific symptoms\n",
      "required for diagnosis (diagnostic criteria), prevalence information (what percent of the population is thought\n",
      "\n",
      "the way that parents approach these decisions depending on whether or not they are also deaf?\n",
      "5.5 The Other Senses\n",
      "LEARNING OBJECTIVES\n",
      "By the end of this section, you will be able to:\n",
      "• Describe the basic functions of the chemical senses\n",
      "\n",
      "4.1 • What Is Consciousness? 113\n",
      "\n",
      "3.1 Human Genetics\n",
      "LEARNING OBJECTIVES\n",
      "By the end of this section, you will be able to:\n",
      "• Explain the basic principles of the theory of evolution by natural selection\n",
      "• Describe the differences between genotype and phenotype\n",
      "\n",
      "20. Explain how the processes of stimulus generalization and stimulus discrimination are considered\n",
      "opposites.\n",
      "21. How does a neutral stimulus become a conditioned stimulus?\n",
      "22. What is a Skinner box and\n",
      "\n",
      "...[обрезано, полный контекст длиннее 2000 символов]\n",
      "================================================================================\n",
      "Block 8B: отладка контекста завершена.\n"
     ]
    }
   ],
   "source": [
    "# === Block 8B. Отладка: смотрим контекст и чанки для одного запроса ===\n",
    "\n",
    "if \"retrieve_relevant_chunks\" not in globals():\n",
    "    print(\"Функция retrieve_relevant_chunks не найдена. Сначала выполни предыдущие блоки.\")\n",
    "else:\n",
    "    # сюда подставь конкретный интересующий вопрос\n",
    "    debug_query = \"What is the general purpose of this document?\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"[DEBUG] ВОПРОС:\")\n",
    "    print(debug_query)\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    try:\n",
    "        retrieval = retrieve_relevant_chunks(\n",
    "            debug_query,\n",
    "            top_k_faiss=20,\n",
    "            top_k_bm25=20,\n",
    "            top_k_final=8,\n",
    "            alpha_faiss=0.6,\n",
    "            max_context_chars=MAX_CONTEXT_CHARS,\n",
    "        )\n",
    "\n",
    "        selected_chunks = retrieval[\"chunks\"]\n",
    "        context_text = retrieval[\"context_text\"]\n",
    "\n",
    "        print(f\"[DEBUG] Количество выбранных чанков: {len(selected_chunks)}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(\"[DEBUG] Список чанков (chunk_id, doc_id, score):\")\n",
    "        for ch in selected_chunks:\n",
    "            print(\n",
    "                f\"- chunk_id={ch['chunk_id']}, \"\n",
    "                f\"doc_id={ch['doc_id']}, \"\n",
    "                f\"score_hybrid={ch['score_hybrid']:.4f}\"\n",
    "            )\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"[DEBUG] КОНТЕКСТ, ПЕРЕДАВАЕМЫЙ В МОДЕЛЬ:\")\n",
    "        print(context_text[:2000])  # при желании можно убрать [:2000], чтобы видеть всё\n",
    "        if len(context_text) > 2000:\n",
    "            print(\"\\n...[обрезано, полный контекст длиннее 2000 символов]\")\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Block 8B: отладка контекста завершена.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Ошибка в retrieve_relevant_chunks:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# получение вопросов и сабмит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUBMISSION_MODE: simple\n"
     ]
    }
   ],
   "source": [
    "# === Block 9B. Универсальная генерация сабмита в разных форматах (обновлённая версия) ===\n",
    "\n",
    "import json\n",
    "\n",
    "# Режимы:\n",
    "# \"simple\"      -> id, answer\n",
    "# \"ru_rag\"      -> ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n",
    "# \"debug_full\"  -> id, question, context, answer, refs_json\n",
    "# \"fr_rag\"      -> id, вопрос, ответ, Контекст, ref_page\n",
    "SUBMISSION_MODE = \"simple\"  # \"simple\" / \"ru_rag\" / \"debug_full\" / \"fr_rag\"\n",
    "\n",
    "print(\"SUBMISSION_MODE:\", SUBMISSION_MODE)\n",
    "\n",
    "\n",
    "def rag_answer_with_context_and_refs(\n",
    "    query: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Возвращает:\n",
    "    - answer: строка ответа модели\n",
    "    - context_text: текст контекста, который ушёл в модель\n",
    "    - refs_json: JSON-строка с ссылками на использованные чанки/доки (chunk_id, doc_id, score, path)\n",
    "    - refs_dict: тот же refs в виде dict (на случай, если формату сабмита нужно вытащить что-то одно)\n",
    "    \"\"\"\n",
    "    retrieval = retrieve_relevant_chunks(query)\n",
    "    context_text = retrieval[\"context_text\"]\n",
    "    selected_chunks = retrieval[\"chunks\"]\n",
    "\n",
    "    if not context_text.strip():\n",
    "        empty_answer = (\n",
    "            \"КРАТКИЙ ОТВЕТ:\\n\"\n",
    "            \"- Я не нашёл релевантной информации в документах.\\n\\n\"\n",
    "            \"ОБОСНОВАНИЕ:\\n\"\n",
    "            \"- Гибридный поиск не вернул подходящих фрагментов, поэтому я не могу ответить на вопрос на основе контекста.\\n\\n\"\n",
    "            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n",
    "            \"- Не хватает любых фрагментов документов, относящихся к заданному вопросу.\"\n",
    "        )\n",
    "        refs_dict = {\"chunks\": []}\n",
    "        refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "        return empty_answer, \"\", refs_json, refs_dict\n",
    "\n",
    "    prompt = build_rag_prompt(query, context_text)\n",
    "\n",
    "    answer = generate_answer(\n",
    "    prompt,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=temperature,\n",
    "    top_p=top_p,\n",
    "    do_sample=True,\n",
    "    )\n",
    "\n",
    "    answer_norm = normalize_rag_answer(answer)\n",
    "\n",
    "    # добавим в refs не только chunk_id/doc_id/score, но и path, если он есть в chunks_df\n",
    "    chunk_info_list = []\n",
    "    for ch in selected_chunks:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        did = int(ch[\"doc_id\"])\n",
    "        score = float(ch[\"score_hybrid\"])\n",
    "\n",
    "        # ищем строку в chunks_df по chunk_id\n",
    "        try:\n",
    "            row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "            path = str(row_match.get(\"path\", \"\"))\n",
    "        except Exception:\n",
    "            path = \"\"\n",
    "\n",
    "        chunk_info_list.append(\n",
    "            {\n",
    "                \"chunk_id\": cid,\n",
    "                \"doc_id\": did,\n",
    "                \"score\": score,\n",
    "                \"path\": path,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    refs_dict = {\"chunks\": chunk_info_list}\n",
    "    refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "\n",
    "    return answer.strip(), context_text, refs_json, refs_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Режимы:\n",
    "# \"simple\"            -> id, answer\n",
    "# \"ru_rag\"            -> ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n",
    "# \"debug_full\"        -> id, question, context, answer, refs_json\n",
    "# \"fr_rag\"            -> id, вопрос, ответ, Контекст, ref_page\n",
    "# \"fr_rag_no_context\" -> id, вопрос, ответ, ref_page\n",
    "# \"id_question_answer\"-> id, вопрос, ответ\n",
    "SUBMISSION_MODE = \"ru_rag\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA_INPUT_PATH: /workspace/data/queries.json\n",
      "QA_INPUT_FORMAT: json\n",
      "QA_OUTPUT_PATH: /workspace/outputs/submission_ru_rag.csv\n",
      "Загружено вопросов: 50\n",
      "   query_id                                      question\n",
      "0         1  What is the scientific method in psychology?\n",
      "1         2         What are the basic parts of a neuron?\n",
      "2         3                 What are the stages of sleep?\n",
      "3         4                 What is operant conditioning?\n",
      "4         5        What is problem-solving in psychology?\n"
     ]
    }
   ],
   "source": [
    "# Путь к файлу с вопросами (подправишь под конкретное соревнование)\n",
    "# === Block 9A. Конфиг и загрузка вопросов (csv/json/jsonl/parquet) ===\n",
    "\n",
    "QA_INPUT_PATH = DATA_DIR / \"queries.json\"   # <- без ведущего слеша\n",
    "QA_INPUT_FORMAT = \"json\"\n",
    "\n",
    "QA_ID_COLUMN = \"query_id\"\n",
    "QA_QUESTION_COLUMN = \"question\"\n",
    "\n",
    "QA_OUTPUT_PATH = OUTPUT_DIR / f\"submission_{SUBMISSION_MODE}.csv\"\n",
    "QA_ANSWER_COLUMN = \"answer\"\n",
    "\n",
    "print(\"QA_INPUT_PATH:\", QA_INPUT_PATH)\n",
    "print(\"QA_INPUT_FORMAT:\", QA_INPUT_FORMAT)\n",
    "print(\"QA_OUTPUT_PATH:\", QA_OUTPUT_PATH)\n",
    "\n",
    "\n",
    "\n",
    "def load_qa_dataframe(\n",
    "    path: Path,\n",
    "    fmt: str,\n",
    "    id_col: str,\n",
    "    question_col: str,\n",
    ") -> pd.DataFrame:\n",
    "    fmt = fmt.lower()\n",
    "    if fmt == \"csv\":\n",
    "        df = pd.read_csv(path)\n",
    "    elif fmt == \"json\":\n",
    "        df = pd.read_json(path)  # обычный JSON-массив объектов\n",
    "    elif fmt == \"jsonl\":\n",
    "        df = pd.read_json(path, lines=True)  # JSON Lines (по строкам)\n",
    "    elif fmt == \"parquet\":\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный формат QA-файла: {fmt}\")\n",
    "\n",
    "    # Проверяем, что нужные колонки есть\n",
    "    missing = [c for c in [id_col, question_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"В файле {path} нет нужных колонок: {missing}. \"\n",
    "            f\"Доступны колонки: {list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if not QA_INPUT_PATH.exists():\n",
    "    print(\"ВНИМАНИЕ: QA_INPUT_PATH не существует:\", QA_INPUT_PATH)\n",
    "    qa_df = None\n",
    "else:\n",
    "    qa_df = load_qa_dataframe(\n",
    "        QA_INPUT_PATH,\n",
    "        QA_INPUT_FORMAT,\n",
    "        QA_ID_COLUMN,\n",
    "        QA_QUESTION_COLUMN,\n",
    "    )\n",
    "    print(\"Загружено вопросов:\", len(qa_df))\n",
    "    print(qa_df[[QA_ID_COLUMN, QA_QUESTION_COLUMN]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаю генерацию ответов для всех вопросов...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824c8cfa836f4fafaa4ecb2ba200ac6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a8c272caac47508d2ed58e8ef28985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642a2344d3244316b9c8b40b5e04a31c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab6560d6fd54f42b1135da7ed27972f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0953011a6c840a38231ae894096705d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2deec3b7188447a88e0d7d9fdedf68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e610dfa56a485286d9e2721d547e97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "429298d074944ee2b4ef7c64df5021df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60a2cb5fd69401f8e447083495030d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad7a504710b4429889bc4cc299f7449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9959fdda7c549b7a177631dafcd9cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf8703daa6b40faa0af946a6bcd5b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "094d4b58ce64496993307a9882f8a1b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59de4b072e5448a89f9685e331b1eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a22f33dea4e17ba934eddef9282c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4c344c37f674ef0a4250551e8328716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241062ad4a31495b917f85be38bb61b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eda83a9948c40138048f3283d132502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b63e9e36c4cf3bcfaed51de891e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bef0b206504922b522916861936a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02960c6b56643e3976ef2d7cd43f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f7033afbd14ecc8eb1bfd3ab3ef4ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ea832b624845078eb2940b94cd9e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d83c50688549bdbcb88f8db7401925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b25a13adf914ac296efc9fd44a417f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3a0a117a77441d9f6a166cb0877af7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3398fdb314454b9a0c0c582ca911ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71add7b9de16482692ff3d81f1177bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46828aaaf6594db996c80fc8e1f73c18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed366396761f48b5a3200b11bab1bed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fec37ca7f64810b7cbcb12afd724b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3249952a1d464638b97f4b5c452c4700",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d0d0b01ca14723902d16e9240ad497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029d5ed72c18495f8d82b4a0aa8a2201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a81c1d892b44a9b8767796b3032f354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7f84b9961d43ad92c8b5ba13a77f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3605b8b07eb4894b00b605d01c12fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fa3ae8774e40ba8f6d1ab1fb8e4ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68ea7636e024d5981990f8fab25c050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ec9605ac034b63829daeeb24b267b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2b375a005c4134a6758d215b0759ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b8fec6382645d9bec979f60814178e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0436d60fc1a248beba4591712955b412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88010b1a5a46475990174455f5df90f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc6168f047e4b909efb7e6a41d25ebd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8a86060abf4f8184fbde912dadc2eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb21fb2e4c94507a7c27bd133ec124a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb9a567a76e4e989e234ddd0117e3cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6c3683568a44eb8920b97a38ca1d74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f8b729212e496aa224661e30b6b4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17879f0423aa4a2e84568742da496988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fdf01c46cd429190a1ebb18c8c773b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24bea27dd0f49d9a2701e7bf7e82397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5976b34996a45c28f6ae4c7479f69eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db064c77ca90497ab4e0efd753b98001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da1fc26b5334ff0820859fb61eb9090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d4899908674e689ffbeefc5e945999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d90203bcde40f0a7101c21106f8450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da29ebeb5dab4c1cab3949c8d6f9c185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecb1c9b4e644c93b76efa8c6211ca7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9708f77b34664b8689457fdee68f6f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b56f96e5a3b450db861065d86439680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b380db7ecbe04c34a17ddb316611c793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "832c5847a6674c948ccb912c2c732787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd427d4781842e2b261d082215f603b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29c85c5589041f68701afb028566cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5efcf1c2d74230a80302704cb24d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595fda789168466985f8baa168102f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1c876adb8047079ebc7a262a30c595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e119343aa8b4d859d3e7a1641075fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bc8ece8ef84faba80b0d5838c52f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6182feff17db4a0cbc5f1ec95e467c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d4d6cd54b40fe8caff4b0a06e99aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e14c0db46f4b2dba3f0f73e8640360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25648fff882f4e7e997ed65bd699b8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd020c8ca58445f92f3d912fe940a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "457a051b4d6a4a7e8aec63484adf7dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9369441bf4cf4ad5abd46e94f0afab3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891eddb93a0b4dc985a60e3930c01576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfb0eddc72842e6b501e3552a41f673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60bedeac9a344518aece7ecfecf06ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8da73137c2742709cd2c230d35aa111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a453a9ad944342a9f28a013c4b4556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "214f8d0b0ec3446286fbf8a6f1c0ddf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5c235c11334740a37fa454fd1c3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a7f5ab2ddd4ce5a2425ee0d721bf67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2745c4ff4444c7f8d3cef1f01b4b1c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8adde3a24c014f428b6a037fe715b50c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92b68826076b4de2a1982065e42e3643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfce19275f74989a119eb6b2532c462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f12427bc6c41f988b87201f8d709d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad4524d33f941e1876add0c92d1b485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d215bc355c4848ab2ad418b5a2ee58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03d014778c945a9b9866a9d2744ca5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8c57a4d913484988a2b7719cc71129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e329a44390384c90b0afefa298dd17d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5387482707df44e9b9b39f07538b1e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed366ff303d4c1db35d6216cfa7fc12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af781a64777647f5a7e5328b300ecb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e525560bdbac4d19bf54aa875d992b05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f644f74f8043b2b268eb0bf719352f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[hybrid] queries: 1 | rows: 50\n"
     ]
    }
   ],
   "source": [
    "if qa_df is None or len(qa_df) == 0:\n",
    "    print(\"qa_df пуст — сначала проверь Block 9A (загрузка вопросов).\")\n",
    "else:\n",
    "    rows = []\n",
    "\n",
    "    print(\"Начинаю генерацию ответов для всех вопросов...\")\n",
    "    for _, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n",
    "        q_id = row[QA_ID_COLUMN]\n",
    "        q_text = str(row[QA_QUESTION_COLUMN])\n",
    "\n",
    "        try:\n",
    "            answer, context_text, refs_json, refs_dict = rag_answer_with_context_and_refs(\n",
    "                q_text,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.2, # советовал 0.8 трайнуть\n",
    "                top_p=0.9,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            answer = f\"Ошибка при генерации ответа: {repr(e)}\"\n",
    "            context_text = \"\"\n",
    "            refs_dict = {\"error\": repr(e)}\n",
    "            refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "\n",
    "        if SUBMISSION_MODE == \"simple\":\n",
    "            # Вариант 1: базовый (id, answer)\n",
    "            rows.append(\n",
    "                {\n",
    "                    QA_ID_COLUMN: q_id,\n",
    "                    QA_ANSWER_COLUMN: answer,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif SUBMISSION_MODE == \"ru_rag\":\n",
    "            # Вариант 2: вариант из русского примера:\n",
    "            # ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"ИДЕНТИФИКАТОР\": q_id,\n",
    "                    \"контекст\": context_text,\n",
    "                    \"отвечать\": answer,\n",
    "                    \"ссылки\": refs_json,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif SUBMISSION_MODE == \"debug_full\":\n",
    "            # Вариант 3: расширенный debug-формат\n",
    "            rows.append(\n",
    "                {\n",
    "                    QA_ID_COLUMN: q_id,\n",
    "                    QA_QUESTION_COLUMN: q_text,\n",
    "                    \"context\": context_text,\n",
    "                    QA_ANSWER_COLUMN: answer,\n",
    "                    \"refs_json\": refs_json,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif SUBMISSION_MODE == \"fr_rag\":\n",
    "            # Вариант 4: полный французский формат:\n",
    "            # id, вопрос, ответ, Контекст, ref_page\n",
    "            if refs_dict.get(\"chunks\"):\n",
    "                first_ref = refs_dict[\"chunks\"][0]\n",
    "                ref_page = first_ref.get(\"path\", \"\")\n",
    "            else:\n",
    "                ref_page = \"\"\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": q_id,\n",
    "                    \"вопрос\": q_text,\n",
    "                    \"ответ\": answer,\n",
    "                    \"Контекст\": context_text,\n",
    "                    \"ref_page\": ref_page,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif SUBMISSION_MODE == \"fr_rag_no_context\":\n",
    "            # Вариант 5: как в твоём примере БЕЗ контекста:\n",
    "            # id, вопрос, ответ, ref_page\n",
    "            if refs_dict.get(\"chunks\"):\n",
    "                first_ref = refs_dict[\"chunks\"][0]\n",
    "                ref_page = first_ref.get(\"path\", \"\")\n",
    "            else:\n",
    "                ref_page = \"\"\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": q_id,\n",
    "                    \"вопрос\": q_text,\n",
    "                    \"ответ\": answer,\n",
    "                    \"ref_page\": ref_page,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif SUBMISSION_MODE == \"id_question_answer\":\n",
    "            # Вариант 6: только id, вопрос, ответ (без ref_page, без контекста)\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"id\": q_id,\n",
    "                    \"вопрос\": q_text,\n",
    "                    \"ответ\": answer,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Неизвестный SUBMISSION_MODE: {SUBMISSION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rag_answer_with_context_and_refs() переопределена и больше не использует build_rag_prompt.\n"
     ]
    }
   ],
   "source": [
    "# === Block 9X. RAG-ответ + контекст + \"ссылки\" для сабмита ===================\n",
    "import json\n",
    "\n",
    "def rag_answer_with_context_and_refs(\n",
    "    query: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    debug: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Обёртка над текущим пайплайном:\n",
    "    1) retrieve_relevant_chunks(query) → контекст + мета по чанкам\n",
    "    2) answer_question(query, ...) → текст ответа (через LLM)\n",
    "    3) собираем refs_dict / refs_json для колонки \"ссылки\" в сабмите\n",
    "\n",
    "    Сейчас refs_dict имеет упрощённый вид:\n",
    "        {\n",
    "          \"chunks\": [\n",
    "            {\n",
    "              \"chunk_id\": ...,\n",
    "              \"doc_id\": ...,\n",
    "              \"score_hybrid\": ...,\n",
    "              \"score_dense\": ...,\n",
    "              \"score_bm25\": ...\n",
    "            },\n",
    "            ...\n",
    "          ]\n",
    "        }\n",
    "\n",
    "    При желании сюда можно будет добавить pages / sections,\n",
    "    если они есть в метаданных chunks_df.\n",
    "    \"\"\"\n",
    "    # 1) ретривал\n",
    "    retrieval = retrieve_relevant_chunks(query)\n",
    "    context_text = retrieval[\"context_text\"]\n",
    "    chunks_meta = retrieval[\"chunks\"]  # список dict'ов из Block 7A\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== RAG DEBUG ===\")\n",
    "        print(\"query:\", query)\n",
    "        print(\"context length:\", len(context_text))\n",
    "        print(\"num chunks:\", len(chunks_meta))\n",
    "\n",
    "    # 2) генерация ответа\n",
    "    answer = answer_question(\n",
    "        query,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    # 3) сбор \"ссылок\" — пока просто список чанков с их score'ами\n",
    "    refs_dict = {\n",
    "        \"chunks\": chunks_meta\n",
    "    }\n",
    "    refs_json = json.dumps(refs_dict, ensure_ascii=False)\n",
    "\n",
    "    return answer, context_text, refs_json, refs_dict\n",
    "\n",
    "\n",
    "print(\"rag_answer_with_context_and_refs() переопределена и больше не использует build_rag_prompt.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество строк в rows: 50\n",
      "Колонки submission_df: ['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки']\n",
      "   ИДЕНТИФИКАТОР                                           контекст  \\\n",
      "0              1  explores questions like these. Psychology refe...   \n",
      "1              2  Neuron Structure\\nNeurons are the central buil...   \n",
      "2              3  behaviors\\nstage 1 sleep first stage of sleep;...   \n",
      "3              4  known as Little Albert. His findings suggest t...   \n",
      "4              5  creativity, language, and problem solving, in ...   \n",
      "\n",
      "                                            отвечать  \\\n",
      "0  The scientific method in psychology involves d...   \n",
      "1  The basic parts of a neuron include:\\n\\n- Soma...   \n",
      "2  The stages of sleep include:\\n\\n1. **Stage 1**...   \n",
      "3  Operant conditioning is a form of learning in ...   \n",
      "4  In psychology, problem-solving refers to the p...   \n",
      "\n",
      "                                              ссылки  \n",
      "0  {\"chunks\": [{\"chunk_id\": 126, \"doc_id\": 0, \"sc...  \n",
      "1  {\"chunks\": [{\"chunk_id\": 1077, \"doc_id\": 0, \"s...  \n",
      "2  {\"chunks\": [{\"chunk_id\": 1814, \"doc_id\": 0, \"s...  \n",
      "3  {\"chunks\": [{\"chunk_id\": 2657, \"doc_id\": 0, \"s...  \n",
      "4  {\"chunks\": [{\"chunk_id\": 2706, \"doc_id\": 0, \"s...  \n"
     ]
    }
   ],
   "source": [
    "# === Собираем submission_df из уже посчитанных rows ===\n",
    "\n",
    "if \"rows\" not in globals():\n",
    "    raise RuntimeError(\"Переменная rows не найдена. Нужно ещё раз запустить ячейку с генерацией (где rows.append(...)).\")\n",
    "\n",
    "print(\"Количество строк в rows:\", len(rows))\n",
    "\n",
    "submission_df = pd.DataFrame(rows)\n",
    "print(\"Колонки submission_df:\", list(submission_df.columns))\n",
    "print(submission_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T00:20:47.557197Z",
     "iopub.status.busy": "2025-11-15T00:20:47.556586Z",
     "iopub.status.idle": "2025-11-15T00:20:47.577454Z",
     "shell.execute_reply": "2025-11-15T00:20:47.576718Z",
     "shell.execute_reply.started": "2025-11-15T00:20:47.557170Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(rows)\n",
    "submission_df.to_csv(QA_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T00:16:03.244474Z",
     "iopub.status.busy": "2025-11-15T00:16:03.243836Z",
     "iopub.status.idle": "2025-11-15T00:16:03.249569Z",
     "shell.execute_reply": "2025-11-15T00:16:03.248936Z",
     "shell.execute_reply.started": "2025-11-15T00:16:03.244448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# === Финальный сабмит: id,answer ===\\n\\nif \"submission_df\" not in globals():\\n    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows (см. предыдущую ячейку).\")\\n\\nprint(\"Текущие колонки:\", list(submission_df.columns))\\n\\n# если твой simple-режим уже делает \\'id\\' и \\'answer\\', то всё просто:\\nif \"id\" in submission_df.columns and \"answer\" in submission_df.columns:\\n    simple_sub = submission_df[[\"id\", \"answer\"]].copy()\\nelse:\\n    # если id-колонка называется иначе (например, QA_ID_COLUMN / \\'query_id\\'):\\n    id_col = QA_ID_COLUMN  # у тебя он уже задан в Block 9A\\n    simple_sub = submission_df[[id_col, QA_ANSWER_COLUMN]].rename(\\n        columns={id_col: \"id\", QA_ANSWER_COLUMN: \"answer\"}\\n    )\\n\\nfinal_path = OUTPUT_DIR / \"submission.csv\"\\nsimple_sub.to_csv(final_path, index=False)\\n\\nprint(\"Сабмит сохранён в:\", final_path)\\nprint(simple_sub.head())'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# === Финальный сабмит: id,answer ===\n",
    "\n",
    "if \"submission_df\" not in globals():\n",
    "    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows (см. предыдущую ячейку).\")\n",
    "\n",
    "print(\"Текущие колонки:\", list(submission_df.columns))\n",
    "\n",
    "# если твой simple-режим уже делает 'id' и 'answer', то всё просто:\n",
    "if \"id\" in submission_df.columns and \"answer\" in submission_df.columns:\n",
    "    simple_sub = submission_df[[\"id\", \"answer\"]].copy()\n",
    "else:\n",
    "    # если id-колонка называется иначе (например, QA_ID_COLUMN / 'query_id'):\n",
    "    id_col = QA_ID_COLUMN  # у тебя он уже задан в Block 9A\n",
    "    simple_sub = submission_df[[id_col, QA_ANSWER_COLUMN]].rename(\n",
    "        columns={id_col: \"id\", QA_ANSWER_COLUMN: \"answer\"}\n",
    "    )\n",
    "\n",
    "final_path = OUTPUT_DIR / \"submission.csv\"\n",
    "simple_sub.to_csv(final_path, index=False)\n",
    "\n",
    "print(\"Сабмит сохранён в:\", final_path)\n",
    "print(simple_sub.head())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# === Финальный сабмит из submission_df в формате id,answer ===\n",
    "\n",
    "if \"submission_df\" not in globals():\n",
    "    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows.\")\n",
    "\n",
    "print(\"Текущие колонки:\", list(submission_df.columns))\n",
    "\n",
    "# Прямо жёстко переименовываем русские имена в нужные:\n",
    "simple_sub = submission_df.rename(\n",
    "    columns={\n",
    "        \"ИДЕНТИФИКАТОР\": \"id\",\n",
    "        \"отвечать\": \"answer\",\n",
    "    }\n",
    ")[[\"id\", \"answer\"]]  # берём только эти две колонки и в нужном порядке\n",
    "\n",
    "final_path = OUTPUT_DIR / \"submission.csv\"\n",
    "simple_sub.to_csv(final_path, index=False)\n",
    "\n",
    "print(\"Сабмит сохранён в:\", final_path)\n",
    "print(simple_sub.head())'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# === Block 9. Формирование сабмита для competition-psycho ===\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# путь к файлу с вопросами\n",
    "QA_INPUT_PATH = Path(\"/kaggle/input/competition-psycho/queries.json\")\n",
    "\n",
    "with open(QA_INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "qa_df = pd.DataFrame(data)\n",
    "print(\"Пример строк из queries.json:\")\n",
    "print(qa_df.head())\n",
    "\n",
    "# ВАЖНО: подправь эти два имени колонок, если они другие в queries.json\n",
    "ID_COL = \"query_id\"       # например: \"id\" или \"query_id\"\n",
    "QUESTION_COL = \"question\"   # например: \"query\" или \"question\"\n",
    "\n",
    "# проверим, что такие колонки есть\n",
    "assert ID_COL in qa_df.columns, f\"В qa_df нет колонки {ID_COL}, есть: {list(qa_df.columns)}\"\n",
    "assert QUESTION_COL in qa_df.columns, f\"В qa_df нет колонки {QUESTION_COL}, есть: {list(qa_df.columns)}\"\n",
    "\n",
    "rows = []\n",
    "\n",
    "print(\"Генерирую ответы для всех вопросов...\")\n",
    "for _, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n",
    "    q_id = row[ID_COL]\n",
    "    q_text = str(row[QUESTION_COL])\n",
    "\n",
    "    ans = answer_question(\n",
    "        q_text,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.2,\n",
    "        top_p=0.9,\n",
    "        debug=False,\n",
    "    )\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"id\": q_id,\n",
    "            \"answer\": ans,\n",
    "        }\n",
    "    )\n",
    "\n",
    "submission_df = pd.DataFrame(rows)\n",
    "\n",
    "# файл сабмита\n",
    "sub_path = OUTPUT_DIR / \"submission.csv\"\n",
    "submission_df.to_csv(sub_path, index=False)\n",
    "\n",
    "print(\"Сабмит сохранён в:\", sub_path)\n",
    "print(submission_df.head())'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "если колонки в другом порядке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(submission_df.columns)\n",
    "# Index(['id', 'вопрос', 'ответ', 'Контекст', 'ref_page'], dtype='object')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"id\", \"вопрос\", \"Контекст\", \"ответ\"]\n",
    "submission_df = submission_df[desired_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Главная идея:\n",
    "сначала формируешь submission_df,\n",
    "потом (перед to_csv) один раз явно задаёшь порядок колонок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = submission_df[[\"id\", \"вопрос\", \"контекст\", \"ответ\"]]\n",
    "submission_df.to_csv(QA_OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# прочие нюансы формирования сабмита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Шпаргалка по колонкам в разных режимах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSION_MODE = \"simple\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.columns == [QA_ID_COLUMN, QA_ANSWER_COLUMN]\n",
    "# например: ['id', 'answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "или"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = submission_df[[QA_ID_COLUMN, QA_ANSWER_COLUMN]]\n",
    "submission_df.rename(columns={QA_ID_COLUMN: \"id\", QA_ANSWER_COLUMN: \"answer\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSION_MODE = \"debug_full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[QA_ID_COLUMN, QA_QUESTION_COLUMN, \"context\", QA_ANSWER_COLUMN, \"refs_json\"]\n",
    "# например: ['id', 'question', 'context', 'answer', 'refs_json']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSION_MODE = \"fr_rag\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['id', 'вопрос', 'ответ', 'Контекст', 'ref_page']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSION_MODE = \"fr_rag_no_context\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['id', 'вопрос', 'ответ', 'ref_page']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBMISSION_MODE = \"id_question_answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['id', 'вопрос', 'ответ']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Универсальный мини-шаблон для «любого конкурса»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_cols = [\"id\", \"question\", \"answer\"]  # подставляешь нужный порядок/имена\n",
    "submission_custom = submission_df[desired_cols].copy()\n",
    "submission_custom.to_csv(OUTPUT_DIR / \"submission_custom.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## переименовать колонки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименовать несколько колонок по словарю"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'context', 'answer', 'references'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "submission_df = submission_df.rename(\n",
    "    columns={\n",
    "        \"ИДЕНТИФИКАТОР\": \"ID\",\n",
    "        \"отвечать\": \"answer\",\n",
    "        \"контекст\": \"context\",\n",
    "        \"ссылки\": \"references\",\n",
    "    }\n",
    ")\n",
    "\n",
    "print(submission_df.columns)\n",
    "# Index(['id', 'context', 'answer', 'refs_json'], dtype='object')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(QA_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>references</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>explores questions like these. Psychology refe...</td>\n",
       "      <td>The scientific method in psychology involves d...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 126, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Neuron Structure\\nNeurons are the central buil...</td>\n",
       "      <td>The basic parts of a neuron include:\\n\\n- Soma...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 1077, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>behaviors\\nstage 1 sleep first stage of sleep;...</td>\n",
       "      <td>The stages of sleep include:\\n\\n1. **Stage 1**...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 1814, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>known as Little Albert. His findings suggest t...</td>\n",
       "      <td>Operant conditioning is a form of learning in ...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 2657, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>creativity, language, and problem solving, in ...</td>\n",
       "      <td>In psychology, problem-solving refers to the p...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 2706, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8.1 How Memory Functions\\nLEARNING OBJECTIVES\\...</td>\n",
       "      <td>The three stages of memory are Sensory Memory,...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 3144, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>emotion and our abilities to recognize those e...</td>\n",
       "      <td>The key components of emotion according to the...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 4445, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>proposed that five trait dimensions are suffic...</td>\n",
       "      <td>The major personality traits in the Five Facto...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Summary\\n12.1 What Is Social Psychology?\\nSoci...</td>\n",
       "      <td>Social psychology is the subfield of psycholog...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 5697, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>DIG DEEPER\\n16.5 • The Sociocultural Model and...</td>\n",
       "      <td>The sociocultural model in therapy involves in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 8361, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>\"bottom middle-right\": modification of work by...</td>\n",
       "      <td>According to the provided context, the history...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 118, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>scholars, Wilhelm Wundt and William James, are...</td>\n",
       "      <td>Wilhelm Wundt and William James were both infl...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 153, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>and related to the justice system\\nfunctionali...</td>\n",
       "      <td>Functionalism in psychology is a philosophical...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 422, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>that Freud has made to the field of psychology...</td>\n",
       "      <td>Freud's contributions to psychology include:\\n...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 3636, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>dots according to the principle of similarity....</td>\n",
       "      <td>Gestalt principles are fundamental concepts in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 2164, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>experiments in classical conditioning (Figure ...</td>\n",
       "      <td>Classical conditioning involves pairing a neut...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 2301, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>B. F. Skinner’s contributions to our understan...</td>\n",
       "      <td>B. F. Skinner's contributions to behaviorism i...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 439, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Abraham Maslow’s hierarchy of needs is a model...</td>\n",
       "      <td>Maslow's hierarchy of needs is a motivational ...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 4582, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>developed by Carl Rogers is known as client-ce...</td>\n",
       "      <td>Client-centered therapy, also known as Rogeria...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 8406, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>psychology is the area of psychology that focu...</td>\n",
       "      <td>Cognitive psychology is the field of psycholog...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 317, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>sensation and perception, thinking and intelli...</td>\n",
       "      <td>Developmental psychology is the scientific stu...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 321, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>biopsychology study of how biology influences ...</td>\n",
       "      <td>Biopsychology is the study of how biology infl...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 417, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>some evolutionary psychologists are content to...</td>\n",
       "      <td>According to the given context, evolutionary p...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 304, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>WHAT DO YOU THINK?\\n156 5 • Sensation and Perc...</td>\n",
       "      <td>Sensation and perception involve how our sense...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 2006, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>5.1 Sensation versus Perception\\nLEARNING OBJE...</td>\n",
       "      <td>The difference between sensation and perceptio...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 1875, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>shape our sense of self. Jean Piaget proposed ...</td>\n",
       "      <td>Jean Piaget's theories of cognitive developmen...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 4117, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>very young children and adults. For instance, ...</td>\n",
       "      <td>Object permanence is the understanding that ev...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 324, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>psychologists know about the functions of neur...</td>\n",
       "      <td>The role of neurotransmitters in psychology in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 1129, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>proposed that five trait dimensions are suffic...</td>\n",
       "      <td>The Big Five personality trait model is a wide...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>disorder developed by the American Psychiatric...</td>\n",
       "      <td>Psychological disorders are conditions charact...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 7098, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>signify the presence of a psychological disord...</td>\n",
       "      <td>The DSM-5 is a comprehensive diagnostic tool d...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 7987, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>of ethical guidelines for conducting psycholog...</td>\n",
       "      <td>Ethical considerations play a crucial role in ...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 353, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Cognitive Psychology\\nAs mentioned in the prev...</td>\n",
       "      <td>The cognitive revolution in psychology refers ...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 316, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>knowledge of the psychology of women.\\nCrawfor...</td>\n",
       "      <td>According to the given context, feminist psych...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 251, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>studying psychological gender differences, and...</td>\n",
       "      <td>Multicultural psychology is the branch of psyc...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>studying psychological gender differences, and...</td>\n",
       "      <td>The challenges of cross-cultural psychology in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>impatient, rushed, and hostile toward others\\n...</td>\n",
       "      <td>In psychology, stress refers to the perception...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 6966, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>messy, careless, disorganized, and prone to em...</td>\n",
       "      <td>The stages of psychosexual development accordi...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 4713, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>replications involve additional measures that ...</td>\n",
       "      <td>Replication plays a crucial role in psychologi...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 807, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>Psychological treatment can occur in a variety...</td>\n",
       "      <td>The different types of psychological therapies...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 8074, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>prevent unused neurotransmitters from being tr...</td>\n",
       "      <td>Neurotransmitters play crucial roles in regula...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 1140, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>Industrial-Organizational Psychology\\nIndustri...</td>\n",
       "      <td>Major themes in industrial-organizational psyc...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 355, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>norms dictate the behavior that is appropriate...</td>\n",
       "      <td>According to the context, social norms play a ...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 5702, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>TABLE 12.2\\n12.5 Prejudice and Discrimination\\...</td>\n",
       "      <td>According to the provided context, prejudice r...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 5443, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>LINK T O LEARNING\\nListen to a podcast about t...</td>\n",
       "      <td>The key takeaways from the Brown v. Board of E...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 264, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>hypotheses are tested, theories are modified a...</td>\n",
       "      <td>The scientific process of hypothesis testing i...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 550, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>An education in psychology is valuable for a n...</td>\n",
       "      <td>The applications of psychology in education in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 142, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>mood disorder one of a group of disorders char...</td>\n",
       "      <td>Mood disorders and personality disorders are d...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 7884, \"doc_id\": 0, \"s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>studying psychological gender differences, and...</td>\n",
       "      <td>Cultural diversity has a significant impact on...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>An education in psychology is valuable for a n...</td>\n",
       "      <td>The role of critical thinking in psychology in...</td>\n",
       "      <td>{\"chunks\": [{\"chunk_id\": 142, \"doc_id\": 0, \"sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID                                            context  \\\n",
       "0    1  explores questions like these. Psychology refe...   \n",
       "1    2  Neuron Structure\\nNeurons are the central buil...   \n",
       "2    3  behaviors\\nstage 1 sleep first stage of sleep;...   \n",
       "3    4  known as Little Albert. His findings suggest t...   \n",
       "4    5  creativity, language, and problem solving, in ...   \n",
       "5    6  8.1 How Memory Functions\\nLEARNING OBJECTIVES\\...   \n",
       "6    7  emotion and our abilities to recognize those e...   \n",
       "7    8  proposed that five trait dimensions are suffic...   \n",
       "8    9  Summary\\n12.1 What Is Social Psychology?\\nSoci...   \n",
       "9   10  DIG DEEPER\\n16.5 • The Sociocultural Model and...   \n",
       "10  11  \"bottom middle-right\": modification of work by...   \n",
       "11  12  scholars, Wilhelm Wundt and William James, are...   \n",
       "12  13  and related to the justice system\\nfunctionali...   \n",
       "13  14  that Freud has made to the field of psychology...   \n",
       "14  15  dots according to the principle of similarity....   \n",
       "15  16  experiments in classical conditioning (Figure ...   \n",
       "16  17  B. F. Skinner’s contributions to our understan...   \n",
       "17  18  Abraham Maslow’s hierarchy of needs is a model...   \n",
       "18  19  developed by Carl Rogers is known as client-ce...   \n",
       "19  20  psychology is the area of psychology that focu...   \n",
       "20  21  sensation and perception, thinking and intelli...   \n",
       "21  22  biopsychology study of how biology influences ...   \n",
       "22  23  some evolutionary psychologists are content to...   \n",
       "23  24  WHAT DO YOU THINK?\\n156 5 • Sensation and Perc...   \n",
       "24  25  5.1 Sensation versus Perception\\nLEARNING OBJE...   \n",
       "25  26  shape our sense of self. Jean Piaget proposed ...   \n",
       "26  27  very young children and adults. For instance, ...   \n",
       "27  28  psychologists know about the functions of neur...   \n",
       "28  29  proposed that five trait dimensions are suffic...   \n",
       "29  30  disorder developed by the American Psychiatric...   \n",
       "30  31  signify the presence of a psychological disord...   \n",
       "31  32  of ethical guidelines for conducting psycholog...   \n",
       "32  33  Cognitive Psychology\\nAs mentioned in the prev...   \n",
       "33  34  knowledge of the psychology of women.\\nCrawfor...   \n",
       "34  35  studying psychological gender differences, and...   \n",
       "35  36  studying psychological gender differences, and...   \n",
       "36  37  impatient, rushed, and hostile toward others\\n...   \n",
       "37  38  messy, careless, disorganized, and prone to em...   \n",
       "38  39  replications involve additional measures that ...   \n",
       "39  40  Psychological treatment can occur in a variety...   \n",
       "40  41  prevent unused neurotransmitters from being tr...   \n",
       "41  42  Industrial-Organizational Psychology\\nIndustri...   \n",
       "42  43  norms dictate the behavior that is appropriate...   \n",
       "43  44  TABLE 12.2\\n12.5 Prejudice and Discrimination\\...   \n",
       "44  45  LINK T O LEARNING\\nListen to a podcast about t...   \n",
       "45  46  hypotheses are tested, theories are modified a...   \n",
       "46  47  An education in psychology is valuable for a n...   \n",
       "47  48  mood disorder one of a group of disorders char...   \n",
       "48  49  studying psychological gender differences, and...   \n",
       "49  50  An education in psychology is valuable for a n...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   The scientific method in psychology involves d...   \n",
       "1   The basic parts of a neuron include:\\n\\n- Soma...   \n",
       "2   The stages of sleep include:\\n\\n1. **Stage 1**...   \n",
       "3   Operant conditioning is a form of learning in ...   \n",
       "4   In psychology, problem-solving refers to the p...   \n",
       "5   The three stages of memory are Sensory Memory,...   \n",
       "6   The key components of emotion according to the...   \n",
       "7   The major personality traits in the Five Facto...   \n",
       "8   Social psychology is the subfield of psycholog...   \n",
       "9   The sociocultural model in therapy involves in...   \n",
       "10  According to the provided context, the history...   \n",
       "11  Wilhelm Wundt and William James were both infl...   \n",
       "12  Functionalism in psychology is a philosophical...   \n",
       "13  Freud's contributions to psychology include:\\n...   \n",
       "14  Gestalt principles are fundamental concepts in...   \n",
       "15  Classical conditioning involves pairing a neut...   \n",
       "16  B. F. Skinner's contributions to behaviorism i...   \n",
       "17  Maslow's hierarchy of needs is a motivational ...   \n",
       "18  Client-centered therapy, also known as Rogeria...   \n",
       "19  Cognitive psychology is the field of psycholog...   \n",
       "20  Developmental psychology is the scientific stu...   \n",
       "21  Biopsychology is the study of how biology infl...   \n",
       "22  According to the given context, evolutionary p...   \n",
       "23  Sensation and perception involve how our sense...   \n",
       "24  The difference between sensation and perceptio...   \n",
       "25  Jean Piaget's theories of cognitive developmen...   \n",
       "26  Object permanence is the understanding that ev...   \n",
       "27  The role of neurotransmitters in psychology in...   \n",
       "28  The Big Five personality trait model is a wide...   \n",
       "29  Psychological disorders are conditions charact...   \n",
       "30  The DSM-5 is a comprehensive diagnostic tool d...   \n",
       "31  Ethical considerations play a crucial role in ...   \n",
       "32  The cognitive revolution in psychology refers ...   \n",
       "33  According to the given context, feminist psych...   \n",
       "34  Multicultural psychology is the branch of psyc...   \n",
       "35  The challenges of cross-cultural psychology in...   \n",
       "36  In psychology, stress refers to the perception...   \n",
       "37  The stages of psychosexual development accordi...   \n",
       "38  Replication plays a crucial role in psychologi...   \n",
       "39  The different types of psychological therapies...   \n",
       "40  Neurotransmitters play crucial roles in regula...   \n",
       "41  Major themes in industrial-organizational psyc...   \n",
       "42  According to the context, social norms play a ...   \n",
       "43  According to the provided context, prejudice r...   \n",
       "44  The key takeaways from the Brown v. Board of E...   \n",
       "45  The scientific process of hypothesis testing i...   \n",
       "46  The applications of psychology in education in...   \n",
       "47  Mood disorders and personality disorders are d...   \n",
       "48  Cultural diversity has a significant impact on...   \n",
       "49  The role of critical thinking in psychology in...   \n",
       "\n",
       "                                           references  \n",
       "0   {\"chunks\": [{\"chunk_id\": 126, \"doc_id\": 0, \"sc...  \n",
       "1   {\"chunks\": [{\"chunk_id\": 1077, \"doc_id\": 0, \"s...  \n",
       "2   {\"chunks\": [{\"chunk_id\": 1814, \"doc_id\": 0, \"s...  \n",
       "3   {\"chunks\": [{\"chunk_id\": 2657, \"doc_id\": 0, \"s...  \n",
       "4   {\"chunks\": [{\"chunk_id\": 2706, \"doc_id\": 0, \"s...  \n",
       "5   {\"chunks\": [{\"chunk_id\": 3144, \"doc_id\": 0, \"s...  \n",
       "6   {\"chunks\": [{\"chunk_id\": 4445, \"doc_id\": 0, \"s...  \n",
       "7   {\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...  \n",
       "8   {\"chunks\": [{\"chunk_id\": 5697, \"doc_id\": 0, \"s...  \n",
       "9   {\"chunks\": [{\"chunk_id\": 8361, \"doc_id\": 0, \"s...  \n",
       "10  {\"chunks\": [{\"chunk_id\": 118, \"doc_id\": 0, \"sc...  \n",
       "11  {\"chunks\": [{\"chunk_id\": 153, \"doc_id\": 0, \"sc...  \n",
       "12  {\"chunks\": [{\"chunk_id\": 422, \"doc_id\": 0, \"sc...  \n",
       "13  {\"chunks\": [{\"chunk_id\": 3636, \"doc_id\": 0, \"s...  \n",
       "14  {\"chunks\": [{\"chunk_id\": 2164, \"doc_id\": 0, \"s...  \n",
       "15  {\"chunks\": [{\"chunk_id\": 2301, \"doc_id\": 0, \"s...  \n",
       "16  {\"chunks\": [{\"chunk_id\": 439, \"doc_id\": 0, \"sc...  \n",
       "17  {\"chunks\": [{\"chunk_id\": 4582, \"doc_id\": 0, \"s...  \n",
       "18  {\"chunks\": [{\"chunk_id\": 8406, \"doc_id\": 0, \"s...  \n",
       "19  {\"chunks\": [{\"chunk_id\": 317, \"doc_id\": 0, \"sc...  \n",
       "20  {\"chunks\": [{\"chunk_id\": 321, \"doc_id\": 0, \"sc...  \n",
       "21  {\"chunks\": [{\"chunk_id\": 417, \"doc_id\": 0, \"sc...  \n",
       "22  {\"chunks\": [{\"chunk_id\": 304, \"doc_id\": 0, \"sc...  \n",
       "23  {\"chunks\": [{\"chunk_id\": 2006, \"doc_id\": 0, \"s...  \n",
       "24  {\"chunks\": [{\"chunk_id\": 1875, \"doc_id\": 0, \"s...  \n",
       "25  {\"chunks\": [{\"chunk_id\": 4117, \"doc_id\": 0, \"s...  \n",
       "26  {\"chunks\": [{\"chunk_id\": 324, \"doc_id\": 0, \"sc...  \n",
       "27  {\"chunks\": [{\"chunk_id\": 1129, \"doc_id\": 0, \"s...  \n",
       "28  {\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...  \n",
       "29  {\"chunks\": [{\"chunk_id\": 7098, \"doc_id\": 0, \"s...  \n",
       "30  {\"chunks\": [{\"chunk_id\": 7987, \"doc_id\": 0, \"s...  \n",
       "31  {\"chunks\": [{\"chunk_id\": 353, \"doc_id\": 0, \"sc...  \n",
       "32  {\"chunks\": [{\"chunk_id\": 316, \"doc_id\": 0, \"sc...  \n",
       "33  {\"chunks\": [{\"chunk_id\": 251, \"doc_id\": 0, \"sc...  \n",
       "34  {\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...  \n",
       "35  {\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...  \n",
       "36  {\"chunks\": [{\"chunk_id\": 6966, \"doc_id\": 0, \"s...  \n",
       "37  {\"chunks\": [{\"chunk_id\": 4713, \"doc_id\": 0, \"s...  \n",
       "38  {\"chunks\": [{\"chunk_id\": 807, \"doc_id\": 0, \"sc...  \n",
       "39  {\"chunks\": [{\"chunk_id\": 8074, \"doc_id\": 0, \"s...  \n",
       "40  {\"chunks\": [{\"chunk_id\": 1140, \"doc_id\": 0, \"s...  \n",
       "41  {\"chunks\": [{\"chunk_id\": 355, \"doc_id\": 0, \"sc...  \n",
       "42  {\"chunks\": [{\"chunk_id\": 5702, \"doc_id\": 0, \"s...  \n",
       "43  {\"chunks\": [{\"chunk_id\": 5443, \"doc_id\": 0, \"s...  \n",
       "44  {\"chunks\": [{\"chunk_id\": 264, \"doc_id\": 0, \"sc...  \n",
       "45  {\"chunks\": [{\"chunk_id\": 550, \"doc_id\": 0, \"sc...  \n",
       "46  {\"chunks\": [{\"chunk_id\": 142, \"doc_id\": 0, \"sc...  \n",
       "47  {\"chunks\": [{\"chunk_id\": 7884, \"doc_id\": 0, \"s...  \n",
       "48  {\"chunks\": [{\"chunk_id\": 252, \"doc_id\": 0, \"sc...  \n",
       "49  {\"chunks\": [{\"chunk_id\": 142, \"doc_id\": 0, \"sc...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименовать только одну колонку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = submission_df.rename(columns={\"ИДЕНТИФИКАТОР\": \"id\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полностью задать новый список имён (если порядок уже правильный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.columns\n",
    "# ['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "и ты хочешь точно такой же порядок, но другие названия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.columns = [\"id\", \"context\", \"answer\", \"refs_json\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# пытаюсь улучшить скор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross-encoder reranker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T02:06:52.516298Z",
     "iopub.status.busy": "2025-11-15T02:06:52.515801Z",
     "iopub.status.idle": "2025-11-15T02:06:55.990414Z",
     "shell.execute_reply": "2025-11-15T02:06:55.989614Z",
     "shell.execute_reply.started": "2025-11-15T02:06:52.516274Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cross-encoder reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a567b6e37ece4add924137eb3f1bed6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1270d4d58b734acea24e059bd3ddb7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4848fe0b09f48cc99313fc20538b1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b4924d27714d01b939d6af65fb762c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2efabc995344bca9bbcd625e7836b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5a87095fa54624be3d81b80d434f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reranker loaded on cuda, num_labels=1\n"
     ]
    }
   ],
   "source": [
    "# === Block 10A. Cross-encoder reranker (HF) ===\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Небольшой, но сильный cross-encoder, обученный на MS MARCO (английский)\n",
    "RERANKER_MODEL_ID = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "\n",
    "# Используем тот же GPU, что и для LLM, если доступен\n",
    "RERANKER_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\"Loading cross-encoder reranker:\", RERANKER_MODEL_ID)\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_ID)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_ID).to(RERANKER_DEVICE)\n",
    "\n",
    "# Проверим, что у модели один выходной логит (score)\n",
    "num_labels = reranker_model.config.num_labels\n",
    "print(f\"Reranker loaded on {RERANKER_DEVICE}, num_labels={num_labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T02:08:23.822526Z",
     "iopub.status.busy": "2025-11-15T02:08:23.821801Z",
     "iopub.status.idle": "2025-11-15T02:08:23.834961Z",
     "shell.execute_reply": "2025-11-15T02:08:23.834211Z",
     "shell.execute_reply.started": "2025-11-15T02:08:23.822501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-encoder reranker и обновлённый retrieve_relevant_chunks() определены.\n"
     ]
    }
   ],
   "source": [
    "# === Block 10B. Rerank кандидатов cross-encoder'ом + обновлённый retrieve_relevant_chunks ===\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Можно будет крутить эти параметры\n",
    "CE_TOP_K_CANDIDATES = 50   # сколько кандидатов берём после hybrid для rerank\n",
    "CE_TOP_K_FINAL = 8         # сколько чанкoв остаётся после rerank (пойдут в контекст)\n",
    "\n",
    "def rerank_chunks_with_ce(query: str, candidates: list, top_k_final: int = CE_TOP_K_FINAL):\n",
    "    \"\"\"\n",
    "    Переранжирует список кандидатов (dict'ы с chunk_id/doc_id/score_hybrid)\n",
    "    с помощью cross-encoder'а.\n",
    "\n",
    "    candidates: список словарей, у которых есть хотя бы 'chunk_id' и 'doc_id'.\n",
    "    Возвращает НОВЫЙ список кандидатов, отсортированный по score_ce (убывание).\n",
    "    \"\"\"\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    # Ограничим количество кандидатов сверху (чтобы не грузить лишнее)\n",
    "    cand = candidates[:CE_TOP_K_CANDIDATES]\n",
    "\n",
    "    # Собираем пары (query, chunk_text)\n",
    "    pair_texts = []\n",
    "    for ch in cand:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        # достаём текст чанка из chunks_df\n",
    "        try:\n",
    "            row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "            chunk_text = str(row_match.get(\"chunk_text\", \"\"))\n",
    "        except Exception:\n",
    "            chunk_text = \"\"\n",
    "        pair_texts.append((query, chunk_text))\n",
    "\n",
    "    # Токенизируем пачкой\n",
    "    inputs = reranker_tokenizer(\n",
    "        [p[0] for p in pair_texts],\n",
    "        [p[1] for p in pair_texts],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "    ).to(RERANKER_DEVICE)\n",
    "\n",
    "    # Считаем скор для каждого (query, chunk)\n",
    "    with torch.no_grad():\n",
    "        logits = reranker_model(**inputs).logits\n",
    "\n",
    "    # Если num_labels=1, logits.shape = [batch, 1], иначе берём логит класса 1\n",
    "    if logits.shape[1] == 1:\n",
    "        scores = logits.squeeze(-1).detach().cpu().numpy()\n",
    "    else:\n",
    "        # предполагаем, что релевантность = логит класса 1\n",
    "        scores = logits[:, 1].detach().cpu().numpy()\n",
    "\n",
    "    # Добавляем score_ce к кандидатам\n",
    "    for ch, sc in zip(cand, scores):\n",
    "        ch[\"score_ce\"] = float(sc)\n",
    "\n",
    "    # Сортируем по score_ce (desc)\n",
    "    cand_sorted = sorted(cand, key=lambda x: x.get(\"score_ce\", 0.0), reverse=True)\n",
    "\n",
    "    # Оставляем top_k_final (можно потом крутить)\n",
    "    return cand_sorted[:top_k_final]\n",
    "\n",
    "\n",
    "def retrieve_relevant_chunks(\n",
    "    query: str,\n",
    "    top_k_faiss: int = 50,\n",
    "    top_k_bm25: int = 50,\n",
    "    top_k_final: int = CE_TOP_K_FINAL,\n",
    "    alpha_faiss: float = 0.6,\n",
    "    max_context_chars: int = MAX_CONTEXT_CHARS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Обновлённая версия:\n",
    "    1) Гибридный поиск (FAISS + BM25) → кандидаты с score_hybrid\n",
    "    2) Cross-encoder reranker (MS MARCO) → score_ce, сортировка\n",
    "    3) Берём top_k_final чанков, собираем контекст\n",
    "\n",
    "    Возвращает:\n",
    "    - dict с полями:\n",
    "        - \"chunks\": список выбранных чанков (dict)\n",
    "        - \"context_text\": текст, который пойдёт в LLM\n",
    "    \"\"\"\n",
    "    # --- 1. Гибридный поиск по эмбеддингам и BM25 ---\n",
    "    hybrid_candidates = hybrid_search_chunks(\n",
    "        query=query,\n",
    "        top_k_faiss=top_k_faiss,\n",
    "        top_k_bm25=top_k_bm25,\n",
    "        alpha_faiss=alpha_faiss,\n",
    "    )\n",
    "\n",
    "    if not hybrid_candidates:\n",
    "        return {\"chunks\": [], \"context_text\": \"\"}\n",
    "\n",
    "    # --- 2. Rerank candidates cross-encoder'ом ---\n",
    "    reranked = rerank_chunks_with_ce(query, hybrid_candidates, top_k_final=top_k_final)\n",
    "\n",
    "    # --- 3. Собираем context_text из reranked чанков ---\n",
    "    # Заодно избегаем дублей по chunk_id\n",
    "    seen_chunk_ids = set()\n",
    "    selected_chunks = []\n",
    "    context_parts = []\n",
    "\n",
    "    for ch in reranked:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        if cid in seen_chunk_ids:\n",
    "            continue\n",
    "        seen_chunk_ids.add(cid)\n",
    "\n",
    "        # достаём текст чанка\n",
    "        row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "        chunk_text = str(row_match.get(\"chunk_text\", \"\"))\n",
    "\n",
    "        selected_chunks.append(ch)\n",
    "        context_parts.append(chunk_text)\n",
    "\n",
    "    # Склеиваем контекст, при необходимости обрезаем по длине\n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    if len(context_text) > max_context_chars:\n",
    "        context_text = context_text[:max_context_chars]\n",
    "\n",
    "    return {\n",
    "        \"chunks\": selected_chunks,\n",
    "        \"context_text\": context_text,\n",
    "    }\n",
    "\n",
    "print(\"Cross-encoder reranker и обновлённый retrieve_relevant_chunks() определены.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "перезапустить (в каком порядке теперь это должно быть):\n",
    "\n",
    "\n",
    "Block 0 (пути, константы).\n",
    "\n",
    "Block 0.5 (копирование book.pdf и queries.json — если есть).\n",
    "\n",
    "Block 1 (список документов).\n",
    "\n",
    "Block 2 (чтение PDF → raw текст).\n",
    "\n",
    "Block 3A / 3B (чанкинг → chunks_df).\n",
    "\n",
    "Block 4A / 4B (эмбеддинги + FAISS-индекс).\n",
    "\n",
    "Block 5A / 5B (BM25 + hybrid_search_chunks).\n",
    "\n",
    "Block 10A (загрузка cross-encoder reranker).\n",
    "\n",
    "Block 10B (rerank + новый retrieve_relevant_chunks).\n",
    "\n",
    "Block 6A / 6B (LLM + generate_answer).\n",
    "\n",
    "Block 7B (build_rag_prompt, answer_question — уже будет использовать новый retrieve_relevant_chunks).\n",
    "\n",
    "Block 8 / 8B (ручная проверка).\n",
    "\n",
    "Block 9A / 9B (загрузка queries.json → генерация сабмита)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-query + Step-back generation (LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 11A. Multi-query generation (paraphrases + step-back) ===\n",
    "\n",
    "def generate_search_queries(\n",
    "    question: str,\n",
    "    num_paraphrases: int = 3,\n",
    "    add_step_back: bool = True,\n",
    "    max_new_tokens: int = 128,\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерирует несколько альтернативных поисковых запросов (multi-query)\n",
    "    + step-back (более общий вопрос).\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Запрос на перефразировки\n",
    "    para_prompt = (\n",
    "        \"Rewrite the question into several diverse search queries that can help \"\n",
    "        \"retrieve relevant passages from a textbook.\\n\"\n",
    "        f\"Original question: {question}\\n\\n\"\n",
    "        f\"Generate {num_paraphrases} short search queries in English.\\n\"\n",
    "        \"Return them as a numbered list. No extra text.\"\n",
    "    )\n",
    "\n",
    "    para_output = gen_pipe(\n",
    "        para_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )[0][\"generated_text\"]\n",
    "\n",
    "    # извлечём строки с запросами\n",
    "    paraphrases = []\n",
    "    for line in para_output.splitlines():\n",
    "        line = line.strip()\n",
    "        if line and (line[0].isdigit() or line.startswith(\"-\")):\n",
    "            q = line.split(\".\", 1)[-1].strip()\n",
    "            if len(q) > 0:\n",
    "                paraphrases.append(q)\n",
    "\n",
    "    # fallback если модель плохо отформатировала\n",
    "    paraphrases = paraphrases[:num_paraphrases]\n",
    "\n",
    "    # 2) step-back запрос\n",
    "    step_back_q = None\n",
    "    if add_step_back:\n",
    "        sb_prompt = (\n",
    "            \"Rewrite the question into a broader, more general version that still helps \"\n",
    "            \"retrieve relevant parts of a textbook.\\n\"\n",
    "            f\"Original question: {question}\\n\\n\"\n",
    "            \"Return only ONE short broader question in English.\"\n",
    "        )\n",
    "\n",
    "        sb_output = gen_pipe(\n",
    "            sb_prompt,\n",
    "            max_new_tokens=80,\n",
    "            do_sample=True,\n",
    "            temperature=0.5,\n",
    "            top_p=0.9,\n",
    "        )[0][\"generated_text\"]\n",
    "\n",
    "        # просто берём первую полноценную строку\n",
    "        sb_line = sb_output.strip().splitlines()[0]\n",
    "        step_back_q = sb_line.strip()\n",
    "\n",
    "    # финальный список запросов\n",
    "    queries = [question]\n",
    "    for q in paraphrases:\n",
    "        if q and q not in queries:\n",
    "            queries.append(q)\n",
    "\n",
    "    if step_back_q and step_back_q not in queries:\n",
    "        queries.append(step_back_q)\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 11B. Multi-query retrieval wrapper ===\n",
    "\n",
    "def get_candidates_multiquery(\n",
    "    question: str,\n",
    "    top_k_faiss: int = 50,\n",
    "    top_k_bm25: int = 50,\n",
    "    alpha_faiss: float = 0.6,\n",
    "):\n",
    "    \"\"\"\n",
    "    Генерирует несколько альтернативных поисковых запросов (multi-query),\n",
    "    делает гибридный поиск по каждому и объединяет кандидатов.\n",
    "    \"\"\"\n",
    "\n",
    "    queries = generate_search_queries(question)\n",
    "\n",
    "    all_candidates = []\n",
    "    for q in queries:\n",
    "        try:\n",
    "            cand = hybrid_search_chunks(\n",
    "                query=q,\n",
    "                top_k_faiss=top_k_faiss,\n",
    "                top_k_bm25=top_k_bm25,\n",
    "                alpha_faiss=alpha_faiss,\n",
    "            )\n",
    "            all_candidates.extend(cand)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] hybrid search failed for query '{q}': {e}\")\n",
    "\n",
    "    # deduplicate by chunk_id (оставляем лучший score_hybrid)\n",
    "    merged = {}\n",
    "    for ch in all_candidates:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        if cid not in merged or ch[\"score_hybrid\"] > merged[cid][\"score_hybrid\"]:\n",
    "            merged[cid] = ch\n",
    "\n",
    "    candidates = list(merged.values())\n",
    "    # сортируем по score_hybrid как первичному\n",
    "    candidates = sorted(candidates, key=lambda x: x[\"score_hybrid\"], reverse=True)\n",
    "\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 11C. Final retrieve_relevant_chunks (multi-query + reranker + adaptive K) ===\n",
    "\n",
    "ADAPTIVE_THRESHOLD = 0.0     # порог уверенности cross-encoder\n",
    "MIN_CHUNKS = 3\n",
    "MAX_CHUNKS = 8\n",
    "\n",
    "def retrieve_relevant_chunks(\n",
    "    query: str,\n",
    "    top_k_faiss: int = 50,\n",
    "    top_k_bm25: int = 50,\n",
    "    alpha_faiss: float = 0.6,\n",
    "    max_context_chars: int = MAX_CONTEXT_CHARS,\n",
    "):\n",
    "    \"\"\"\n",
    "    Улучшенная версия:\n",
    "    1. Генерация multi-query (paraphrases + step-back)\n",
    "    2. Hybrid search (FAISS + BM25) для каждого запроса\n",
    "    3. Дедупликация кандидатов\n",
    "    4. Cross-encoder reranking\n",
    "    5. Adaptive K (по уверенности, а не фиксированному количеству)\n",
    "    6. Формирование контекста\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Multi-query candidates\n",
    "    multi_candidates = get_candidates_multiquery(\n",
    "        question=query,\n",
    "        top_k_faiss=top_k_faiss,\n",
    "        top_k_bm25=top_k_bm25,\n",
    "        alpha_faiss=alpha_faiss,\n",
    "    )\n",
    "\n",
    "    if not multi_candidates:\n",
    "        return {\"chunks\": [], \"context_text\": \"\"}\n",
    "\n",
    "    # 2. Cross-encoder rerank\n",
    "    reranked = rerank_chunks_with_ce(\n",
    "        query=query,\n",
    "        candidates=multi_candidates,\n",
    "        top_k_final=MAX_CHUNKS * 3,   # временно берём много, потом фильтруем\n",
    "    )\n",
    "\n",
    "    # 3. Adaptive K\n",
    "    confident = [ch for ch in reranked if ch.get(\"score_ce\", 0) >= ADAPTIVE_THRESHOLD]\n",
    "\n",
    "    if len(confident) < MIN_CHUNKS:\n",
    "        selected = reranked[:MIN_CHUNKS]\n",
    "    else:\n",
    "        selected = confident[:MAX_CHUNKS]\n",
    "\n",
    "    # 4. Формируем context_text\n",
    "    seen = set()\n",
    "    parts = []\n",
    "    final_chunks = []\n",
    "\n",
    "    for ch in selected:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        if cid in seen:\n",
    "            continue\n",
    "        seen.add(cid)\n",
    "\n",
    "        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "        text = str(row[\"chunk_text\"])\n",
    "\n",
    "        final_chunks.append(ch)\n",
    "        parts.append(text)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(parts)\n",
    "    if len(context_text) > max_context_chars:\n",
    "        context_text = context_text[:max_context_chars]\n",
    "\n",
    "    return {\n",
    "        \"chunks\": final_chunks,\n",
    "        \"context_text\": context_text,\n",
    "    }\n",
    "\n",
    "print(\"MULTI-QUERY + RERANKER + ADAPTIVE-K retrieve_relevant_chunks loaded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПЕРВЫЙ ПРОХОД (полностью после изменения кода):\n",
    "\n",
    "Block 0 — импорты, пути\n",
    "\n",
    "Block 0.5 — копирование данных (если есть)\n",
    "\n",
    "Block 1 — список документов\n",
    "\n",
    "Block 2 — PDF → raw text\n",
    "\n",
    "Block 3A / 3B — чанкинг → chunks_df\n",
    "\n",
    "Block 4A / 4B — Embeddings → FAISS\n",
    "\n",
    "Block 5A / 5B — BM25 → hybrid_search_chunks\n",
    "\n",
    "Block 6A — LLM загрузка (gen_pipe)\n",
    "\n",
    "Block 10A — загрузка cross-encoder (reranker)\n",
    "\n",
    "Block 10B — определение rerank_chunks_with_ce\n",
    "\n",
    "Block 7B — build_rag_prompt, answer_question\n",
    "\n",
    "Block 11A — multi-query generation\n",
    "\n",
    "Block 11B — multiquery candidate gatherer\n",
    "\n",
    "Block 11C — замена retrieve_relevant_chunks\n",
    "\n",
    "Block 8 / 8B — тестирование\n",
    "\n",
    "Block 9A / 9B — генерация submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ПРИ ПОВТОРНОМ ЗАПУСКЕ (если ядро не перезагружалось):\n",
    "\n",
    "Можно запускать только:\n",
    "\n",
    "Block 10A\n",
    "\n",
    "Block 10B\n",
    "\n",
    "Block 11A\n",
    "\n",
    "Block 11B\n",
    "\n",
    "Block 11C\n",
    "\n",
    "Block 7B\n",
    "\n",
    "Block 8 или Block 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-RAG helper (draft + verify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 12A. Self-RAG helper: draft + verify ===\n",
    "\n",
    "def build_verify_prompt(question: str, context_text: str, draft_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Промпт для второго шага Self-RAG:\n",
    "    проверка и переписывание ответа строго по контексту.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are a verification and refinement module for a RAG system.\\n\"\n",
    "        \"Your job is to check whether the draft answer is FULLY supported by the provided context.\\n\"\n",
    "        \"If any part of the answer is not supported, you must either remove it or rephrase it so that it is strictly grounded in the context.\\n\"\n",
    "        \"If the context does not contain enough information to answer the question, you must explicitly say that.\\n\\n\"\n",
    "        \"=== CONTEXT ===\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"=== QUESTION ===\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"=== DRAFT ANSWER ===\\n\"\n",
    "        f\"{draft_answer}\\n\\n\"\n",
    "        \"=== TASK ===\\n\"\n",
    "        \"1) Identify unsupported or speculative parts.\\n\"\n",
    "        \"2) Rewrite the answer so that EVERY statement is supported by the context.\\n\"\n",
    "        \"3) Keep the answer clear and concise.\\n\"\n",
    "        \"4) Answer in English.\\n\\n\"\n",
    "        \"Return ONLY the final improved answer, without any meta-comments.\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def self_rag_answer(\n",
    "    question: str,\n",
    "    context_text: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature_draft: float = 0.4,\n",
    "    temperature_verify: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Двухшаговый ответ:\n",
    "    1) draft-ответ по стандартному RAG-промпту\n",
    "    2) проверка и переписывание ответа строго по контексту (Self-RAG)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Шаг 1. Черновой ответ ---\n",
    "    draft_prompt = build_rag_prompt(\n",
    "        query=question,\n",
    "        context_text=context_text,\n",
    "    )\n",
    "    draft_answer = generate_answer(\n",
    "        prompt=draft_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature_draft,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "    )\n",
    "\n",
    "    # --- Шаг 2. Верификация и улучшение ---\n",
    "    verify_prompt = build_verify_prompt(\n",
    "        question=question,\n",
    "        context_text=context_text,\n",
    "        draft_answer=draft_answer,\n",
    "    )\n",
    "    final_answer = generate_answer(\n",
    "        prompt=verify_prompt,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature_verify,\n",
    "        top_p=top_p,\n",
    "        do_sample=False,  # на верификации можно не сэмплить\n",
    "    )\n",
    "\n",
    "    return final_answer.strip(), draft_answer.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 12B. Обновлённый answer_question() с Self-RAG ===\n",
    "\n",
    "def answer_question(\n",
    "    question: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature_draft: float = 0.4,\n",
    "    temperature_verify: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    debug: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Высокоуровневая функция:\n",
    "    1) достаёт релевантные чанки (retrieve_relevant_chunks)\n",
    "    2) собирает context_text\n",
    "    3) делает двухшаговый Self-RAG:\n",
    "       - draft ответ по стандартному промпту\n",
    "       - verify+refine ответ строго по контексту\n",
    "    \"\"\"\n",
    "\n",
    "    retrieval = retrieve_relevant_chunks(\n",
    "        query=question,\n",
    "        top_k_faiss=50,\n",
    "        top_k_bm25=50,\n",
    "        alpha_faiss=0.6,\n",
    "        max_context_chars=MAX_CONTEXT_CHARS,\n",
    "    )\n",
    "    chunks = retrieval.get(\"chunks\", [])\n",
    "    context_text = retrieval.get(\"context_text\", \"\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== DEBUG: выбрано чанков ===\", len(chunks))\n",
    "        for ch in chunks:\n",
    "            cid = ch.get(\"chunk_id\")\n",
    "            did = ch.get(\"doc_id\")\n",
    "            s_h = ch.get(\"score_hybrid\", None)\n",
    "            s_ce = ch.get(\"score_ce\", None)\n",
    "            line = f\"- chunk_id={cid}, doc_id={did}, hybrid={s_h:.4f}\" if s_h is not None else f\"- chunk_id={cid}, doc_id={did}\"\n",
    "            if s_ce is not None:\n",
    "                line += f\", ce={s_ce:.4f}\"\n",
    "            print(line)\n",
    "        print()\n",
    "\n",
    "    final_answer, draft_answer = self_rag_answer(\n",
    "        question=question,\n",
    "        context_text=context_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature_draft=temperature_draft,\n",
    "        temperature_verify=temperature_verify,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    if debug:\n",
    "        print(\"[DRAFT ANSWER]:\")\n",
    "        print(draft_answer)\n",
    "        print(\"\\n[FINAL ANSWER]:\")\n",
    "        print(final_answer)\n",
    "\n",
    "    return final_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 12C. Обновлённый rag_answer_with_context_and_refs() с Self-RAG ===\n",
    "\n",
    "def rag_answer_with_context_and_refs(\n",
    "    question: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature_draft: float = 0.4,\n",
    "    temperature_verify: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Расширенная версия ответа для сабмитов:\n",
    "    - делает retrieve_relevant_chunks (multi-query + reranker)\n",
    "    - считает final_answer через Self-RAG (draft + verify)\n",
    "    - возвращает:\n",
    "        answer, context_text, refs_json, refs_dict\n",
    "    \"\"\"\n",
    "\n",
    "    retrieval = retrieve_relevant_chunks(\n",
    "        query=question,\n",
    "        top_k_faiss=50,\n",
    "        top_k_bm25=50,\n",
    "        alpha_faiss=0.6,\n",
    "        max_context_chars=MAX_CONTEXT_CHARS,\n",
    "    )\n",
    "    chunks = retrieval.get(\"chunks\", [])\n",
    "    context_text = retrieval.get(\"context_text\", \"\")\n",
    "\n",
    "    # Self-RAG\n",
    "    final_answer, draft_answer = self_rag_answer(\n",
    "        question=question,\n",
    "        context_text=context_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature_draft=temperature_draft,\n",
    "        temperature_verify=temperature_verify,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    # Собираем refs_dict\n",
    "    refs = {\"chunks\": []}\n",
    "    for ch in chunks:\n",
    "        cid = int(ch.get(\"chunk_id\"))\n",
    "        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "\n",
    "        ref_item = {\n",
    "            \"chunk_id\": cid,\n",
    "            \"doc_id\": int(ch.get(\"doc_id\", row.get(\"doc_id\", 0))),\n",
    "            \"path\": str(row.get(\"path\", \"\")),\n",
    "            \"score_faiss\": float(ch.get(\"score_faiss\", 0.0)),\n",
    "            \"score_bm25\": float(ch.get(\"score_bm25\", 0.0)),\n",
    "            \"score_hybrid\": float(ch.get(\"score_hybrid\", 0.0)),\n",
    "        }\n",
    "        if \"score_ce\" in ch:\n",
    "            ref_item[\"score_ce\"] = float(ch[\"score_ce\"])\n",
    "\n",
    "        # если у тебя в chunks_df есть 'page' или 'page_start' — можно добавить сюда:\n",
    "        if \"page\" in row.index:\n",
    "            ref_item[\"page\"] = int(row[\"page\"])\n",
    "\n",
    "        refs[\"chunks\"].append(ref_item)\n",
    "\n",
    "    refs_json = json.dumps(refs, ensure_ascii=False)\n",
    "\n",
    "    return final_answer.strip(), context_text, refs_json, refs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block 12A\n",
    "\n",
    "Block 12B\n",
    "\n",
    "Block 12C\n",
    "\n",
    "и дальше:\n",
    "\n",
    "Block 8 — для проверки качества ответов\n",
    "\n",
    "Block 9B — для нового сабмита"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## к слову о параметрах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''1. Retrieval Parameters (FAISS + BM25 + Multi-Query + Reranker)\n",
    "\n",
    "Эти параметры находятся в:\n",
    "\n",
    "Block 10B — Cross-encoder reranker\n",
    "\n",
    "Block 11A — Multi-query generation\n",
    "\n",
    "Block 11B — Candidate merging\n",
    "\n",
    "Block 11C — Final retrieve_relevant_chunks\n",
    "\n",
    "🔹 1.1. Adaptive K (Block 11C)\n",
    "ADAPTIVE_THRESHOLD = 0.0\n",
    "MIN_CHUNKS = 3\n",
    "MAX_CHUNKS = 8\n",
    "\n",
    "\n",
    "ADAPTIVE_THRESHOLD: порог по cross-encoder score.\n",
    "\n",
    "↑ больше (0.2–0.5) → меньше мусора\n",
    "\n",
    "↓ меньше (0.0) → шире покрытие\n",
    "\n",
    "MIN_CHUNKS: минимум чанков.\n",
    "\n",
    "увеличить до 4 → если LLM слишком поверхностна\n",
    "\n",
    "уменьшить до 2 → если слишком многословно\n",
    "\n",
    "MAX_CHUNKS: максимум чанков.\n",
    "\n",
    "увеличить до 10–12 → лучше покрытие\n",
    "\n",
    "уменьшить до 6 → меньше шума\n",
    "\n",
    "🔹 1.2. Hybrid Search Parameters (Block 11B)\n",
    "top_k_faiss = 50\n",
    "top_k_bm25 = 50\n",
    "alpha_faiss = 0.6\n",
    "\n",
    "\n",
    "top_k_faiss, top_k_bm25:\n",
    "\n",
    "↑ до 80–100 → больше кандидатов для reranker\n",
    "\n",
    "↓ до 30 → быстрее, но хуже качество\n",
    "\n",
    "alpha_faiss:\n",
    "\n",
    "ближе к 0.7–0.8 → лучше семантический поиск\n",
    "\n",
    "ближе к 0.4–0.5 → лучше по ключевым словам (если вопросы прямые)\n",
    "\n",
    "🔹 1.3. Reranker Parameters (Block 10B)\n",
    "CE_TOP_K_CANDIDATES = 50\n",
    "CE_TOP_K_FINAL = 8\n",
    "\n",
    "\n",
    "CE_TOP_K_CANDIDATES: сколько кандидатов проверяет cross-encoder\n",
    "\n",
    "↑ до 80–100 → выше качество поиска\n",
    "\n",
    "↓ до 30 → быстрее\n",
    "\n",
    "CE_TOP_K_FINAL: сколько берём после reranker до adaptive K\n",
    "\n",
    "обычно от 8 до 16.\n",
    "\n",
    "🔹 1.4. Multi-Query Generation Parameters (Block 11A)\n",
    "num_paraphrases = 3\n",
    "temperature (paraphrases) = 0.7\n",
    "temperature (step-back) = 0.5\n",
    "\n",
    "\n",
    "num_paraphrases:\n",
    "\n",
    "↑ до 4–5 → более устойчивый поиск\n",
    "\n",
    "↓ до 2 → быстрее\n",
    "\n",
    "Температуры:\n",
    "\n",
    "↑ → разнообразные формулировки\n",
    "\n",
    "↓ → более точные формулировки\n",
    "\n",
    "#️⃣ 2. Self-RAG Parameters (Block 12A / 12B / 12C)\n",
    "\n",
    "Эти параметры управляют качеством финального ответа.\n",
    "\n",
    "🔹 2.1. Draft generation\n",
    "temperature_draft = 0.4\n",
    "top_p = 0.9\n",
    "\n",
    "\n",
    "↑ до 0.5–0.6 → ответы богаче и разнообразнее\n",
    "\n",
    "↓ до 0.2–0.3 → меньше галлюцинаций, строже привязка к контексту\n",
    "\n",
    "🔹 2.2. Verification stage\n",
    "temperature_verify = 0.2\n",
    "\n",
    "\n",
    "не стоит сильно поднимать — это «строгая» стадия\n",
    "\n",
    "↑ до 0.3–0.4 → чуть плавнее формулировки\n",
    "\n",
    "↓ до 0.1 → максимально строгий ответ «по контексту»\n",
    "\n",
    "🔹 2.3. Token Limits\n",
    "max_new_tokens = 256\n",
    "\n",
    "\n",
    "↑ до 320–384 → если ответы обрезаются\n",
    "\n",
    "↓ до 192 → если модель многословна\n",
    "\n",
    "#️⃣ 3. Prompting Parameters (Block 7B + Block 12A)\n",
    "🔹 3.1. Output Structure Strictness\n",
    "\n",
    "Можно усилить:\n",
    "\n",
    "более строгие инструкции\n",
    "\n",
    "указание «не использовать информацию вне контекста»\n",
    "\n",
    "требование ссылок (pages / sections)\n",
    "\n",
    "🔹 3.2. Chain-of-Thought\n",
    "\n",
    "Можно включить короткий CoT:\n",
    "\n",
    "Think step-by-step, but keep reasoning concise (no more than 3–4 steps).\n",
    "\n",
    "\n",
    "в draft-промпт.\n",
    "\n",
    "#️⃣ 4. Chunking & Context Parameters (Block 3A/3B)\n",
    "🔹 4.1. Overlap\n",
    "\n",
    "↑ overlap до 40–50% → больше связности\n",
    "\n",
    "↓ до 20% → меньше шума, короче индекс\n",
    "\n",
    "🔹 4.2. Max context length\n",
    "\n",
    "В блоке retrieve_relevant_chunks:\n",
    "\n",
    "max_context_chars = MAX_CONTEXT_CHARS\n",
    "\n",
    "\n",
    "↑ до 50k → больше контекста (если модель тянет)\n",
    "\n",
    "↓ до 20k → если ответы слишком загромождены\n",
    "\n",
    "#️⃣ 5. Что влияет сильнее всего (приоритет)\n",
    "🥇 ТОП-5 ПАРАМЕТРОВ, которые дают реальный прирост:\n",
    "\n",
    "ADAPTIVE_THRESHOLD\n",
    "\n",
    "MAX_CHUNKS / MIN_CHUNKS\n",
    "\n",
    "num_paraphrases (multi-query)\n",
    "\n",
    "top_k_faiss / top_k_bm25\n",
    "\n",
    "temperature_draft\n",
    "\n",
    "🥈 Средней важности:\n",
    "\n",
    "alpha_faiss\n",
    "\n",
    "temperatures при multi-query\n",
    "\n",
    "chunk overlap\n",
    "\n",
    "CE_TOP_K_CANDIDATES\n",
    "\n",
    "🥉 Наименее критичные:\n",
    "\n",
    "temperature_verify\n",
    "\n",
    "top_p\n",
    "\n",
    "max_new_tokens'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## новая версия build_verify_prompt с указанием источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 13A. Обновлённый build_verify_prompt с указанием источников ===\n",
    "\n",
    "def build_verify_prompt(question: str, context_text: str, draft_answer: str) -> str:\n",
    "    \"\"\"\n",
    "    Промпт для второго шага Self-RAG:\n",
    "    проверка и переписывание ответа строго по контексту + указание источников.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = (\n",
    "        \"You are a verification and refinement module for a RAG system.\\n\"\n",
    "        \"Your task is to check whether the DRAFT ANSWER is fully supported by the CONTEXT.\\n\"\n",
    "        \"If any part of the answer is not supported, you must remove it or rewrite it so that it is strictly grounded in the context.\\n\"\n",
    "        \"If the context does not contain enough information to answer the question, you must explicitly say that in the final answer.\\n\\n\"\n",
    "        \"=== CONTEXT ===\\n\"\n",
    "        f\"{context_text}\\n\\n\"\n",
    "        \"=== QUESTION ===\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"=== DRAFT ANSWER ===\\n\"\n",
    "        f\"{draft_answer}\\n\\n\"\n",
    "        \"=== TASK ===\\n\"\n",
    "        \"1) Identify unsupported, speculative, or hallucinated parts of the draft answer.\\n\"\n",
    "        \"2) Rewrite the answer so that EVERY statement is supported by the context.\\n\"\n",
    "        \"3) The final answer MUST be clear, concise and in ENGLISH.\\n\"\n",
    "        \"4) After the main answer, add a short 'SOURCES:' section where you briefly list which parts of the context support the key points of the answer.\\n\"\n",
    "        \"   For example: 'SOURCES: paragraph about Pavlov's experiments; section describing behaviorism; part about Taylor's management theory'.\\n\"\n",
    "        \"5) Do NOT invent page numbers; use only descriptions that can be inferred from the context.\\n\\n\"\n",
    "        \"=== OUTPUT FORMAT (STRICT) ===\\n\"\n",
    "        \"FINAL ANSWER:\\n\"\n",
    "        \"- <your final, verified answer here>\\n\\n\"\n",
    "        \"SOURCES:\\n\"\n",
    "        \"- <short description of the main supporting fragment 1>\\n\"\n",
    "        \"- <short description of the main supporting fragment 2>\\n\"\n",
    "        \"- ...\\n\\n\"\n",
    "        \"Return ONLY the 'FINAL ANSWER' and 'SOURCES' sections, without any extra commentary.\\n\"\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запусти Block 13A.\n",
    "\n",
    "Запусти Block 12A не нужно заново (если ядро не перезапускалось) — мы просто переопределили функцию, она уже в памяти.\n",
    "\n",
    "Запусти Block 12B и 12C, если ты их менял после этого (не обязательно, но безопасно).\n",
    "\n",
    "Запусти Block 8\n",
    "\n",
    "я уже засыпаю мм"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## лёгкий sentence-level trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Block 14. Sentence-level trimming контекста под вопрос ===\n",
    "# ИДЕЯ:\n",
    "#   - каждый выбранный чанк мы режем на предложения;\n",
    "#   - для каждого предложения считаем \"похожесть\" на вопрос по пересечению ключевых слов;\n",
    "#   - оставляем N самых релевантных предложений, сохраняя исходный порядок;\n",
    "#   - если текст и так короткий — оставляем как есть.\n",
    "#\n",
    "# ПЛЮС:\n",
    "#   - меньше шума в контексте → LLM проще держаться фактов;\n",
    "#   - self-RAG видит более сфокусированный текст.\n",
    "#\n",
    "# ВАЖНО:\n",
    "#   - мы НЕ трогаем FAISS/BM25/reranker, только финальный context_text.\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Небольшой набор английских стоп-слов, чтобы не учитывать \"the, and, of, to, ...\"\n",
    "_TRIM_STOPWORDS = {\n",
    "    # русские\n",
    "    \"и\", \"в\", \"во\", \"не\", \"что\", \"он\", \"она\", \"оно\", \"они\",\n",
    "    \"как\", \"а\", \"но\", \"yet\", \"да\", \"или\", \"либо\",\n",
    "    \"к\", \"ко\", \"от\", \"до\", \"из\", \"за\", \"над\", \"под\", \"при\", \"по\", \"о\", \"об\", \"обо\",\n",
    "    \"на\", \"с\", \"со\", \"у\", \"про\", \"для\", \"без\", \"через\", \"между\",\n",
    "    \"это\", \"этот\", \"эта\", \"это\", \"эти\", \"того\", \"той\", \"тех\",\n",
    "    \"тот\", \"та\", \"те\", \"там\", \"тут\", \"здесь\",\n",
    "    \"же\", \"уж\", \"ли\", \"бы\", \"то\", \"же\", \"уже\", \"ещё\", \"ещё\", \"тоже\",\n",
    "    \"быть\", \"есть\", \"был\", \"была\", \"были\", \"будет\", \"будут\",\n",
    "    \"мы\", \"вы\", \"ты\", \"они\", \"он\", \"она\",\n",
    "    \"мой\", \"моя\", \"мои\", \"твой\", \"твоя\", \"твои\", \"наш\", \"наша\", \"наши\",\n",
    "    \"их\", \"его\", \"ее\", \"её\", \"сам\", \"сама\", \"сами\",\n",
    "    \"там\", \"здесь\", \"сюда\", \"туда\",\n",
    "    # английские (на всякий случай — вдруг смешанный текст)\n",
    "    \"the\", \"a\", \"an\", \"and\", \"or\", \"for\", \"of\", \"to\", \"in\", \"on\", \"at\",\n",
    "    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n",
    "    \"this\", \"that\", \"these\", \"those\",\n",
    "    \"by\", \"with\", \"from\", \"as\", \"it\", \"its\",\n",
    "    \"about\", \"into\", \"over\", \"under\", \"between\", \"through\",\n",
    "    \"he\", \"she\", \"they\", \"them\", \"his\", \"her\", \"their\",\n",
    "}\n",
    "\n",
    "def _simple_tokenize(text: str):\n",
    "    \"\"\"\n",
    "    Очень простой токенайзер:\n",
    "    - приводим к нижнему регистру\n",
    "    - выкидываем все, что не буква/цифра/пробел\n",
    "    - сплитим по пробелам\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n",
    "    tokens = [t for t in text.split() if t]\n",
    "    return tokens\n",
    "\n",
    "def _keywords(text: str):\n",
    "    \"\"\"\n",
    "    Ключевые слова: токены без стоп-слов и длиной >= 3.\n",
    "    \"\"\"\n",
    "    toks = _simple_tokenize(text)\n",
    "    return [t for t in toks if len(t) >= 3 and t not in _TRIM_STOPWORDS]\n",
    "\n",
    "def split_into_sentences(text: str):\n",
    "    \"\"\"\n",
    "    Очень простой сплиттер на предложения:\n",
    "    режем по .!? с сохранением базовой структуры.\n",
    "    \"\"\"\n",
    "    # Заменяем перевод строки на пробел — часто в pdf они стоят внутри предложений\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    # Грубый сплит по конечным знакам. Это не идеально, но для учебника ок.\n",
    "    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    sentences = [s.strip() for s in parts if s.strip()]\n",
    "    return sentences\n",
    "\n",
    "def score_sentence_relevance(sentence: str, question: str) -> float:\n",
    "    \"\"\"\n",
    "    Оценка релевантности предложения к вопросу.\n",
    "    Очень простой скор:\n",
    "      score = |пересечение ключевых слов| / (1 + |ключевые слова предложения|)\n",
    "    \"\"\"\n",
    "    q_kw = set(_keywords(question))\n",
    "    s_kw = _keywords(sentence)\n",
    "    if not s_kw or not q_kw:\n",
    "        return 0.0\n",
    "    inter = q_kw.intersection(s_kw)\n",
    "    score = len(inter) / (1.0 + len(s_kw))\n",
    "    return score\n",
    "\n",
    "def trim_chunk_for_question(\n",
    "    chunk_text: str,\n",
    "    question: str,\n",
    "    max_sentences: int = 3,\n",
    "    min_chars: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Основная функция тримминга чанка:\n",
    "    - если чанк короткий (<= min_chars) — возвращаем как есть;\n",
    "    - иначе:\n",
    "        * режем на предложения\n",
    "        * если предложений мало (<= max_sentences) — тоже возвращаем как есть\n",
    "        * считаем score_sentence_relevance для каждого предложения\n",
    "        * выбираем top-k предложений по score\n",
    "        * сортируем их в исходном порядке и склеиваем обратно\n",
    "        * если по каким-то причинам всё пусто — fallback к исходному тексту\n",
    "    \"\"\"\n",
    "    text = chunk_text.strip()\n",
    "    if len(text) <= min_chars:\n",
    "        return text\n",
    "\n",
    "    sentences = split_into_sentences(text)\n",
    "    if len(sentences) <= max_sentences:\n",
    "        return text\n",
    "\n",
    "    # считаем скор для каждой фразы\n",
    "    scored = []\n",
    "    for idx, sent in enumerate(sentences):\n",
    "        sc = score_sentence_relevance(sent, question)\n",
    "        scored.append((idx, sent, sc))\n",
    "\n",
    "    # если все скоры нулевые — лучше ничего не трогать\n",
    "    if all(sc == 0.0 for _, _, sc in scored):\n",
    "        return text\n",
    "\n",
    "    # сортируем по score (по убыванию), берём top-k,\n",
    "    # затем сортируем по исходному индексу, чтобы сохранить порядок\n",
    "    scored_sorted = sorted(scored, key=lambda x: x[2], reverse=True)\n",
    "    top = scored_sorted[:max_sentences]\n",
    "    top_sorted_by_idx = sorted(top, key=lambda x: x[0])\n",
    "\n",
    "    trimmed_sentences = [s for _, s, _ in top_sorted_by_idx]\n",
    "    trimmed_text = \" \".join(trimmed_sentences).strip()\n",
    "\n",
    "    # на всякий случай fallback\n",
    "    if len(trimmed_text) < 1:\n",
    "        return text\n",
    "\n",
    "    return trimmed_text\n",
    "\n",
    "\n",
    "# --- Переопределяем retrieve_relevant_chunks, чтобы использовать тримминг внутри ---\n",
    "# ВАЖНО:\n",
    "#   - предполагается, что уже определены:\n",
    "#       * get_candidates_multiquery(...)\n",
    "#       * rerank_chunks_with_ce(...)\n",
    "#       * chunks_df\n",
    "#       * MAX_CONTEXT_CHARS\n",
    "#   - то есть этот блок надо запускать ПОСЛЕ Block 11A/11B/11C и 10A/10B.\n",
    "\n",
    "ADAPTIVE_THRESHOLD = 0.0     # можно тюнить\n",
    "MIN_CHUNKS = 3\n",
    "MAX_CHUNKS = 8\n",
    "\n",
    "def retrieve_relevant_chunks(\n",
    "    query: str,\n",
    "    top_k_faiss: int = 50,\n",
    "    top_k_bm25: int = 50,\n",
    "    alpha_faiss: float = 0.6,\n",
    "    max_context_chars: int = MAX_CONTEXT_CHARS,\n",
    "    trim_max_sentences: int = 3,\n",
    "    trim_min_chars: int = 300,\n",
    "):\n",
    "    \"\"\"\n",
    "    Финальная версия:\n",
    "    1) multi-query (paraphrases + step-back)\n",
    "    2) hybrid search (FAISS + BM25)\n",
    "    3) cross-encoder rerank\n",
    "    4) Adaptive K (по score_ce)\n",
    "    5) sentence-level trimming внутри каждого чанка\n",
    "    6) склейка контекста\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Multi-query candidates\n",
    "    multi_candidates = get_candidates_multiquery(\n",
    "        question=query,\n",
    "        top_k_faiss=top_k_faiss,\n",
    "        top_k_bm25=top_k_bm25,\n",
    "        alpha_faiss=alpha_faiss,\n",
    "    )\n",
    "\n",
    "    if not multi_candidates:\n",
    "        return {\"chunks\": [], \"context_text\": \"\"}\n",
    "\n",
    "    # 2. Cross-encoder rerank\n",
    "    reranked = rerank_chunks_with_ce(\n",
    "        query=query,\n",
    "        candidates=multi_candidates,\n",
    "        top_k_final=MAX_CHUNKS * 3,   # сначала берём побольше, потом фильтруем\n",
    "    )\n",
    "\n",
    "    # 3. Adaptive K\n",
    "    confident = [ch for ch in reranked if ch.get(\"score_ce\", 0) >= ADAPTIVE_THRESHOLD]\n",
    "\n",
    "    if len(confident) < MIN_CHUNKS:\n",
    "        selected = reranked[:MIN_CHUNKS]\n",
    "    else:\n",
    "        selected = confident[:MAX_CHUNKS]\n",
    "\n",
    "    # 4. Формируем context_text с триммингом предложений\n",
    "    seen = set()\n",
    "    parts = []\n",
    "    final_chunks = []\n",
    "\n",
    "    for ch in selected:\n",
    "        cid = int(ch[\"chunk_id\"])\n",
    "        if cid in seen:\n",
    "            continue\n",
    "        seen.add(cid)\n",
    "\n",
    "        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "        raw_text = str(row[\"chunk_text\"])\n",
    "\n",
    "        trimmed = trim_chunk_for_question(\n",
    "            chunk_text=raw_text,\n",
    "            question=query,\n",
    "            max_sentences=trim_max_sentences,\n",
    "            min_chars=trim_min_chars,\n",
    "        )\n",
    "\n",
    "        final_chunks.append(ch)\n",
    "        parts.append(trimmed)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(parts)\n",
    "    if len(context_text) > max_context_chars:\n",
    "        context_text = context_text[:max_context_chars]\n",
    "\n",
    "    return {\n",
    "        \"chunks\": final_chunks,\n",
    "        \"context_text\": context_text,\n",
    "    }\n",
    "\n",
    "print(\"Sentence-level trimming для retrieve_relevant_chunks() активирован.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# формирование сабмита иначе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T04:53:57.978242Z",
     "iopub.status.busy": "2025-11-15T04:53:57.977495Z",
     "iopub.status.idle": "2025-11-15T04:53:57.987027Z",
     "shell.execute_reply": "2025-11-15T04:53:57.986211Z",
     "shell.execute_reply.started": "2025-11-15T04:53:57.978216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_question() обновлён: поддерживает temperature=..., возвращает plain answer.\n"
     ]
    }
   ],
   "source": [
    "# === Block 15. Post-processing: извлечение \"чистого\" ответа без служебных заголовков ===\n",
    "\n",
    "import re\n",
    "\n",
    "def _extract_section(text: str, start_label: str, end_labels: list):\n",
    "    \"\"\"\n",
    "    Вырезает кусок текста между start_label и ближайшим из end_labels.\n",
    "    Если не найдено — возвращает None.\n",
    "    \"\"\"\n",
    "    if start_label not in text:\n",
    "        return None\n",
    "    part = text.split(start_label, 1)[1]\n",
    "    for end in end_labels:\n",
    "        if end in part:\n",
    "            part = part.split(end, 1)[0]\n",
    "    return part.strip()\n",
    "\n",
    "\n",
    "def extract_plain_answer(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Преобразует \"форматный\" ответ (с заголовками вроде SHORT ANSWER:, FINAL ANSWER:, SOURCES:)\n",
    "    в обычный ответ, без служебных меток.\n",
    "    Приоритет:\n",
    "      1) Если есть FINAL ANSWER + SOURCES -> берём только FINAL ANSWER.\n",
    "      2) Иначе, если есть SHORT ANSWER -> берём только SHORT ANSWER.\n",
    "      3) Иначе возвращаем текст как есть (обрезанный по краям).\n",
    "    Также убираем начальные маркеры \"- \" и пустые строки.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text)\n",
    "\n",
    "    raw = text.strip()\n",
    "\n",
    "    # 1) Пытаемся вытащить FINAL ANSWER\n",
    "    section = _extract_section(\n",
    "        raw,\n",
    "        start_label=\"FINAL ANSWER:\",\n",
    "        end_labels=[\"SOURCES:\", \"Sources:\", \"SOURCE:\", \"КРАТКИЙ ОТВЕТ:\", \"SHORT ANSWER:\", \"ОБОСНОВАНИЕ:\"],\n",
    "    )\n",
    "    if section:\n",
    "        cleaned_lines = []\n",
    "        for line in section.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"- \"):\n",
    "                line = line[2:].strip()\n",
    "            cleaned_lines.append(line)\n",
    "        if cleaned_lines:\n",
    "            return \" \".join(cleaned_lines).strip()\n",
    "\n",
    "    # 2) Пытаемся вытащить SHORT ANSWER\n",
    "    section = _extract_section(\n",
    "        raw,\n",
    "        start_label=\"SHORT ANSWER:\",\n",
    "        end_labels=[\"REASONING:\", \"MISSING INFORMATION:\", \"SOURCES:\", \"ОБОСНОВАНИЕ:\", \"НЕДОСТАЮЩИЕ ДАННЫЕ:\"],\n",
    "    )\n",
    "    if section:\n",
    "        cleaned_lines = []\n",
    "        for line in section.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"- \"):\n",
    "                line = line[2:].strip()\n",
    "            cleaned_lines.append(line)\n",
    "        if cleaned_lines:\n",
    "            return \" \".join(cleaned_lines).strip()\n",
    "\n",
    "    # 3) Пытаемся вытащить КРАТКИЙ ОТВЕТ (если вдруг модель всё ещё отвечает по-русски)\n",
    "    section = _extract_section(\n",
    "        raw,\n",
    "        start_label=\"КРАТКИЙ ОТВЕТ:\",\n",
    "        end_labels=[\"ОБОСНОВАНИЕ:\", \"НЕДОСТАЮЩИЕ ДАННЫЕ:\", \"SOURCES:\", \"REASONING:\"],\n",
    "    )\n",
    "    if section:\n",
    "        cleaned_lines = []\n",
    "        for line in section.splitlines():\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"- \"):\n",
    "                line = line[2:].strip()\n",
    "            cleaned_lines.append(line)\n",
    "        if cleaned_lines:\n",
    "            return \" \".join(cleaned_lines).strip()\n",
    "\n",
    "    # 4) Ничего не нашли — просто возвращаем текст как есть, без лишних переносов\n",
    "    return \" \".join(raw.split())\n",
    "\n",
    "\n",
    "# --- Обновляем answer_question, чтобы он возвращал PLAIN ANSWER для сабмита ---\n",
    "\n",
    "\n",
    "def answer_question(\n",
    "    question: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature_draft: float = 0.4,\n",
    "    temperature_verify: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    debug: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Та же логика Self-RAG, но наружу (и в сабмит) возвращаем уже \"очищенный\" ответ\n",
    "    без служебных заголовков.\n",
    "    \"\"\"\n",
    "\n",
    "    retrieval = retrieve_relevant_chunks(\n",
    "        query=question,\n",
    "        top_k_faiss=50,\n",
    "        top_k_bm25=50,\n",
    "        alpha_faiss=0.6,\n",
    "        max_context_chars=MAX_CONTEXT_CHARS,\n",
    "    )\n",
    "    chunks = retrieval.get(\"chunks\", [])\n",
    "    context_text = retrieval.get(\"context_text\", \"\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"=== DEBUG: выбрано чанков ===\", len(chunks))\n",
    "        for ch in chunks:\n",
    "            cid = ch.get(\"chunk_id\")\n",
    "            did = ch.get(\"doc_id\")\n",
    "            s_h = ch.get(\"score_hybrid\", None)\n",
    "            s_ce = ch.get(\"score_ce\", None)\n",
    "            line = f\"- chunk_id={cid}, doc_id={did}, hybrid={s_h:.4f}\" if s_h is not None else f\"- chunk_id={cid}, doc_id={did}\"\n",
    "            if s_ce is not None:\n",
    "                line += f\", ce={s_ce:.4f}\"\n",
    "            print(line)\n",
    "        print()\n",
    "\n",
    "    final_answer, draft_answer = self_rag_answer(\n",
    "        question=question,\n",
    "        context_text=context_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature_draft=temperature_draft,\n",
    "        temperature_verify=temperature_verify,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    plain_answer = extract_plain_answer(final_answer)\n",
    "\n",
    "    if debug:\n",
    "        print(\"[DRAFT ANSWER]:\")\n",
    "        print(draft_answer)\n",
    "        print(\"\\n[FINAL ANSWER RAW]:\")\n",
    "        print(final_answer)\n",
    "        print(\"\\n[PLAIN ANSWER]:\")\n",
    "        print(plain_answer)\n",
    "\n",
    "    return plain_answer\n",
    "\n",
    "\n",
    "# --- Обновляем rag_answer_with_context_and_refs, чтобы в сабмит уходил PLAIN ANSWER ---\n",
    "\n",
    "\n",
    "def rag_answer_with_context_and_refs(\n",
    "    question: str,\n",
    "    max_new_tokens: int = 256,\n",
    "    temperature_draft: float = 0.4,\n",
    "    temperature_verify: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "):\n",
    "    \"\"\"\n",
    "    Как и раньше, но теперь:\n",
    "      - финальный ответ, который возвращаем первым — уже очищенный plain answer.\n",
    "      - refs_dict, context_text остаются без изменений.\n",
    "    \"\"\"\n",
    "\n",
    "    retrieval = retrieve_relevant_chunks(\n",
    "        query=question,\n",
    "        top_k_faiss=50,\n",
    "        top_k_bm25=50,\n",
    "        alpha_faiss=0.6,\n",
    "        max_context_chars=MAX_CONTEXT_CHARS,\n",
    "    )\n",
    "    chunks = retrieval.get(\"chunks\", [])\n",
    "    context_text = retrieval.get(\"context_text\", \"\")\n",
    "\n",
    "    final_answer, draft_answer = self_rag_answer(\n",
    "        question=question,\n",
    "        context_text=context_text,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature_draft=temperature_draft,\n",
    "        temperature_verify=temperature_verify,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "    plain_answer = extract_plain_answer(final_answer)\n",
    "\n",
    "    # Собираем refs_dict\n",
    "    refs = {\"chunks\": []}\n",
    "    for ch in chunks:\n",
    "        cid = int(ch.get(\"chunk_id\"))\n",
    "        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n",
    "\n",
    "        ref_item = {\n",
    "            \"chunk_id\": cid,\n",
    "            \"doc_id\": int(ch.get(\"doc_id\", row.get(\"doc_id\", 0))),\n",
    "            \"path\": str(row.get(\"path\", \"\")),\n",
    "            \"score_faiss\": float(ch.get(\"score_faiss\", 0.0)),\n",
    "            \"score_bm25\": float(ch.get(\"score_bm25\", 0.0)),\n",
    "            \"score_hybrid\": float(ch.get(\"score_hybrid\", 0.0)),\n",
    "        }\n",
    "        if \"score_ce\" in ch:\n",
    "            ref_item[\"score_ce\"] = float(ch[\"score_ce\"])\n",
    "        if \"page\" in row.index:\n",
    "            ref_item[\"page\"] = int(row[\"page\"])\n",
    "\n",
    "        refs[\"chunks\"].append(ref_item)\n",
    "\n",
    "    refs_json = json.dumps(refs, ensure_ascii=False)\n",
    "\n",
    "    return plain_answer, context_text, refs_json, refs\n",
    "\n",
    "print(\"Post-processing plain answer (без SHORT ANSWER/FINAL ANSWER) активирован.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8739556,
     "sourceId": 13735613,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
