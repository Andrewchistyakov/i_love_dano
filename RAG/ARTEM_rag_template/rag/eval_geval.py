import json
from pathlib import Path
from statistics import mean
from typing import Dict, Any, List

from deepeval.metrics import (
    AnswerRelevancyMetric,
    FaithfulnessMetric,
    GEval,
)
from deepeval.test_case import LLMTestCase, LLMTestCaseParams

from .config import AppConfig
from .pipeline import RAGPipeline


def run_geval_eval(cfg: AppConfig, pipeline: RAGPipeline, dataset_path: str) -> None:
    dataset_path = Path(dataset_path)
    if not dataset_path.exists():
        raise FileNotFoundError(f"Eval dataset not found: {dataset_path}")

    results_dir = Path(cfg.eval.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)
    out_path = results_dir / "geval_results.jsonl"

    eval_model = cfg.eval.eval_model or cfg.llm.model

    # –ú–µ—Ç—Ä–∏–∫–∏
    answer_rel = AnswerRelevancyMetric(model=eval_model)
    faithfulness = FaithfulnessMetric(model=eval_model)
    correctness = GEval(
        name="Correctness",
        model=eval_model,
        evaluation_params=[
            LLMTestCaseParams.ACTUAL_OUTPUT,
            LLMTestCaseParams.EXPECTED_OUTPUT,
        ],
        evaluation_steps=[
            "–ü—Ä–æ–≤–µ—Ä—å, –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∞—Ç –ª–∏ —Ñ–∞–∫—Ç—ã –≤ 'actual output' —Ñ–∞–∫—Ç–∞–º –≤ 'expected output'.",
            "–°–∏–ª—å–Ω–æ —à—Ç—Ä–∞—Ñ—É–π –ø—Ä–æ–ø—É—Å–∫–∏ –∫–ª—é—á–µ–≤—ã—Ö —Ñ–∞–∫—Ç–æ–≤.",
            "–õ—ë–≥–∫–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞—Ö –¥–æ–ø—É—Å—Ç–∏–º–∞.",
        ],
        threshold=0.6,
    )

    print(f"üìä –ó–∞–ø—É—Å–∫–∞–µ–º eval –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É {dataset_path} —Å –º–æ–¥–µ–ª—å—é {eval_model}")
    scores_rel: List[float] = []
    scores_faith: List[float] = []
    scores_corr: List[float] = []

    with open(dataset_path, "r", encoding="utf-8") as f_in, \
         open(out_path, "w", encoding="utf-8") as f_out:

        for line in f_in:
            if not line.strip():
                continue
            item = json.loads(line)

            q = item["query"]
            expected = item.get("expected_answer", "")

            rag_result = pipeline.answer(q)
            answer = rag_result["answer"]
            contexts = [c["text"] for c in rag_result["contexts"]]

            # LLMTestCase –¥–ª—è RAG
            tc = LLMTestCase(
                input=q,
                actual_output=answer,
                expected_output=expected or None,
                retrieval_context=contexts,
                context=item.get("gold_context", None),
            )

            # —Å—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
            answer_rel.measure(tc)
            faithfulness.measure(tc)
            correctness.measure(tc)

            res_record: Dict[str, Any] = {
                "id": item.get("id"),
                "query": q,
                "expected_answer": expected,
                "answer": answer,
                "contexts": contexts,
                "metrics": {
                    "answer_relevancy": {
                        "score": answer_rel.score,
                        "reason": getattr(answer_rel, "reason", None),
                    },
                    "faithfulness": {
                        "score": faithfulness.score,
                        "reason": getattr(faithfulness, "reason", None),
                    },
                    "correctness": {
                        "score": correctness.score,
                        "reason": getattr(correctness, "reason", None),
                    },
                },
            }
            f_out.write(json.dumps(res_record, ensure_ascii=False) + "\n")

            scores_rel.append(answer_rel.score)
            scores_faith.append(faithfulness.score)
            scores_corr.append(correctness.score)

    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã eval —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {out_path}")
    if scores_rel:
        print(f"AnswerRelevancy: mean={mean(scores_rel):.3f}")
        print(f"Faithfulness:    mean={mean(scores_faith):.3f}")
        print(f"Correctness:     mean={mean(scores_corr):.3f}")