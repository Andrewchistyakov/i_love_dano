{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b5c1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3424ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cbc207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "db_name = 'CASML - Generative AI Hackathon'\n",
    "LLM_model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "cross_encoder_id = \"BAAI/bge-reranker-v2-m3\"\n",
    "embedding_model_id = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "chunk_overlap = 300\n",
    "chunk_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d823a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "# texts = {}\n",
    "\n",
    "# with pdfplumber.open(\"data/book.pdf\") as pdf:\n",
    "#     text = \"\"\n",
    "#     for i, page in enumerate(pdf.pages):\n",
    "#         if i < 18 or i > 642:\n",
    "#             continue\n",
    "#         text = page.extract_text()\n",
    "#         texts[i - 18] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf9d4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(texts, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09a830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052f1f5",
   "metadata": {},
   "source": [
    "**ОЧИСТКА ТЕКСТА**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_headers_footers(text, header_patterns=None, footer_patterns=None):\n",
    "    if header_patterns is None:\n",
    "        header_patterns = [r'^.*Header.*$']\n",
    "    if footer_patterns is None:\n",
    "        footer_patterns = [r'^.*Footer.*$']\n",
    "\n",
    "    for pattern in header_patterns + footer_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def remove_special_characters(text, special_chars=None):\n",
    "    if special_chars is None:\n",
    "        special_chars = r'[^A-Za-z0-9\\s\\.,;:\\'\\\"\\?\\!\\-]'\n",
    "\n",
    "    text = re.sub(special_chars, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_repeated_substrings(text, pattern=r'\\.{2,}'):\n",
    "    text = re.sub(pattern, '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text, metadata=None, language=None):\n",
    "    # optional: detect language, skip if not target\n",
    "    # remove headers/footers\n",
    "    text = remove_headers_footers(text)\n",
    "    # fix unicode quirks\n",
    "    # text = ftfy.fix_text(text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # remove URLs/emails\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # normalize whitespace\n",
    "    text = remove_extra_spaces(text)\n",
    "    # remove special characters\n",
    "    text = remove_special_characters(text)\n",
    "    # lowercase (optional)\n",
    "    text = text.lower()\n",
    "    # lemmatize (optional) — using spaCy or any other\n",
    "    # chunking can happen here or after\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0aa3c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_num in texts:\n",
    "    texts[page_num] = texts[page_num].replace(\"\\n\", \" \")\n",
    "    texts[page_num] = preprocess_text(texts[page_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfe39f",
   "metadata": {},
   "source": [
    "**ЧАНКИНГ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63bf8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Базовый чанкинг\n",
    "\n",
    "text_chunks = [{i: text_splitter.split_text(texts[str(i)])} for i in tqdm(range(len(texts)))]\n",
    "\n",
    "text_chunks_numbered = []\n",
    "\n",
    "for chunk_dict in text_chunks:\n",
    "    key, values = next(iter(chunk_dict.items()))\n",
    "    for chunk in values:\n",
    "        text_chunks_numbered.append((key, chunk))\n",
    "\n",
    "# Добавляем глобальный индекс\n",
    "global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "\n",
    "global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4f36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 2782.45it/s]\n"
     ]
    }
   ],
   "source": [
    "# # Не всегда лучше \n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# semantic_chunks = {}\n",
    "\n",
    "# for i in tqdm(range(len(texts))):\n",
    "#     text = texts[str(i)]\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     joined_sentences = []\n",
    "#     buffer = \"\"\n",
    "\n",
    "#     # объединяем предложения в блоки <= chunk_size\n",
    "#     for sent in sentences:\n",
    "#         if len(buffer) + len(sent) + 1 <= chunk_size:\n",
    "#             buffer += \" \" + sent\n",
    "#         else:\n",
    "#             joined_sentences.append(buffer.strip())\n",
    "#             buffer = sent\n",
    "#     if buffer:\n",
    "#         joined_sentences.append(buffer.strip())\n",
    "\n",
    "#     # если отдельный блок всё ещё длинный — применяем RecursiveCharacterTextSplitter\n",
    "#     refined_chunks = []\n",
    "#     for chunk in joined_sentences:\n",
    "#         if len(chunk) > chunk_size:\n",
    "#             refined_chunks.extend(text_splitter.split_text(chunk))\n",
    "#         else:\n",
    "#             refined_chunks.append(chunk)\n",
    "\n",
    "#     semantic_chunks[i] = refined_chunks\n",
    "\n",
    "# # выравниваем в общий список\n",
    "# text_chunks_numbered = [(i, chunk) for i, chunks in semantic_chunks.items() for chunk in chunks]\n",
    "\n",
    "# # глобальная нумерация чанков\n",
    "# global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "# global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6779cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_id, model_kwargs={'dtype': torch.float16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb0ad915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a113bc23bd854042906d35ad2d807fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = embedding_model.encode(text_chunks, batch_size=32, device=device, normalize_embeddings=True, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b2fc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_np = np.asarray(vectors, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27cb15c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=db_name,\n",
    "    on_disk_payload=True,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1024,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c2d6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2431/2431 [00:01<00:00, 1250.67it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(vectors))):\n",
    "    client.upsert(\n",
    "        collection_name=db_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors[i],\n",
    "                payload={\n",
    "                    'text': text_chunks[i],\n",
    "                    'page': page_numbers[i] + 7,\n",
    "                    'chunk_index': global_chunk_ids[i]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17e3310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('data/queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f77e6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f25dc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.5, top_p=0.9) -> str:\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "383bb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_prompt = \"You are a query rewriter specialized in mental health and psychology topics. Your task is to rewrite the user’s question into short, clear search queries for vector retrieval.\\n\\nSTRICT RULES:\\n- Output EXACTLY THREE rewritten query variants.\\n- Separate them each on a NEW LINE.\\n- DO NOT explain, comment, justify, or describe anything.\\n- DO NOT answer the question.\\n- Output ONLY the rewritten queries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b223123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_query(query_text) -> list[str]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rephrase_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Rephrase this question into exactly three short search queries separated by '\\n'. Do not explain anything. Question: {query_text}\"},\n",
    "    ]\n",
    "    output = generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.3, top_p=0.9)\n",
    "    parts = output.split(\"\\n\")\n",
    "    result = [p.strip() for p in parts][:3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8811fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем перефразированные запросы\n",
    "\n",
    "# for i in tqdm(range(len(queries))):\n",
    "#     print(queries[i]['question'])\n",
    "#     new_queries = rephrase_query(queries[i]['question'])\n",
    "#     queries[i]['new_queries'] = new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "003cb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/new_queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(queries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8027c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e9923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/new_queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2168bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_corpus = [word_tokenize(t.lower()) for t in text_chunks]\n",
    "bm25 = BM25Okapi(bm25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72e5fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(queries, top_k_db=5, alpha_vec=0.9, alpha_bm25=0.1):\n",
    "    results = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query['query_id']\n",
    "        query_texts = [query['question']] + query['new_queries']\n",
    "\n",
    "        all_hits = []\n",
    "\n",
    "        for qt in query_texts:\n",
    "            query_vec = embedding_model.encode(qt, normalize_embeddings=True, device=device).tolist()\n",
    "            query_vec_np = np.asarray(query_vec, dtype=np.float32)\n",
    "\n",
    "            # --- Векторный поиск ---\n",
    "            hits = client.search(collection_name=db_name, query_vector=query_vec, limit=top_k_db)\n",
    "\n",
    "            # --- BM25 ---\n",
    "            tokenized_query = word_tokenize(qt.lower())\n",
    "            bm25_scores = bm25.get_scores(tokenized_query)\n",
    "            bm25_scores = np.array(bm25_scores)\n",
    "            bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n",
    "            bm25_norm = np.power(bm25_scores, 0.5)\n",
    "\n",
    "            # --- Собираем кандидатов ---\n",
    "            seen_indices = set()\n",
    "            for hit in hits:\n",
    "                seen_indices.add(hit.payload['chunk_index'])\n",
    "                all_hits.append({\n",
    "                    \"chunk_index\": hit.payload['chunk_index'],\n",
    "                    \"text\": hit.payload['text'],\n",
    "                    \"page\": hit.payload['page'],\n",
    "                    \"score_vec\": hit.score,\n",
    "                    \"score_bm25\": bm25_norm[hit.payload['chunk_index']],\n",
    "                    \"source\": \"qdrant\"\n",
    "                })\n",
    "\n",
    "            # Добавляем недостающих кандидатов из BM25-топа\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-top_k_db:][::-1]\n",
    "            for idx in top_bm25_idx:\n",
    "                if idx not in seen_indices:\n",
    "                    score_vec_direct = float(np.dot(query_vec_np, vectors_np[idx]))\n",
    "                    score_vec_direct = min(max(score_vec_direct, 0.0), 1.0)\n",
    "                    all_hits.append({\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"text\": text_chunks[idx],\n",
    "                        \"page\": page_numbers[idx] + 7,\n",
    "                        \"score_vec\": score_vec_direct,\n",
    "                        \"score_bm25\": bm25_norm[idx],\n",
    "                        \"source\": \"bm25\"\n",
    "                    })\n",
    "\n",
    "        # --- Комбинируем по весам ---\n",
    "        dedup = {}\n",
    "        for item in all_hits:\n",
    "            ci = item[\"chunk_index\"]\n",
    "            combined_score = alpha_vec * item[\"score_vec\"] + alpha_bm25 * item[\"score_bm25\"]\n",
    "            if ci not in dedup or combined_score > dedup[ci][\"score\"]:\n",
    "                dedup[ci] = {\n",
    "                    \"text\": item[\"text\"],\n",
    "                    \"page\": item[\"page\"],\n",
    "                    \"score\": combined_score,\n",
    "                    \"chunk_index\": ci,\n",
    "                    \"source\": item[\"source\"],\n",
    "                    \"score_vec\": item[\"score_vec\"],\n",
    "                    \"score_bm25\": item[\"score_bm25\"]\n",
    "                }\n",
    "\n",
    "        final_items = sorted(dedup.values(), key=lambda x: x[\"score\"], reverse=True)[:top_k_db]\n",
    "\n",
    "        if final_items:\n",
    "            scores = [i[\"score\"] for i in final_items]\n",
    "            vec_scores = [i[\"score_vec\"] for i in final_items]\n",
    "            bm25_scores = [i[\"score_bm25\"] for i in final_items]\n",
    "            qdrant_count = sum(1 for i in final_items if i[\"source\"] == \"qdrant\")\n",
    "            bm25_count = sum(1 for i in final_items if i[\"source\"] == \"bm25\")\n",
    "\n",
    "            print(f\"[{qid}] total={len(final_items)} | qdrant={qdrant_count}, bm25={bm25_count}\")\n",
    "            print(f\"   combined  -> min={min(scores):.4f}, max={max(scores):.4f}\")\n",
    "            print(f\"   qdrant(sc)-> min={min(vec_scores):.4f}, max={max(vec_scores):.4f}\")\n",
    "            print(f\"   bm25(sc)  -> min={min(bm25_scores):.4f}, max={max(bm25_scores):.4f}\")\n",
    "\n",
    "        results[qid] = final_items\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bbfec84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]C:\\Users\\Степан\\AppData\\Local\\Temp\\ipykernel_5928\\3240959167.py:15: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(collection_name=db_name, query_vector=query_vec, limit=top_k_db)\n",
      "  4%|▍         | 2/50 [00:00<00:09,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] total=100 | qdrant=78, bm25=22\n",
      "   combined  -> min=0.7966, max=0.8509\n",
      "   qdrant(sc)-> min=0.7857, max=0.8422\n",
      "   bm25(sc)  -> min=0.6102, max=1.0000\n",
      "[2] total=100 | qdrant=83, bm25=17\n",
      "   combined  -> min=0.7860, max=0.8614\n",
      "   qdrant(sc)-> min=0.7690, max=0.8460\n",
      "   bm25(sc)  -> min=0.5801, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:06,  7.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] total=100 | qdrant=84, bm25=16\n",
      "   combined  -> min=0.7842, max=0.8638\n",
      "   qdrant(sc)-> min=0.7734, max=0.8579\n",
      "   bm25(sc)  -> min=0.5785, max=1.0000\n",
      "[4] total=100 | qdrant=84, bm25=16\n",
      "   combined  -> min=0.7767, max=0.8769\n",
      "   qdrant(sc)-> min=0.7738, max=0.8721\n",
      "   bm25(sc)  -> min=0.3652, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:00<00:05,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] total=100 | qdrant=71, bm25=29\n",
      "   combined  -> min=0.7877, max=0.8303\n",
      "   qdrant(sc)-> min=0.7776, max=0.8252\n",
      "   bm25(sc)  -> min=0.6193, max=1.0000\n",
      "[6] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.8005, max=0.8566\n",
      "   qdrant(sc)-> min=0.7959, max=0.8507\n",
      "   bm25(sc)  -> min=0.6633, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:01<00:05,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] total=100 | qdrant=80, bm25=20\n",
      "   combined  -> min=0.7966, max=0.8373\n",
      "   qdrant(sc)-> min=0.7750, max=0.8366\n",
      "   bm25(sc)  -> min=0.6756, max=1.0000\n",
      "[8] total=100 | qdrant=84, bm25=16\n",
      "   combined  -> min=0.7767, max=0.8780\n",
      "   qdrant(sc)-> min=0.7756, max=0.8668\n",
      "   bm25(sc)  -> min=0.5524, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:01<00:04,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] total=100 | qdrant=80, bm25=20\n",
      "   combined  -> min=0.7972, max=0.8705\n",
      "   qdrant(sc)-> min=0.7828, max=0.8716\n",
      "   bm25(sc)  -> min=0.6329, max=1.0000\n",
      "[10] total=100 | qdrant=83, bm25=17\n",
      "   combined  -> min=0.7784, max=0.8809\n",
      "   qdrant(sc)-> min=0.7697, max=0.8676\n",
      "   bm25(sc)  -> min=0.5647, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:01<00:04,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] total=100 | qdrant=93, bm25=7\n",
      "   combined  -> min=0.7948, max=0.8423\n",
      "   qdrant(sc)-> min=0.7831, max=0.8380\n",
      "   bm25(sc)  -> min=0.6683, max=1.0000\n",
      "[12] total=100 | qdrant=89, bm25=11\n",
      "   combined  -> min=0.7306, max=0.8461\n",
      "   qdrant(sc)-> min=0.7314, max=0.8305\n",
      "   bm25(sc)  -> min=0.4867, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:01<00:04,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] total=100 | qdrant=67, bm25=33\n",
      "   combined  -> min=0.7778, max=0.8560\n",
      "   qdrant(sc)-> min=0.7627, max=0.8571\n",
      "   bm25(sc)  -> min=0.5843, max=1.0000\n",
      "[14] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.7763, max=0.8364\n",
      "   qdrant(sc)-> min=0.7755, max=0.8333\n",
      "   bm25(sc)  -> min=0.4637, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:02<00:03,  8.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] total=100 | qdrant=78, bm25=22\n",
      "   combined  -> min=0.7892, max=0.8605\n",
      "   qdrant(sc)-> min=0.7709, max=0.8536\n",
      "   bm25(sc)  -> min=0.6481, max=1.0000\n",
      "[16] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.7769, max=0.8725\n",
      "   qdrant(sc)-> min=0.7767, max=0.8722\n",
      "   bm25(sc)  -> min=0.3714, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:02<00:03,  8.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] total=100 | qdrant=77, bm25=23\n",
      "   combined  -> min=0.7670, max=0.8490\n",
      "   qdrant(sc)-> min=0.7571, max=0.8528\n",
      "   bm25(sc)  -> min=0.4844, max=1.0000\n",
      "[18] total=100 | qdrant=87, bm25=13\n",
      "   combined  -> min=0.7677, max=0.8924\n",
      "   qdrant(sc)-> min=0.7663, max=0.8891\n",
      "   bm25(sc)  -> min=0.4063, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:02<00:03,  8.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] total=100 | qdrant=84, bm25=16\n",
      "   combined  -> min=0.7658, max=0.8610\n",
      "   qdrant(sc)-> min=0.7538, max=0.8610\n",
      "   bm25(sc)  -> min=0.5151, max=1.0000\n",
      "[20] total=100 | qdrant=81, bm25=19\n",
      "   combined  -> min=0.7924, max=0.8767\n",
      "   qdrant(sc)-> min=0.7807, max=0.8759\n",
      "   bm25(sc)  -> min=0.6163, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:02<00:03,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] total=100 | qdrant=87, bm25=13\n",
      "   combined  -> min=0.7870, max=0.8594\n",
      "   qdrant(sc)-> min=0.7790, max=0.8511\n",
      "   bm25(sc)  -> min=0.5849, max=1.0000\n",
      "[22] total=100 | qdrant=75, bm25=25\n",
      "   combined  -> min=0.7922, max=0.8516\n",
      "   qdrant(sc)-> min=0.7792, max=0.8508\n",
      "   bm25(sc)  -> min=0.5959, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:02<00:02,  8.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23] total=100 | qdrant=69, bm25=31\n",
      "   combined  -> min=0.7695, max=0.8538\n",
      "   qdrant(sc)-> min=0.7568, max=0.8456\n",
      "   bm25(sc)  -> min=0.4558, max=1.0000\n",
      "[24] total=100 | qdrant=77, bm25=23\n",
      "   combined  -> min=0.7814, max=0.8551\n",
      "   qdrant(sc)-> min=0.7746, max=0.8436\n",
      "   bm25(sc)  -> min=0.4392, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:03<00:02,  8.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] total=100 | qdrant=72, bm25=28\n",
      "   combined  -> min=0.7863, max=0.8655\n",
      "   qdrant(sc)-> min=0.7902, max=0.8556\n",
      "   bm25(sc)  -> min=0.4873, max=1.0000\n",
      "[26] total=100 | qdrant=84, bm25=16\n",
      "   combined  -> min=0.7852, max=0.8571\n",
      "   qdrant(sc)-> min=0.7816, max=0.8640\n",
      "   bm25(sc)  -> min=0.5049, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:03<00:02,  8.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27] total=100 | qdrant=63, bm25=37\n",
      "   combined  -> min=0.7623, max=0.8660\n",
      "   qdrant(sc)-> min=0.7691, max=0.8512\n",
      "   bm25(sc)  -> min=0.0000, max=1.0000\n",
      "[28] total=100 | qdrant=81, bm25=19\n",
      "   combined  -> min=0.7997, max=0.8604\n",
      "   qdrant(sc)-> min=0.7814, max=0.8533\n",
      "   bm25(sc)  -> min=0.5444, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:03<00:02,  8.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29] total=100 | qdrant=88, bm25=12\n",
      "   combined  -> min=0.7781, max=0.8650\n",
      "   qdrant(sc)-> min=0.7764, max=0.8631\n",
      "   bm25(sc)  -> min=0.5031, max=1.0000\n",
      "[30] total=100 | qdrant=86, bm25=14\n",
      "   combined  -> min=0.7953, max=0.8702\n",
      "   qdrant(sc)-> min=0.7869, max=0.8618\n",
      "   bm25(sc)  -> min=0.5168, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [00:03<00:02,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.7850, max=0.8640\n",
      "   qdrant(sc)-> min=0.7668, max=0.8604\n",
      "   bm25(sc)  -> min=0.5793, max=1.0000\n",
      "[32] total=100 | qdrant=85, bm25=15\n",
      "   combined  -> min=0.7974, max=0.8463\n",
      "   qdrant(sc)-> min=0.7822, max=0.8375\n",
      "   bm25(sc)  -> min=0.6297, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:04<00:01,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.7840, max=0.8696\n",
      "   qdrant(sc)-> min=0.7798, max=0.8551\n",
      "   bm25(sc)  -> min=0.5988, max=1.0000\n",
      "[34] total=100 | qdrant=86, bm25=14\n",
      "   combined  -> min=0.7766, max=0.8516\n",
      "   qdrant(sc)-> min=0.7653, max=0.8351\n",
      "   bm25(sc)  -> min=0.5680, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:04<00:01,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35] total=100 | qdrant=67, bm25=33\n",
      "   combined  -> min=0.7819, max=0.8304\n",
      "   qdrant(sc)-> min=0.7590, max=0.8327\n",
      "   bm25(sc)  -> min=0.5987, max=1.0000\n",
      "[36] total=100 | qdrant=78, bm25=22\n",
      "   combined  -> min=0.7998, max=0.8413\n",
      "   qdrant(sc)-> min=0.7873, max=0.8327\n",
      "   bm25(sc)  -> min=0.6481, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:04<00:01,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37] total=100 | qdrant=87, bm25=13\n",
      "   combined  -> min=0.8034, max=0.8557\n",
      "   qdrant(sc)-> min=0.7905, max=0.8510\n",
      "   bm25(sc)  -> min=0.5151, max=1.0000\n",
      "[38] total=100 | qdrant=89, bm25=11\n",
      "   combined  -> min=0.8041, max=0.8740\n",
      "   qdrant(sc)-> min=0.7959, max=0.8654\n",
      "   bm25(sc)  -> min=0.5360, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [00:04<00:01,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.8007, max=0.8858\n",
      "   qdrant(sc)-> min=0.7874, max=0.8753\n",
      "   bm25(sc)  -> min=0.6242, max=1.0000\n",
      "[40] total=100 | qdrant=75, bm25=25\n",
      "   combined  -> min=0.7930, max=0.8561\n",
      "   qdrant(sc)-> min=0.7799, max=0.8401\n",
      "   bm25(sc)  -> min=0.5913, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [00:05<00:00,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41] total=100 | qdrant=70, bm25=30\n",
      "   combined  -> min=0.8007, max=0.8672\n",
      "   qdrant(sc)-> min=0.7808, max=0.8525\n",
      "   bm25(sc)  -> min=0.4806, max=1.0000\n",
      "[42] total=100 | qdrant=86, bm25=14\n",
      "   combined  -> min=0.8008, max=0.8742\n",
      "   qdrant(sc)-> min=0.7821, max=0.8651\n",
      "   bm25(sc)  -> min=0.5988, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [00:05<00:00,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43] total=100 | qdrant=88, bm25=12\n",
      "   combined  -> min=0.7741, max=0.8566\n",
      "   qdrant(sc)-> min=0.7683, max=0.8408\n",
      "   bm25(sc)  -> min=0.4954, max=1.0000\n",
      "[44] total=100 | qdrant=79, bm25=21\n",
      "   combined  -> min=0.7803, max=0.8558\n",
      "   qdrant(sc)-> min=0.7686, max=0.8564\n",
      "   bm25(sc)  -> min=0.5467, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [00:05<00:00,  8.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45] total=100 | qdrant=79, bm25=21\n",
      "   combined  -> min=0.7860, max=0.8238\n",
      "   qdrant(sc)-> min=0.7778, max=0.8302\n",
      "   bm25(sc)  -> min=0.5431, max=1.0000\n",
      "[46] total=100 | qdrant=72, bm25=28\n",
      "   combined  -> min=0.7824, max=0.8554\n",
      "   qdrant(sc)-> min=0.7670, max=0.8483\n",
      "   bm25(sc)  -> min=0.5411, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [00:05<00:00,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47] total=100 | qdrant=76, bm25=24\n",
      "   combined  -> min=0.7997, max=0.8403\n",
      "   qdrant(sc)-> min=0.7895, max=0.8277\n",
      "   bm25(sc)  -> min=0.5898, max=1.0000\n",
      "[48] total=100 | qdrant=88, bm25=12\n",
      "   combined  -> min=0.7976, max=0.8506\n",
      "   qdrant(sc)-> min=0.7861, max=0.8456\n",
      "   bm25(sc)  -> min=0.6306, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49] total=100 | qdrant=82, bm25=18\n",
      "   combined  -> min=0.7995, max=0.8303\n",
      "   qdrant(sc)-> min=0.7834, max=0.8401\n",
      "   bm25(sc)  -> min=0.6795, max=1.0000\n",
      "[50] total=100 | qdrant=88, bm25=12\n",
      "   combined  -> min=0.7863, max=0.8791\n",
      "   qdrant(sc)-> min=0.7829, max=0.8668\n",
      "   bm25(sc)  -> min=0.4536, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = get_candidates(queries, top_k_db=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf094e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_db_score(relevant, score_threshold=0.0):\n",
    "    filtered = {}\n",
    "\n",
    "    for qid, items in relevant.items():\n",
    "        selected = [item for item in items if item[\"score\"] >= score_threshold]\n",
    "        filtered[qid] = selected\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# relevant = filter_by_db_score(relevant, score_threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931a32f",
   "metadata": {},
   "source": [
    "*Список моделей для Cross-Encoder*\n",
    "\n",
    "\n",
    "-BAAI/bge-reranker-v2-m3\n",
    "\n",
    "-cross‑encoder/ms‑marco‑MiniLM‑L6‑v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5c15d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_encoder_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18c909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реранк на основе только исходного вопроса\n",
    "\n",
    "# def rerank_results(queries, relevant, top_k_rerank=3, rerank_threshold=None):\n",
    "#     reranked = {}\n",
    "\n",
    "#     for query in tqdm(queries):\n",
    "#         qid = query['query_id']\n",
    "#         q_text = query['question']\n",
    "\n",
    "#         candidates = relevant[qid]\n",
    "#         texts = [c[\"text\"] for c in candidates]\n",
    "\n",
    "#         if len(texts) == 0:\n",
    "#             reranked[qid] = []\n",
    "#             continue\n",
    "\n",
    "#         pairs = [(q_text, t) for t in texts]\n",
    "#         scores = cross_encoder.predict(pairs)  # по аналогии с твоим примером\n",
    "\n",
    "#         scored = [\n",
    "#             {**c, \"rerank_score\": s}\n",
    "#             for c, s in zip(candidates, scores)\n",
    "#         ]\n",
    "\n",
    "#         scored = sorted(scored, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "#         scored = scored[:top_k_rerank]\n",
    "\n",
    "#         if rerank_threshold is not None:\n",
    "#             scored = [s for s in scored if s[\"rerank_score\"] >= rerank_threshold]\n",
    "\n",
    "#         reranked[qid] = scored\n",
    "\n",
    "#     return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02d650ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(queries, relevant, top_k_rerank=3, rerank_threshold=None):\n",
    "    reranked = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query[\"query_id\"]\n",
    "        query_texts = [query[\"question\"]] + query.get(\"new_queries\", [])\n",
    "        candidates = relevant[qid]\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "\n",
    "        if not texts:\n",
    "            reranked[qid] = []\n",
    "            continue\n",
    "\n",
    "        # Считаем средний скор по всем перефразам\n",
    "        total_scores = np.zeros(len(texts))\n",
    "        for q_text in query_texts:\n",
    "            pairs = [(q_text, t) for t in texts]\n",
    "            scores = cross_encoder.predict(pairs)\n",
    "            total_scores += np.array(scores)\n",
    "\n",
    "        avg_scores = total_scores / len(query_texts)\n",
    "\n",
    "        scored = [\n",
    "            {**c, \"rerank_score\": s}\n",
    "            for c, s in zip(candidates, avg_scores)\n",
    "        ]\n",
    "\n",
    "        scored = sorted(scored, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "        scored = scored[:top_k_rerank]\n",
    "\n",
    "        if rerank_threshold is not None:\n",
    "            scored = [s for s in scored if s[\"rerank_score\"] >= rerank_threshold]\n",
    "\n",
    "        reranked[qid] = scored\n",
    "\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c6bad970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [02:50<00:00,  3.41s/it]\n"
     ]
    }
   ],
   "source": [
    "relevant_reranked = rerank_results(\n",
    "    queries,\n",
    "    relevant,\n",
    "    top_k_rerank=6,         # регулируй\n",
    "    rerank_threshold=None   # или поставь, например, 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "149dc9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': '1',\n",
       " 'question': 'What is the scientific method in psychology?',\n",
       " 'new_queries': ['What does the scientific method entail in psychology?',\n",
       "  'How can I understand the scientific method used in psychology research?',\n",
       "  'What are the key steps of the scientific method applied in psychological studies?']}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "120cc51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'thus, psychological science is empirical, based on measurable data. in general, science deals only with matter and energy, that is, those things that can be measured, and it cannot arrive at knowledge about values and morality. this is one reason why our scientific understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. the scientific method is also a form of empiricism. anempirical methodfor acquiring knowledge is one based on observation, including experimentation, rather than a method based only on forms of logical argument or previous authorities. it was not until the late 1800s that psychology became accepted as its own academic discipline. before this time, the workings of the mind were considered under the auspices of philosophy. given that any behavior is, at its roots, biological, some areas of psychology take on aspects of a natural science like biology.',\n",
       "  'page': 8,\n",
       "  'score': 0.83656948781172,\n",
       "  'chunk_index': 4,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8347081806543848,\n",
       "  'score_bm25': 0.8533212522277366,\n",
       "  'rerank_score': 0.6067520342767239},\n",
       " {'text': 'given demographic shifts occurring in the united states and increased access to higher educational opportunities among historically underrepresented populations, there is reason to hope that the diversity of the field will increasingly match the larger population, and that the research contributions made by the psychologists of the future will better serve people of all backgrounds women and minorities in psychology, n.d.. the process of scientific research scientific knowledge is advanced through a process known as thescientific method. basically, ideas in the form of theories and hypotheses are tested against the real world in the form of empirical observations, and those empirical observations lead to more ideas that are tested against the real world, and so on. in this sense, the scientific process is circular. the types of reasoning within the circle are called deductive and inductive.',\n",
       "  'page': 39,\n",
       "  'score': 0.8508862636336328,\n",
       "  'chunk_index': 119,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8407043020728162,\n",
       "  'score_bm25': 0.9425239176809814,\n",
       "  'rerank_score': 0.447895810008049},\n",
       " {'text': 'psychology is defined as the scientific study of mind and behavior. students of psychology develop critical thinking skills, become familiar with the scientific method, and recognize the complexity of behavior. 1.2history of psychology before the time of wundt and james, questions about the mind were considered by philosophers. however, both wundt and james helped create psychology as a distinct scientific discipline. wundt was a structuralist, which meant he believed that our cognitive experience was best understood by breaking that experience into its component parts. he thought this was best accomplished by introspection. access for free at openstax.org',\n",
       "  'page': 30,\n",
       "  'score': 0.8169413701964303,\n",
       "  'chunk_index': 90,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8178444751397844,\n",
       "  'score_bm25': 0.8088134257062433,\n",
       "  'rerank_score': 0.40150532498955727},\n",
       " {'text': '8 1  introduction to psychology 2002. nash was the subject of the 2001 moviea beautiful mind. why did these people have these experiences? how does the human brain work? and what is the connection between the brains internal processes and peoples external behaviors? this textbook will introduce you to various ways that the field of psychology has explored these questions. 1.1what is psychology? learning objectives by the end of this section, you will be able to:  define psychology  understand the merits of an education in psychology what is creativity? what are prejudice and discrimination? what is consciousness? the field of psychology explores questions like these.psychologyrefers to the scientific study of the mind and behavior. psychologists use the scientific method to acquire knowledge. to apply the scientific method, a researcher with a question about how or why something happens will propose a tentative explanation, called a hypothesis, to explain the phenomenon.',\n",
       "  'page': 8,\n",
       "  'score': 0.8431833831634912,\n",
       "  'chunk_index': 2,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8257593146282105,\n",
       "  'score_bm25': 0.9999999999810173,\n",
       "  'rerank_score': 0.3881250377744436},\n",
       " {'text': 'in fact, this research has been conducted and while the emotional experiences of people deprived of an awareness of their physiological arousal may be less intense, they still experience emotion chwalisz, diener,  gallagher, 1988. scientific researchs dependence on falsifiability allows for great confidence in the information that it produces. typically, by the time information is accepted by the scientific community, it has been tested repeatedly. 2.2approaches to research learning objectives by the end of this section, you will be able to:  describe the different research methods used by psychologists  discuss the strengths and weaknesses of case studies, naturalistic observation, surveys, and archival research  compare longitudinal and cross-sectional approaches to research  compare and contrast correlation and causation there are many research methods available to psychologists in their efforts to understand, describe, and',\n",
       "  'page': 41,\n",
       "  'score': 0.8295688643534667,\n",
       "  'chunk_index': 127,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8269891782664003,\n",
       "  'score_bm25': 0.8527860391370635,\n",
       "  'rerank_score': 0.3797128200531006},\n",
       " {'text': '1.2  history of psychology 9 an education in psychology is valuable for a number of reasons. psychology students hone critical thinking skills and are trained in the use of the scientific method. critical thinking is the active application of a set of skills to information for the understanding and evaluation of that information. the evaluation of informationassessing its reliability and usefulness is an important skill in a world full of competing facts, many of which are designed to be misleading. for example, critical thinking involves maintaining an attitude of skepticism, recognizing internal biases, making use of logical thinking, asking appropriate questions, and making observations.',\n",
       "  'page': 9,\n",
       "  'score': 0.8227083421669824,\n",
       "  'chunk_index': 7,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8145755466664738,\n",
       "  'score_bm25': 0.8959035016715601,\n",
       "  'rerank_score': 0.3723264615982771}]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_reranked[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00941f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очищаем память, чтобы LLM было вкусно\n",
    "\n",
    "# del cross_encoder\n",
    "# del embedding_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3175a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO почитать про промпты\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert in psychology.\\n\\n\"\n",
    "    \"Your task is to answer the user’s question using strictly and only the information \"\n",
    "    \"from the retrieved context chunks provided below.\\n\\n\"\n",
    "    \"Each chunk contains a label indicating which section of the original source it came from. \"\n",
    "    \"When making each factual claim, explicitly reference the section label of the chunk it came from. \"\n",
    "    \"If the answer cannot be fully supported by the provided chunks, respond: \\\"Not enough information to answer the question.\\\"\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. Do not add external knowledge, assumptions, or general reasoning not grounded in the retrieved chunks.\\n\"\n",
    "    \"2. Do not merge or generalize facts unless they are directly stated in the chunks.\\n\"\n",
    "    \"3. Each factual statement must include a source label in parentheses, e.g. (section: ...).\\n\"\n",
    "    \"4. If multiple chunks contradict each other, do not resolve the contradiction; instead, state that there is a contradiction.\\n\\n\"\n",
    "    \"Output format:\\n\"\n",
    "    \"- First, provide the final answer based solely on the evidence.\\n\"\n",
    "    \"- Then, list each used factual statement with its corresponding section reference.\\n\\n\"\n",
    "    \"Now wait for the user’s question and the retrieved context chunks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a98881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer(query, context):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    answer = generation_pipeline(messages, temperature=0.7, max_new_tokens=1024)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def llm_answer(query, context):\n",
    "#     system_msg = (\n",
    "#         \"You are an expert in psychology. Using only the provided retrieved documents, answer the following question. Do not add any external knowledge.\"\n",
    "#     )\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_msg},\n",
    "#         {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "#         {\"role\": \"user\", \"content\": query}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     )\n",
    "#     prompt = \"Привет, как дела?\"\n",
    "#     payload = {\n",
    "#         \"prompt\": prompt,\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.9,\n",
    "#         \"top_p\": 0.9\n",
    "#     }\n",
    "\n",
    "#     r = requests.post(\"https://shortly-pleasant-democrats-discussion.trycloudflare.com/generate\", json=payload)\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     # Модель возвращает полный текст, нужно вырезать только продолжение\n",
    "#     response_text = r.json()[\"response\"]\n",
    "\n",
    "#     print(response_text)\n",
    "#     # Если нужно вырезать ввод с контекстом, делаем так:\n",
    "#     if response_text.startswith(prompt):\n",
    "#         response_text = response_text[len(prompt):].strip()\n",
    "\n",
    "#     return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reorder_chunks(chunks):\n",
    "#     n = len(chunks)\n",
    "#     if n <= 3:\n",
    "#         return chunks  # просто как есть\n",
    "\n",
    "#     first_three = chunks[:3]\n",
    "#     # далее позиции считаем относительно chunks[3:]\n",
    "#     odds = chunks[3::2]   # 4-й, 6-й, 8-й...\n",
    "#     evens = chunks[4::2]  # 5-й, 7-й, 9-й...\n",
    "#     evens = evens[::-1]   # развернуть\n",
    "\n",
    "#     return first_three + odds + evens\n",
    "\n",
    "# # пример формирования вывода\n",
    "# def format_chunks(chunks):\n",
    "#     return \"\\n\".join(f\"{i}. {c}\" for i, c in enumerate(reorder_chunks(chunks), start=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eafd076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toc import toc\n",
    "\n",
    "def find_section_for_page(toc, page):\n",
    "    all_sections = []\n",
    "\n",
    "    for chapter_data in toc.values():\n",
    "        chapter_title = chapter_data[\"title\"]\n",
    "        for section_title, start in chapter_data[\"sections\"].items():\n",
    "            all_sections.append((chapter_title, section_title, start))\n",
    "\n",
    "    # сортируем по старту секции\n",
    "    all_sections.sort(key=lambda x: x[2])\n",
    "\n",
    "    # ищем последнюю секцию, начавшуюся не позже страницы\n",
    "    candidates = [(ch, sec, start) for ch, sec, start in all_sections if start <= page]\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    ch, sec, _ = candidates[-1]\n",
    "    return f\"{ch}/{sec}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdd2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pairs(pairs):\n",
    "    # pairs: список кортежей (chunk_text, page)\n",
    "    n = len(pairs)\n",
    "    if n <= 3:\n",
    "        return pairs\n",
    "\n",
    "    first_three = pairs[:3]\n",
    "    odds = pairs[3::2]   # 4-й, 6-й, 8-й...\n",
    "    evens = pairs[4::2]  # 5-й, 7-й, 9-й...\n",
    "    evens = evens[::-1]\n",
    "\n",
    "    return first_three + odds + evens\n",
    "\n",
    "def format_pairs(pairs):\n",
    "    lines = []\n",
    "    for i, (chunk, page) in enumerate(pairs, start=1):\n",
    "        sec = find_section_for_page(toc, page)\n",
    "        if sec is None:\n",
    "            sec = \"Unknown\"\n",
    "        lines.append(f\"{i}. {sec} - {chunk}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d8e2f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:46<00:00, 12.94s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "result_pages = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    qid = query['query_id']\n",
    "    q_text = query['question']\n",
    "    data = relevant_reranked[qid]\n",
    "\n",
    "    # собираем пары (текст, страница)\n",
    "    pairs = [(item[\"text\"], item[\"page\"]) for item in data]\n",
    "\n",
    "    # переставляем\n",
    "    reordered = reorder_pairs(pairs)\n",
    "\n",
    "    # делаем контекст с секциями\n",
    "    context = format_pairs(reordered)\n",
    "    answer = llm_answer(q_text, context)\n",
    "\n",
    "    outputs.append(answer)\n",
    "    result_pages.append([p for _, p in reordered])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4dd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequential(items, overlap: int) -> str:\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    items = sorted(items, key=lambda x: x[\"chunk_index\"])\n",
    "\n",
    "    merged_groups = []\n",
    "    cur_text = items[0][\"text\"]\n",
    "    prev_idx = items[0][\"chunk_index\"]\n",
    "\n",
    "    for it in items[1:]:\n",
    "        idx, t = it[\"chunk_index\"], it[\"text\"]\n",
    "        if idx == prev_idx + 1:\n",
    "            # учёт overlap: убираем дублирующийся префикс t\n",
    "            max_k = min(overlap, len(cur_text), len(t))\n",
    "            cut = 0\n",
    "            for k in range(max_k, 0, -1):\n",
    "                if cur_text[-k:] == t[:k]:\n",
    "                    cut = k\n",
    "                    break\n",
    "            cur_text += t[cut:]\n",
    "        else:\n",
    "            merged_groups.append(cur_text)\n",
    "            cur_text = t\n",
    "        prev_idx = idx\n",
    "\n",
    "    merged_groups.append(cur_text)\n",
    "    return \"\\n\".join(merged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f012889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [q['query_id'] for q in queries]\n",
    "references = [{\"sections\": [], \"pages\": list(set(pages))} for pages in result_pages]\n",
    "\n",
    "for ref in references:\n",
    "    for page in ref[\"pages\"]:\n",
    "        section = find_section_for_page(toc, page)\n",
    "        if section:\n",
    "            section = section.lower().replace(\" \", \"_\")\n",
    "            ref[\"sections\"].append(section)\n",
    "\n",
    "context_text = [\n",
    "    merge_sequential(relevant_reranked[qid], chunk_overlap)\n",
    "    for qid in ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2faa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"context\": context_text,\n",
    "    \"answer\": outputs,\n",
    "    \"references\": references\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8e2ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"data/submission10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
