{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5c1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3424ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from rank_bm25 import BM25Okapi\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cbc207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "db_name = 'CASML - Generative AI Hackathon'\n",
    "LLM_model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "cross_encoder_id = \"BAAI/bge-reranker-v2-m3\"\n",
    "embedding_model_id = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "chunk_overlap = 300\n",
    "chunk_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d823a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "# texts = {}\n",
    "\n",
    "# with pdfplumber.open(\"data/book.pdf\") as pdf:\n",
    "#     text = \"\"\n",
    "#     for i, page in enumerate(pdf.pages):\n",
    "#         if i < 18 or i > 642:\n",
    "#             continue\n",
    "#         text = page.extract_text()\n",
    "#         texts[i - 18] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf9d4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(texts, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09a830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6052f1f5",
   "metadata": {},
   "source": [
    "**ОЧИСТКА ТЕКСТА**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3a023cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_headers_footers(text, header_patterns=None, footer_patterns=None):\n",
    "    if header_patterns is None:\n",
    "        header_patterns = [r'^.*Header.*$']\n",
    "    if footer_patterns is None:\n",
    "        footer_patterns = [r'^.*Footer.*$']\n",
    "\n",
    "    for pattern in header_patterns + footer_patterns:\n",
    "        text = re.sub(pattern, '', text, flags=re.MULTILINE)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def remove_special_characters(text, special_chars=None):\n",
    "    if special_chars is None:\n",
    "        special_chars = r'[^A-Za-z0-9\\s\\.,;:\\'\\\"\\?\\!\\-]'\n",
    "\n",
    "    text = re.sub(special_chars, '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_repeated_substrings(text, pattern=r'\\.{2,}'):\n",
    "    text = re.sub(pattern, '.', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text, metadata=None, language=None):\n",
    "    # optional: detect language, skip if not target\n",
    "    # remove headers/footers\n",
    "    text = remove_headers_footers(text)\n",
    "    # fix unicode quirks\n",
    "    # text = ftfy.fix_text(text)\n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    # remove URLs/emails\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    # normalize whitespace\n",
    "    text = remove_extra_spaces(text)\n",
    "    # remove special characters\n",
    "    text = remove_special_characters(text)\n",
    "    # lowercase (optional)\n",
    "    text = text.lower()\n",
    "    # lemmatize (optional) — using spaCy or any other\n",
    "    # chunking can happen here or after\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aa3c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_num in texts:\n",
    "    texts[page_num] = texts[page_num].replace(\"\\n\", \" \")\n",
    "    texts[page_num] = preprocess_text(texts[page_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfe39f",
   "metadata": {},
   "source": [
    "**ЧАНКИНГ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63bf8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c97c5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 2729.28it/s]\n"
     ]
    }
   ],
   "source": [
    "#Базовый чанкинг\n",
    "\n",
    "text_chunks = [{i: text_splitter.split_text(texts[str(i)])} for i in tqdm(range(len(texts)))]\n",
    "\n",
    "text_chunks_numbered = []\n",
    "\n",
    "for chunk_dict in text_chunks:\n",
    "    key, values = next(iter(chunk_dict.items()))\n",
    "    for chunk in values:\n",
    "        text_chunks_numbered.append((key, chunk))\n",
    "\n",
    "# Добавляем глобальный индекс\n",
    "global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "\n",
    "global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae4f36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Чанкинг по предложениям\n",
    "\n",
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "# semantic_chunks = {}\n",
    "\n",
    "# for i in tqdm(range(len(texts))):\n",
    "#     text = texts[str(i)]\n",
    "#     sentences = sent_tokenize(text)\n",
    "#     joined_sentences = []\n",
    "#     buffer = \"\"\n",
    "\n",
    "#     # объединяем предложения в блоки <= chunk_size\n",
    "#     for sent in sentences:\n",
    "#         if len(buffer) + len(sent) + 1 <= chunk_size:\n",
    "#             buffer += \" \" + sent\n",
    "#         else:\n",
    "#             joined_sentences.append(buffer.strip())\n",
    "#             buffer = sent\n",
    "#     if buffer:\n",
    "#         joined_sentences.append(buffer.strip())\n",
    "\n",
    "#     # если отдельный блок всё ещё длинный — применяем RecursiveCharacterTextSplitter\n",
    "#     refined_chunks = []\n",
    "#     for chunk in joined_sentences:\n",
    "#         if len(chunk) > chunk_size:\n",
    "#             refined_chunks.extend(text_splitter.split_text(chunk))\n",
    "#         else:\n",
    "#             refined_chunks.append(chunk)\n",
    "\n",
    "#     semantic_chunks[i] = refined_chunks\n",
    "\n",
    "# # выравниваем в общий список\n",
    "# text_chunks_numbered = [(i, chunk) for i, chunks in semantic_chunks.items() for chunk in chunks]\n",
    "\n",
    "# # глобальная нумерация чанков\n",
    "# global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "# global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7974f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "# from statistics import mean, median\n",
    "\n",
    "\n",
    "# # Чанкинг по смысловым блокам\n",
    "\n",
    "# cross_encoder = CrossEncoder(cross_encoder_id, device=device)\n",
    "\n",
    "# semantic_chunks = {}\n",
    "\n",
    "# semantic_similarity_threshold = 0.15\n",
    "\n",
    "\n",
    "# buffer_lengths = []  # сюда будем сохранять длины чанков по числу предложений\n",
    "# scores = []\n",
    "\n",
    "# for i in tqdm(range(len(texts))):\n",
    "#     text = texts[str(i)]\n",
    "#     sentences = sent_tokenize(text)\n",
    "\n",
    "#     joined_sentences = []\n",
    "#     buffer = sentences[0]\n",
    "#     max_buffer = 1\n",
    "\n",
    "#     for j in range(1, len(sentences)):\n",
    "#         s1 = buffer.split()[-50:]\n",
    "#         s2 = sentences[j]\n",
    "#         score = cross_encoder.predict([(\" \".join(s1), s2)])[0]\n",
    "#         scores.append(score)\n",
    "\n",
    "#         if score > semantic_similarity_threshold:\n",
    "#             buffer += \" \" + s2\n",
    "#             max_buffer += 1\n",
    "#         else:\n",
    "#             joined_sentences.append(buffer.strip())\n",
    "#             buffer_lengths.append(max_buffer)\n",
    "#             buffer = s2\n",
    "#             max_buffer = 1\n",
    "\n",
    "#     if buffer:\n",
    "#         joined_sentences.append(buffer.strip())\n",
    "#         buffer_lengths.append(max_buffer)\n",
    "\n",
    "#     refined_chunks = []\n",
    "#     for chunk in joined_sentences:\n",
    "#         if len(chunk) > chunk_size:\n",
    "#             refined_chunks.extend(text_splitter.split_text(chunk))\n",
    "#         else:\n",
    "#             refined_chunks.append(chunk)\n",
    "\n",
    "#     semantic_chunks[i] = refined_chunks\n",
    "\n",
    "# # выравниваем в общий список\n",
    "# text_chunks_numbered = [(i, chunk) for i, chunks in semantic_chunks.items() for chunk in chunks]\n",
    "\n",
    "# # глобальная нумерация чанков\n",
    "# global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "# global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)\n",
    "\n",
    "\n",
    "# # статистика по длинам\n",
    "# print(\"\\n--- Chunk length statistics ---\")\n",
    "# print(f\"Total chunks: {len(buffer_lengths)}\")\n",
    "# print(f\"Min length: {min(buffer_lengths)}\")\n",
    "# print(f\"Max length: {max(buffer_lengths)}\")\n",
    "# print(f\"Mean length: {mean(buffer_lengths):.2f}\")\n",
    "# print(f\"Median length: {median(buffer_lengths):.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1626b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.hist(scores, bins=30, edgecolor='black', alpha=0.7)\n",
    "# plt.axvline(semantic_similarity_threshold, color='red', linestyle='--', label='Current threshold')\n",
    "# plt.title('Распределение семантических скоров между предложениями')\n",
    "# plt.xlabel('Semantic similarity score')\n",
    "# plt.ylabel('Количество пар предложений')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e1d82d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.hist(buffer_lengths, bins=range(1, max(buffer_lengths) + 2), edgecolor='black')\n",
    "# plt.title(\"Распределение длин семантических чанков (по числу предложений)\")\n",
    "# plt.xlabel(\"Количество предложений в чанке\")\n",
    "# plt.ylabel(\"Количество чанков\")\n",
    "# plt.grid(alpha=0.3)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4255a301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Очищаем память, чтобы мне было вкусно\n",
    "\n",
    "# del cross_encoder\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4fc26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "347dea51",
   "metadata": {},
   "source": [
    "**ВЕКТОРИЗАЦИЯ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6779cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_id, model_kwargs={'dtype': torch.float16})\n",
    "\n",
    "state_dict = load_file(\"triplet_finetuned_encoder/model.safetensors\")\n",
    "embedding_model[0].auto_model.load_state_dict(state_dict, strict=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb0ad915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9011905e505a4830b4483c2861e73a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/84 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = embedding_model.encode(text_chunks, batch_size=32, device=device, normalize_embeddings=True, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b2fc820",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_np = np.asarray(vectors, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27cb15c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=db_name,\n",
    "    on_disk_payload=True,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1024,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c2d6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2684/2684 [00:02<00:00, 1245.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(vectors))):\n",
    "    client.upsert(\n",
    "        collection_name=db_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors[i],\n",
    "                payload={\n",
    "                    'text': text_chunks[i],\n",
    "                    'page': page_numbers[i] + 7,\n",
    "                    'chunk_index': global_chunk_ids[i]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17e3310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('data/queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f77e6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     LLM_model_id,\n",
    "#     dtype=torch.float16,\n",
    "#     device_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25dc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.5, top_p=0.9) -> str:\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "383bb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_prompt = \"You are a query rewriter specialized in mental health and psychology topics. Your task is to rewrite the user’s question into short, clear search queries for vector retrieval.\\n\\nSTRICT RULES:\\n- Output EXACTLY THREE rewritten query variants.\\n- Separate them each on a NEW LINE.\\n- DO NOT explain, comment, justify, or describe anything.\\n- DO NOT answer the question.\\n- Output ONLY the rewritten queries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b223123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_query(query_text) -> list[str]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rephrase_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Rephrase this question into exactly three short search queries separated by '\\n'. Do not explain anything. Question: {query_text}\"},\n",
    "    ]\n",
    "    output = generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.3, top_p=0.9)\n",
    "    parts = output.split(\"\\n\")\n",
    "    result = [p.strip() for p in parts][:3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8811fd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем перефразированные запросы\n",
    "\n",
    "# for i in tqdm(range(len(queries))):\n",
    "#     print(queries[i]['question'])\n",
    "#     new_queries = rephrase_query(queries[i]['question'])\n",
    "#     queries[i]['new_queries'] = new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "003cb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/new_queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(queries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8027c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e9923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/new_queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e2168bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_corpus = [word_tokenize(t.lower()) for t in text_chunks]\n",
    "bm25 = BM25Okapi(bm25_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72e5fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_candidates(queries, top_k_db=5, alpha_vec=0.9, alpha_bm25=0.1):\n",
    "    results = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query['query_id']\n",
    "        query_texts = [query['question']] + query['new_queries']\n",
    "\n",
    "        all_hits = []\n",
    "\n",
    "        for qt in query_texts:\n",
    "            query_vec = embedding_model.encode(qt, normalize_embeddings=True, device=device).tolist()\n",
    "            query_vec_np = np.asarray(query_vec, dtype=np.float32)\n",
    "\n",
    "            # --- Векторный поиск ---\n",
    "            hits = client.search(collection_name=db_name, query_vector=query_vec, limit=top_k_db)\n",
    "\n",
    "            # --- BM25 ---\n",
    "            tokenized_query = word_tokenize(qt.lower())\n",
    "            bm25_scores = bm25.get_scores(tokenized_query)\n",
    "            bm25_scores = np.array(bm25_scores)\n",
    "            bm25_scores = (bm25_scores - bm25_scores.min()) / (bm25_scores.max() - bm25_scores.min() + 1e-9)\n",
    "            bm25_norm = np.power(bm25_scores, 0.5)\n",
    "\n",
    "            # --- Собираем кандидатов ---\n",
    "            seen_indices = set()\n",
    "            for hit in hits:\n",
    "                seen_indices.add(hit.payload['chunk_index'])\n",
    "                all_hits.append({\n",
    "                    \"chunk_index\": hit.payload['chunk_index'],\n",
    "                    \"text\": hit.payload['text'],\n",
    "                    \"page\": hit.payload['page'],\n",
    "                    \"score_vec\": hit.score,\n",
    "                    \"score_bm25\": bm25_norm[hit.payload['chunk_index']],\n",
    "                    \"source\": \"qdrant\"\n",
    "                })\n",
    "\n",
    "            # Добавляем недостающих кандидатов из BM25-топа\n",
    "            top_bm25_idx = np.argsort(bm25_scores)[-top_k_db:][::-1]\n",
    "            for idx in top_bm25_idx:\n",
    "                if idx not in seen_indices:\n",
    "                    score_vec_direct = float(np.dot(query_vec_np, vectors_np[idx]))\n",
    "                    score_vec_direct = min(max(score_vec_direct, 0.0), 1.0)\n",
    "                    all_hits.append({\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"text\": text_chunks[idx],\n",
    "                        \"page\": page_numbers[idx] + 7,\n",
    "                        \"score_vec\": score_vec_direct,\n",
    "                        \"score_bm25\": bm25_norm[idx],\n",
    "                        \"source\": \"bm25\"\n",
    "                    })\n",
    "\n",
    "        # --- Комбинируем по весам ---\n",
    "        dedup = {}\n",
    "        for item in all_hits:\n",
    "            ci = item[\"chunk_index\"]\n",
    "            combined_score = alpha_vec * item[\"score_vec\"] + alpha_bm25 * item[\"score_bm25\"]\n",
    "            if ci not in dedup or combined_score > dedup[ci][\"score\"]:\n",
    "                dedup[ci] = {\n",
    "                    \"text\": item[\"text\"],\n",
    "                    \"page\": item[\"page\"],\n",
    "                    \"score\": combined_score,\n",
    "                    \"chunk_index\": ci,\n",
    "                    \"source\": item[\"source\"],\n",
    "                    \"score_vec\": item[\"score_vec\"],\n",
    "                    \"score_bm25\": item[\"score_bm25\"]\n",
    "                }\n",
    "\n",
    "        final_items = sorted(dedup.values(), key=lambda x: x[\"score\"], reverse=True)[:top_k_db]\n",
    "\n",
    "        if final_items:\n",
    "            scores = [i[\"score\"] for i in final_items]\n",
    "            vec_scores = [i[\"score_vec\"] for i in final_items]\n",
    "            bm25_scores = [i[\"score_bm25\"] for i in final_items]\n",
    "            qdrant_count = sum(1 for i in final_items if i[\"source\"] == \"qdrant\")\n",
    "            bm25_count = sum(1 for i in final_items if i[\"source\"] == \"bm25\")\n",
    "\n",
    "            print(f\"[{qid}] total={len(final_items)} | qdrant={qdrant_count}, bm25={bm25_count}\")\n",
    "            print(f\"   combined  -> min={min(scores):.4f}, max={max(scores):.4f}\")\n",
    "            print(f\"   qdrant(sc)-> min={min(vec_scores):.4f}, max={max(vec_scores):.4f}\")\n",
    "            print(f\"   bm25(sc)  -> min={min(bm25_scores):.4f}, max={max(bm25_scores):.4f}\")\n",
    "\n",
    "        results[qid] = final_items\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbfec84e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]C:\\Users\\Степан\\AppData\\Local\\Temp\\ipykernel_18296\\3240959167.py:15: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(collection_name=db_name, query_vector=query_vec, limit=top_k_db)\n",
      "  4%|▍         | 2/50 [00:00<00:09,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8072, max=0.9321\n",
      "   qdrant(sc)-> min=0.8005, max=0.9467\n",
      "   bm25(sc)  -> min=0.4789, max=1.0000\n",
      "[2] total=100 | qdrant=98, bm25=2\n",
      "   combined  -> min=0.5419, max=0.8586\n",
      "   qdrant(sc)-> min=0.5141, max=0.8429\n",
      "   bm25(sc)  -> min=0.3687, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [00:00<00:06,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.8054, max=0.9462\n",
      "   qdrant(sc)-> min=0.8101, max=0.9402\n",
      "   bm25(sc)  -> min=0.5687, max=1.0000\n",
      "[4] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7923, max=0.9598\n",
      "   qdrant(sc)-> min=0.8151, max=0.9582\n",
      "   bm25(sc)  -> min=0.3212, max=0.9979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:00<00:05,  7.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7351, max=0.8959\n",
      "   qdrant(sc)-> min=0.7346, max=0.9093\n",
      "   bm25(sc)  -> min=0.3340, max=1.0000\n",
      "[6] total=100 | qdrant=97, bm25=3\n",
      "   combined  -> min=0.8514, max=0.9542\n",
      "   qdrant(sc)-> min=0.8609, max=0.9527\n",
      "   bm25(sc)  -> min=0.5754, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [00:01<00:05,  7.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6827, max=0.9573\n",
      "   qdrant(sc)-> min=0.6795, max=0.9526\n",
      "   bm25(sc)  -> min=0.4352, max=1.0000\n",
      "[8] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.8007, max=0.9569\n",
      "   qdrant(sc)-> min=0.7985, max=0.9585\n",
      "   bm25(sc)  -> min=0.3617, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 10/50 [00:01<00:04,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9] total=100 | qdrant=97, bm25=3\n",
      "   combined  -> min=0.8474, max=0.9397\n",
      "   qdrant(sc)-> min=0.8474, max=0.9488\n",
      "   bm25(sc)  -> min=0.4713, max=1.0000\n",
      "[10] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.7862, max=0.9630\n",
      "   qdrant(sc)-> min=0.7747, max=0.9589\n",
      "   bm25(sc)  -> min=0.5322, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 12/50 [00:01<00:04,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6432, max=0.9428\n",
      "   qdrant(sc)-> min=0.6392, max=0.9639\n",
      "   bm25(sc)  -> min=0.5003, max=0.9628\n",
      "[12] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6690, max=0.9282\n",
      "   qdrant(sc)-> min=0.6787, max=0.9335\n",
      "   bm25(sc)  -> min=0.3480, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [00:01<00:04,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.6890, max=0.9272\n",
      "   qdrant(sc)-> min=0.6698, max=0.9354\n",
      "   bm25(sc)  -> min=0.3673, max=1.0000\n",
      "[14] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7095, max=0.9229\n",
      "   qdrant(sc)-> min=0.7026, max=0.9144\n",
      "   bm25(sc)  -> min=0.3086, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 16/50 [00:02<00:04,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7706, max=0.9023\n",
      "   qdrant(sc)-> min=0.7636, max=0.9224\n",
      "   bm25(sc)  -> min=0.3557, max=1.0000\n",
      "[16] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.7738, max=0.9452\n",
      "   qdrant(sc)-> min=0.7573, max=0.9452\n",
      "   bm25(sc)  -> min=0.2720, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 18/50 [00:02<00:04,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6873, max=0.9480\n",
      "   qdrant(sc)-> min=0.6836, max=0.9556\n",
      "   bm25(sc)  -> min=0.3891, max=0.9667\n",
      "[18] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6824, max=0.8989\n",
      "   qdrant(sc)-> min=0.6736, max=0.8914\n",
      "   bm25(sc)  -> min=0.2705, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:02<00:03,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] total=100 | qdrant=98, bm25=2\n",
      "   combined  -> min=0.8551, max=0.9709\n",
      "   qdrant(sc)-> min=0.8647, max=0.9742\n",
      "   bm25(sc)  -> min=0.5311, max=0.9957\n",
      "[20] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6626, max=0.9053\n",
      "   qdrant(sc)-> min=0.6543, max=0.9122\n",
      "   bm25(sc)  -> min=0.3110, max=0.9691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 22/50 [00:02<00:03,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] total=100 | qdrant=95, bm25=5\n",
      "   combined  -> min=0.8362, max=0.9626\n",
      "   qdrant(sc)-> min=0.8400, max=0.9622\n",
      "   bm25(sc)  -> min=0.3184, max=1.0000\n",
      "[22] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.5782, max=0.8343\n",
      "   qdrant(sc)-> min=0.5637, max=0.8277\n",
      "   bm25(sc)  -> min=0.3851, max=0.9967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 24/50 [00:03<00:03,  8.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6146, max=0.9157\n",
      "   qdrant(sc)-> min=0.6048, max=0.9251\n",
      "   bm25(sc)  -> min=0.0000, max=1.0000\n",
      "[24] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.8094, max=0.9542\n",
      "   qdrant(sc)-> min=0.8095, max=0.9577\n",
      "   bm25(sc)  -> min=0.0000, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:03<00:02,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25] total=100 | qdrant=97, bm25=3\n",
      "   combined  -> min=0.8134, max=0.9524\n",
      "   qdrant(sc)-> min=0.8132, max=0.9542\n",
      "   bm25(sc)  -> min=0.3895, max=1.0000\n",
      "[26] total=100 | qdrant=97, bm25=3\n",
      "   combined  -> min=0.8738, max=0.9670\n",
      "   qdrant(sc)-> min=0.8958, max=0.9767\n",
      "   bm25(sc)  -> min=0.4521, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 28/50 [00:03<00:02,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7303, max=0.8519\n",
      "   qdrant(sc)-> min=0.7541, max=0.8806\n",
      "   bm25(sc)  -> min=0.0000, max=1.0000\n",
      "[28] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.6896, max=0.9485\n",
      "   qdrant(sc)-> min=0.6680, max=0.9428\n",
      "   bm25(sc)  -> min=0.3641, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 30/50 [00:03<00:02,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8135, max=0.9674\n",
      "   qdrant(sc)-> min=0.8218, max=0.9638\n",
      "   bm25(sc)  -> min=0.3613, max=1.0000\n",
      "[30] total=100 | qdrant=95, bm25=5\n",
      "   combined  -> min=0.8623, max=0.9451\n",
      "   qdrant(sc)-> min=0.8653, max=0.9529\n",
      "   bm25(sc)  -> min=0.3847, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [00:04<00:02,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31] total=100 | qdrant=95, bm25=5\n",
      "   combined  -> min=0.7971, max=0.8806\n",
      "   qdrant(sc)-> min=0.7869, max=0.8873\n",
      "   bm25(sc)  -> min=0.3723, max=0.9115\n",
      "[32] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8468, max=0.9425\n",
      "   qdrant(sc)-> min=0.8450, max=0.9413\n",
      "   bm25(sc)  -> min=0.5233, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 34/50 [00:04<00:02,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.7033, max=0.9474\n",
      "   qdrant(sc)-> min=0.6900, max=0.9474\n",
      "   bm25(sc)  -> min=0.4552, max=1.0000\n",
      "[34] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.6695, max=0.9390\n",
      "   qdrant(sc)-> min=0.6581, max=0.9355\n",
      "   bm25(sc)  -> min=0.4633, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 36/50 [00:04<00:01,  7.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6684, max=0.9142\n",
      "   qdrant(sc)-> min=0.6477, max=0.9139\n",
      "   bm25(sc)  -> min=0.2830, max=1.0000\n",
      "[36] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8370, max=0.9343\n",
      "   qdrant(sc)-> min=0.8452, max=0.9293\n",
      "   bm25(sc)  -> min=0.3616, max=0.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:04<00:01,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37] total=100 | qdrant=92, bm25=8\n",
      "   combined  -> min=0.8837, max=0.9443\n",
      "   qdrant(sc)-> min=0.8813, max=0.9598\n",
      "   bm25(sc)  -> min=0.4533, max=0.9733\n",
      "[38] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8292, max=0.9679\n",
      "   qdrant(sc)-> min=0.8380, max=0.9645\n",
      "   bm25(sc)  -> min=0.4465, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 40/50 [00:05<00:01,  7.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8744, max=0.9435\n",
      "   qdrant(sc)-> min=0.8785, max=0.9424\n",
      "   bm25(sc)  -> min=0.5511, max=1.0000\n",
      "[40] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8492, max=0.9656\n",
      "   qdrant(sc)-> min=0.8678, max=0.9795\n",
      "   bm25(sc)  -> min=0.3793, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 42/50 [00:05<00:01,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6907, max=0.9100\n",
      "   qdrant(sc)-> min=0.6762, max=0.9154\n",
      "   bm25(sc)  -> min=0.0000, max=1.0000\n",
      "[42] total=100 | qdrant=98, bm25=2\n",
      "   combined  -> min=0.8590, max=0.9549\n",
      "   qdrant(sc)-> min=0.8630, max=0.9621\n",
      "   bm25(sc)  -> min=0.4792, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [00:05<00:00,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.8154, max=0.9307\n",
      "   qdrant(sc)-> min=0.8281, max=0.9302\n",
      "   bm25(sc)  -> min=0.4874, max=1.0000\n",
      "[44] total=100 | qdrant=97, bm25=3\n",
      "   combined  -> min=0.7916, max=0.9485\n",
      "   qdrant(sc)-> min=0.8006, max=0.9427\n",
      "   bm25(sc)  -> min=0.3874, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 46/50 [00:05<00:00,  7.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7535, max=0.9030\n",
      "   qdrant(sc)-> min=0.7487, max=0.8937\n",
      "   bm25(sc)  -> min=0.3354, max=0.9867\n",
      "[46] total=100 | qdrant=99, bm25=1\n",
      "   combined  -> min=0.8636, max=0.9620\n",
      "   qdrant(sc)-> min=0.8810, max=0.9630\n",
      "   bm25(sc)  -> min=0.4572, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 48/50 [00:06<00:00,  7.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.6910, max=0.9143\n",
      "   qdrant(sc)-> min=0.6740, max=0.9286\n",
      "   bm25(sc)  -> min=0.3720, max=1.0000\n",
      "[48] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.8388, max=0.9244\n",
      "   qdrant(sc)-> min=0.8431, max=0.9383\n",
      "   bm25(sc)  -> min=0.3945, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:06<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7188, max=0.9161\n",
      "   qdrant(sc)-> min=0.7123, max=0.9272\n",
      "   bm25(sc)  -> min=0.2923, max=1.0000\n",
      "[50] total=100 | qdrant=100, bm25=0\n",
      "   combined  -> min=0.7187, max=0.9127\n",
      "   qdrant(sc)-> min=0.7170, max=0.9242\n",
      "   bm25(sc)  -> min=0.3361, max=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = get_candidates(queries, top_k_db=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf094e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_db_score(relevant, score_threshold=0.0):\n",
    "    filtered = {}\n",
    "\n",
    "    for qid, items in relevant.items():\n",
    "        selected = [item for item in items if item[\"score\"] >= score_threshold]\n",
    "        filtered[qid] = selected\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# relevant = filter_by_db_score(relevant, score_threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0931a32f",
   "metadata": {},
   "source": [
    "*Список моделей для Cross-Encoder*\n",
    "\n",
    "\n",
    "-BAAI/bge-reranker-v2-m3\n",
    "\n",
    "-cross‑encoder/ms‑marco‑MiniLM‑L6‑v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c15d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_encoder_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18c909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реранк на основе только исходного вопроса\n",
    "\n",
    "# def rerank_results(queries, relevant, top_k_rerank=3, rerank_threshold=None):\n",
    "#     reranked = {}\n",
    "\n",
    "#     for query in tqdm(queries):\n",
    "#         qid = query['query_id']\n",
    "#         q_text = query['question']\n",
    "\n",
    "#         candidates = relevant[qid]\n",
    "#         texts = [c[\"text\"] for c in candidates]\n",
    "\n",
    "#         if len(texts) == 0:\n",
    "#             reranked[qid] = []\n",
    "#             continue\n",
    "\n",
    "#         pairs = [(q_text, t) for t in texts]\n",
    "#         scores = cross_encoder.predict(pairs)  # по аналогии с твоим примером\n",
    "\n",
    "#         scored = [\n",
    "#             {**c, \"rerank_score\": s}\n",
    "#             for c, s in zip(candidates, scores)\n",
    "#         ]\n",
    "\n",
    "#         scored = sorted(scored, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "#         scored = scored[:top_k_rerank]\n",
    "\n",
    "#         if rerank_threshold is not None:\n",
    "#             scored = [s for s in scored if s[\"rerank_score\"] >= rerank_threshold]\n",
    "\n",
    "#         reranked[qid] = scored\n",
    "\n",
    "#     return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02d650ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(queries, relevant, top_k_rerank=3, rerank_threshold=None):\n",
    "    reranked = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query[\"query_id\"]\n",
    "        query_texts = [query[\"question\"]] + query.get(\"new_queries\", [])\n",
    "        candidates = relevant[qid]\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "\n",
    "        if not texts:\n",
    "            reranked[qid] = []\n",
    "            continue\n",
    "\n",
    "        # Считаем средний скор по всем перефразам\n",
    "        total_scores = np.zeros(len(texts))\n",
    "        for q_text in query_texts:\n",
    "            pairs = [(q_text, t) for t in texts]\n",
    "            scores = cross_encoder.predict(pairs)\n",
    "            total_scores += np.array(scores)\n",
    "\n",
    "        avg_scores = total_scores / len(query_texts)\n",
    "\n",
    "        scored = [\n",
    "            {**c, \"rerank_score\": s}\n",
    "            for c, s in zip(candidates, avg_scores)\n",
    "        ]\n",
    "\n",
    "        scored = sorted(scored, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "        scored = scored[:top_k_rerank]\n",
    "\n",
    "        if rerank_threshold is not None:\n",
    "            scored = [s for s in scored if s[\"rerank_score\"] >= rerank_threshold]\n",
    "\n",
    "        reranked[qid] = scored\n",
    "\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6bad970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [03:04<00:00,  3.69s/it]\n"
     ]
    }
   ],
   "source": [
    "relevant_reranked = rerank_results(\n",
    "    queries,\n",
    "    relevant,\n",
    "    top_k_rerank=6,         # регулируй\n",
    "    rerank_threshold=None   # или поставь, например, 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "149dc9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query_id': '1',\n",
       " 'question': 'What is the scientific method in psychology?',\n",
       " 'new_queries': ['What does the scientific method entail in psychology?',\n",
       "  'How can I understand the scientific method used in psychology research?',\n",
       "  'What are the key steps of the scientific method applied in psychological studies?']}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "120cc51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'scientific study of the mind and behavior. psychologists use the scientific method to acquire knowledge. to apply the scientific method, a researcher with a question about how or why something happens will propose a tentative explanation, called a hypothesis, to explain the phenomenon. a hypothesis should fit into the context of a scientific theory, which is a broad explanation or group of explanations for some aspect of the natural world that is consistently supported by evidence over time. a theory is the best understanding we have of that part of the natural world. the researcher then makes observations or carries out an experiment to test the validity of the hypothesis. those results are then published or presented at research conferences so that others can replicate or build on the results. scientists test that which is perceivable and measurable. for example, the hypothesis that a bird sings because it is happy is not a hypothesis that can be tested since we have no way to',\n",
       "  'page': 8,\n",
       "  'score': 0.9308790968364365,\n",
       "  'chunk_index': 3,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.9432886424498714,\n",
       "  'score_bm25': 0.8191931863155222,\n",
       "  'rerank_score': 0.5443105176091194},\n",
       " {'text': 'can be measured, and it cannot arrive at knowledge about values and morality. this is one reason why our scientific understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. the scientific method is also a form of empiricism. anempirical methodfor acquiring knowledge is one based on observation, including experimentation, rather than a method based only on forms of logical argument or previous authorities. it was not until the late 1800s that psychology became accepted as its own academic discipline. before this time, the workings of the mind were considered under the auspices of philosophy. given that any behavior is, at its roots, biological, some areas of psychology take on aspects of a natural science like biology. no biological organism exists in isolation, and our behavior is influenced by our interactions with others. therefore, psychology is also a social science. why study psychology? often, students take their',\n",
       "  'page': 8,\n",
       "  'score': 0.9188768974693247,\n",
       "  'chunk_index': 5,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.9276195945802246,\n",
       "  'score_bm25': 0.8401926234712247,\n",
       "  'rerank_score': 0.4933552034199238},\n",
       " {'text': 'are then published or presented at research conferences so that others can replicate or build on the results. scientists test that which is perceivable and measurable. for example, the hypothesis that a bird sings because it is happy is not a hypothesis that can be tested since we have no way to measure the happiness of a bird. we must ask a different question, perhaps about the brain state of the bird, since this can be measured. however, we can ask individuals about whether they sing because they are happy since they are able to tell us. thus, psychological science is empirical, based on measurable data. in general, science deals only with matter and energy, that is, those things that can be measured, and it cannot arrive at knowledge about values and morality. this is one reason why our scientific understanding of the mind is so limited, since thoughts, at least as we experience them, are neither matter nor energy. the scientific method is also a form of empiricism. anempirical',\n",
       "  'page': 8,\n",
       "  'score': 0.92588015768921,\n",
       "  'chunk_index': 4,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.945232803990005,\n",
       "  'score_bm25': 0.7517063409820547,\n",
       "  'rerank_score': 0.4221100900322199},\n",
       " {'text': 'psychology is defined as the scientific study of mind and behavior. students of psychology develop critical thinking skills, become familiar with the scientific method, and recognize the complexity of behavior. 1.2history of psychology before the time of wundt and james, questions about the mind were considered by philosophers. however, both wundt and james helped create psychology as a distinct scientific discipline. wundt was a structuralist, which meant he believed that our cognitive experience was best understood by breaking that experience into its component parts. he thought this was best accomplished by introspection. access for free at openstax.org',\n",
       "  'page': 30,\n",
       "  'score': 0.8746213188221461,\n",
       "  'chunk_index': 100,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8784405358972086,\n",
       "  'score_bm25': 0.8402483651465832,\n",
       "  'rerank_score': 0.4015053119510412},\n",
       " {'text': '8 1  introduction to psychology 2002. nash was the subject of the 2001 moviea beautiful mind. why did these people have these experiences? how does the human brain work? and what is the connection between the brains internal processes and peoples external behaviors? this textbook will introduce you to various ways that the field of psychology has explored these questions. 1.1what is psychology? learning objectives by the end of this section, you will be able to:  define psychology  understand the merits of an education in psychology what is creativity? what are prejudice and discrimination? what is consciousness? the field of psychology explores questions like these.psychologyrefers to the scientific study of the mind and behavior. psychologists use the scientific method to acquire knowledge. to apply the scientific method, a researcher with a question about how or why something happens will propose a tentative explanation, called a hypothesis, to explain the phenomenon. a hypothesis',\n",
       "  'page': 8,\n",
       "  'score': 0.8441802497828381,\n",
       "  'chunk_index': 2,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.8268669442052017,\n",
       "  'score_bm25': 0.999999999981565,\n",
       "  'rerank_score': 0.3710613511502743},\n",
       " {'text': '1.2  history of psychology 9 an education in psychology is valuable for a number of reasons. psychology students hone critical thinking skills and are trained in the use of the scientific method. critical thinking is the active application of a set of skills to information for the understanding and evaluation of that information. the evaluation of informationassessing its reliability and usefulness is an important skill in a world full of competing facts, many of which are designed to be misleading. for example, critical thinking involves maintaining an attitude of skepticism, recognizing internal biases, making use of logical thinking, asking appropriate questions, and making observations. psychology students also can develop better communication skills during the course of their undergraduate coursework american psychological association, 2011. together, these factors increase students scientific literacy and prepare students to critically evaluate the various sources of information',\n",
       "  'page': 9,\n",
       "  'score': 0.8944283049458429,\n",
       "  'chunk_index': 8,\n",
       "  'source': 'qdrant',\n",
       "  'score_vec': 0.9021726013819319,\n",
       "  'score_bm25': 0.8247296370210419,\n",
       "  'rerank_score': 0.3675398789346218}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_reranked[\"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00941f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очищаем память, чтобы LLM было вкусно\n",
    "\n",
    "del cross_encoder\n",
    "del embedding_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3175a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b813c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO почитать про промпты\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an expert in psychology.\\n\\n\"\n",
    "    \"Your task is to answer the user’s question using strictly and only the information \"\n",
    "    \"from the retrieved context chunks provided below.\\n\\n\"\n",
    "    \"Each chunk contains a label indicating which section of the original source it came from. \"\n",
    "    \"When making each factual claim, explicitly reference the section label of the chunk it came from. \"\n",
    "    \"If the answer cannot be fully supported by the provided chunks, respond: \\\"Not enough information to answer the question.\\\"\\n\\n\"\n",
    "    \"Rules:\\n\"\n",
    "    \"1. Do not add external knowledge, assumptions, or general reasoning not grounded in the retrieved chunks.\\n\"\n",
    "    \"2. Do not merge or generalize facts unless they are directly stated in the chunks.\\n\"\n",
    "    \"3. Each factual statement must include a source label in parentheses, e.g. (section: ...).\\n\"\n",
    "    \"4. If multiple chunks contradict each other, do not resolve the contradiction; instead, state that there is a contradiction.\\n\\n\"\n",
    "    \"Output format:\\n\"\n",
    "    \"- First, provide the final answer based solely on the evidence.\\n\"\n",
    "    \"- Then, list each used factual statement with its corresponding section reference.\\n\\n\"\n",
    "    \"Now wait for the user’s question and the retrieved context chunks.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a98881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer(query, context):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    answer = generation_pipeline(messages, temperature=0.7, max_new_tokens=1024)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def llm_answer(query, context):\n",
    "#     system_msg = (\n",
    "#         \"You are an expert in psychology. Using only the provided retrieved documents, answer the following question. Do not add any external knowledge.\"\n",
    "#     )\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_msg},\n",
    "#         {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "#         {\"role\": \"user\", \"content\": query}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     )\n",
    "#     prompt = \"Привет, как дела?\"\n",
    "#     payload = {\n",
    "#         \"prompt\": prompt,\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.9,\n",
    "#         \"top_p\": 0.9\n",
    "#     }\n",
    "\n",
    "#     r = requests.post(\"https://shortly-pleasant-democrats-discussion.trycloudflare.com/generate\", json=payload)\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     # Модель возвращает полный текст, нужно вырезать только продолжение\n",
    "#     response_text = r.json()[\"response\"]\n",
    "\n",
    "#     print(response_text)\n",
    "#     # Если нужно вырезать ввод с контекстом, делаем так:\n",
    "#     if response_text.startswith(prompt):\n",
    "#         response_text = response_text[len(prompt):].strip()\n",
    "\n",
    "#     return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reorder_chunks(chunks):\n",
    "#     n = len(chunks)\n",
    "#     if n <= 3:\n",
    "#         return chunks  # просто как есть\n",
    "\n",
    "#     first_three = chunks[:3]\n",
    "#     # далее позиции считаем относительно chunks[3:]\n",
    "#     odds = chunks[3::2]   # 4-й, 6-й, 8-й...\n",
    "#     evens = chunks[4::2]  # 5-й, 7-й, 9-й...\n",
    "#     evens = evens[::-1]   # развернуть\n",
    "\n",
    "#     return first_three + odds + evens\n",
    "\n",
    "# # пример формирования вывода\n",
    "# def format_chunks(chunks):\n",
    "#     return \"\\n\".join(f\"{i}. {c}\" for i, c in enumerate(reorder_chunks(chunks), start=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eafd076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toc import toc\n",
    "\n",
    "def find_section_for_page(toc, page):\n",
    "    all_sections = []\n",
    "\n",
    "    for chapter_data in toc.values():\n",
    "        chapter_title = chapter_data[\"title\"]\n",
    "        for section_title, start in chapter_data[\"sections\"].items():\n",
    "            all_sections.append((chapter_title, section_title, start))\n",
    "\n",
    "    # сортируем по старту секции\n",
    "    all_sections.sort(key=lambda x: x[2])\n",
    "\n",
    "    # ищем последнюю секцию, начавшуюся не позже страницы\n",
    "    candidates = [(ch, sec, start) for ch, sec, start in all_sections if start <= page]\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    ch, sec, _ = candidates[-1]\n",
    "    return f\"{ch}/{sec}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bdd2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pairs(pairs):\n",
    "    # pairs: список кортежей (chunk_text, page)\n",
    "    n = len(pairs)\n",
    "    if n <= 3:\n",
    "        return pairs\n",
    "\n",
    "    first_three = pairs[:3]\n",
    "    odds = pairs[3::2]   # 4-й, 6-й, 8-й...\n",
    "    evens = pairs[4::2]  # 5-й, 7-й, 9-й...\n",
    "    evens = evens[::-1]\n",
    "\n",
    "    return first_three + odds + evens\n",
    "\n",
    "def format_pairs(pairs):\n",
    "    lines = []\n",
    "    for i, (chunk, page) in enumerate(pairs, start=1):\n",
    "        sec = find_section_for_page(toc, page)\n",
    "        if sec is None:\n",
    "            sec = \"Unknown\"\n",
    "        lines.append(f\"{i}. {sec} - {chunk}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5d8e2f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:46<00:00, 12.94s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "result_pages = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    qid = query['query_id']\n",
    "    q_text = query['question']\n",
    "    data = relevant_reranked[qid]\n",
    "\n",
    "    # собираем пары (текст, страница)\n",
    "    pairs = [(item[\"text\"], item[\"page\"]) for item in data]\n",
    "\n",
    "    # переставляем\n",
    "    reordered = reorder_pairs(pairs)\n",
    "\n",
    "    # делаем контекст с секциями\n",
    "    context = format_pairs(reordered)\n",
    "    answer = llm_answer(q_text, context)\n",
    "\n",
    "    outputs.append(answer)\n",
    "    result_pages.append([p for _, p in reordered])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d4dd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequential(items, overlap: int) -> str:\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    items = sorted(items, key=lambda x: x[\"chunk_index\"])\n",
    "\n",
    "    merged_groups = []\n",
    "    cur_text = items[0][\"text\"]\n",
    "    prev_idx = items[0][\"chunk_index\"]\n",
    "\n",
    "    for it in items[1:]:\n",
    "        idx, t = it[\"chunk_index\"], it[\"text\"]\n",
    "        if idx == prev_idx + 1:\n",
    "            # учёт overlap: убираем дублирующийся префикс t\n",
    "            max_k = min(overlap, len(cur_text), len(t))\n",
    "            cut = 0\n",
    "            for k in range(max_k, 0, -1):\n",
    "                if cur_text[-k:] == t[:k]:\n",
    "                    cut = k\n",
    "                    break\n",
    "            cur_text += t[cut:]\n",
    "        else:\n",
    "            merged_groups.append(cur_text)\n",
    "            cur_text = t\n",
    "        prev_idx = idx\n",
    "\n",
    "    merged_groups.append(cur_text)\n",
    "    return \"\\n\".join(merged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f012889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [q['query_id'] for q in queries]\n",
    "references = [{\"sections\": [], \"pages\": list(set(pages))} for pages in result_pages]\n",
    "\n",
    "for ref in references:\n",
    "    for page in ref[\"pages\"]:\n",
    "        section = find_section_for_page(toc, page)\n",
    "        if section:\n",
    "            section = section.lower().replace(\" \", \"_\")\n",
    "            ref[\"sections\"].append(section)\n",
    "\n",
    "context_text = [\n",
    "    merge_sequential(relevant_reranked[qid], chunk_overlap)\n",
    "    for qid in ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "e2faa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"context\": context_text,\n",
    "    \"answer\": outputs,\n",
    "    \"references\": references\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8e2ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"data/submission10.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
