{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3424ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbc207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "db_name = 'CASML - Generative AI Hackathon'\n",
    "LLM_model_id = \"Qwen/Qwen1.5-1.8B-Chat\"\n",
    "cross_encoder_id = \"BAAI/bge-reranker-v2-m3\"\n",
    "embedding_model_id = \"intfloat/multilingual-e5-large\"\n",
    "\n",
    "chunk_overlap=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d823a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pdfplumber\n",
    "# texts = {}\n",
    "\n",
    "# with pdfplumber.open(\"data/book.pdf\") as pdf:\n",
    "#     text = \"\"\n",
    "#     for i, page in enumerate(pdf.pages):\n",
    "#         if i < 18 or i > 642:\n",
    "#             continue\n",
    "#         text = page.extract_text()\n",
    "#         texts[i - 18] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9d4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/texts.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(texts, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a830bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/texts.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa3c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page_num in texts:\n",
    "    texts[page_num] = texts[page_num].replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bf8cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c97c5514",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:00<00:00, 2648.33it/s]\n"
     ]
    }
   ],
   "source": [
    "text_chunks = [{i: text_splitter.split_text(texts[str(i)])} for i in tqdm(range(len(texts)))]\n",
    "\n",
    "text_chunks_numbered = []\n",
    "\n",
    "for chunk_dict in text_chunks:\n",
    "    key, values = next(iter(chunk_dict.items()))\n",
    "    for chunk in values:\n",
    "        text_chunks_numbered.append((key, chunk))\n",
    "\n",
    "# Добавляем глобальный индекс\n",
    "global_indexed = [(idx, key, chunk) for idx, (key, chunk) in enumerate(text_chunks_numbered)]\n",
    "\n",
    "global_chunk_ids, page_numbers, text_chunks = zip(*global_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6779cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(embedding_model_id, model_kwargs={'dtype': torch.float16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb0ad915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be11dad32bc041cda23f84d67b94cfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vectors = embedding_model.encode(text_chunks, batch_size=32, device=device, normalize_embeddings=True, show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cb15c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient, models\n",
    "\n",
    "client = QdrantClient(\":memory:\")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=db_name,\n",
    "    on_disk_payload=True,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=1024,\n",
    "        distance=models.Distance.COSINE,\n",
    "        on_disk=True\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2d6bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2715/2715 [00:02<00:00, 1222.71it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(vectors))):\n",
    "    client.upsert(\n",
    "        collection_name=db_name,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=vectors[i],\n",
    "                payload={\n",
    "                    'text': text_chunks[i],\n",
    "                    'page': page_numbers[i] + 7,\n",
    "                    'chunk_index': global_chunk_ids[i]\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17e3310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('data/queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e6ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f25dc02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.5, top_p=0.9) -> str:\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=do_sample,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    gen_ids = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
    "    answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "383bb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rephrase_prompt = \"You are a query rewriter specialized in mental health and psychology topics. Your task is to rewrite the user’s question into short, clear search queries for vector retrieval.\\n\\nSTRICT RULES:\\n- Output EXACTLY THREE rewritten query variants.\\n- Separate them each on a NEW LINE.\\n- DO NOT explain, comment, justify, or describe anything.\\n- DO NOT answer the question.\\n- Output ONLY the rewritten queries.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2b223123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_query(query_text) -> list[str]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": rephrase_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Rephrase this question into exactly three short search queries separated by '\\n'. Do not explain anything. Question: {query_text}\"},\n",
    "    ]\n",
    "    output = generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=0.3, top_p=0.9)\n",
    "    parts = output.split(\"\\n\")\n",
    "    result = [p.strip() for p in parts][:3]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8811fd81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the scientific method in psychology?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"What is the scientific method in psychology?\"\n",
      "2. \"How does the scientific method apply to psychological research?\"\n",
      "3. \"What principles guide the process of conducting a scientific study in psychology?\"\n",
      "What are the basic parts of a neuron?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Basic Neuron Parts Query: \"What are the fundamental components of a neuron?\"\n",
      "2. Simplified Neuron Parts Query: \"Identify the essential elements of a neuron.\"\n",
      "3. Detailed Neuron Parts Query: \"Provide a comprehensive list of the primary components found within a neuron.\"\n",
      "What are the stages of sleep?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Stage 1 (NREM): This is the first stage of sleep, characterized by rapid eye movements, increased muscle tone, and reduced brain activity. It lasts about 30 minutes and is followed by deeper stages of sleep, such as NREM II and III, which last around 90 minutes.\n",
      "\n",
      "2. Stage 2 (REM): The second stage of sleep, also known as Rapid Eye Movement (REM), occurs when the brain is more active and focused, producing vivid dreams and dreaming experiences. REM sleep typically lasts for about an hour and a half, followed by another stage called Stage 3 (Non-Rapid Eye Movement) that lasts for about 45 minutes.\n",
      "\n",
      "3. Stage 3 (Non-Rapid Eye Movement): This stage of sleep, also known as Non-Rapid Eye Movement (NREM), is characterized by slow-wave sleep, during which the body relaxes and the brain releases hormones like melatonin. NREM sleep can last for up to two hours and is essential for memory consolidation, mood regulation, and overall physical well-being.\n",
      "What is operant conditioning?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"What is operant conditioning and how does it work?\"\n",
      "2. \"How does operant conditioning impact behavior through reinforcement learning?\"\n",
      "3. \"In operant conditioning, what is the concept of 'negative reinforcement' and its role in shaping behavior?\"\n",
      "What is problem-solving in psychology?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"Problem-solving in psychology: Definition, techniques, and applications\"\n",
      "2. \"Exploring the role of problem-solving in psychological research and practice\"\n",
      "3. \"The impact of problem-solving skills on mental health outcomes\"\n",
      "What are the three stages of memory?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [00:13<01:59,  2.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(queries))):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28mprint\u001b[39m(queries[i][\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     new_queries = \u001b[43mrephrase_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     queries[i][\u001b[33m'\u001b[39m\u001b[33mnew_queries\u001b[39m\u001b[33m'\u001b[39m] = new_queries\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrephrase_query\u001b[39m\u001b[34m(query_text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrephrase_query\u001b[39m(query_text) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      2\u001b[39m     messages = [\n\u001b[32m      3\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: rephrase_prompt},\n\u001b[32m      4\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRephrase this question into exactly three short search queries separated by \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Do not explain anything. Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m},\n\u001b[32m      5\u001b[39m     ]\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     output = \u001b[43mgeneration_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     parts = output.split(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     result = [p.strip() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parts][:\u001b[32m3\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mgeneration_pipeline\u001b[39m\u001b[34m(messages, max_new_tokens, do_sample, temperature, top_p)\u001b[39m\n\u001b[32m      8\u001b[39m inputs = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m gen_ids = outputs[\u001b[32m0\u001b[39m][inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].shape[-\u001b[32m1\u001b[39m]:]\n\u001b[32m     22\u001b[39m answer = tokenizer.decode(gen_ids, skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m).strip()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\generation\\utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\utils\\generic.py:918\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    917\u001b[39m     return_dict = return_dict_passed\n\u001b[32m--> \u001b[39m\u001b[32m918\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    920\u001b[39m     output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:449\u001b[39m, in \u001b[36mQwen2ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[32m    418\u001b[39m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   (...)\u001b[39m\u001b[32m    430\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    431\u001b[39m ) -> CausalLMOutputWithPast:\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    433\u001b[39m \u001b[33;03m    Example:\u001b[39;00m\n\u001b[32m    434\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    447\u001b[39m \u001b[33;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[32m    448\u001b[39m \u001b[33;03m    ```\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     outputs: BaseModelOutputWithPast = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     hidden_states = outputs.last_hidden_state\n\u001b[32m    461\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\utils\\generic.py:1064\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1061\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1066\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1069\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:384\u001b[39m, in \u001b[36mQwen2Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    381\u001b[39m position_embeddings = \u001b[38;5;28mself\u001b[39m.rotary_emb(hidden_states, position_ids)\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers[: \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers]:\n\u001b[32m--> \u001b[39m\u001b[32m384\u001b[39m     hidden_states = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattention_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.norm(hidden_states)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[32m    397\u001b[39m     last_hidden_state=hidden_states,\n\u001b[32m    398\u001b[39m     past_key_values=past_key_values \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    399\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:232\u001b[39m, in \u001b[36mQwen2DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mpast_key_value\u001b[39m\u001b[33m\"\u001b[39m, new_name=\u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m, version=\u001b[33m\"\u001b[39m\u001b[33m4.58\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    221\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    229\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m    230\u001b[39m ) -> torch.Tensor:\n\u001b[32m    231\u001b[39m     residual = hidden_states\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    233\u001b[39m     \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m    234\u001b[39m     hidden_states, _ = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    235\u001b[39m         hidden_states=hidden_states,\n\u001b[32m    236\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    242\u001b[39m         **kwargs,\n\u001b[32m    243\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\ProgramData\\anaconda3\\envs\\DL\\Lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:199\u001b[39m, in \u001b[36mQwen2RMSNorm.forward\u001b[39m\u001b[34m(self, hidden_states)\u001b[39m\n\u001b[32m    197\u001b[39m input_dtype = hidden_states.dtype\n\u001b[32m    198\u001b[39m hidden_states = hidden_states.to(torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m variance = \u001b[43mhidden_states\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m.mean(-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    200\u001b[39m hidden_states = hidden_states * torch.rsqrt(variance + \u001b[38;5;28mself\u001b[39m.variance_epsilon)\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weight * hidden_states.to(input_dtype)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Генерируем перефразированные запросы\n",
    "\n",
    "# for i in tqdm(range(len(queries))):\n",
    "#     print(queries[i]['question'])\n",
    "#     new_queries = rephrase_query(queries[i]['question'])\n",
    "#     queries[i]['new_queries'] = new_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cb6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/new_queries.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(queries, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8027c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e9923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/new_queries.json') as files:\n",
    "    queries = json.load(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "72e5fa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Степан\\AppData\\Local\\Temp\\ipykernel_29232\\1456174109.py:18: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n",
      "  hits = client.search(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] score min: 0.8150, max: 0.8551\n",
      "[2] score min: 0.8090, max: 0.8496\n",
      "[3] score min: 0.8137, max: 0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4] score min: 0.8222, max: 0.8636\n",
      "[5] score min: 0.8065, max: 0.8348\n",
      "[6] score min: 0.8155, max: 0.8431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [00:00<00:04,  8.86it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7] score min: 0.8104, max: 0.8425\n",
      "[8] score min: 0.8046, max: 0.8744\n",
      "[9] score min: 0.8163, max: 0.8656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 9/50 [00:01<00:04,  9.47it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] score min: 0.8081, max: 0.8656\n",
      "[11] score min: 0.8107, max: 0.8377\n",
      "[12] score min: 0.7686, max: 0.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13] score min: 0.8011, max: 0.8445\n",
      "[14] score min: 0.8096, max: 0.8466\n",
      "[15] score min: 0.8077, max: 0.8586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 15/50 [00:01<00:03, 10.18it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16] score min: 0.8284, max: 0.8846\n",
      "[17] score min: 0.7945, max: 0.8766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] score min: 0.8003, max: 0.8684\n",
      "[19] score min: 0.7989, max: 0.8505\n",
      "[20] score min: 0.8122, max: 0.8748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [00:02<00:03,  9.89it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21] score min: 0.8098, max: 0.8494\n",
      "[22] score min: 0.8105, max: 0.8576\n",
      "[23] score min: 0.7998, max: 0.8759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24] score min: 0.8092, max: 0.8531\n",
      "[25] score min: 0.8150, max: 0.8641\n",
      "[26] score min: 0.8135, max: 0.8610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [00:02<00:02,  9.93it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27] score min: 0.8124, max: 0.8487\n",
      "[28] score min: 0.8208, max: 0.8538\n",
      "[29] score min: 0.8062, max: 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [00:03<00:02,  9.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30] score min: 0.8223, max: 0.8650\n",
      "[31] score min: 0.8182, max: 0.8584\n",
      "[32] score min: 0.8204, max: 0.8542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33] score min: 0.8059, max: 0.8603\n",
      "[34] score min: 0.7960, max: 0.8498\n",
      "[35] score min: 0.7981, max: 0.8415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [00:03<00:01, 10.22it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36] score min: 0.8166, max: 0.8420\n",
      "[37] score min: 0.8235, max: 0.8461\n",
      "[38] score min: 0.8271, max: 0.8768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [00:03<00:01,  9.75it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39] score min: 0.8183, max: 0.8701\n",
      "[40] score min: 0.8147, max: 0.8505\n",
      "[41] score min: 0.8235, max: 0.8537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [00:04<00:00,  9.91it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42] score min: 0.8234, max: 0.8682\n",
      "[43] score min: 0.8062, max: 0.8449\n",
      "[44] score min: 0.8095, max: 0.8446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45] score min: 0.8102, max: 0.8338\n",
      "[46] score min: 0.8080, max: 0.8441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47] score min: 0.8148, max: 0.8370\n",
      "[48] score min: 0.8187, max: 0.8436\n",
      "[49] score min: 0.8151, max: 0.8465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.65it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50] score min: 0.8126, max: 0.8590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_candidates(queries, top_k_db=5):\n",
    "    results = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query['query_id']\n",
    "        query_texts = [query['question']] + query['new_queries']\n",
    "\n",
    "        # собираем все попадания отсюда\n",
    "        all_hits = []\n",
    "\n",
    "        for qt in query_texts:\n",
    "            vec = embedding_model.encode(\n",
    "                qt,\n",
    "                normalize_embeddings=True,\n",
    "                device=device\n",
    "            ).tolist()\n",
    "\n",
    "            hits = client.search(\n",
    "                collection_name=db_name,\n",
    "                query_vector=vec,\n",
    "                limit=top_k_db\n",
    "            )\n",
    "\n",
    "            for hit in hits:\n",
    "                all_hits.append({\n",
    "                    \"text\": hit.payload['text'],\n",
    "                    \"page\": hit.payload['page'],\n",
    "                    \"score\": hit.score,\n",
    "                    \"chunk_index\": hit.payload['chunk_index']\n",
    "                })\n",
    "\n",
    "        dedup = {}\n",
    "        for item in all_hits:\n",
    "            ci = item[\"chunk_index\"]\n",
    "            if ci not in dedup or item[\"score\"] > dedup[ci][\"score\"]:\n",
    "                dedup[ci] = item\n",
    "\n",
    "        # Берем top_k_db\n",
    "        final_items = sorted(dedup.values(), key=lambda x: x[\"score\"], reverse=True)[:top_k_db]\n",
    "\n",
    "        # логгирование\n",
    "        if final_items:\n",
    "            scores = [i[\"score\"] for i in final_items]\n",
    "            print(f\"[{qid}] score min: {min(scores):.4f}, max: {max(scores):.4f}\")\n",
    "\n",
    "        results[qid] = final_items\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "relevant = get_candidates(queries, top_k_db=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf094e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_db_score(relevant, score_threshold=0.0):\n",
    "    filtered = {}\n",
    "\n",
    "    for qid, items in relevant.items():\n",
    "        selected = [item for item in items if item[\"score\"] >= score_threshold]\n",
    "        filtered[qid] = selected\n",
    "\n",
    "    return filtered\n",
    "\n",
    "# relevant = filter_by_db_score(relevant, score_threshold=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5c15d8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "cross_encoder = CrossEncoder(cross_encoder_id, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "18c909ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_results(queries, relevant, top_k_rerank=3, rerank_threshold=None):\n",
    "    reranked = {}\n",
    "\n",
    "    for query in tqdm(queries):\n",
    "        qid = query['query_id']\n",
    "        q_text = query['question']\n",
    "\n",
    "        candidates = relevant[qid]\n",
    "        texts = [c[\"text\"] for c in candidates]\n",
    "\n",
    "        if len(texts) == 0:\n",
    "            reranked[qid] = []\n",
    "            continue\n",
    "\n",
    "        pairs = [(q_text, t) for t in texts]\n",
    "        scores = cross_encoder.predict(pairs)  # по аналогии с твоим примером\n",
    "\n",
    "        scored = [\n",
    "            {**c, \"rerank_score\": s}\n",
    "            for c, s in zip(candidates, scores)\n",
    "        ]\n",
    "\n",
    "        scored = sorted(scored, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "        scored = scored[:top_k_rerank]\n",
    "\n",
    "        if rerank_threshold is not None:\n",
    "            scored = [s for s in scored if s[\"rerank_score\"] >= rerank_threshold]\n",
    "\n",
    "        reranked[qid] = scored\n",
    "\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6bad970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:19<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "relevant_reranked = rerank_results(\n",
    "    queries,\n",
    "    relevant,\n",
    "    top_k_rerank=6,         # регулируй\n",
    "    rerank_threshold=None   # или поставь, например, 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "00941f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очищаем память, чтобы LLM было вкусно\n",
    "\n",
    "# del cross_encoder\n",
    "# del embedding_model\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3175a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(LLM_model_id, use_fast=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLM_model_id,\n",
    "    dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a98881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_answer(query, context):\n",
    "    system_msg = (\n",
    "        \"You are an expert in psychology. Using only the provided retrieved documents, answer the following question. Do not add any external knowledge.\"\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_msg},\n",
    "        {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ]\n",
    "\n",
    "    answer = generation_pipeline(messages, temperature=0.7, max_new_tokens=1024)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# def llm_answer(query, context):\n",
    "#     system_msg = (\n",
    "#         \"You are an expert in psychology. Using only the provided retrieved documents, answer the following question. Do not add any external knowledge.\"\n",
    "#     )\n",
    "\n",
    "#     messages = [\n",
    "#         {\"role\": \"system\", \"content\": system_msg},\n",
    "#         {\"role\": \"system\", \"content\": f\"Context documents:\\n{context}\"},\n",
    "#         {\"role\": \"user\", \"content\": query}\n",
    "#     ]\n",
    "\n",
    "#     prompt = tokenizer.apply_chat_template(\n",
    "#         messages,\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True\n",
    "#     )\n",
    "#     prompt = \"Привет, как дела?\"\n",
    "#     payload = {\n",
    "#         \"prompt\": prompt,\n",
    "#         \"max_new_tokens\": 512,\n",
    "#         \"do_sample\": True,\n",
    "#         \"temperature\": 0.9,\n",
    "#         \"top_p\": 0.9\n",
    "#     }\n",
    "\n",
    "#     r = requests.post(\"https://shortly-pleasant-democrats-discussion.trycloudflare.com/generate\", json=payload)\n",
    "#     r.raise_for_status()\n",
    "\n",
    "#     # Модель возвращает полный текст, нужно вырезать только продолжение\n",
    "#     response_text = r.json()[\"response\"]\n",
    "\n",
    "#     print(response_text)\n",
    "#     # Если нужно вырезать ввод с контекстом, делаем так:\n",
    "#     if response_text.startswith(prompt):\n",
    "#         response_text = response_text[len(prompt):].strip()\n",
    "\n",
    "#     return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reorder_chunks(chunks):\n",
    "#     n = len(chunks)\n",
    "#     if n <= 3:\n",
    "#         return chunks  # просто как есть\n",
    "\n",
    "#     first_three = chunks[:3]\n",
    "#     # далее позиции считаем относительно chunks[3:]\n",
    "#     odds = chunks[3::2]   # 4-й, 6-й, 8-й...\n",
    "#     evens = chunks[4::2]  # 5-й, 7-й, 9-й...\n",
    "#     evens = evens[::-1]   # развернуть\n",
    "\n",
    "#     return first_three + odds + evens\n",
    "\n",
    "# # пример формирования вывода\n",
    "# def format_chunks(chunks):\n",
    "#     return \"\\n\".join(f\"{i}. {c}\" for i, c in enumerate(reorder_chunks(chunks), start=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eafd076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toc import toc\n",
    "\n",
    "def find_section_for_page(toc, page):\n",
    "    all_sections = []\n",
    "\n",
    "    for chapter_data in toc.values():\n",
    "        chapter_title = chapter_data[\"title\"]\n",
    "        for section_title, start in chapter_data[\"sections\"].items():\n",
    "            all_sections.append((chapter_title, section_title, start))\n",
    "\n",
    "    # сортируем по старту секции\n",
    "    all_sections.sort(key=lambda x: x[2])\n",
    "\n",
    "    # ищем последнюю секцию, начавшуюся не позже страницы\n",
    "    candidates = [(ch, sec, start) for ch, sec, start in all_sections if start <= page]\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    ch, sec, _ = candidates[-1]\n",
    "    return f\"{ch}/{sec}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bdd2cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_pairs(pairs):\n",
    "    # pairs: список кортежей (chunk_text, page)\n",
    "    n = len(pairs)\n",
    "    if n <= 3:\n",
    "        return pairs\n",
    "\n",
    "    first_three = pairs[:3]\n",
    "    odds = pairs[3::2]   # 4-й, 6-й, 8-й...\n",
    "    evens = pairs[4::2]  # 5-й, 7-й, 9-й...\n",
    "    evens = evens[::-1]\n",
    "\n",
    "    return first_three + odds + evens\n",
    "\n",
    "def format_pairs(pairs):\n",
    "    lines = []\n",
    "    for i, (chunk, page) in enumerate(pairs, start=1):\n",
    "        sec = find_section_for_page(toc, page)\n",
    "        if sec is None:\n",
    "            sec = \"Unknown\"\n",
    "        lines.append(f\"{i}. {sec} - {chunk}\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5d8e2f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [10:26<00:00, 12.53s/it]\n"
     ]
    }
   ],
   "source": [
    "outputs = []\n",
    "result_pages = []\n",
    "\n",
    "for query in tqdm(queries):\n",
    "    qid = query['query_id']\n",
    "    q_text = query['question']\n",
    "    data = relevant_reranked[qid]\n",
    "\n",
    "    # собираем пары (текст, страница)\n",
    "    pairs = [(item[\"text\"], item[\"page\"]) for item in data]\n",
    "\n",
    "    # переставляем\n",
    "    reordered = reorder_pairs(pairs)\n",
    "\n",
    "    # делаем контекст с секциями\n",
    "    context = format_pairs(reordered)\n",
    "\n",
    "    answer = llm_answer(q_text, context)\n",
    "\n",
    "    outputs.append(answer)\n",
    "    result_pages.append([p for _, p in reordered])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d4dd9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sequential(items, overlap: int) -> str:\n",
    "    if not items:\n",
    "        return \"\"\n",
    "    items = sorted(items, key=lambda x: x[\"chunk_index\"])\n",
    "\n",
    "    merged_groups = []\n",
    "    cur_text = items[0][\"text\"]\n",
    "    prev_idx = items[0][\"chunk_index\"]\n",
    "\n",
    "    for it in items[1:]:\n",
    "        idx, t = it[\"chunk_index\"], it[\"text\"]\n",
    "        if idx == prev_idx + 1:\n",
    "            # учёт overlap: убираем дублирующийся префикс t\n",
    "            max_k = min(overlap, len(cur_text), len(t))\n",
    "            cut = 0\n",
    "            for k in range(max_k, 0, -1):\n",
    "                if cur_text[-k:] == t[:k]:\n",
    "                    cut = k\n",
    "                    break\n",
    "            cur_text += t[cut:]\n",
    "        else:\n",
    "            merged_groups.append(cur_text)\n",
    "            cur_text = t\n",
    "        prev_idx = idx\n",
    "\n",
    "    merged_groups.append(cur_text)\n",
    "    return \"\\n\".join(merged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f012889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [q['query_id'] for q in queries]\n",
    "references = [{\"sections\": [], \"pages\": list(set(pages))} for pages in result_pages]\n",
    "\n",
    "for ref in references:\n",
    "    for page in ref[\"pages\"]:\n",
    "        section = find_section_for_page(toc, page)\n",
    "        if section:\n",
    "            section = section.lower().replace(\" \", \"_\")\n",
    "            ref[\"sections\"].append(section)\n",
    "\n",
    "context_text = [\n",
    "    merge_sequential(relevant_reranked[qid], chunk_overlap)\n",
    "    for qid in ids\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2faa4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    \"ID\": ids,\n",
    "    \"context\": context_text,\n",
    "    \"answer\": outputs,\n",
    "    \"references\": references\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8e2ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"data/submission8.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
