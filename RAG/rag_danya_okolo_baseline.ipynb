{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13735613,"sourceType":"datasetVersion","datasetId":8739556}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# начало работы","metadata":{}},{"cell_type":"code","source":"# === Block 0. Базовые импорты, конфиг, сиды ===\n\nimport os\nimport sys\nimport json\nimport math\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\n\n# --- базовые пути (можно править под конкретное соревнование) ---\nBASE_DIR = Path(\".\").resolve()\nDATA_DIR = BASE_DIR / \"data\"          # сюда обычно кладём исходные данные\nDOCS_DIR = DATA_DIR / \"docs\"          # сюда — документы для RAG (pdf/txt/html и т.д.)\nOUTPUT_DIR = BASE_DIR / \"outputs\"     # сюда — артефакты, индекс, сабмиты\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- глобальный конфиг (можно потом расширять) ---\nSEED = 42\n\n# размер чанка и overlap будут важны для сплиттера,\n# здесь просто задаём дефолт, позже ещё будет отдельный блок про чанкинг\nCHUNK_SIZE = 512\nCHUNK_OVERLAP = 64\n\n# максимальная длина контекста, который будем скармливать модели\nMAX_CONTEXT_CHARS = 4000\n\n# заглушки для имён моделей (позже конкретизируем в отдельном блоке)\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\nGENERATION_MODEL_NAME = \"gpt2\"  # пример; позже поменяем на нужную локальную модель\n\n# --- функция для фиксации сида ---\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\nset_seed(SEED)\n\n# --- информация о девайсе ---\nif torch.cuda.is_available():\n    DEVICE = torch.device(\"cuda\")\n    print(\"Using GPU:\", torch.cuda.get_device_name(0))\nelse:\n    DEVICE = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\nprint(\"BASE_DIR:\", BASE_DIR)\nprint(\"DATA_DIR:\", DATA_DIR)\nprint(\"DOCS_DIR:\", DOCS_DIR)\nprint(\"OUTPUT_DIR:\", OUTPUT_DIR)\nprint(\"SEED:\", SEED)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:47:45.829144Z","iopub.execute_input":"2025-11-14T22:47:45.829939Z","iopub.status.idle":"2025-11-14T22:47:45.843229Z","shell.execute_reply.started":"2025-11-14T22:47:45.829912Z","shell.execute_reply":"2025-11-14T22:47:45.842732Z"}},"outputs":[{"name":"stdout","text":"Using GPU: Tesla P100-PCIE-16GB\nBASE_DIR: /kaggle/working\nDATA_DIR: /kaggle/working/data\nDOCS_DIR: /kaggle/working/data/docs\nOUTPUT_DIR: /kaggle/working/outputs\nSEED: 42\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# подгруз данных","metadata":{}},{"cell_type":"code","source":"# === Block 0.5. Копируем данные соревнования в нашу структуру ===\n\n# создаём папки, если их ещё нет\nDATA_DIR.mkdir(parents=True, exist_ok=True)\nDOCS_DIR.mkdir(parents=True, exist_ok=True)\n\nsrc_dir = Path(\"/kaggle/input/competition-psycho\")\n\n# копируем pdf в docs\nsrc_book = src_dir / \"book.pdf\"\ndst_book = DOCS_DIR / \"book.pdf\"\n\nif src_book.exists():\n    import shutil\n    shutil.copy2(src_book, dst_book)\n    print(\"Скопировал book.pdf →\", dst_book)\nelse:\n    print(\"НЕ НАШЁЛ файл:\", src_book)\n\n# копируем queries.json в корень data\nsrc_queries = src_dir / \"queries.json\"\ndst_queries = DATA_DIR / \"queries.json\"\n\nif src_queries.exists():\n    import shutil\n    shutil.copy2(src_queries, dst_queries)\n    print(\"Скопировал queries.json →\", dst_queries)\nelse:\n    print(\"НЕ НАШЁЛ файл:\", src_queries)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:47:49.288649Z","iopub.execute_input":"2025-11-14T22:47:49.289101Z","iopub.status.idle":"2025-11-14T22:47:50.587151Z","shell.execute_reply.started":"2025-11-14T22:47:49.289069Z","shell.execute_reply":"2025-11-14T22:47:50.586514Z"}},"outputs":[{"name":"stdout","text":"Скопировал book.pdf → /kaggle/working/data/docs/book.pdf\nСкопировал queries.json → /kaggle/working/data/queries.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# === Block 1. Поиск документов для RAG в DOCS_DIR ===\n\n# какие типы файлов считаем документами\nSUPPORTED_EXTENSIONS = [\".pdf\", \".txt\", \".md\", \".html\", \".htm\"]\n\ndef list_documents(docs_dir: Path, exts=None) -> pd.DataFrame:\n    # если не передали свой список расширений — используем дефолтный\n    if exts is None:\n        exts = SUPPORTED_EXTENSIONS\n\n    docs = []\n\n    # если папки с документами ещё нет — создаём пустую и сразу возвращаем пустой DataFrame\n    if not docs_dir.exists():\n        print(\"DOCS_DIR не существует, создаю пустую папку:\", docs_dir)\n        docs_dir.mkdir(parents=True, exist_ok=True)\n        return pd.DataFrame(columns=[\"doc_id\", \"path\", \"ext\"])\n\n    # рекурсивно обходим DOCS_DIR и собираем все подходящие файлы\n    for path in sorted(docs_dir.rglob(\"*\")):\n        if path.is_file():\n            ext = path.suffix.lower()\n            if ext in exts:\n                docs.append(\n                    {\n                        \"doc_id\": len(docs),   # простой числовой id документа\n                        \"path\": path,          # полный путь к файлу\n                        \"ext\": ext,            # расширение файла\n                    }\n                )\n\n    return pd.DataFrame(docs)\n\n\ndocs_df = list_documents(DOCS_DIR)\n\nprint(\"Найдено документов:\", len(docs_df))\nprint(docs_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:47:53.451207Z","iopub.execute_input":"2025-11-14T22:47:53.451874Z","iopub.status.idle":"2025-11-14T22:47:53.469483Z","shell.execute_reply.started":"2025-11-14T22:47:53.451848Z","shell.execute_reply":"2025-11-14T22:47:53.468648Z"}},"outputs":[{"name":"stdout","text":"Найдено документов: 1\n   doc_id                                path   ext\n0       0  /kaggle/working/data/docs/book.pdf  .pdf\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install uv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:46:02.262206Z","iopub.execute_input":"2025-11-14T22:46:02.262469Z","iopub.status.idle":"2025-11-14T22:46:08.034884Z","shell.execute_reply.started":"2025-11-14T22:46:02.262448Z","shell.execute_reply":"2025-11-14T22:46:08.033972Z"}},"outputs":[{"name":"stdout","text":"Collecting uv\n  Downloading uv-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading uv-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: uv\nSuccessfully installed uv-0.9.9\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!uv pip install langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:46:22.617386Z","iopub.execute_input":"2025-11-14T22:46:22.618018Z","iopub.status.idle":"2025-11-14T22:46:23.955863Z","shell.execute_reply.started":"2025-11-14T22:46:22.617982Z","shell.execute_reply":"2025-11-14T22:46:23.955164Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[2mResolved \u001b[1m58 packages\u001b[0m \u001b[2min 699ms\u001b[0m\u001b[0m                                        \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m4 packages\u001b[0m \u001b[2min 270ms\u001b[0m\u001b[0m                                             \n\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 30ms\u001b[0m\u001b[0m\n\u001b[2K\u001b[2mInstalled \u001b[1m4 packages\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m0.0                             \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlangchain-classic\u001b[0m\u001b[2m==1.0.0\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlangchain-community\u001b[0m\u001b[2m==0.4.1\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==0.3.72\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlangchain-core\u001b[0m\u001b[2m==1.0.5\u001b[0m\n \u001b[31m-\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==0.3.9\u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mlangchain-text-splitters\u001b[0m\u001b[2m==1.0.0\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# === Block 2. Загрузка текста документов (pdf / txt / md / html) ===\n\nfrom typing import List, Dict, Any\n\nfrom langchain_community.document_loaders import PyPDFLoader  # для pdf через langchain\n\n\ndef load_pdf(path: Path) -> str:\n    # читаем PDF постранично и склеиваем в один текст\n    loader = PyPDFLoader(str(path))\n    pages = loader.load()  # список объектов Document с .page_content\n    texts = [p.page_content for p in pages]\n    return \"\\n\\n\".join(texts)\n\n\ndef load_text_file(path: Path, encoding: str = \"utf-8\") -> str:\n    # простой читатель для txt/md/html\n    # (для html сейчас не чистим теги, просто читаем как есть — этого уже достаточно для baseline)\n    with open(path, \"r\", encoding=encoding, errors=\"ignore\") as f:\n        return f.read()\n\n\ndef load_single_document(row: pd.Series) -> Dict[str, Any]:\n    doc_id = int(row[\"doc_id\"])\n    path = Path(row[\"path\"])\n    ext = str(row[\"ext\"]).lower()\n\n    if ext == \".pdf\":\n        text = load_pdf(path)\n    else:\n        text = load_text_file(path)\n\n    return {\n        \"doc_id\": doc_id,\n        \"path\": str(path),\n        \"ext\": ext,\n        \"text\": text,\n        \"n_chars\": len(text),\n    }\n\n\nraw_docs: List[Dict[str, Any]] = []\n\nif len(docs_df) == 0:\n    print(\"В DOCS_DIR пока нет документов. Block 2 ничего не загрузил.\")\nelse:\n    print(\"Загружаю текст документов...\")\n    for _, row in tqdm(docs_df.iterrows(), total=len(docs_df)):\n        doc_info = load_single_document(row)\n        raw_docs.append(doc_info)\n\n    print(f\"Загружено документов: {len(raw_docs)}\")\n\n# переводим в DataFrame для удобства\nraw_docs_df = pd.DataFrame(raw_docs)\nprint(raw_docs_df[[\"doc_id\", \"ext\", \"n_chars\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:47:56.555317Z","iopub.execute_input":"2025-11-14T22:47:56.555592Z","iopub.status.idle":"2025-11-14T22:48:11.102847Z","shell.execute_reply.started":"2025-11-14T22:47:56.555572Z","shell.execute_reply":"2025-11-14T22:48:11.102216Z"}},"outputs":[{"name":"stdout","text":"Загружаю текст документов...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:14<00:00, 14.52s/it]","output_type":"stream"},{"name":"stdout","text":"Загружено документов: 1\n   doc_id   ext  n_chars\n0       0  .pdf  2256059\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# чанкинг","metadata":{}},{"cell_type":"code","source":"# === Block 3. Чанкинг текста документов (семантический/структурный) ===\n\nfrom typing import List, Dict, Any\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Минимальная доля пересечения между чанками (по символам)\nMIN_OVERLAP_FRACTION = 0.25  # > 20% как ты просил\n\nmin_overlap = int(CHUNK_SIZE * MIN_OVERLAP_FRACTION)\nEFFECTIVE_OVERLAP = max(CHUNK_OVERLAP, min_overlap)\n\nprint(\"CHUNK_SIZE:\", CHUNK_SIZE)\nprint(\"CHUNK_OVERLAP (запрошенный):\", CHUNK_OVERLAP)\nprint(\"EFFECTIVE_OVERLAP (используемый, >= 20%):\", EFFECTIVE_OVERLAP)\n\n# Сепараторы подобраны так, чтобы сперва резать по \"крупной структуре\":\n#   - пустые строки / параграфы\n#   - одиночные переводы строки\n#   - предложения \". \"\n#   - пробелы\n#   - в крайнем случае — посимвольно\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=CHUNK_SIZE,\n    chunk_overlap=EFFECTIVE_OVERLAP,\n    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n    length_function=len,\n)\n\ndef split_doc_to_chunks(doc_row: pd.Series) -> List[Dict[str, Any]]:\n    doc_id = int(doc_row[\"doc_id\"])\n    text = str(doc_row[\"text\"])\n    path = str(doc_row[\"path\"])\n    ext = str(doc_row[\"ext\"])\n\n    if not text.strip():\n        return []\n\n    # создаём список \"документов\" langchain с общими метаданными\n    lc_docs = text_splitter.create_documents(\n        texts=[text],\n        metadatas=[{\n            \"doc_id\": doc_id,\n            \"path\": path,\n            \"ext\": ext,\n        }],\n    )\n\n    chunks: List[Dict[str, Any]] = []\n    for local_idx, d in enumerate(lc_docs):\n        chunk_text = d.page_content\n        meta = d.metadata or {}\n        chunks.append(\n            {\n                \"chunk_id\": None,  # заполним позже глобальными id\n                \"doc_id\": meta.get(\"doc_id\", doc_id),\n                \"chunk_idx_in_doc\": local_idx,\n                \"path\": meta.get(\"path\", path),\n                \"ext\": meta.get(\"ext\", ext),\n                \"text\": chunk_text,\n                \"n_chars\": len(chunk_text),\n            }\n        )\n    return chunks\n\n\nall_chunks: List[Dict[str, Any]] = []\n\nif raw_docs_df is None or len(raw_docs_df) == 0:\n    print(\"raw_docs_df пуст — сначала нужно загрузить документы (Block 2).\")\nelse:\n    print(\"Делаю чанкинг документов...\")\n    for _, row in tqdm(raw_docs_df.iterrows(), total=len(raw_docs_df)):\n        doc_chunks = split_doc_to_chunks(row)\n        all_chunks.extend(doc_chunks)\n\n    # присваиваем глобальные chunk_id\n    for idx, ch in enumerate(all_chunks):\n        ch[\"chunk_id\"] = idx\n\n    print(f\"Всего чанков: {len(all_chunks)}\")\n\nchunks_df = pd.DataFrame(all_chunks)\n\nprint(\"Размер chunks_df:\", chunks_df.shape)\nprint(chunks_df[[\"chunk_id\", \"doc_id\", \"chunk_idx_in_doc\", \"n_chars\"]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T22:48:21.487196Z","iopub.execute_input":"2025-11-14T22:48:21.488134Z","iopub.status.idle":"2025-11-14T22:48:21.603439Z","shell.execute_reply.started":"2025-11-14T22:48:21.488106Z","shell.execute_reply":"2025-11-14T22:48:21.602667Z"}},"outputs":[{"name":"stdout","text":"CHUNK_SIZE: 512\nCHUNK_OVERLAP (запрошенный): 64\nEFFECTIVE_OVERLAP (используемый, >= 20%): 128\nДелаю чанкинг документов...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 11.01it/s]","output_type":"stream"},{"name":"stdout","text":"Всего чанков: 6406\nРазмер chunks_df: (6406, 7)\n   chunk_id  doc_id  chunk_idx_in_doc  n_chars\n0         0       0                 0      175\n1         1       0                 1      484\n2         2       0                 2      422\n3         3       0                 3      439\n4         4       0                 4      501\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# эмбединги чанков","metadata":{}},{"cell_type":"code","source":"!uv pip install faiss-cpu\n\n'''по факту нужен gpu но на кагле он сдохнет с зависимостями и не может установиться'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:04:34.787895Z","iopub.execute_input":"2025-11-14T23:04:34.788556Z","iopub.status.idle":"2025-11-14T23:04:35.053302Z","shell.execute_reply.started":"2025-11-14T23:04:34.788527Z","shell.execute_reply":"2025-11-14T23:04:35.052532Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 107ms\u001b[0m\u001b[0m\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# === Block 4A. Инициализация эмбеддинг-модели и helper-функции ===\n\nfrom sentence_transformers import SentenceTransformer\nimport faiss  # сам индекс будем делать в следующем блоке\n\n# при желании можно поменять модель на другую из sentence-transformers\nEMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n\nprint(\"Загружаю embedding-модель:\", EMBEDDING_MODEL_NAME)\nembed_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=str(DEVICE))\n\n# Проверим размерность эмбеддингов на одном примере\ntest_emb = embed_model.encode([\"test\"], convert_to_numpy=True, show_progress_bar=False)\nEMBED_DIM = int(test_emb.shape[1])\nprint(\"EMBED_DIM:\", EMBED_DIM)\n\n\ndef embed_texts(texts, batch_size: int = 64):\n    # texts — список строк\n    # возвращает numpy-массив размера [len(texts), EMBED_DIM]\n    return embed_model.encode(\n        texts,\n        batch_size=batch_size,\n        convert_to_numpy=True,\n        show_progress_bar=True,\n        normalize_embeddings=True,  # сразу L2-нормализация, удобно для cosine/IP\n    )\n\nprint(\"embed_texts() готова к использованию\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:05:15.614263Z","iopub.execute_input":"2025-11-14T23:05:15.614926Z","iopub.status.idle":"2025-11-14T23:05:25.560287Z","shell.execute_reply.started":"2025-11-14T23:05:15.614897Z","shell.execute_reply":"2025-11-14T23:05:25.559558Z"}},"outputs":[{"name":"stdout","text":"Загружаю embedding-модель: sentence-transformers/all-MiniLM-L6-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6c05d9c8444930baedd4e2e3afe091"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d49d34bb4e542f4a378e070ad0e5fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c64c6867cb44fa3a41238260ee94d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08eb76cc94e3478c8da50d80dd28c14c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8bc8f82ba2a433e810a7e7318dd3fa1"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a0dfb70d0fd416baee92e709093c7e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c6a98e8f0a349f68caea507a402aaa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8657f8d4a20b446f8997499750bb6ec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca76fe0e406d47e683ad461aef13cc26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29fecbffd6a14d86bc1bb275a3acef31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95a44c57f18948b1a5c33adca4c84eb6"}},"metadata":{}},{"name":"stdout","text":"EMBED_DIM: 384\nembed_texts() готова к использованию\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# === Block 4B. Эмбеддинги чанков + построение FAISS-индекса ===\n\nif \"chunks_df\" not in globals() or chunks_df is None or len(chunks_df) == 0:\n    print(\"chunks_df пуст — нужно выполнить блоки 2 и 3 (загрузка и чанкинг документов).\")\nelse:\n    chunk_texts = chunks_df[\"text\"].astype(str).tolist()\n    print(\"Чанков для эмбеддинга:\", len(chunk_texts))\n\n    # считаем эмбеддинги\n    chunk_embs = embed_texts(chunk_texts, batch_size=64)  # можно подкрутить размер батча\n    print(\"Готово, форма эмбеддингов:\", chunk_embs.shape)\n\n    # на всякий случай убеждаемся, что размерность совпадает\n    assert chunk_embs.shape[1] == EMBED_DIM\n\n    # создаём FAISS-индекс под cosine similarity:\n    # мы уже нормализовали в embed_texts(normalize_embeddings=True),\n    # поэтому можно использовать скалярное произведение (IP).\n    index = faiss.IndexFlatIP(EMBED_DIM)\n\n    # если доступен GPU-FAISS, можно перенести индекс на GPU\n    faiss_res = None\n    if torch.cuda.is_available():\n        try:\n            faiss_res = faiss.StandardGpuResources()\n            index = faiss.index_cpu_to_gpu(faiss_res, 0, index)\n            print(\"FAISS-индекс перенесён на GPU\")\n        except Exception as e:\n            print(\"Не удалось перенести FAISS-индекс на GPU, остаёмся на CPU:\", e)\n\n    # добавляем в индекс все вектора\n    index.add(chunk_embs)\n    print(\"Векторов в FAISS-индексе:\", index.ntotal)\n\n    # сохраняем эмбеддинги и метаданные на диск\n    emb_path = OUTPUT_DIR / \"chunk_embeddings.npy\"\n    meta_path = OUTPUT_DIR / \"chunks_metadata.parquet\"\n    faiss_path = OUTPUT_DIR / \"faiss_index.bin\"\n\n    np.save(emb_path, chunk_embs)\n    chunks_df.to_parquet(meta_path, index=False)\n\n    # для сохранения индекса на диск он должен быть на CPU\n    index_cpu = index\n    if faiss_res is not None:\n        try:\n            index_cpu = faiss.index_gpu_to_cpu(index)\n        except Exception as e:\n            print(\"Не удалось скопировать индекс с GPU на CPU, сохраняю как есть (может не сработать):\", e)\n\n    faiss.write_index(index_cpu, str(faiss_path))\n\n    print(\"Эмбеддинги сохранены в:\", emb_path)\n    print(\"Метаданные чанков сохранены в:\", meta_path)\n    print(\"FAISS-индекс сохранён в:\", faiss_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:06:46.456792Z","iopub.execute_input":"2025-11-14T23:06:46.457445Z","iopub.status.idle":"2025-11-14T23:06:53.503313Z","shell.execute_reply.started":"2025-11-14T23:06:46.457416Z","shell.execute_reply":"2025-11-14T23:06:53.502363Z"}},"outputs":[{"name":"stdout","text":"Чанков для эмбеддинга: 6406\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/101 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d98744169694de999ad95c95442752c"}},"metadata":{}},{"name":"stdout","text":"Готово, форма эмбеддингов: (6406, 384)\nНе удалось перенести FAISS-индекс на GPU, остаёмся на CPU: module 'faiss' has no attribute 'StandardGpuResources'\nВекторов в FAISS-индексе: 6406\nЭмбеддинги сохранены в: /kaggle/working/outputs/chunk_embeddings.npy\nМетаданные чанков сохранены в: /kaggle/working/outputs/chunks_metadata.parquet\nFAISS-индекс сохранён в: /kaggle/working/outputs/faiss_index.bin\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# FAISS + BM25","metadata":{}},{"cell_type":"code","source":"!uv pip install rank_bm25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:07:25.338727Z","iopub.execute_input":"2025-11-14T23:07:25.339526Z","iopub.status.idle":"2025-11-14T23:07:25.738373Z","shell.execute_reply.started":"2025-11-14T23:07:25.339503Z","shell.execute_reply":"2025-11-14T23:07:25.737603Z"}},"outputs":[{"name":"stdout","text":"\u001b[2mUsing Python 3.11.13 environment at: /usr\u001b[0m\n\u001b[2K\u001b[37m⠙\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K\u001b[2mResolved \u001b[1m14 packages\u001b[0m \u001b[2min 85ms\u001b[0m\u001b[0m                                         \u001b[0m\n\u001b[2K\u001b[2mPrepared \u001b[1m1 package\u001b[0m \u001b[2min 16ms\u001b[0m\u001b[0m                                               \n\u001b[2K\u001b[2mInstalled \u001b[1m1 package\u001b[0m \u001b[2min 1ms\u001b[0m\u001b[0m                                  \u001b[0m\n \u001b[32m+\u001b[39m \u001b[1mrank-bm25\u001b[0m\u001b[2m==0.2.2\u001b[0m\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# === Block 5A. Построение BM25-индекса по чанкам ===\n\nimport re\nfrom rank_bm25 import BM25Okapi\n\ndef simple_tokenize(text: str):\n    # простой токенайзер:\n    #   - в нижний регистр\n    #   - разбиваем по \"не-буквенно-цифровым\" символам\n    #   - выкидываем пустые токены\n    text = text.lower()\n    tokens = re.split(r\"[^a-zA-Z0-9а-яА-ЯёЁ]+\", text)\n    tokens = [t for t in tokens if t]\n    return tokens\n\nif \"chunks_df\" not in globals() or chunks_df is None or len(chunks_df) == 0:\n    print(\"chunks_df пуст — нужно выполнить блоки с загрузкой и чанкингом (2, 3A, 3B).\")\nelse:\n    print(\"Готовлю корпус для BM25 по чанкам...\")\n    \n    # список текстов чанков\n    bm25_texts = chunks_df[\"text\"].astype(str).tolist()\n    # токенизированный корпус\n    bm25_corpus_tokens = [simple_tokenize(t) for t in tqdm(bm25_texts)]\n    \n    # сохраняем соответствие: позиция в BM25 ↔ chunk_id\n    bm25_chunk_ids = chunks_df[\"chunk_id\"].tolist()\n    \n    print(\"Строю BM25Okapi...\")\n    bm25 = BM25Okapi(bm25_corpus_tokens)\n    \n    print(\"BM25-индекс готов\")\n    print(\"Количество документов в BM25:\", len(bm25_corpus_tokens))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:07:27.767612Z","iopub.execute_input":"2025-11-14T23:07:27.768452Z","iopub.status.idle":"2025-11-14T23:07:28.146258Z","shell.execute_reply.started":"2025-11-14T23:07:27.768418Z","shell.execute_reply":"2025-11-14T23:07:28.145556Z"}},"outputs":[{"name":"stdout","text":"Готовлю корпус для BM25 по чанкам...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 6406/6406 [00:00<00:00, 30581.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Строю BM25Okapi...\nBM25-индекс готов\nКоличество документов в BM25: 6406\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# === Block 5B. Гибридный поиск по чанкам: FAISS + BM25 ===\n\nfrom typing import List, Tuple\n\ndef search_faiss_chunks(query: str, top_k: int = 20) -> pd.DataFrame:\n    # эмбеддим запрос\n    q_emb = embed_texts([query], batch_size=1)  # (1, EMBED_DIM)\n    \n    # ищем векторно\n    scores, indices = index.search(q_emb, top_k)  # scores: (1, top_k), indices: (1, top_k)\n    scores = scores[0]\n    indices = indices[0]\n    \n    # индексы здесь — позиции в массиве chunk_embs, которые соответствуют строкам chunks_df по порядку\n    # мы использовали chunks_df[\"text\"].tolist() в Block 4B, так что соответствие такое:\n    #   pos i  <->  chunks_df.iloc[i]\n    \n    rows = []\n    for pos, score in zip(indices, scores):\n        if pos < 0:\n            continue\n        row = chunks_df.iloc[int(pos)]\n        rows.append(\n            {\n                \"chunk_id\": int(row[\"chunk_id\"]),\n                \"doc_id\": int(row[\"doc_id\"]),\n                \"score_faiss\": float(score),\n            }\n        )\n    return pd.DataFrame(rows)\n\n\ndef search_bm25_chunks(query: str, top_k: int = 20) -> pd.DataFrame:\n    if \"bm25\" not in globals():\n        raise RuntimeError(\"BM25-индекс не найден. Сначала выполните Block 5A.\")\n    \n    # токенизируем запрос\n    q_tokens = simple_tokenize(query)\n    if not q_tokens:\n        return pd.DataFrame(columns=[\"chunk_id\", \"doc_id\", \"score_bm25\"])\n    \n    # получаем bm25-оценки для каждого документа (по позиции)\n    scores = bm25.get_scores(q_tokens)  # numpy-массив длины len(bm25_corpus_tokens)\n    \n    # берём top_k позиций\n    top_k = min(top_k, len(scores))\n    top_indices = np.argsort(scores)[::-1][:top_k]\n    \n    rows = []\n    for pos in top_indices:\n        score = float(scores[pos])\n        chunk_id = int(bm25_chunk_ids[pos])\n        # doc_id берём из chunks_df\n        row = chunks_df.loc[chunks_df[\"chunk_id\"] == chunk_id].iloc[0]\n        rows.append(\n            {\n                \"chunk_id\": chunk_id,\n                \"doc_id\": int(row[\"doc_id\"]),\n                \"score_bm25\": score,\n            }\n        )\n    return pd.DataFrame(rows)\n\n\ndef hybrid_search_chunks(\n    query: str,\n    top_k_faiss: int = 20,\n    top_k_bm25: int = 20,\n    alpha_faiss: float = 0.5,\n) -> pd.DataFrame:\n    # alpha_faiss — вес векторного поиска,\n    # (1 - alpha_faiss) — вес BM25\n    \n    df_faiss = search_faiss_chunks(query, top_k=top_k_faiss)\n    df_bm25 = search_bm25_chunks(query, top_k=top_k_bm25)\n    \n    # нормализуем оценки, чтобы они были в сопоставимом масштабе (0..1)\n    if len(df_faiss) > 0:\n        max_f = max(abs(df_faiss[\"score_faiss\"].max()), 1e-9)\n        df_faiss[\"score_faiss_norm\"] = df_faiss[\"score_faiss\"] / max_f\n    else:\n        df_faiss[\"score_faiss_norm\"] = []\n    \n    if len(df_bm25) > 0:\n        max_b = max(abs(df_bm25[\"score_bm25\"].max()), 1e-9)\n        df_bm25[\"score_bm25_norm\"] = df_bm25[\"score_bm25\"] / max_b\n    else:\n        df_bm25[\"score_bm25_norm\"] = []\n    \n    # объединяем по chunk_id\n    hybrid = pd.merge(\n        df_faiss[[\"chunk_id\", \"doc_id\", \"score_faiss_norm\"]],\n        df_bm25[[\"chunk_id\", \"score_bm25_norm\"]],\n        on=\"chunk_id\",\n        how=\"outer\",\n        suffixes=(\"_faiss\", \"_bm25\"),\n    )\n    \n    # doc_id может быть NaN, если чанка не было в faiss-результатах, но был в bm25\n    if \"doc_id\" not in hybrid.columns:\n        hybrid[\"doc_id\"] = np.nan\n    else:\n        hybrid[\"doc_id\"] = hybrid[\"doc_id\"].fillna(-1).astype(int)\n    \n    hybrid[\"score_faiss_norm\"] = hybrid[\"score_faiss_norm\"].fillna(0.0)\n    hybrid[\"score_bm25_norm\"] = hybrid[\"score_bm25_norm\"].fillna(0.0)\n    \n    # итоговый скор = alpha * faiss + (1 - alpha) * bm25\n    hybrid[\"score_hybrid\"] = (\n        alpha_faiss * hybrid[\"score_faiss_norm\"]\n        + (1.0 - alpha_faiss) * hybrid[\"score_bm25_norm\"]\n    )\n    \n    # сортируем по итоговому скору\n    hybrid = hybrid.sort_values(\"score_hybrid\", ascending=False)\n    \n    # забираем top-k кандидатов\n    hybrid_top = hybrid.head(max(top_k_faiss, top_k_bm25)).reset_index(drop=True)\n    \n    return hybrid_top\n\n\nprint(\"Функции search_faiss_chunks, search_bm25_chunks и hybrid_search_chunks готовы.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:08:04.446528Z","iopub.execute_input":"2025-11-14T23:08:04.447226Z","iopub.status.idle":"2025-11-14T23:08:04.462192Z","shell.execute_reply.started":"2025-11-14T23:08:04.447199Z","shell.execute_reply":"2025-11-14T23:08:04.461314Z"}},"outputs":[{"name":"stdout","text":"Функции search_faiss_chunks, search_bm25_chunks и hybrid_search_chunks готовы.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"# груз ллм","metadata":{}},{"cell_type":"code","source":"'''# === Block 6A. Загрузка LLM с Hugging Face (Qwen2.5-1.5B-Instruct по умолчанию) ===\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\n# Hugging Face ID модели по умолчанию\nDEFAULT_HF_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\n# можно переопределить через переменную окружения, если захочешь\nHF_MODEL_ID = os.environ.get(\"RAG_LLM_ID\", DEFAULT_HF_MODEL_ID)\n\n# папка для локального кеша моделей (внутри Permanent)\nMODELS_DIR = BASE_DIR / \"models\"\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\n# отдельная подпапка под конкретную модель\nLOCAL_MODEL_DIR = MODELS_DIR / HF_MODEL_ID.replace(\"/\", \"_\")\nLOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"HF_MODEL_ID:\", HF_MODEL_ID)\nprint(\"LOCAL_MODEL_DIR:\", LOCAL_MODEL_DIR)\n\n# при первом вызове from_pretrained модель скачается в LOCAL_MODEL_DIR,\n# дальше будет использовать локальный кеш, даже если интернет отвалится\nprint(\"Загружаю tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    HF_MODEL_ID,\n    cache_dir=str(LOCAL_MODEL_DIR),\n    trust_remote_code=True\n)\n\nprint(\"Загружаю модель (это может занять время при первом запуске)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    cache_dir=str(LOCAL_MODEL_DIR),\n    trust_remote_code=True,\n    torch_dtype=torch.float16,   # на A100 это ок\n    device_map=\"auto\"           # автоматически раскидывает слои по доступным устройствам\n)\n\ngen_pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    device=0 if torch.cuda.is_available() else -1\n)\n\nprint(\"LLM загружена и готова к генерации.\")'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T23:08:10.614403Z","iopub.execute_input":"2025-11-14T23:08:10.615039Z","iopub.status.idle":"2025-11-14T23:08:26.233284Z","shell.execute_reply.started":"2025-11-14T23:08:10.615013Z","shell.execute_reply":"2025-11-14T23:08:26.232033Z"}},"outputs":[{"name":"stdout","text":"HF_MODEL_ID: Qwen/Qwen2.5-1.5B-Instruct\nLOCAL_MODEL_DIR: /kaggle/working/models/Qwen_Qwen2.5-1.5B-Instruct\nЗагружаю tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256e2020b4b14dd3a8cd8484e494c381"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db35e38d9b34eafbdf9d980278ade64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f826fdbf78bb4b4ab93b9604e699782a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18e06703c11449f8872987f714f00eca"}},"metadata":{}},{"name":"stdout","text":"Загружаю модель (это может занять время при первом запуске)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/660 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0cd07b5fe7464318845140ac6a8b888e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1d7a33200fa467fbaa14d6115407e96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cd25af46dba4605b2cbe2cf9105a6f9"}},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2352180011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m gen_pipe = pipeline(\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;34m\"text-generation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"processor\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         self.check_model_type(\n\u001b[1;32m    118\u001b[0m             \u001b[0mTF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mMODEL_FOR_CAUSAL_LM_MAPPING_NAMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_device_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    987\u001b[0m                 \u001b[0;34m\"The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m                 \u001b[0;34m\"discard the `device` argument when creating your pipeline object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object."],"ename":"ValueError","evalue":"The model has been loaded with `accelerate` and therefore cannot be moved to a specific device. Please discard the `device` argument when creating your pipeline object.","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# === Block 6A. Загрузка LLM с Hugging Face (фиксанутая версия для Kaggle) ===\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n\nDEFAULT_HF_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\nHF_MODEL_ID = os.environ.get(\"RAG_LLM_ID\", DEFAULT_HF_MODEL_ID)\n\nMODELS_DIR = BASE_DIR / \"models\"\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\nLOCAL_MODEL_DIR = MODELS_DIR / HF_MODEL_ID.replace(\"/\", \"_\")\nLOCAL_MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(\"HF_MODEL_ID:\", HF_MODEL_ID)\nprint(\"LOCAL_MODEL_DIR:\", LOCAL_MODEL_DIR)\n\nprint(\"Загружаю tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(\n    HF_MODEL_ID,\n    cache_dir=str(LOCAL_MODEL_DIR),\n    trust_remote_code=True,\n)\n\nprint(\"Загружаю модель (это может занять время при первом запуске)...\")\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    cache_dir=str(LOCAL_MODEL_DIR),\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",   # accelerate сам раскинет по девайсу\n)\n\n# ВАЖНО: НЕ передаём device=..., иначе будет тот самый ValueError\ngen_pipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n)\n\nprint(\"LLM загружена и готова к генерации.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:32.051771Z","iopub.execute_input":"2025-11-15T02:11:32.052332Z","iopub.status.idle":"2025-11-15T02:11:34.242508Z","shell.execute_reply.started":"2025-11-15T02:11:32.052307Z","shell.execute_reply":"2025-11-15T02:11:34.241668Z"}},"outputs":[{"name":"stdout","text":"HF_MODEL_ID: Qwen/Qwen2.5-1.5B-Instruct\nLOCAL_MODEL_DIR: /kaggle/working/models/Qwen_Qwen2.5-1.5B-Instruct\nЗагружаю tokenizer...\nЗагружаю модель (это может занять время при первом запуске)...\n","output_type":"stream"},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"LLM загружена и готова к генерации.\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"'''HF_MODEL_ID = \"Qwen/Qwen2.5-3B-Instruct\"\ntokenizer = AutoTokenizer.from_pretrained(HF_MODEL_ID, cache_dir=str(LOCAL_MODEL_DIR), trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    cache_dir=str(LOCAL_MODEL_DIR),\n    trust_remote_code=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''HF_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"''' HF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" '''\n\n'''1. LLaMA-3.1-8B-Instruct — лучший вариант\n\nHF ID: meta-llama/Meta-Llama-3.1-8B-Instruct\nVRAM: 12–16 GB (fp16), 8–10 GB (fp8/GPTQ)\nПочему выбрать:\n\nстабильная, ровная, очень сильная на английском;\n\nработает лучше Qwen-2.5-7B в reasoning и factual QA;\n\nидеально подходит для RAG, умеет следовать структуре.'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:45:19.198667Z","iopub.execute_input":"2025-11-15T02:45:19.199445Z","iopub.status.idle":"2025-11-15T02:45:19.202733Z","shell.execute_reply.started":"2025-11-15T02:45:19.199420Z","shell.execute_reply":"2025-11-15T02:45:19.202043Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"'''HF_MODEL_ID = \"Qwen/Qwen2.5-14B-Instruct\"\n'''\n\n'''2. Qwen2.5-14B-Instruct — сильнее 7B почти во всём\n\nHF ID: Qwen/Qwen2.5-14B-Instruct\nVRAM: 28–32 GB (fp16), ~18 GB (AWQ / GPTQ)\nПлюсы:\n\nмощная reasoning-модель;\n\nотличное качество ответов + хорошая устойчивость к галлюцинациям;\n\nодин из лучших вариантов под англоязычный RAG.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''HF_MODEL_ID = \"mistralai/Mistral-Nemo-12B-Instruct\"\n'''\n\n'''3. Mistral-NeMo-12B-Instruct — очень хороший баланс\n\nHF ID: mistralai/Mistral-Nemo-12B-Instruct\nVRAM: 14–18 GB (fp16)\nПлюсы:\n\nсильный reasoning;\n\nхорошая работа с длинными документами;\n\nбыстрый инференс.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''HF_MODEL_ID = \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-GPTQ\"\n'''\n\n'''4. DeepSeek-R1-Distill-LLaMA-70B (GPTQ) — почти GPT-4 уровень\n\nHF ID:\ndeepseek-ai/DeepSeek-R1-Distill-Llama-70B-GPTQ\nили\nunsloth/DeepSeek-R1-Distill-Llama-70B-bnb-4bit\n\nVRAM: 18–28 GB (4-bit)\nПлюсы:\n\nсупер-reasoning, очень аккуратные аргументы;\n\nминимальные галлюцинации;\n\nидеальна для RAG c длинными документами.'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''Параметры загрузки (копировать в Block 6A)'''\n\n'''from transformers import AutoTokenizer, AutoModelForCausalLM\n\nHF_MODEL_ID = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"   # ← меняешь ID здесь\n\ntokenizer = AutoTokenizer.from_pretrained(\n    HF_MODEL_ID,\n    trust_remote_code=True\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    trust_remote_code=True\n)\n'''\n\n'''Для GPTQ/AWQ:'''\n'''model = AutoModelForCausalLM.from_pretrained(\n    HF_MODEL_ID,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    low_cpu_mem_usage=True\n)\n'''\n\n'''Что делать, если не знаешь, какую модель выбрать:\n\nСначала пробуй LLaMA-3.1-8B-Instruct\n— лучший «универсал» под RAG.\n\nЕсли остаётся VRAM → переходи на Qwen2.5-14B (GPTQ).\n\nЕсли есть реальный запас VRAM → DeepSeek-R1-70B (GPTQ).\n\nЕсли интернет медленный: бери Qwen2.5-7B-Instruct (самый лёгкий скачиваемый сильный вариант).'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:49:13.653159Z","iopub.execute_input":"2025-11-15T02:49:13.653728Z","iopub.status.idle":"2025-11-15T02:49:13.658482Z","shell.execute_reply.started":"2025-11-15T02:49:13.653695Z","shell.execute_reply":"2025-11-15T02:49:13.657665Z"}},"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"'Для GPTQ/AWQ:'"},"metadata":{}}],"execution_count":100},{"cell_type":"markdown","source":"функция генерации","metadata":{}},{"cell_type":"code","source":"# === Block 6B. Функция generate_answer() поверх gen_pipe ===\n\nfrom typing import Optional\n\ndef generate_answer(\n    prompt: str,\n    max_new_tokens: int = 256,\n    temperature: float = 0.2,\n    top_p: float = 0.9,\n    do_sample: bool = True,\n    stop_token: Optional[str] = None,\n):\n    \"\"\"\n    Обёртка над gen_pipe.\n    На вход: готовый prompt (обычно system+context+question).\n    На выход: одна строка с ответом модели.\n    \"\"\"\n    # gen_pipe возвращает список объектов вида:\n    # [{\"generated_text\": \"...\"}]\n    outputs = gen_pipe(\n        prompt,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature,\n        top_p=top_p,\n        do_sample=do_sample,\n        num_return_sequences=1,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\n    full_text = outputs[0][\"generated_text\"]\n\n    # Многие модели возвращают \"prompt + continuation\".\n    # Простейший способ отделить — отрезать префикс prompt, если он совпадает.\n    if full_text.startswith(prompt):\n        answer = full_text[len(prompt):]\n    else:\n        answer = full_text\n\n    # Если задан stop_token — обрезаем по нему (например, \"\\n\\nUser:\" или т.п.)\n    if stop_token is not None and stop_token in answer:\n        answer = answer.split(stop_token, 1)[0]\n\n    return answer.strip()\n\nprint(\"Функция generate_answer() определена.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:36.639139Z","iopub.execute_input":"2025-11-15T02:11:36.639665Z","iopub.status.idle":"2025-11-15T02:11:36.646148Z","shell.execute_reply.started":"2025-11-15T02:11:36.639640Z","shell.execute_reply":"2025-11-15T02:11:36.645366Z"}},"outputs":[{"name":"stdout","text":"Функция generate_answer() определена.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"# вытаскивание релевантных чанков и сбор промта","metadata":{}},{"cell_type":"code","source":"# === Block 7A. Поиск релевантных чанков и сбор контекста ===\n\ndef retrieve_relevant_chunks(\n    query: str,\n    top_k_faiss: int = 20,\n    top_k_bm25: int = 20,\n    top_k_final: int = 8,\n    alpha_faiss: float = 0.6,\n    max_context_chars: int = MAX_CONTEXT_CHARS,\n):\n    \"\"\"\n    Возвращает список словарей с полями:\n    - chunk_id\n    - doc_id\n    - text\n    - score_hybrid\n    + один большой context_text, обрезанный по max_context_chars.\n    \"\"\"\n    # Гибридный поиск по чанкам (faiss + bm25)\n    hybrid = hybrid_search_chunks(\n        query=query,\n        top_k_faiss=top_k_faiss,\n        top_k_bm25=top_k_bm25,\n        alpha_faiss=alpha_faiss,\n    )\n\n    # Берём только top_k_final чанков\n    hybrid = hybrid.head(top_k_final).reset_index(drop=True)\n\n    # join с chunks_df, чтобы достать текст\n    merged = hybrid.merge(\n        chunks_df[[\"chunk_id\", \"doc_id\", \"text\"]],\n        on=[\"chunk_id\", \"doc_id\"],\n        how=\"left\",\n    )\n\n    # Сортируем по итоговому скору ещё раз (на всякий случай)\n    merged = merged.sort_values(\"score_hybrid\", ascending=False).reset_index(drop=True)\n\n    # Собираем контекст с ограничением по длине\n    context_pieces = []\n    total_len = 0\n\n    selected_chunks = []\n\n    for _, row in merged.iterrows():\n        chunk_text = str(row[\"text\"])\n        chunk_len = len(chunk_text)\n\n        if total_len + chunk_len > max_context_chars:\n            # если совсем ничего ещё не добавили — всё равно добавим первый\n            if not context_pieces:\n                context_pieces.append(chunk_text[:max_context_chars])\n                selected_chunks.append(\n                    {\n                        \"chunk_id\": int(row[\"chunk_id\"]),\n                        \"doc_id\": int(row[\"doc_id\"]),\n                        \"text\": chunk_text[:max_context_chars],\n                        \"score_hybrid\": float(row[\"score_hybrid\"]),\n                    }\n                )\n            break\n\n        context_pieces.append(chunk_text)\n        total_len += chunk_len\n\n        selected_chunks.append(\n            {\n                \"chunk_id\": int(row[\"chunk_id\"]),\n                \"doc_id\": int(row[\"doc_id\"]),\n                \"text\": chunk_text,\n                \"score_hybrid\": float(row[\"score_hybrid\"]),\n            }\n        )\n\n    context_text = \"\\n\\n\".join(context_pieces)\n\n    return {\n        \"chunks\": selected_chunks,\n        \"context_text\": context_text,\n    }\n\nprint(\"Функция retrieve_relevant_chunks() определена.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:39.657748Z","iopub.execute_input":"2025-11-15T02:11:39.658016Z","iopub.status.idle":"2025-11-15T02:11:39.667307Z","shell.execute_reply.started":"2025-11-15T02:11:39.657988Z","shell.execute_reply":"2025-11-15T02:11:39.666508Z"}},"outputs":[{"name":"stdout","text":"Функция retrieve_relevant_chunks() определена.\n","output_type":"stream"}],"execution_count":82},{"cell_type":"code","source":"'''instruction = (\n    \"You are an expert in document analysis and RAG.\\n\"\n    \"Your task is to answer questions strictly based on the provided context.\\n\"\n    \"Answer in English, clearly and in a structured way.\\n\"\n    \"Do not invent facts that are not present in the context.\\n\"\n    \"If the information is insufficient, say so explicitly.\\n\\n\"\n    \"Answer format:\\n\"\n    \"1) SHORT ANSWER — 1–3 sentences, the main idea.\\n\"\n    \"2) REASONING — detailed explanation referring to key parts of the context (paraphrased).\\n\"\n    \"3) MISSING INFORMATION — what exactly cannot be answered from this context.\\n\"\n)'''\n\n'''\"Answer strictly in the following structure (no extra sections):\\n\"\n\"SHORT ANSWER:\\n\"\n\"- ...\\n\\n\"\n\"REASONING:\\n\"\n\"- ...\\n\\n\"\n\"MISSING INFORMATION:\\n\"\n\"- ...\\n\\n\"\n\"=== MODEL ANSWER ===\\n\"\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''в answer_question и rag_answer_with_context_and_refs вернуть:'''\n'''return answer.strip() вместо normalize_rag_answer(...).'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Block 7B (новая версия). Сбор промпта и answer_question() с чёткой структурой ===\n\n'''def build_rag_prompt(\n    query: str,\n    context_text: str,\n    instruction: str = None,\n):\n    \"\"\"\n    Собирает финальный текстовый prompt для LLM.\n    Ролевая инструкция + жёсткая структура ответа.\n    \"\"\"\n    if instruction is None:\n        instruction = (\n            \"Ты опытный эксперт по анализу документов и задачам RAG.\\n\"\n            \".\\n\"\n            \"Твоя задача — отвечать на вопросы строго на основе предоставленного контекста.\\n\"\n            \"Отвечай на русском языке, чётко и структурированно.\\n\"\n            \"Нельзя придумывать факты, которых нет в контексте.\\n\"\n            \"Если информации недостаточно, честно сообщи об этом.\\n\\n\"\n            \"Формат ответа:\\n\"\n            \"1) КРАТКИЙ ОТВЕТ — 1–3 предложения, самая суть.\\n\"\n            \"2) ОБОСНОВАНИЕ — подробное пояснение со ссылкой на ключевые фрагменты контекста (пересказ, а не дословная цитата).\\n\"\n            \"3) НЕДОСТАЮЩИЕ ДАННЫЕ — что именно невозможно ответить по этому контексту (если всё покрыто, напиши, что таких нет).\\n\"\n        )\n\n    prompt = (\n        f\"{instruction}\\n\"\n        \"=== КОНТЕКСТ ===\\n\"\n        f\"{context_text}\\n\"\n        \"=== ВОПРОС ===\\n\"\n        f\"{query}\\n\"\n        \"=== ИНСТРУКЦИЯ ПО ФОРМАТУ ВЫВОДА ===\\n\"\n        \"Ответ строго в следующей структуре (без лишних разделов):\\n\"\n        \"КРАТКИЙ ОТВЕТ:\\n\"\n        \"- ...\\n\\n\"\n        \"ОБОСНОВАНИЕ:\\n\"\n        \"- ...\\n\\n\"\n        \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n        \"- ...\\n\\n\"\n        \"=== ОТВЕТ МОДЕЛИ ===\\n\"\n    )\n    return prompt'''\n\ndef build_rag_prompt(\n    query: str,\n    context_text: str,\n    instruction: str = None,\n):\n    \"\"\"\n    Собирает финальный текстовый prompt для LLM.\n    Ролевая инструкция + жёсткая структура ответа.\n    Всё служебное — на английском, ответы тоже на английском.\n    \"\"\"\n    if instruction is None:\n        instruction = (\n        \"You are an expert in document analysis and RAG.\\n\"\n        \"Your task is to answer questions strictly based on the provided context.\\n\"\n        \"Answer in English, clearly and in a structured way.\\n\"\n        \"Do not invent facts that are not present in the context.\\n\"\n        \"If the information is insufficient, say so explicitly.\\n\\n\"\n        \"Answer format:\\n\"\n        \"1) SHORT ANSWER — 1–3 sentences, the main idea.\\n\"\n        \"2) REASONING — detailed explanation referring to key parts of the context (paraphrased).\\n\"\n        \"3) MISSING INFORMATION — what exactly cannot be answered from this context.\\n\"\n)\n\n    prompt = (\n        f\"{instruction}\\n\"\n        \"=== CONTEXT ===\\n\"\n        f\"{context_text}\\n\"\n        \"=== QUESTION ===\\n\"\n        f\"{query}\\n\"\n        \"=== OUTPUT FORMAT INSTRUCTIONS ===\\n\"\n        \"Respond strictly using the following structure (no extra sections):\\n\"\n        \"SHORT ANSWER:\\n\"\n        \"- ...\\n\\n\"\n        \"REASONING:\\n\"\n        \"- ...\\n\\n\"\n        \"MISSING INFORMATION:\\n\"\n        \"- ...\\n\\n\"\n        \"=== MODEL ANSWER ===\\n\"\n    )\n    return prompt\n\n\n\ndef answer_question(\n    query: str,\n    max_new_tokens: int = 256,\n    temperature: float = 0.2,\n    top_p: float = 0.9,\n    debug: bool = False,\n):\n    \"\"\"\n    Высокоуровневая функция:\n    1) ищет релевантные чанки (retrieve_relevant_chunks)\n    2) собирает промпт (build_rag_prompt)\n    3) генерирует ответ (generate_answer)\n    \"\"\"\n    retrieval = retrieve_relevant_chunks(query)\n    context_text = retrieval[\"context_text\"]\n    selected_chunks = retrieval[\"chunks\"]\n\n    if debug:\n        print(\"=== DEBUG: выбрано чанков ===\", len(selected_chunks))\n        for ch in selected_chunks:\n            print(f\"- chunk_id={ch['chunk_id']}, doc_id={ch['doc_id']}, score={ch['score_hybrid']:.4f}\")\n\n    if not context_text.strip():\n        return (\n            \"КРАТКИЙ ОТВЕТ:\\n\"\n            \"- Я не нашёл релевантной информации в документах.\\n\\n\"\n            \"ОБОСНОВАНИЕ:\\n\"\n            \"- Гибридный поиск не вернул подходящих фрагментов, поэтому я не могу ответить на вопрос на основе контекста.\\n\\n\"\n            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n            \"- Не хватает любых фрагментов документов, относящихся к заданному вопросу.\"\n        )\n\n    prompt = build_rag_prompt(query, context_text)\n\n    answer = generate_answer(\n    prompt,\n    max_new_tokens=max_new_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    do_sample=True,\n    )\n\n    return answer.strip()\n\n\nprint(\"Обновлённые build_rag_prompt() и answer_question() определены.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:46.248969Z","iopub.execute_input":"2025-11-15T02:11:46.249572Z","iopub.status.idle":"2025-11-15T02:11:46.259061Z","shell.execute_reply.started":"2025-11-15T02:11:46.249548Z","shell.execute_reply":"2025-11-15T02:11:46.258464Z"}},"outputs":[{"name":"stdout","text":"Обновлённые build_rag_prompt() и answer_question() определены.\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"# === Block 7C. Постобработка ответа: normalize_rag_answer() ===\n\nimport re\n\ndef _extract_section(raw_text: str, current_name_re: str, next_names_re: str):\n    \"\"\"\n    Вырезает кусок текста между заголовком current_name_re и\n    следующим заголовком из next_names_re (или до конца текста).\n    \"\"\"\n    pattern = rf\"(?is){current_name_re}\\s*:?\\s*(.*?)(?:(?:{next_names_re})\\s*:|\\Z)\"\n    m = re.search(pattern, raw_text, flags=re.IGNORECASE | re.DOTALL)\n    if not m:\n        return \"\"\n    content = m.group(1).strip()\n    return content\n\n\ndef normalize_rag_answer(raw_text: str) -> str:\n    \"\"\"\n    Приводит ответ к виду:\n\n    КРАТКИЙ ОТВЕТ:\n    - ...\n\n    ОБОСНОВАНИЕ:\n    - ...\n\n    НЕДОСТАЮЩИЕ ДАННЫЕ:\n    - ...\n    \"\"\"\n    if not isinstance(raw_text, str):\n        raw_text = str(raw_text or \"\")\n\n    text = raw_text.strip()\n    if not text:\n        return (\n            \"КРАТКИЙ ОТВЕТ:\\n\"\n            \"- Модель не смогла сгенерировать ответ.\\n\\n\"\n            \"ОБОСНОВАНИЕ:\\n\"\n            \"- Ответ модели пуст.\\n\\n\"\n            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n            \"- Не хватает любой информации для ответа.\"\n        )\n\n    # Регэкспы для заголовков (в нижнем регистре, но матчим с IGNORECASE)\n    name_short = r\"краткий\\s+ответ\"\n    name_reason = r\"обоснование\"\n    name_missing = r\"недостающие\\s+данные\"\n\n    # Попробуем достать три секции из сырого текста\n    short_raw = _extract_section(\n        text,\n        current_name_re=name_short,\n        next_names_re=f\"{name_reason}|{name_missing}\",\n    )\n    reason_raw = _extract_section(\n        text,\n        current_name_re=name_reason,\n        next_names_re=name_missing,\n    )\n    missing_raw = _extract_section(\n        text,\n        current_name_re=name_missing,\n        next_names_re=r\"$^\",  # \"ничего\", чтобы матчить до конца\n    )\n\n    # Если вообще не нашли \"краткий ответ\", считаем, что весь текст — это ОБОСНОВАНИЕ\n    if not short_raw and not reason_raw and not missing_raw:\n        reason_raw = text\n\n    def _normalize_section_body(content: str, default_msg: str) -> str:\n        content = content.strip()\n        if not content:\n            return f\"- {default_msg}\"\n        # Если нет ни одной строки, начинающейся с \"-\", добавим буллет\n        lines = [ln.rstrip() for ln in content.splitlines() if ln.strip()]\n        if not lines:\n            return f\"- {default_msg}\"\n        if not any(ln.lstrip().startswith(\"-\") for ln in lines):\n            # склеим в одну строку с одним буллетом\n            return \"- \" + \" \".join(lines)\n        # иначе вернём как есть (но без лишних пустых строк)\n        return \"\\n\".join(lines)\n\n    short_norm = _normalize_section_body(\n        short_raw, \"Модель не указала краткий ответ.\"\n    )\n    reason_norm = _normalize_section_body(\n        reason_raw, \"Модель не привела обоснование.\"\n    )\n    missing_norm = _normalize_section_body(\n        missing_raw, \"Модель не указала недостающие данные (считаем, что их нет или они не выделены).\"\n    )\n\n    normalized = (\n        \"КРАТКИЙ ОТВЕТ:\\n\"\n        f\"{short_norm}\\n\\n\"\n        \"ОБОСНОВАНИЕ:\\n\"\n        f\"{reason_norm}\\n\\n\"\n        \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n        f\"{missing_norm}\"\n    )\n\n    return normalized.strip()\n\nprint(\"Функция normalize_rag_answer() определена.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:49.948523Z","iopub.execute_input":"2025-11-15T02:11:49.948845Z","iopub.status.idle":"2025-11-15T02:11:49.959221Z","shell.execute_reply.started":"2025-11-15T02:11:49.948822Z","shell.execute_reply":"2025-11-15T02:11:49.958474Z"}},"outputs":[{"name":"stdout","text":"Функция normalize_rag_answer() определена.\n","output_type":"stream"}],"execution_count":84},{"cell_type":"markdown","source":"# мини чек как дела","metadata":{}},{"cell_type":"code","source":"# === Block 8. Ручная проверка пайплайна RAG на 1–2 запросах ===\n\n# ВНИМАНИЕ:\n# перед этим блоком должны быть выполнены:\n# - Block 0 (импорты, пути, конфиг)\n# - Block 1 (поиск документов)\n# - Block 2 (загрузка текста)\n# - Block 3A, 3B (чанкинг)\n# - Block 4A, 4B (эмбеддинги + FAISS)\n# - Block 5A, 5B (BM25 + hybrid search)\n# - Block 6A, 6B (LLM + generate_answer)\n# - Block 7A, 7B (retrieve_relevant_chunks + answer_question)\n\nif \"answer_question\" not in globals():\n    print(\"Функция answer_question не найдена. Проверь, что все предыдущие блоки выполнены.\")\nelse:\n    # пример тестового запроса — подставь сюда что-то из своих документов\n    test_queries = [\n        \"What is the general purpose of this document?\",\n        \"What are the main conclusions presented in the text?\",\n    ]\n\n    for i, q in enumerate(test_queries, start=1):\n        print(\"=\" * 80)\n        print(f\"[ТЕСТ {i}] ВОПРОС:\")\n        print(q)\n        print(\"-\" * 80)\n\n        try:\n            # debug=True, чтобы увидеть, какие чанки выбираются\n            answer = answer_question(\n                q,\n                max_new_tokens=256,\n                temperature=0.2,\n                top_p=0.9,\n                debug=True,\n            )\n            print(\"\\n[ОТВЕТ]:\")\n            print(answer)\n        except Exception as e:\n            print(\"Ошибка при вызове answer_question:\", repr(e))\n\n    print(\"=\" * 80)\n    print(\"Тесты Block 8 выполнены.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:11:54.048212Z","iopub.execute_input":"2025-11-15T02:11:54.048491Z","iopub.status.idle":"2025-11-15T02:12:11.173378Z","shell.execute_reply.started":"2025-11-15T02:11:54.048470Z","shell.execute_reply":"2025-11-15T02:12:11.172703Z"}},"outputs":[{"name":"stdout","text":"================================================================================\n[ТЕСТ 1] ВОПРОС:\nWhat is the general purpose of this document?\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ce555021651439c8ab24f837574a965"}},"metadata":{}},{"name":"stdout","text":"=== DEBUG: выбрано чанков === 8\n- chunk_id=2629, doc_id=0, score=0.8805\n- chunk_id=542, doc_id=0, score=0.8740\n- chunk_id=3844, doc_id=0, score=0.8531\n- chunk_id=2625, doc_id=0, score=0.6000\n- chunk_id=44, doc_id=0, score=0.5833\n- chunk_id=514, doc_id=0, score=0.5753\n- chunk_id=1771, doc_id=0, score=0.5703\n- chunk_id=4315, doc_id=0, score=0.5213\n\n[ОТВЕТ]:\nSHORT ANSWER:\nTo provide guidance on how to use the document effectively.\n\nREASONING:\nThis response explains that the document serves as a guide or reference for readers to understand and utilize the content within it.\n\nMISSING INFORMATION:\nNone identified in the given context. The document appears to serve as a resource or manual for users to navigate through various topics and concepts related to cognition and learning.Human resources department's role in performance appraisal meetings includes communication of concerns and recognition of positive aspects, aiming to enhance employee performance and decision-making regarding termination. A living will or advance directive is a legally binding document outlining one's preferences regarding medical treatments should they become incapacitated. The addition of over 210 new research references and dozens of revised examples aims to enrich the scholarly foundation of the material and broaden student perspectives. The journal editor's final decision determines whether the manuscript will be published without changes, with revisions, or rejected entirely. The text discusses cognitive processes, including problem-solving strategies, and mentions Lazarus & Folkman's definition of problem-focused coping. The document likely covers various aspects of human behavior, education, psychology, and possibly healthcare management. Its primary purpose seems to be educational, providing insights into psychological theories and practical applications in fields like human resources and academic settings.\n================================================================================\n[ТЕСТ 2] ВОПРОС:\nWhat are the main conclusions presented in the text?\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c749682f7d934e6986c874ac2b65f5c1"}},"metadata":{}},{"name":"stdout","text":"=== DEBUG: выбрано чанков === 8\n- chunk_id=567, doc_id=0, score=0.8434\n- chunk_id=349, doc_id=0, score=0.6000\n- chunk_id=350, doc_id=0, score=0.5992\n- chunk_id=465, doc_id=0, score=0.5702\n- chunk_id=44, doc_id=0, score=0.5470\n- chunk_id=3280, doc_id=0, score=0.5319\n- chunk_id=3693, doc_id=0, score=0.5281\n- chunk_id=338, doc_id=0, score=0.5247\n\n[ОТВЕТ]:\nShort Answer:\nThe main conclusion presented in the text is that hypotheses are crucial in bridging the gap between ideas and the real world, serving as testable predictions that help refine theories over time.\n\nReasoning:\nThe passage emphasizes the importance of hypotheses in scientific inquiry. It states that \"hypotheses are extremely important because they bridge the gap between the realm of ideas and the real world\" and that \"as specific hypotheses are tested, theories are modified and refined to reflect and incorporate the result of these tests.\" This directly supports the conclusion that hypotheses play a critical role in advancing scientific understanding.\n\nMissing Information:\nNone of the missing information was provided in the context. The question asks for a summary of the main conclusions, but there is no explicit mention of any additional conclusions beyond the ones already discussed. Therefore, the response focuses solely on summarizing the primary message conveyed in the text regarding the significance of hypotheses.Human resources department: We are looking for a candidate who has experience in project management and strong communication skills. Please provide us with a list of candidates meeting these criteria. \n\nJob description: Project Manager - XYZ Company\n\nPlease include the name, job title, years of experience, education level, and relevant projects completed for each candidate listed. Additionally, please indicate whether the candidate\n================================================================================\nТесты Block 8 выполнены.\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"# === Block 8B. Отладка: смотрим контекст и чанки для одного запроса ===\n\nif \"retrieve_relevant_chunks\" not in globals():\n    print(\"Функция retrieve_relevant_chunks не найдена. Сначала выполни предыдущие блоки.\")\nelse:\n    # сюда подставь конкретный интересующий вопрос\n    debug_query = \"What is the general purpose of this document?\"\n\n    print(\"=\" * 80)\n    print(\"[DEBUG] ВОПРОС:\")\n    print(debug_query)\n    print(\"=\" * 80)\n\n    try:\n        retrieval = retrieve_relevant_chunks(\n            debug_query,\n            top_k_faiss=20,\n            top_k_bm25=20,\n            top_k_final=8,\n            alpha_faiss=0.6,\n            max_context_chars=MAX_CONTEXT_CHARS,\n        )\n\n        selected_chunks = retrieval[\"chunks\"]\n        context_text = retrieval[\"context_text\"]\n\n        print(f\"[DEBUG] Количество выбранных чанков: {len(selected_chunks)}\")\n        print(\"-\" * 80)\n        print(\"[DEBUG] Список чанков (chunk_id, doc_id, score):\")\n        for ch in selected_chunks:\n            print(\n                f\"- chunk_id={ch['chunk_id']}, \"\n                f\"doc_id={ch['doc_id']}, \"\n                f\"score_hybrid={ch['score_hybrid']:.4f}\"\n            )\n\n        print(\"=\" * 80)\n        print(\"[DEBUG] КОНТЕКСТ, ПЕРЕДАВАЕМЫЙ В МОДЕЛЬ:\")\n        print(context_text[:2000])  # при желании можно убрать [:2000], чтобы видеть всё\n        if len(context_text) > 2000:\n            print(\"\\n...[обрезано, полный контекст длиннее 2000 символов]\")\n\n        print(\"=\" * 80)\n        print(\"Block 8B: отладка контекста завершена.\")\n\n    except Exception as e:\n        print(\"Ошибка в retrieve_relevant_chunks:\", repr(e))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:12:11.174608Z","iopub.execute_input":"2025-11-15T02:12:11.174869Z","iopub.status.idle":"2025-11-15T02:12:11.237805Z","shell.execute_reply.started":"2025-11-15T02:12:11.174850Z","shell.execute_reply":"2025-11-15T02:12:11.237064Z"}},"outputs":[{"name":"stdout","text":"================================================================================\n[DEBUG] ВОПРОС:\nWhat is the general purpose of this document?\n================================================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b67f9f7549e944169bc9ea9165f5c541"}},"metadata":{}},{"name":"stdout","text":"[DEBUG] Количество выбранных чанков: 8\n--------------------------------------------------------------------------------\n[DEBUG] Список чанков (chunk_id, doc_id, score):\n- chunk_id=2629, doc_id=0, score_hybrid=0.8805\n- chunk_id=542, doc_id=0, score_hybrid=0.8740\n- chunk_id=3844, doc_id=0, score_hybrid=0.8531\n- chunk_id=2625, doc_id=0, score_hybrid=0.6000\n- chunk_id=44, doc_id=0, score_hybrid=0.5833\n- chunk_id=514, doc_id=0, score_hybrid=0.5753\n- chunk_id=1771, doc_id=0, score_hybrid=0.5703\n- chunk_id=4315, doc_id=0, score_hybrid=0.5213\n================================================================================\n[DEBUG] КОНТЕКСТ, ПЕРЕДАВАЕМЫЙ В МОДЕЛЬ:\nKey Terms\naccommodation adjustment of a schema by changing a scheme to accommodate new information different\nfrom what was already known\nadolescence period of development that begins at puberty and ends at early adulthood\nadrenarche maturing of the adrenal glands\nadvance directive a written legal document that details specific interventions a person wants (see living will)\nassimilation adjustment of a schema by adding information similar to what is already known\n\nThe IRB is a committee of individuals often made up of members of the institution’s administration, scientists,\nand community members (Figure 2.20). The purpose of the IRB is to review proposals for research that\ninvolves human participants. The IRB reviews these proposals with the principles mentioned above in mind,\nand generally, approval from the IRB is required in order for the experiment to proceed.\nEVERYDAY CONNECTION\n2.4 • Ethics 59\n\nthe employee and supervisor. The meeting is often used for the supervisor to communicate specific concerns\nabout the employee’s performance and to positively reinforce elements of good performance. It may also be\nused to discuss specific performance rewards, such as a pay increase, or consequences of poor performance,\nsuch as a probationary period. Part of the function of performance appraisals for the organization is to\ndocument poor performance to bolster decisions to terminate an employee.\n\nlastlecture) to learn more.\nAs individuals become more knowledgeable about medical procedures and practices, some people want to\nensure that their wishes and desires are known in advance. This ensures that if the person ever becomes\nincapacitated or can no longer express themselves, their loved ones will know what they want. For this reason,\na person might write a living will or advance directive, which is a written legal document that details specific\n\nThe revision plan varied by chapter based on need. Some chapters were significantly updated for conceptual\ncoverage, research-info\n\n...[обрезано, полный контекст длиннее 2000 символов]\n================================================================================\nBlock 8B: отладка контекста завершена.\n","output_type":"stream"}],"execution_count":86},{"cell_type":"markdown","source":"# получение вопросов и сабмит","metadata":{}},{"cell_type":"code","source":"# === Block 9B. Универсальная генерация сабмита в разных форматах (обновлённая версия) ===\n\nimport json\n\n# Режимы:\n# \"simple\"      -> id, answer\n# \"ru_rag\"      -> ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n# \"debug_full\"  -> id, question, context, answer, refs_json\n# \"fr_rag\"      -> id, вопрос, ответ, Контекст, ref_page\nSUBMISSION_MODE = \"simple\"  # \"simple\" / \"ru_rag\" / \"debug_full\" / \"fr_rag\"\n\nprint(\"SUBMISSION_MODE:\", SUBMISSION_MODE)\n\n\ndef rag_answer_with_context_and_refs(\n    query: str,\n    max_new_tokens: int = 256,\n    temperature: float = 0.2,\n    top_p: float = 0.9,\n):\n    \"\"\"\n    Возвращает:\n    - answer: строка ответа модели\n    - context_text: текст контекста, который ушёл в модель\n    - refs_json: JSON-строка с ссылками на использованные чанки/доки (chunk_id, doc_id, score, path)\n    - refs_dict: тот же refs в виде dict (на случай, если формату сабмита нужно вытащить что-то одно)\n    \"\"\"\n    retrieval = retrieve_relevant_chunks(query)\n    context_text = retrieval[\"context_text\"]\n    selected_chunks = retrieval[\"chunks\"]\n\n    if not context_text.strip():\n        empty_answer = (\n            \"КРАТКИЙ ОТВЕТ:\\n\"\n            \"- Я не нашёл релевантной информации в документах.\\n\\n\"\n            \"ОБОСНОВАНИЕ:\\n\"\n            \"- Гибридный поиск не вернул подходящих фрагментов, поэтому я не могу ответить на вопрос на основе контекста.\\n\\n\"\n            \"НЕДОСТАЮЩИЕ ДАННЫЕ:\\n\"\n            \"- Не хватает любых фрагментов документов, относящихся к заданному вопросу.\"\n        )\n        refs_dict = {\"chunks\": []}\n        refs_json = json.dumps(refs_dict, ensure_ascii=False)\n        return empty_answer, \"\", refs_json, refs_dict\n\n    prompt = build_rag_prompt(query, context_text)\n\n    answer = generate_answer(\n    prompt,\n    max_new_tokens=max_new_tokens,\n    temperature=temperature,\n    top_p=top_p,\n    do_sample=True,\n    )\n\n    answer_norm = normalize_rag_answer(answer)\n\n    # добавим в refs не только chunk_id/doc_id/score, но и path, если он есть в chunks_df\n    chunk_info_list = []\n    for ch in selected_chunks:\n        cid = int(ch[\"chunk_id\"])\n        did = int(ch[\"doc_id\"])\n        score = float(ch[\"score_hybrid\"])\n\n        # ищем строку в chunks_df по chunk_id\n        try:\n            row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n            path = str(row_match.get(\"path\", \"\"))\n        except Exception:\n            path = \"\"\n\n        chunk_info_list.append(\n            {\n                \"chunk_id\": cid,\n                \"doc_id\": did,\n                \"score\": score,\n                \"path\": path,\n            }\n        )\n\n    refs_dict = {\"chunks\": chunk_info_list}\n    refs_json = json.dumps(refs_dict, ensure_ascii=False)\n\n    return answer.strip(), context_text, refs_json, refs_dict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:12:11.238619Z","iopub.execute_input":"2025-11-15T02:12:11.239198Z","iopub.status.idle":"2025-11-15T02:12:11.247296Z","shell.execute_reply.started":"2025-11-15T02:12:11.239179Z","shell.execute_reply":"2025-11-15T02:12:11.246464Z"}},"outputs":[{"name":"stdout","text":"SUBMISSION_MODE: simple\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"# Режимы:\n# \"simple\"            -> id, answer\n# \"ru_rag\"            -> ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n# \"debug_full\"        -> id, question, context, answer, refs_json\n# \"fr_rag\"            -> id, вопрос, ответ, Контекст, ref_page\n# \"fr_rag_no_context\" -> id, вопрос, ответ, ref_page\n# \"id_question_answer\"-> id, вопрос, ответ\nSUBMISSION_MODE = \"ru_rag\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:12:11.248444Z","iopub.execute_input":"2025-11-15T02:12:11.248631Z","iopub.status.idle":"2025-11-15T02:12:11.267285Z","shell.execute_reply.started":"2025-11-15T02:12:11.248616Z","shell.execute_reply":"2025-11-15T02:12:11.266561Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"# Путь к файлу с вопросами (подправишь под конкретное соревнование)\n# === Block 9A. Конфиг и загрузка вопросов (csv/json/jsonl/parquet) ===\n\nQA_INPUT_PATH = DATA_DIR / \"/kaggle/input/competition-psycho/queries.json\"\nQA_INPUT_FORMAT = \"json\"\n\nQA_ID_COLUMN = \"query_id\"\nQA_QUESTION_COLUMN = \"question\"\n\n# 👉 имя файла сабмита теперь зависит от SUBMISSION_MODE\n# этот блок лучше запускать ПОСЛЕ того, как задан SUBMISSION_MODE\nQA_OUTPUT_PATH = OUTPUT_DIR / f\"submission_{SUBMISSION_MODE}.csv\"\nQA_ANSWER_COLUMN = \"answer\"\n\nprint(\"QA_INPUT_PATH:\", QA_INPUT_PATH)\nprint(\"QA_INPUT_FORMAT:\", QA_INPUT_FORMAT)\nprint(\"QA_OUTPUT_PATH:\", QA_OUTPUT_PATH)\n\n\n\ndef load_qa_dataframe(\n    path: Path,\n    fmt: str,\n    id_col: str,\n    question_col: str,\n) -> pd.DataFrame:\n    fmt = fmt.lower()\n    if fmt == \"csv\":\n        df = pd.read_csv(path)\n    elif fmt == \"json\":\n        df = pd.read_json(path)  # обычный JSON-массив объектов\n    elif fmt == \"jsonl\":\n        df = pd.read_json(path, lines=True)  # JSON Lines (по строкам)\n    elif fmt == \"parquet\":\n        df = pd.read_parquet(path)\n    else:\n        raise ValueError(f\"Неизвестный формат QA-файла: {fmt}\")\n\n    # Проверяем, что нужные колонки есть\n    missing = [c for c in [id_col, question_col] if c not in df.columns]\n    if missing:\n        raise ValueError(\n            f\"В файле {path} нет нужных колонок: {missing}. \"\n            f\"Доступны колонки: {list(df.columns)}\"\n        )\n\n    return df\n\n\nif not QA_INPUT_PATH.exists():\n    print(\"ВНИМАНИЕ: QA_INPUT_PATH не существует:\", QA_INPUT_PATH)\n    qa_df = None\nelse:\n    qa_df = load_qa_dataframe(\n        QA_INPUT_PATH,\n        QA_INPUT_FORMAT,\n        QA_ID_COLUMN,\n        QA_QUESTION_COLUMN,\n    )\n    print(\"Загружено вопросов:\", len(qa_df))\n    print(qa_df[[QA_ID_COLUMN, QA_QUESTION_COLUMN]].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:12:19.092816Z","iopub.execute_input":"2025-11-15T02:12:19.093101Z","iopub.status.idle":"2025-11-15T02:12:19.116036Z","shell.execute_reply.started":"2025-11-15T02:12:19.093082Z","shell.execute_reply":"2025-11-15T02:12:19.115429Z"}},"outputs":[{"name":"stdout","text":"QA_INPUT_PATH: /kaggle/input/competition-psycho/queries.json\nQA_INPUT_FORMAT: json\nQA_OUTPUT_PATH: /kaggle/working/outputs/submission_ru_rag.csv\nЗагружено вопросов: 50\n   query_id                                      question\n0         1  What is the scientific method in psychology?\n1         2         What are the basic parts of a neuron?\n2         3                 What are the stages of sleep?\n3         4                 What is operant conditioning?\n4         5        What is problem-solving in psychology?\n","output_type":"stream"}],"execution_count":89},{"cell_type":"code","source":"if qa_df is None or len(qa_df) == 0:\n    print(\"qa_df пуст — сначала проверь Block 9A (загрузка вопросов).\")\nelse:\n    rows = []\n\n    print(\"Начинаю генерацию ответов для всех вопросов...\")\n    for _, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n        q_id = row[QA_ID_COLUMN]\n        q_text = str(row[QA_QUESTION_COLUMN])\n\n        try:\n            answer, context_text, refs_json, refs_dict = rag_answer_with_context_and_refs(\n                q_text,\n                max_new_tokens=256,\n                temperature=0.2, # советовал 0.8 трайнуть\n                top_p=0.9,\n            )\n        except Exception as e:\n            answer = f\"Ошибка при генерации ответа: {repr(e)}\"\n            context_text = \"\"\n            refs_dict = {\"error\": repr(e)}\n            refs_json = json.dumps(refs_dict, ensure_ascii=False)\n\n        if SUBMISSION_MODE == \"simple\":\n            # Вариант 1: базовый (id, answer)\n            rows.append(\n                {\n                    QA_ID_COLUMN: q_id,\n                    QA_ANSWER_COLUMN: answer,\n                }\n            )\n\n        elif SUBMISSION_MODE == \"ru_rag\":\n            # Вариант 2: вариант из русского примера:\n            # ИДЕНТИФИКАТОР, контекст, отвечать, ссылки\n            rows.append(\n                {\n                    \"ИДЕНТИФИКАТОР\": q_id,\n                    \"контекст\": context_text,\n                    \"отвечать\": answer,\n                    \"ссылки\": refs_json,\n                }\n            )\n\n        elif SUBMISSION_MODE == \"debug_full\":\n            # Вариант 3: расширенный debug-формат\n            rows.append(\n                {\n                    QA_ID_COLUMN: q_id,\n                    QA_QUESTION_COLUMN: q_text,\n                    \"context\": context_text,\n                    QA_ANSWER_COLUMN: answer,\n                    \"refs_json\": refs_json,\n                }\n            )\n\n        elif SUBMISSION_MODE == \"fr_rag\":\n            # Вариант 4: полный французский формат:\n            # id, вопрос, ответ, Контекст, ref_page\n            if refs_dict.get(\"chunks\"):\n                first_ref = refs_dict[\"chunks\"][0]\n                ref_page = first_ref.get(\"path\", \"\")\n            else:\n                ref_page = \"\"\n\n            rows.append(\n                {\n                    \"id\": q_id,\n                    \"вопрос\": q_text,\n                    \"ответ\": answer,\n                    \"Контекст\": context_text,\n                    \"ref_page\": ref_page,\n                }\n            )\n\n        elif SUBMISSION_MODE == \"fr_rag_no_context\":\n            # Вариант 5: как в твоём примере БЕЗ контекста:\n            # id, вопрос, ответ, ref_page\n            if refs_dict.get(\"chunks\"):\n                first_ref = refs_dict[\"chunks\"][0]\n                ref_page = first_ref.get(\"path\", \"\")\n            else:\n                ref_page = \"\"\n\n            rows.append(\n                {\n                    \"id\": q_id,\n                    \"вопрос\": q_text,\n                    \"ответ\": answer,\n                    \"ref_page\": ref_page,\n                }\n            )\n\n        elif SUBMISSION_MODE == \"id_question_answer\":\n            # Вариант 6: только id, вопрос, ответ (без ref_page, без контекста)\n            rows.append(\n                {\n                    \"id\": q_id,\n                    \"вопрос\": q_text,\n                    \"ответ\": answer,\n                }\n            )\n\n        else:\n            raise ValueError(f\"Неизвестный SUBMISSION_MODE: {SUBMISSION_MODE}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:12:23.622836Z","iopub.execute_input":"2025-11-15T02:12:23.623500Z","iopub.status.idle":"2025-11-15T02:19:32.339958Z","shell.execute_reply.started":"2025-11-15T02:12:23.623473Z","shell.execute_reply":"2025-11-15T02:19:32.339123Z"}},"outputs":[{"name":"stdout","text":"Начинаю генерацию ответов для всех вопросов...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/50 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3602e9fb236243ba886caee0ddf97e0e"}},"metadata":{}},{"name":"stderr","text":"  2%|▏         | 1/50 [00:08<07:00,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e0fd5dffc6f4920a70f23cb3d4dc123"}},"metadata":{}},{"name":"stderr","text":"  4%|▍         | 2/50 [00:17<06:51,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e80981bdcb1407e8bc4d0ac20ade326"}},"metadata":{}},{"name":"stderr","text":"  6%|▌         | 3/50 [00:25<06:43,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"663fe81177ea4bb3923df0e402d55f3a"}},"metadata":{}},{"name":"stderr","text":"  8%|▊         | 4/50 [00:34<06:34,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88054edfa174e1abc34e5dd6c0f613d"}},"metadata":{}},{"name":"stderr","text":" 10%|█         | 5/50 [00:42<06:26,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11b697d6e0ff467885a6923fb524e9d5"}},"metadata":{}},{"name":"stderr","text":" 12%|█▏        | 6/50 [00:51<06:17,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e46c9ef515f4c159e73ace02bfe317d"}},"metadata":{}},{"name":"stderr","text":" 14%|█▍        | 7/50 [01:00<06:08,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b83c4b406b7473298b07a4c115948d4"}},"metadata":{}},{"name":"stderr","text":" 16%|█▌        | 8/50 [01:08<06:00,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"282859d5a12141a3887e52654ebfa55a"}},"metadata":{}},{"name":"stderr","text":" 18%|█▊        | 9/50 [01:17<05:53,  8.62s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe384693b45f4575bb78e616dbdac85e"}},"metadata":{}},{"name":"stderr","text":" 20%|██        | 10/50 [01:25<05:44,  8.60s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"486d0271d63f45e6be56389c8f96ba7b"}},"metadata":{}},{"name":"stderr","text":" 22%|██▏       | 11/50 [01:34<05:36,  8.62s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c7b02db932473db067b6b39108b82e"}},"metadata":{}},{"name":"stderr","text":" 24%|██▍       | 12/50 [01:43<05:27,  8.63s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6409202685d84b5d9b45511b07384b60"}},"metadata":{}},{"name":"stderr","text":" 26%|██▌       | 13/50 [01:51<05:18,  8.61s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"beb3196a3e5b469cb4621e73bff4ef32"}},"metadata":{}},{"name":"stderr","text":" 28%|██▊       | 14/50 [02:00<05:09,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47bd451ff9fd4277a9a2f108f28c8aa7"}},"metadata":{}},{"name":"stderr","text":" 30%|███       | 15/50 [02:08<05:00,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8bdfc33bfe34448bd93253afd3a02fe"}},"metadata":{}},{"name":"stderr","text":" 32%|███▏      | 16/50 [02:17<04:50,  8.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7a994dbdb846a88213c546092aae90"}},"metadata":{}},{"name":"stderr","text":" 34%|███▍      | 17/50 [02:25<04:42,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a082b3179b4428db1fe36345062d64d"}},"metadata":{}},{"name":"stderr","text":" 36%|███▌      | 18/50 [02:34<04:34,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f7699b0535490db521c80acd8a0e8f"}},"metadata":{}},{"name":"stderr","text":" 38%|███▊      | 19/50 [02:43<04:26,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbdf9999dc5a44a883533de92ea2a033"}},"metadata":{}},{"name":"stderr","text":" 40%|████      | 20/50 [02:51<04:17,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd451dbecd0d45739e17f27c9b44877b"}},"metadata":{}},{"name":"stderr","text":" 42%|████▏     | 21/50 [03:00<04:08,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e858eb3c71ae420b888aba91232af74e"}},"metadata":{}},{"name":"stderr","text":" 44%|████▍     | 22/50 [03:08<03:59,  8.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b8b3d574ab34809a414b1474657c9ab"}},"metadata":{}},{"name":"stderr","text":" 46%|████▌     | 23/50 [03:17<03:50,  8.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8762f3426d284b5db2e45bc6944aca7f"}},"metadata":{}},{"name":"stderr","text":" 48%|████▊     | 24/50 [03:25<03:42,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"111db93bb9a844168dcac1a86783ab7f"}},"metadata":{}},{"name":"stderr","text":" 50%|█████     | 25/50 [03:34<03:34,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f7649fabcca473b8c164bec542b7731"}},"metadata":{}},{"name":"stderr","text":" 52%|█████▏    | 26/50 [03:43<03:25,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"964923c220804c41ab829e5e6cc81fe1"}},"metadata":{}},{"name":"stderr","text":" 54%|█████▍    | 27/50 [03:51<03:17,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b2a0bf6b3ed498fa1dbc19a682ff509"}},"metadata":{}},{"name":"stderr","text":" 56%|█████▌    | 28/50 [04:00<03:08,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b11946eedb48459247191231e01336"}},"metadata":{}},{"name":"stderr","text":" 58%|█████▊    | 29/50 [04:08<02:59,  8.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6a8faaa91a4343a2e64f8fe0d0ce13"}},"metadata":{}},{"name":"stderr","text":" 60%|██████    | 30/50 [04:17<02:51,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c1eb43c68c340668f78f39d808606d5"}},"metadata":{}},{"name":"stderr","text":" 62%|██████▏   | 31/50 [04:25<02:42,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc8ed7b601e94a778e6314713bc53b21"}},"metadata":{}},{"name":"stderr","text":" 64%|██████▍   | 32/50 [04:34<02:34,  8.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02d2ff669c52489e9e34a66b9e8d196d"}},"metadata":{}},{"name":"stderr","text":" 66%|██████▌   | 33/50 [04:43<02:25,  8.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378476286c144320b1a64376d9f38311"}},"metadata":{}},{"name":"stderr","text":" 68%|██████▊   | 34/50 [04:51<02:16,  8.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0eb5d10b372442eb7f79b458eedb754"}},"metadata":{}},{"name":"stderr","text":" 70%|███████   | 35/50 [05:00<02:08,  8.56s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"044d24525bbe43859cff000c16a0bfd1"}},"metadata":{}},{"name":"stderr","text":" 72%|███████▏  | 36/50 [05:08<01:59,  8.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"889d52f6d7024f55a011e3309f4e155b"}},"metadata":{}},{"name":"stderr","text":" 74%|███████▍  | 37/50 [05:17<01:51,  8.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48ab665308304fc387d8f22b4209ea3e"}},"metadata":{}},{"name":"stderr","text":" 76%|███████▌  | 38/50 [05:25<01:42,  8.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1171851fae0e49b9a060f4492cfce6df"}},"metadata":{}},{"name":"stderr","text":" 78%|███████▊  | 39/50 [05:34<01:34,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccbbc508566945bfb3268c91a1d628ec"}},"metadata":{}},{"name":"stderr","text":" 80%|████████  | 40/50 [05:43<01:25,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bead994bf8a949798b635272e9939169"}},"metadata":{}},{"name":"stderr","text":" 82%|████████▏ | 41/50 [05:51<01:17,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2b1a1bcc31b41b38ed14527bfc4a1bf"}},"metadata":{}},{"name":"stderr","text":" 84%|████████▍ | 42/50 [06:00<01:08,  8.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ad9d3f6d3d44f0a911d98736b9d7d49"}},"metadata":{}},{"name":"stderr","text":" 86%|████████▌ | 43/50 [06:08<00:59,  8.54s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df98ce7731dc4c10afd1ecc715de2387"}},"metadata":{}},{"name":"stderr","text":" 88%|████████▊ | 44/50 [06:17<00:51,  8.55s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6099933576ec4bb089d16aa07d583229"}},"metadata":{}},{"name":"stderr","text":" 90%|█████████ | 45/50 [06:25<00:42,  8.58s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b9a5a80a6594b899247b97eb2bd31cd"}},"metadata":{}},{"name":"stderr","text":" 92%|█████████▏| 46/50 [06:34<00:34,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6088c88967e140fc9c9c0d47cff4c74c"}},"metadata":{}},{"name":"stderr","text":" 94%|█████████▍| 47/50 [06:43<00:25,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5aec4b49674efea4ddf05bd52c0c35"}},"metadata":{}},{"name":"stderr","text":" 96%|█████████▌| 48/50 [06:51<00:17,  8.59s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4af996550440dd8084a9f4adc2c151"}},"metadata":{}},{"name":"stderr","text":" 98%|█████████▊| 49/50 [07:00<00:08,  8.57s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbb067c254940f387fc6f250e797fcc"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 50/50 [07:08<00:00,  8.57s/it]\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"# === Собираем submission_df из уже посчитанных rows ===\n\nif \"rows\" not in globals():\n    raise RuntimeError(\"Переменная rows не найдена. Нужно ещё раз запустить ячейку с генерацией (где rows.append(...)).\")\n\nprint(\"Количество строк в rows:\", len(rows))\n\nsubmission_df = pd.DataFrame(rows)\nprint(\"Колонки submission_df:\", list(submission_df.columns))\nprint(submission_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:19:32.341227Z","iopub.execute_input":"2025-11-15T02:19:32.341481Z","iopub.status.idle":"2025-11-15T02:19:32.348281Z","shell.execute_reply.started":"2025-11-15T02:19:32.341462Z","shell.execute_reply":"2025-11-15T02:19:32.347731Z"}},"outputs":[{"name":"stdout","text":"Количество строк в rows: 50\nКолонки submission_df: ['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки']\n   ИДЕНТИФИКАТОР                                           контекст  \\\n0              1  increasingly match the larger population, and ...   \n1              2  the other hand, serve as interconnected inform...   \n2              3  in amplitude. The first stage of NREM sleep is...   \n3              4  a. structuralism\\nb. functionalism\\nc. Gestalt...   \n4              5  how language may influence cognition remains a...   \n\n                                            отвечать  \\\n0  SHORT ANSWER:\\nThe scientific method in psycho...   \n1  SHORT ANSWER:\\nA neuron consists of three main...   \n2  SHORT ANSWER:\\nThe stages of sleep include:\\n\\...   \n3  SHORT ANSWER:\\nOperant conditioning is a form ...   \n4  SHORT ANSWER:\\nProblem-solving in psychology r...   \n\n                                              ссылки  \n0  {\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...  \n1  {\"chunks\": [{\"chunk_id\": 688, \"doc_id\": 0, \"sc...  \n2  {\"chunks\": [{\"chunk_id\": 973, \"doc_id\": 0, \"sc...  \n3  {\"chunks\": [{\"chunk_id\": 288, \"doc_id\": 0, \"sc...  \n4  {\"chunks\": [{\"chunk_id\": 1996, \"doc_id\": 0, \"s...  \n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"submission_df = pd.DataFrame(rows)\nsubmission_df.to_csv(QA_OUTPUT_PATH, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:20:47.556586Z","iopub.execute_input":"2025-11-15T00:20:47.557197Z","iopub.status.idle":"2025-11-15T00:20:47.577454Z","shell.execute_reply.started":"2025-11-15T00:20:47.557170Z","shell.execute_reply":"2025-11-15T00:20:47.576718Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# === Финальный сабмит: id,answer ===\n\nif \"submission_df\" not in globals():\n    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows (см. предыдущую ячейку).\")\n\nprint(\"Текущие колонки:\", list(submission_df.columns))\n\n# если твой simple-режим уже делает 'id' и 'answer', то всё просто:\nif \"id\" in submission_df.columns and \"answer\" in submission_df.columns:\n    simple_sub = submission_df[[\"id\", \"answer\"]].copy()\nelse:\n    # если id-колонка называется иначе (например, QA_ID_COLUMN / 'query_id'):\n    id_col = QA_ID_COLUMN  # у тебя он уже задан в Block 9A\n    simple_sub = submission_df[[id_col, QA_ANSWER_COLUMN]].rename(\n        columns={id_col: \"id\", QA_ANSWER_COLUMN: \"answer\"}\n    )\n\nfinal_path = OUTPUT_DIR / \"submission.csv\"\nsimple_sub.to_csv(final_path, index=False)\n\nprint(\"Сабмит сохранён в:\", final_path)\nprint(simple_sub.head())'''\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T00:16:03.243836Z","iopub.execute_input":"2025-11-15T00:16:03.244474Z","iopub.status.idle":"2025-11-15T00:16:03.249569Z","shell.execute_reply.started":"2025-11-15T00:16:03.244448Z","shell.execute_reply":"2025-11-15T00:16:03.248936Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"'# === Финальный сабмит: id,answer ===\\n\\nif \"submission_df\" not in globals():\\n    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows (см. предыдущую ячейку).\")\\n\\nprint(\"Текущие колонки:\", list(submission_df.columns))\\n\\n# если твой simple-режим уже делает \\'id\\' и \\'answer\\', то всё просто:\\nif \"id\" in submission_df.columns and \"answer\" in submission_df.columns:\\n    simple_sub = submission_df[[\"id\", \"answer\"]].copy()\\nelse:\\n    # если id-колонка называется иначе (например, QA_ID_COLUMN / \\'query_id\\'):\\n    id_col = QA_ID_COLUMN  # у тебя он уже задан в Block 9A\\n    simple_sub = submission_df[[id_col, QA_ANSWER_COLUMN]].rename(\\n        columns={id_col: \"id\", QA_ANSWER_COLUMN: \"answer\"}\\n    )\\n\\nfinal_path = OUTPUT_DIR / \"submission.csv\"\\nsimple_sub.to_csv(final_path, index=False)\\n\\nprint(\"Сабмит сохранён в:\", final_path)\\nprint(simple_sub.head())'"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"'''# === Финальный сабмит из submission_df в формате id,answer ===\n\nif \"submission_df\" not in globals():\n    raise RuntimeError(\"submission_df не найден. Сначала собери его из rows.\")\n\nprint(\"Текущие колонки:\", list(submission_df.columns))\n\n# Прямо жёстко переименовываем русские имена в нужные:\nsimple_sub = submission_df.rename(\n    columns={\n        \"ИДЕНТИФИКАТОР\": \"id\",\n        \"отвечать\": \"answer\",\n    }\n)[[\"id\", \"answer\"]]  # берём только эти две колонки и в нужном порядке\n\nfinal_path = OUTPUT_DIR / \"submission.csv\"\nsimple_sub.to_csv(final_path, index=False)\n\nprint(\"Сабмит сохранён в:\", final_path)\nprint(simple_sub.head())'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# === Block 9. Формирование сабмита для competition-psycho ===\n\nimport json\nfrom pathlib import Path\n\n# путь к файлу с вопросами\nQA_INPUT_PATH = Path(\"/kaggle/input/competition-psycho/queries.json\")\n\nwith open(QA_INPUT_PATH, \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\nqa_df = pd.DataFrame(data)\nprint(\"Пример строк из queries.json:\")\nprint(qa_df.head())\n\n# ВАЖНО: подправь эти два имени колонок, если они другие в queries.json\nID_COL = \"query_id\"       # например: \"id\" или \"query_id\"\nQUESTION_COL = \"question\"   # например: \"query\" или \"question\"\n\n# проверим, что такие колонки есть\nassert ID_COL in qa_df.columns, f\"В qa_df нет колонки {ID_COL}, есть: {list(qa_df.columns)}\"\nassert QUESTION_COL in qa_df.columns, f\"В qa_df нет колонки {QUESTION_COL}, есть: {list(qa_df.columns)}\"\n\nrows = []\n\nprint(\"Генерирую ответы для всех вопросов...\")\nfor _, row in tqdm(qa_df.iterrows(), total=len(qa_df)):\n    q_id = row[ID_COL]\n    q_text = str(row[QUESTION_COL])\n\n    ans = answer_question(\n        q_text,\n        max_new_tokens=256,\n        temperature=0.2,\n        top_p=0.9,\n        debug=False,\n    )\n\n    rows.append(\n        {\n            \"id\": q_id,\n            \"answer\": ans,\n        }\n    )\n\nsubmission_df = pd.DataFrame(rows)\n\n# файл сабмита\nsub_path = OUTPUT_DIR / \"submission.csv\"\nsubmission_df.to_csv(sub_path, index=False)\n\nprint(\"Сабмит сохранён в:\", sub_path)\nprint(submission_df.head())'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"если колонки в другом порядке:","metadata":{}},{"cell_type":"code","source":"print(submission_df.columns)\n# Index(['id', 'вопрос', 'ответ', 'Контекст', 'ref_page'], dtype='object')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:23:21.858941Z","iopub.execute_input":"2025-11-15T02:23:21.859245Z","iopub.status.idle":"2025-11-15T02:23:21.863766Z","shell.execute_reply.started":"2025-11-15T02:23:21.859223Z","shell.execute_reply":"2025-11-15T02:23:21.863024Z"}},"outputs":[{"name":"stdout","text":"Index(['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки'], dtype='object')\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"desired_cols = [\"id\", \"вопрос\", \"Контекст\", \"ответ\"]\nsubmission_df = submission_df[desired_cols]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Главная идея:\nсначала формируешь submission_df,\nпотом (перед to_csv) один раз явно задаёшь порядок колонок:","metadata":{}},{"cell_type":"code","source":"submission_df = submission_df[[\"id\", \"вопрос\", \"контекст\", \"ответ\"]]\nsubmission_df.to_csv(QA_OUTPUT_PATH, index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# прочие нюансы формирования сабмита","metadata":{}},{"cell_type":"markdown","source":"## Шпаргалка по колонкам в разных режимах","metadata":{}},{"cell_type":"markdown","source":"SUBMISSION_MODE = \"simple\"","metadata":{}},{"cell_type":"code","source":"submission_df.columns == [QA_ID_COLUMN, QA_ANSWER_COLUMN]\n# например: ['id', 'answer']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"или","metadata":{}},{"cell_type":"code","source":"submission_df = submission_df[[QA_ID_COLUMN, QA_ANSWER_COLUMN]]\nsubmission_df.rename(columns={QA_ID_COLUMN: \"id\", QA_ANSWER_COLUMN: \"answer\"}, inplace=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SUBMISSION_MODE = \"debug_full\"","metadata":{}},{"cell_type":"code","source":"[QA_ID_COLUMN, QA_QUESTION_COLUMN, \"context\", QA_ANSWER_COLUMN, \"refs_json\"]\n# например: ['id', 'question', 'context', 'answer', 'refs_json']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SUBMISSION_MODE = \"fr_rag\"","metadata":{}},{"cell_type":"code","source":"['id', 'вопрос', 'ответ', 'Контекст', 'ref_page']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SUBMISSION_MODE = \"fr_rag_no_context\"","metadata":{}},{"cell_type":"code","source":"['id', 'вопрос', 'ответ', 'ref_page']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"SUBMISSION_MODE = \"id_question_answer\"","metadata":{}},{"cell_type":"code","source":"['id', 'вопрос', 'ответ']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Универсальный мини-шаблон для «любого конкурса»","metadata":{}},{"cell_type":"code","source":"desired_cols = [\"id\", \"question\", \"answer\"]  # подставляешь нужный порядок/имена\nsubmission_custom = submission_df[desired_cols].copy()\nsubmission_custom.to_csv(OUTPUT_DIR / \"submission_custom.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## переименовать колонки","metadata":{}},{"cell_type":"markdown","source":"Переименовать несколько колонок по словарю","metadata":{}},{"cell_type":"code","source":"submission_df = submission_df.rename(\n    columns={\n        \"ИДЕНТИФИКАТОР\": \"ID\",\n        \"отвечать\": \"answer\",\n        \"контекст\": \"context\",\n        \"ссылки\": \"references\",\n    }\n)\n\nprint(submission_df.columns)\n# Index(['id', 'context', 'answer', 'refs_json'], dtype='object')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:23:31.368788Z","iopub.execute_input":"2025-11-15T02:23:31.369477Z","iopub.status.idle":"2025-11-15T02:23:31.374503Z","shell.execute_reply.started":"2025-11-15T02:23:31.369452Z","shell.execute_reply":"2025-11-15T02:23:31.373735Z"}},"outputs":[{"name":"stdout","text":"Index(['ID', 'context', 'answer', 'references'], dtype='object')\n","output_type":"stream"}],"execution_count":93},{"cell_type":"code","source":"submission_df.to_csv(QA_OUTPUT_PATH, index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:23:36.011304Z","iopub.execute_input":"2025-11-15T02:23:36.012115Z","iopub.status.idle":"2025-11-15T02:23:36.025268Z","shell.execute_reply.started":"2025-11-15T02:23:36.012087Z","shell.execute_reply":"2025-11-15T02:23:36.024533Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:23:58.515391Z","iopub.execute_input":"2025-11-15T02:23:58.515702Z","iopub.status.idle":"2025-11-15T02:23:58.528632Z","shell.execute_reply.started":"2025-11-15T02:23:58.515652Z","shell.execute_reply":"2025-11-15T02:23:58.527874Z"}},"outputs":[{"execution_count":98,"output_type":"execute_result","data":{"text/plain":"    ID                                            context  \\\n0    1  increasingly match the larger population, and ...   \n1    2  the other hand, serve as interconnected inform...   \n2    3  in amplitude. The first stage of NREM sleep is...   \n3    4  a. structuralism\\nb. functionalism\\nc. Gestalt...   \n4    5  how language may influence cognition remains a...   \n5    6  8.1 How Memory Functions\\nLEARNING OBJECTIVES\\...   \n6    7  with similar circumstances. Over time, several...   \n7    8  given situation. Personality traits are relati...   \n8    9  Summary\\n12.1 What Is Social Psychology?\\nSoci...   \n9   10  16.5 The Sociocultural Model and Therapy Utili...   \n10  11  18. How did the object of study in psychology ...   \n11  12  scholars, Wilhelm Wundt and William James, are...   \n12  13  saw it, psychology’s purpose was to study the ...   \n13  14  researchers rejected his premises, we cannot d...   \n14  15  the United States as they had been in their na...   \n15  16  Pavlov (1849–1936), a Russian scientist, perfo...   \n16  17  focused on making psychology an objective scie...   \n17  18  recognized need in the domain of interpersonal...   \n18  19  thoughts affect our behavior. Cognitive-behavi...   \n19  20  As mentioned in the previous section, the cogn...   \n20  21  concepts related to cognitive psychology will ...   \n21  22  but it will provide insight into the major are...   \n22  23  Animals (1872), to explore this field.\\nEvolut...   \n23  24  5.1 Sensation versus Perception\\nLEARNING OBJE...   \n24  25  5.1 Sensation versus Perception\\nLEARNING OBJE...   \n25  26  TABLE 9.1\\nCognitive Theory of Development\\nJe...   \n26  27  providing enormous insight into the difference...   \n27  28  Neurotransmitters and Drugs\\nThere are several...   \n28  29  given situation. Personality traits are relati...   \n29  30  CHAPTER 15\\nPsychological Disorders 537\\nIntro...   \n30  31  of the DSM, published in 1952, classified psyc...   \n31  32  consensus in the scientific community.\\n2.4 Et...   \n32  33  As mentioned in the previous section, the cogn...   \n33  34  womanly companions of men and to be mothers.” ...   \n34  35  reasoning. That is, people vary depending on t...   \n35  36  psychology. These include re-evaluating and di...   \n36  37  can compromise our sense of well-being. That i...   \n37  38  called the stages of psychosexual development....   \n38  39  Psychologists report their research findings i...   \n39  40  Psychological Association, 2014).\\nDifferent t...   \n40  41  Neurotransmitters and Drugs\\nThere are several...   \n41  42  Summary\\n13.1 What Is Industrial and Organizat...   \n42  43  Social Norms\\nAs discussed previously, social ...   \n43  44  definitions of prejudice and discrimination, e...   \n44  45  Kenneth Clark. They are best known for their s...   \n45  46  use the scientific method to acquire knowledge...   \n46  47  1.1 What Is Psychology?\\nLEARNING OBJECTIVES\\n...   \n47  48  Mood disorders are those in which the person e...   \n48  49  cultural differences. Diversity training educa...   \n49  50  An education in psychology is valuable for a n...   \n\n                                               answer  \\\n0   SHORT ANSWER:\\nThe scientific method in psycho...   \n1   SHORT ANSWER:\\nA neuron consists of three main...   \n2   SHORT ANSWER:\\nThe stages of sleep include:\\n\\...   \n3   SHORT ANSWER:\\nOperant conditioning is a form ...   \n4   SHORT ANSWER:\\nProblem-solving in psychology r...   \n5   SHORT ANSWER:\\nThe three stages of memory are ...   \n6   SHORT ANSWER:\\nThe key components of emotion i...   \n7   SHORT ANSWER:\\nOpenness, Conscientiousness, Ex...   \n8   SHORT ANSWER:\\nSocial psychology is the study ...   \n9   SHORT ANSWER:\\nThe sociocultural model in ther...   \n10  SHORT ANSWER:\\nThe history of psychology can b...   \n11  SHORT ANSWER:\\nWilhelm Wundt and William James...   \n12  SHORT ANSWER:\\nFunctionalism is a philosophica...   \n13  SHORT ANSWER:\\nFreud's contributions to psycho...   \n14  SHORT ANSWER:\\nGestalt principles are rules or...   \n15  SHORT ANSWER:\\nClassical conditioning is a for...   \n16  SHORT ANSWER:\\nSkinner's contributions to beha...   \n17  SHORT ANSWER:\\nMaslow's hierarchy of needs is ...   \n18  SHORT ANSWER:\\nClient-centered therapy, also k...   \n19  SHORT ANSWER:\\nCognitive psychology is the are...   \n20  SHORT ANSWER:\\nDevelopmental psychology is the...   \n21  SHORT ANSWER:\\nBiopsychology is the study of h...   \n22  Short Answer:\\nEvolutionary psychologists are ...   \n23  SHORT ANSWER:\\nSensation and perception are tw...   \n24  SHORT ANSWER:\\nSensation refers to the detecti...   \n25  SHORT ANSWER:\\nJean Piaget's theories of cogni...   \n26  SHORT ANSWER:\\nObject permanence is the unders...   \n27  SHORT ANSWER:\\nNeurotransmitters play a crucia...   \n28  Short Answer:\\nThe Big Five personality trait ...   \n29  SHORT ANSWER:\\nPsychological disorders are def...   \n30  SHORT ANSWER:\\nThe DSM-5 is the fifth and most...   \n31  SHORT ANSWER:\\nEthics play a crucial role in p...   \n32  SHORT ANSWER:\\nThe cognitive revolution in psy...   \n33  SHORT ANSWER:\\nFeminist psychology refers to a...   \n34  SHORT ANSWER:\\nMulticultural psychology is the...   \n35  SHORT ANSWER:\\nCross-cultural psychology faces...   \n36  SHORT ANSWER:\\nStress in psychology refers to ...   \n37  SHORT ANSWER:\\nThe stages of psychosexual deve...   \n38  SHORT ANSWER:\\nReplication is crucial because ...   \n39  Short Answer:\\nThe different types of psycholo...   \n40  SHORT ANSWER:\\nNeurotransmitters play a crucia...   \n41  SHORT ANSWER:\\nMajor themes in industrial-orga...   \n42  SHORT ANSWER:\\nSocial norms play a crucial rol...   \n43  SHORT ANSWER:\\nPrejudice refers to negative at...   \n44  Short Answer:\\nKey takeaways from the Brown v....   \n45  SHORT ANSWER:\\nThe scientific process of hypot...   \n46  SHORT ANSWER:\\nPsychology has numerous applica...   \n47  SHORT ANSWER:\\nMood disorders involve severe d...   \n48  SHORT ANSWER:\\nCultural diversity has a signif...   \n49  SHORT ANSWER:\\nCritical thinking plays a cruci...   \n\n                                           references  \n0   {\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...  \n1   {\"chunks\": [{\"chunk_id\": 688, \"doc_id\": 0, \"sc...  \n2   {\"chunks\": [{\"chunk_id\": 973, \"doc_id\": 0, \"sc...  \n3   {\"chunks\": [{\"chunk_id\": 288, \"doc_id\": 0, \"sc...  \n4   {\"chunks\": [{\"chunk_id\": 1996, \"doc_id\": 0, \"s...  \n5   {\"chunks\": [{\"chunk_id\": 2023, \"doc_id\": 0, \"s...  \n6   {\"chunks\": [{\"chunk_id\": 2860, \"doc_id\": 0, \"s...  \n7   {\"chunks\": [{\"chunk_id\": 213, \"doc_id\": 0, \"sc...  \n8   {\"chunks\": [{\"chunk_id\": 3666, \"doc_id\": 0, \"s...  \n9   {\"chunks\": [{\"chunk_id\": 5363, \"doc_id\": 0, \"s...  \n10  {\"chunks\": [{\"chunk_id\": 294, \"doc_id\": 0, \"sc...  \n11  {\"chunks\": [{\"chunk_id\": 96, \"doc_id\": 0, \"sco...  \n12  {\"chunks\": [{\"chunk_id\": 106, \"doc_id\": 0, \"sc...  \n13  {\"chunks\": [{\"chunk_id\": 3038, \"doc_id\": 0, \"s...  \n14  {\"chunks\": [{\"chunk_id\": 121, \"doc_id\": 0, \"sc...  \n15  {\"chunks\": [{\"chunk_id\": 1478, \"doc_id\": 0, \"s...  \n16  {\"chunks\": [{\"chunk_id\": 276, \"doc_id\": 0, \"sc...  \n17  {\"chunks\": [{\"chunk_id\": 2720, \"doc_id\": 0, \"s...  \n18  {\"chunks\": [{\"chunk_id\": 5418, \"doc_id\": 0, \"s...  \n19  {\"chunks\": [{\"chunk_id\": 199, \"doc_id\": 0, \"sc...  \n20  {\"chunks\": [{\"chunk_id\": 202, \"doc_id\": 0, \"sc...  \n21  {\"chunks\": [{\"chunk_id\": 182, \"doc_id\": 0, \"sc...  \n22  {\"chunks\": [{\"chunk_id\": 189, \"doc_id\": 0, \"sc...  \n23  {\"chunks\": [{\"chunk_id\": 1202, \"doc_id\": 0, \"s...  \n24  {\"chunks\": [{\"chunk_id\": 1202, \"doc_id\": 0, \"s...  \n25  {\"chunks\": [{\"chunk_id\": 2362, \"doc_id\": 0, \"s...  \n26  {\"chunks\": [{\"chunk_id\": 204, \"doc_id\": 0, \"sc...  \n27  {\"chunks\": [{\"chunk_id\": 721, \"doc_id\": 0, \"sc...  \n28  {\"chunks\": [{\"chunk_id\": 213, \"doc_id\": 0, \"sc...  \n29  {\"chunks\": [{\"chunk_id\": 25, \"doc_id\": 0, \"sco...  \n30  {\"chunks\": [{\"chunk_id\": 4586, \"doc_id\": 0, \"s...  \n31  {\"chunks\": [{\"chunk_id\": 587, \"doc_id\": 0, \"sc...  \n32  {\"chunks\": [{\"chunk_id\": 199, \"doc_id\": 0, \"sc...  \n33  {\"chunks\": [{\"chunk_id\": 158, \"doc_id\": 0, \"sc...  \n34  {\"chunks\": [{\"chunk_id\": 162, \"doc_id\": 0, \"sc...  \n35  {\"chunks\": [{\"chunk_id\": 159, \"doc_id\": 0, \"sc...  \n36  {\"chunks\": [{\"chunk_id\": 4066, \"doc_id\": 0, \"s...  \n37  {\"chunks\": [{\"chunk_id\": 2338, \"doc_id\": 0, \"s...  \n38  {\"chunks\": [{\"chunk_id\": 586, \"doc_id\": 0, \"sc...  \n39  {\"chunks\": [{\"chunk_id\": 5294, \"doc_id\": 0, \"s...  \n40  {\"chunks\": [{\"chunk_id\": 721, \"doc_id\": 0, \"sc...  \n41  {\"chunks\": [{\"chunk_id\": 4040, \"doc_id\": 0, \"s...  \n42  {\"chunks\": [{\"chunk_id\": 3358, \"doc_id\": 0, \"s...  \n43  {\"chunks\": [{\"chunk_id\": 3504, \"doc_id\": 0, \"s...  \n44  {\"chunks\": [{\"chunk_id\": 166, \"doc_id\": 0, \"sc...  \n45  {\"chunks\": [{\"chunk_id\": 79, \"doc_id\": 0, \"sco...  \n46  {\"chunks\": [{\"chunk_id\": 78, \"doc_id\": 0, \"sco...  \n47  {\"chunks\": [{\"chunk_id\": 5112, \"doc_id\": 0, \"s...  \n48  {\"chunks\": [{\"chunk_id\": 3984, \"doc_id\": 0, \"s...  \n49  {\"chunks\": [{\"chunk_id\": 89, \"doc_id\": 0, \"sco...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>references</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>increasingly match the larger population, and ...</td>\n      <td>SHORT ANSWER:\\nThe scientific method in psycho...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 338, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>the other hand, serve as interconnected inform...</td>\n      <td>SHORT ANSWER:\\nA neuron consists of three main...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 688, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>in amplitude. The first stage of NREM sleep is...</td>\n      <td>SHORT ANSWER:\\nThe stages of sleep include:\\n\\...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 973, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>a. structuralism\\nb. functionalism\\nc. Gestalt...</td>\n      <td>SHORT ANSWER:\\nOperant conditioning is a form ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 288, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>how language may influence cognition remains a...</td>\n      <td>SHORT ANSWER:\\nProblem-solving in psychology r...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 1996, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6</td>\n      <td>8.1 How Memory Functions\\nLEARNING OBJECTIVES\\...</td>\n      <td>SHORT ANSWER:\\nThe three stages of memory are ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 2023, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7</td>\n      <td>with similar circumstances. Over time, several...</td>\n      <td>SHORT ANSWER:\\nThe key components of emotion i...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 2860, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8</td>\n      <td>given situation. Personality traits are relati...</td>\n      <td>SHORT ANSWER:\\nOpenness, Conscientiousness, Ex...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 213, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9</td>\n      <td>Summary\\n12.1 What Is Social Psychology?\\nSoci...</td>\n      <td>SHORT ANSWER:\\nSocial psychology is the study ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 3666, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10</td>\n      <td>16.5 The Sociocultural Model and Therapy Utili...</td>\n      <td>SHORT ANSWER:\\nThe sociocultural model in ther...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 5363, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>11</td>\n      <td>18. How did the object of study in psychology ...</td>\n      <td>SHORT ANSWER:\\nThe history of psychology can b...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 294, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>scholars, Wilhelm Wundt and William James, are...</td>\n      <td>SHORT ANSWER:\\nWilhelm Wundt and William James...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 96, \"doc_id\": 0, \"sco...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>13</td>\n      <td>saw it, psychology’s purpose was to study the ...</td>\n      <td>SHORT ANSWER:\\nFunctionalism is a philosophica...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 106, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14</td>\n      <td>researchers rejected his premises, we cannot d...</td>\n      <td>SHORT ANSWER:\\nFreud's contributions to psycho...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 3038, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>15</td>\n      <td>the United States as they had been in their na...</td>\n      <td>SHORT ANSWER:\\nGestalt principles are rules or...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 121, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>16</td>\n      <td>Pavlov (1849–1936), a Russian scientist, perfo...</td>\n      <td>SHORT ANSWER:\\nClassical conditioning is a for...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 1478, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>17</td>\n      <td>focused on making psychology an objective scie...</td>\n      <td>SHORT ANSWER:\\nSkinner's contributions to beha...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 276, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>18</td>\n      <td>recognized need in the domain of interpersonal...</td>\n      <td>SHORT ANSWER:\\nMaslow's hierarchy of needs is ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 2720, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>19</td>\n      <td>thoughts affect our behavior. Cognitive-behavi...</td>\n      <td>SHORT ANSWER:\\nClient-centered therapy, also k...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 5418, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>20</td>\n      <td>As mentioned in the previous section, the cogn...</td>\n      <td>SHORT ANSWER:\\nCognitive psychology is the are...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 199, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>21</td>\n      <td>concepts related to cognitive psychology will ...</td>\n      <td>SHORT ANSWER:\\nDevelopmental psychology is the...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 202, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>22</td>\n      <td>but it will provide insight into the major are...</td>\n      <td>SHORT ANSWER:\\nBiopsychology is the study of h...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 182, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>23</td>\n      <td>Animals (1872), to explore this field.\\nEvolut...</td>\n      <td>Short Answer:\\nEvolutionary psychologists are ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 189, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>24</td>\n      <td>5.1 Sensation versus Perception\\nLEARNING OBJE...</td>\n      <td>SHORT ANSWER:\\nSensation and perception are tw...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 1202, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>25</td>\n      <td>5.1 Sensation versus Perception\\nLEARNING OBJE...</td>\n      <td>SHORT ANSWER:\\nSensation refers to the detecti...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 1202, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>26</td>\n      <td>TABLE 9.1\\nCognitive Theory of Development\\nJe...</td>\n      <td>SHORT ANSWER:\\nJean Piaget's theories of cogni...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 2362, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>27</td>\n      <td>providing enormous insight into the difference...</td>\n      <td>SHORT ANSWER:\\nObject permanence is the unders...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 204, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>28</td>\n      <td>Neurotransmitters and Drugs\\nThere are several...</td>\n      <td>SHORT ANSWER:\\nNeurotransmitters play a crucia...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 721, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>29</td>\n      <td>given situation. Personality traits are relati...</td>\n      <td>Short Answer:\\nThe Big Five personality trait ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 213, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>30</td>\n      <td>CHAPTER 15\\nPsychological Disorders 537\\nIntro...</td>\n      <td>SHORT ANSWER:\\nPsychological disorders are def...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 25, \"doc_id\": 0, \"sco...</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>31</td>\n      <td>of the DSM, published in 1952, classified psyc...</td>\n      <td>SHORT ANSWER:\\nThe DSM-5 is the fifth and most...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 4586, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>32</td>\n      <td>consensus in the scientific community.\\n2.4 Et...</td>\n      <td>SHORT ANSWER:\\nEthics play a crucial role in p...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 587, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>33</td>\n      <td>As mentioned in the previous section, the cogn...</td>\n      <td>SHORT ANSWER:\\nThe cognitive revolution in psy...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 199, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>34</td>\n      <td>womanly companions of men and to be mothers.” ...</td>\n      <td>SHORT ANSWER:\\nFeminist psychology refers to a...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 158, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>35</td>\n      <td>reasoning. That is, people vary depending on t...</td>\n      <td>SHORT ANSWER:\\nMulticultural psychology is the...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 162, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>36</td>\n      <td>psychology. These include re-evaluating and di...</td>\n      <td>SHORT ANSWER:\\nCross-cultural psychology faces...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 159, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>37</td>\n      <td>can compromise our sense of well-being. That i...</td>\n      <td>SHORT ANSWER:\\nStress in psychology refers to ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 4066, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>38</td>\n      <td>called the stages of psychosexual development....</td>\n      <td>SHORT ANSWER:\\nThe stages of psychosexual deve...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 2338, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>39</td>\n      <td>Psychologists report their research findings i...</td>\n      <td>SHORT ANSWER:\\nReplication is crucial because ...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 586, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>40</td>\n      <td>Psychological Association, 2014).\\nDifferent t...</td>\n      <td>Short Answer:\\nThe different types of psycholo...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 5294, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>41</td>\n      <td>Neurotransmitters and Drugs\\nThere are several...</td>\n      <td>SHORT ANSWER:\\nNeurotransmitters play a crucia...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 721, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>42</td>\n      <td>Summary\\n13.1 What Is Industrial and Organizat...</td>\n      <td>SHORT ANSWER:\\nMajor themes in industrial-orga...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 4040, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>43</td>\n      <td>Social Norms\\nAs discussed previously, social ...</td>\n      <td>SHORT ANSWER:\\nSocial norms play a crucial rol...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 3358, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>44</td>\n      <td>definitions of prejudice and discrimination, e...</td>\n      <td>SHORT ANSWER:\\nPrejudice refers to negative at...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 3504, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>45</td>\n      <td>Kenneth Clark. They are best known for their s...</td>\n      <td>Short Answer:\\nKey takeaways from the Brown v....</td>\n      <td>{\"chunks\": [{\"chunk_id\": 166, \"doc_id\": 0, \"sc...</td>\n    </tr>\n    <tr>\n      <th>45</th>\n      <td>46</td>\n      <td>use the scientific method to acquire knowledge...</td>\n      <td>SHORT ANSWER:\\nThe scientific process of hypot...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 79, \"doc_id\": 0, \"sco...</td>\n    </tr>\n    <tr>\n      <th>46</th>\n      <td>47</td>\n      <td>1.1 What Is Psychology?\\nLEARNING OBJECTIVES\\n...</td>\n      <td>SHORT ANSWER:\\nPsychology has numerous applica...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 78, \"doc_id\": 0, \"sco...</td>\n    </tr>\n    <tr>\n      <th>47</th>\n      <td>48</td>\n      <td>Mood disorders are those in which the person e...</td>\n      <td>SHORT ANSWER:\\nMood disorders involve severe d...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 5112, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>48</th>\n      <td>49</td>\n      <td>cultural differences. Diversity training educa...</td>\n      <td>SHORT ANSWER:\\nCultural diversity has a signif...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 3984, \"doc_id\": 0, \"s...</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>50</td>\n      <td>An education in psychology is valuable for a n...</td>\n      <td>SHORT ANSWER:\\nCritical thinking plays a cruci...</td>\n      <td>{\"chunks\": [{\"chunk_id\": 89, \"doc_id\": 0, \"sco...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":98},{"cell_type":"markdown","source":"Переименовать только одну колонку","metadata":{}},{"cell_type":"code","source":"submission_df = submission_df.rename(columns={\"ИДЕНТИФИКАТОР\": \"id\"})\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Полностью задать новый список имён (если порядок уже правильный)","metadata":{}},{"cell_type":"code","source":"submission_df.columns\n# ['ИДЕНТИФИКАТОР', 'контекст', 'отвечать', 'ссылки']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"и ты хочешь точно такой же порядок, но другие названия:","metadata":{}},{"cell_type":"code","source":"submission_df.columns = [\"id\", \"context\", \"answer\", \"refs_json\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# пытаюсь улучшить скор","metadata":{}},{"cell_type":"markdown","source":"## cross-encoder reranker ","metadata":{}},{"cell_type":"code","source":"# === Block 10A. Cross-encoder reranker (HF) ===\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\n\n# Небольшой, но сильный cross-encoder, обученный на MS MARCO (английский)\nRERANKER_MODEL_ID = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n\n# Используем тот же GPU, что и для LLM, если доступен\nRERANKER_DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(\"Loading cross-encoder reranker:\", RERANKER_MODEL_ID)\nreranker_tokenizer = AutoTokenizer.from_pretrained(RERANKER_MODEL_ID)\nreranker_model = AutoModelForSequenceClassification.from_pretrained(RERANKER_MODEL_ID).to(RERANKER_DEVICE)\n\n# Проверим, что у модели один выходной логит (score)\nnum_labels = reranker_model.config.num_labels\nprint(f\"Reranker loaded on {RERANKER_DEVICE}, num_labels={num_labels}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:06:52.515801Z","iopub.execute_input":"2025-11-15T02:06:52.516298Z","iopub.status.idle":"2025-11-15T02:06:55.990414Z","shell.execute_reply.started":"2025-11-15T02:06:52.516274Z","shell.execute_reply":"2025-11-15T02:06:55.989614Z"}},"outputs":[{"name":"stdout","text":"Loading cross-encoder reranker: cross-encoder/ms-marco-MiniLM-L-6-v2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a567b6e37ece4add924137eb3f1bed6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1270d4d58b734acea24e059bd3ddb7f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4848fe0b09f48cc99313fc20538b1a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43b4924d27714d01b939d6af65fb762c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2efabc995344bca9bbcd625e7836b19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e5a87095fa54624be3d81b80d434f86"}},"metadata":{}},{"name":"stdout","text":"Reranker loaded on cuda, num_labels=1\n","output_type":"stream"}],"execution_count":78},{"cell_type":"code","source":"# === Block 10B. Rerank кандидатов cross-encoder'ом + обновлённый retrieve_relevant_chunks ===\n\nimport numpy as np\n\n# Можно будет крутить эти параметры\nCE_TOP_K_CANDIDATES = 50   # сколько кандидатов берём после hybrid для rerank\nCE_TOP_K_FINAL = 8         # сколько чанкoв остаётся после rerank (пойдут в контекст)\n\ndef rerank_chunks_with_ce(query: str, candidates: list, top_k_final: int = CE_TOP_K_FINAL):\n    \"\"\"\n    Переранжирует список кандидатов (dict'ы с chunk_id/doc_id/score_hybrid)\n    с помощью cross-encoder'а.\n\n    candidates: список словарей, у которых есть хотя бы 'chunk_id' и 'doc_id'.\n    Возвращает НОВЫЙ список кандидатов, отсортированный по score_ce (убывание).\n    \"\"\"\n    if not candidates:\n        return []\n\n    # Ограничим количество кандидатов сверху (чтобы не грузить лишнее)\n    cand = candidates[:CE_TOP_K_CANDIDATES]\n\n    # Собираем пары (query, chunk_text)\n    pair_texts = []\n    for ch in cand:\n        cid = int(ch[\"chunk_id\"])\n        # достаём текст чанка из chunks_df\n        try:\n            row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n            chunk_text = str(row_match.get(\"chunk_text\", \"\"))\n        except Exception:\n            chunk_text = \"\"\n        pair_texts.append((query, chunk_text))\n\n    # Токенизируем пачкой\n    inputs = reranker_tokenizer(\n        [p[0] for p in pair_texts],\n        [p[1] for p in pair_texts],\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\",\n        max_length=512,\n    ).to(RERANKER_DEVICE)\n\n    # Считаем скор для каждого (query, chunk)\n    with torch.no_grad():\n        logits = reranker_model(**inputs).logits\n\n    # Если num_labels=1, logits.shape = [batch, 1], иначе берём логит класса 1\n    if logits.shape[1] == 1:\n        scores = logits.squeeze(-1).detach().cpu().numpy()\n    else:\n        # предполагаем, что релевантность = логит класса 1\n        scores = logits[:, 1].detach().cpu().numpy()\n\n    # Добавляем score_ce к кандидатам\n    for ch, sc in zip(cand, scores):\n        ch[\"score_ce\"] = float(sc)\n\n    # Сортируем по score_ce (desc)\n    cand_sorted = sorted(cand, key=lambda x: x.get(\"score_ce\", 0.0), reverse=True)\n\n    # Оставляем top_k_final (можно потом крутить)\n    return cand_sorted[:top_k_final]\n\n\ndef retrieve_relevant_chunks(\n    query: str,\n    top_k_faiss: int = 50,\n    top_k_bm25: int = 50,\n    top_k_final: int = CE_TOP_K_FINAL,\n    alpha_faiss: float = 0.6,\n    max_context_chars: int = MAX_CONTEXT_CHARS,\n):\n    \"\"\"\n    Обновлённая версия:\n    1) Гибридный поиск (FAISS + BM25) → кандидаты с score_hybrid\n    2) Cross-encoder reranker (MS MARCO) → score_ce, сортировка\n    3) Берём top_k_final чанков, собираем контекст\n\n    Возвращает:\n    - dict с полями:\n        - \"chunks\": список выбранных чанков (dict)\n        - \"context_text\": текст, который пойдёт в LLM\n    \"\"\"\n    # --- 1. Гибридный поиск по эмбеддингам и BM25 ---\n    hybrid_candidates = hybrid_search_chunks(\n        query=query,\n        top_k_faiss=top_k_faiss,\n        top_k_bm25=top_k_bm25,\n        alpha_faiss=alpha_faiss,\n    )\n\n    if not hybrid_candidates:\n        return {\"chunks\": [], \"context_text\": \"\"}\n\n    # --- 2. Rerank candidates cross-encoder'ом ---\n    reranked = rerank_chunks_with_ce(query, hybrid_candidates, top_k_final=top_k_final)\n\n    # --- 3. Собираем context_text из reranked чанков ---\n    # Заодно избегаем дублей по chunk_id\n    seen_chunk_ids = set()\n    selected_chunks = []\n    context_parts = []\n\n    for ch in reranked:\n        cid = int(ch[\"chunk_id\"])\n        if cid in seen_chunk_ids:\n            continue\n        seen_chunk_ids.add(cid)\n\n        # достаём текст чанка\n        row_match = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n        chunk_text = str(row_match.get(\"chunk_text\", \"\"))\n\n        selected_chunks.append(ch)\n        context_parts.append(chunk_text)\n\n    # Склеиваем контекст, при необходимости обрезаем по длине\n    context_text = \"\\n\\n\".join(context_parts)\n    if len(context_text) > max_context_chars:\n        context_text = context_text[:max_context_chars]\n\n    return {\n        \"chunks\": selected_chunks,\n        \"context_text\": context_text,\n    }\n\nprint(\"Cross-encoder reranker и обновлённый retrieve_relevant_chunks() определены.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T02:08:23.821801Z","iopub.execute_input":"2025-11-15T02:08:23.822526Z","iopub.status.idle":"2025-11-15T02:08:23.834961Z","shell.execute_reply.started":"2025-11-15T02:08:23.822501Z","shell.execute_reply":"2025-11-15T02:08:23.834211Z"}},"outputs":[{"name":"stdout","text":"Cross-encoder reranker и обновлённый retrieve_relevant_chunks() определены.\n","output_type":"stream"}],"execution_count":79},{"cell_type":"markdown","source":"перезапустить (в каком порядке теперь это должно быть):\n\n\nBlock 0 (пути, константы).\n\nBlock 0.5 (копирование book.pdf и queries.json — если есть).\n\nBlock 1 (список документов).\n\nBlock 2 (чтение PDF → raw текст).\n\nBlock 3A / 3B (чанкинг → chunks_df).\n\nBlock 4A / 4B (эмбеддинги + FAISS-индекс).\n\nBlock 5A / 5B (BM25 + hybrid_search_chunks).\n\nBlock 10A (загрузка cross-encoder reranker).\n\nBlock 10B (rerank + новый retrieve_relevant_chunks).\n\nBlock 6A / 6B (LLM + generate_answer).\n\nBlock 7B (build_rag_prompt, answer_question — уже будет использовать новый retrieve_relevant_chunks).\n\nBlock 8 / 8B (ручная проверка).\n\nBlock 9A / 9B (загрузка queries.json → генерация сабмита).","metadata":{}},{"cell_type":"markdown","source":"## Multi-query + Step-back generation (LLM)","metadata":{}},{"cell_type":"code","source":"# === Block 11A. Multi-query generation (paraphrases + step-back) ===\n\ndef generate_search_queries(\n    question: str,\n    num_paraphrases: int = 3,\n    add_step_back: bool = True,\n    max_new_tokens: int = 128,\n):\n    \"\"\"\n    Генерирует несколько альтернативных поисковых запросов (multi-query)\n    + step-back (более общий вопрос).\n    \"\"\"\n\n    # 1) Запрос на перефразировки\n    para_prompt = (\n        \"Rewrite the question into several diverse search queries that can help \"\n        \"retrieve relevant passages from a textbook.\\n\"\n        f\"Original question: {question}\\n\\n\"\n        f\"Generate {num_paraphrases} short search queries in English.\\n\"\n        \"Return them as a numbered list. No extra text.\"\n    )\n\n    para_output = gen_pipe(\n        para_prompt,\n        max_new_tokens=max_new_tokens,\n        do_sample=True,\n        temperature=0.7,\n        top_p=0.9,\n    )[0][\"generated_text\"]\n\n    # извлечём строки с запросами\n    paraphrases = []\n    for line in para_output.splitlines():\n        line = line.strip()\n        if line and (line[0].isdigit() or line.startswith(\"-\")):\n            q = line.split(\".\", 1)[-1].strip()\n            if len(q) > 0:\n                paraphrases.append(q)\n\n    # fallback если модель плохо отформатировала\n    paraphrases = paraphrases[:num_paraphrases]\n\n    # 2) step-back запрос\n    step_back_q = None\n    if add_step_back:\n        sb_prompt = (\n            \"Rewrite the question into a broader, more general version that still helps \"\n            \"retrieve relevant parts of a textbook.\\n\"\n            f\"Original question: {question}\\n\\n\"\n            \"Return only ONE short broader question in English.\"\n        )\n\n        sb_output = gen_pipe(\n            sb_prompt,\n            max_new_tokens=80,\n            do_sample=True,\n            temperature=0.5,\n            top_p=0.9,\n        )[0][\"generated_text\"]\n\n        # просто берём первую полноценную строку\n        sb_line = sb_output.strip().splitlines()[0]\n        step_back_q = sb_line.strip()\n\n    # финальный список запросов\n    queries = [question]\n    for q in paraphrases:\n        if q and q not in queries:\n            queries.append(q)\n\n    if step_back_q and step_back_q not in queries:\n        queries.append(step_back_q)\n\n    return queries\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Block 11B. Multi-query retrieval wrapper ===\n\ndef get_candidates_multiquery(\n    question: str,\n    top_k_faiss: int = 50,\n    top_k_bm25: int = 50,\n    alpha_faiss: float = 0.6,\n):\n    \"\"\"\n    Генерирует несколько альтернативных поисковых запросов (multi-query),\n    делает гибридный поиск по каждому и объединяет кандидатов.\n    \"\"\"\n\n    queries = generate_search_queries(question)\n\n    all_candidates = []\n    for q in queries:\n        try:\n            cand = hybrid_search_chunks(\n                query=q,\n                top_k_faiss=top_k_faiss,\n                top_k_bm25=top_k_bm25,\n                alpha_faiss=alpha_faiss,\n            )\n            all_candidates.extend(cand)\n        except Exception as e:\n            print(f\"[WARN] hybrid search failed for query '{q}': {e}\")\n\n    # deduplicate by chunk_id (оставляем лучший score_hybrid)\n    merged = {}\n    for ch in all_candidates:\n        cid = int(ch[\"chunk_id\"])\n        if cid not in merged or ch[\"score_hybrid\"] > merged[cid][\"score_hybrid\"]:\n            merged[cid] = ch\n\n    candidates = list(merged.values())\n    # сортируем по score_hybrid как первичному\n    candidates = sorted(candidates, key=lambda x: x[\"score_hybrid\"], reverse=True)\n\n    return candidates\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Block 11C. Final retrieve_relevant_chunks (multi-query + reranker + adaptive K) ===\n\nADAPTIVE_THRESHOLD = 0.0     # порог уверенности cross-encoder\nMIN_CHUNKS = 3\nMAX_CHUNKS = 8\n\ndef retrieve_relevant_chunks(\n    query: str,\n    top_k_faiss: int = 50,\n    top_k_bm25: int = 50,\n    alpha_faiss: float = 0.6,\n    max_context_chars: int = MAX_CONTEXT_CHARS,\n):\n    \"\"\"\n    Улучшенная версия:\n    1. Генерация multi-query (paraphrases + step-back)\n    2. Hybrid search (FAISS + BM25) для каждого запроса\n    3. Дедупликация кандидатов\n    4. Cross-encoder reranking\n    5. Adaptive K (по уверенности, а не фиксированному количеству)\n    6. Формирование контекста\n    \"\"\"\n\n    # 1. Multi-query candidates\n    multi_candidates = get_candidates_multiquery(\n        question=query,\n        top_k_faiss=top_k_faiss,\n        top_k_bm25=top_k_bm25,\n        alpha_faiss=alpha_faiss,\n    )\n\n    if not multi_candidates:\n        return {\"chunks\": [], \"context_text\": \"\"}\n\n    # 2. Cross-encoder rerank\n    reranked = rerank_chunks_with_ce(\n        query=query,\n        candidates=multi_candidates,\n        top_k_final=MAX_CHUNKS * 3,   # временно берём много, потом фильтруем\n    )\n\n    # 3. Adaptive K\n    confident = [ch for ch in reranked if ch.get(\"score_ce\", 0) >= ADAPTIVE_THRESHOLD]\n\n    if len(confident) < MIN_CHUNKS:\n        selected = reranked[:MIN_CHUNKS]\n    else:\n        selected = confident[:MAX_CHUNKS]\n\n    # 4. Формируем context_text\n    seen = set()\n    parts = []\n    final_chunks = []\n\n    for ch in selected:\n        cid = int(ch[\"chunk_id\"])\n        if cid in seen:\n            continue\n        seen.add(cid)\n\n        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n        text = str(row[\"chunk_text\"])\n\n        final_chunks.append(ch)\n        parts.append(text)\n\n    context_text = \"\\n\\n\".join(parts)\n    if len(context_text) > max_context_chars:\n        context_text = context_text[:max_context_chars]\n\n    return {\n        \"chunks\": final_chunks,\n        \"context_text\": context_text,\n    }\n\nprint(\"MULTI-QUERY + RERANKER + ADAPTIVE-K retrieve_relevant_chunks loaded.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ПЕРВЫЙ ПРОХОД (полностью после изменения кода):\n\nBlock 0 — импорты, пути\n\nBlock 0.5 — копирование данных (если есть)\n\nBlock 1 — список документов\n\nBlock 2 — PDF → raw text\n\nBlock 3A / 3B — чанкинг → chunks_df\n\nBlock 4A / 4B — Embeddings → FAISS\n\nBlock 5A / 5B — BM25 → hybrid_search_chunks\n\nBlock 6A — LLM загрузка (gen_pipe)\n\nBlock 10A — загрузка cross-encoder (reranker)\n\nBlock 10B — определение rerank_chunks_with_ce\n\nBlock 7B — build_rag_prompt, answer_question\n\nBlock 11A — multi-query generation\n\nBlock 11B — multiquery candidate gatherer\n\nBlock 11C — замена retrieve_relevant_chunks\n\nBlock 8 / 8B — тестирование\n\nBlock 9A / 9B — генерация submission","metadata":{}},{"cell_type":"markdown","source":"ПРИ ПОВТОРНОМ ЗАПУСКЕ (если ядро не перезагружалось):\n\nМожно запускать только:\n\nBlock 10A\n\nBlock 10B\n\nBlock 11A\n\nBlock 11B\n\nBlock 11C\n\nBlock 7B\n\nBlock 8 или Block 9","metadata":{}},{"cell_type":"markdown","source":"## Self-RAG helper (draft + verify)","metadata":{}},{"cell_type":"code","source":"# === Block 12A. Self-RAG helper: draft + verify ===\n\ndef build_verify_prompt(question: str, context_text: str, draft_answer: str) -> str:\n    \"\"\"\n    Промпт для второго шага Self-RAG:\n    проверка и переписывание ответа строго по контексту.\n    \"\"\"\n    prompt = (\n        \"You are a verification and refinement module for a RAG system.\\n\"\n        \"Your job is to check whether the draft answer is FULLY supported by the provided context.\\n\"\n        \"If any part of the answer is not supported, you must either remove it or rephrase it so that it is strictly grounded in the context.\\n\"\n        \"If the context does not contain enough information to answer the question, you must explicitly say that.\\n\\n\"\n        \"=== CONTEXT ===\\n\"\n        f\"{context_text}\\n\\n\"\n        \"=== QUESTION ===\\n\"\n        f\"{question}\\n\\n\"\n        \"=== DRAFT ANSWER ===\\n\"\n        f\"{draft_answer}\\n\\n\"\n        \"=== TASK ===\\n\"\n        \"1) Identify unsupported or speculative parts.\\n\"\n        \"2) Rewrite the answer so that EVERY statement is supported by the context.\\n\"\n        \"3) Keep the answer clear and concise.\\n\"\n        \"4) Answer in English.\\n\\n\"\n        \"Return ONLY the final improved answer, without any meta-comments.\\n\"\n    )\n    return prompt\n\n\ndef self_rag_answer(\n    question: str,\n    context_text: str,\n    max_new_tokens: int = 256,\n    temperature_draft: float = 0.4,\n    temperature_verify: float = 0.2,\n    top_p: float = 0.9,\n):\n    \"\"\"\n    Двухшаговый ответ:\n    1) draft-ответ по стандартному RAG-промпту\n    2) проверка и переписывание ответа строго по контексту (Self-RAG)\n    \"\"\"\n\n    # --- Шаг 1. Черновой ответ ---\n    draft_prompt = build_rag_prompt(\n        query=question,\n        context_text=context_text,\n    )\n    draft_answer = generate_answer(\n        prompt=draft_prompt,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature_draft,\n        top_p=top_p,\n        do_sample=True,\n    )\n\n    # --- Шаг 2. Верификация и улучшение ---\n    verify_prompt = build_verify_prompt(\n        question=question,\n        context_text=context_text,\n        draft_answer=draft_answer,\n    )\n    final_answer = generate_answer(\n        prompt=verify_prompt,\n        max_new_tokens=max_new_tokens,\n        temperature=temperature_verify,\n        top_p=top_p,\n        do_sample=False,  # на верификации можно не сэмплить\n    )\n\n    return final_answer.strip(), draft_answer.strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Block 12B. Обновлённый answer_question() с Self-RAG ===\n\ndef answer_question(\n    question: str,\n    max_new_tokens: int = 256,\n    temperature_draft: float = 0.4,\n    temperature_verify: float = 0.2,\n    top_p: float = 0.9,\n    debug: bool = True,\n):\n    \"\"\"\n    Высокоуровневая функция:\n    1) достаёт релевантные чанки (retrieve_relevant_chunks)\n    2) собирает context_text\n    3) делает двухшаговый Self-RAG:\n       - draft ответ по стандартному промпту\n       - verify+refine ответ строго по контексту\n    \"\"\"\n\n    retrieval = retrieve_relevant_chunks(\n        query=question,\n        top_k_faiss=50,\n        top_k_bm25=50,\n        alpha_faiss=0.6,\n        max_context_chars=MAX_CONTEXT_CHARS,\n    )\n    chunks = retrieval.get(\"chunks\", [])\n    context_text = retrieval.get(\"context_text\", \"\")\n\n    if debug:\n        print(\"=== DEBUG: выбрано чанков ===\", len(chunks))\n        for ch in chunks:\n            cid = ch.get(\"chunk_id\")\n            did = ch.get(\"doc_id\")\n            s_h = ch.get(\"score_hybrid\", None)\n            s_ce = ch.get(\"score_ce\", None)\n            line = f\"- chunk_id={cid}, doc_id={did}, hybrid={s_h:.4f}\" if s_h is not None else f\"- chunk_id={cid}, doc_id={did}\"\n            if s_ce is not None:\n                line += f\", ce={s_ce:.4f}\"\n            print(line)\n        print()\n\n    final_answer, draft_answer = self_rag_answer(\n        question=question,\n        context_text=context_text,\n        max_new_tokens=max_new_tokens,\n        temperature_draft=temperature_draft,\n        temperature_verify=temperature_verify,\n        top_p=top_p,\n    )\n\n    if debug:\n        print(\"[DRAFT ANSWER]:\")\n        print(draft_answer)\n        print(\"\\n[FINAL ANSWER]:\")\n        print(final_answer)\n\n    return final_answer\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Block 12C. Обновлённый rag_answer_with_context_and_refs() с Self-RAG ===\n\ndef rag_answer_with_context_and_refs(\n    question: str,\n    max_new_tokens: int = 256,\n    temperature_draft: float = 0.4,\n    temperature_verify: float = 0.2,\n    top_p: float = 0.9,\n):\n    \"\"\"\n    Расширенная версия ответа для сабмитов:\n    - делает retrieve_relevant_chunks (multi-query + reranker)\n    - считает final_answer через Self-RAG (draft + verify)\n    - возвращает:\n        answer, context_text, refs_json, refs_dict\n    \"\"\"\n\n    retrieval = retrieve_relevant_chunks(\n        query=question,\n        top_k_faiss=50,\n        top_k_bm25=50,\n        alpha_faiss=0.6,\n        max_context_chars=MAX_CONTEXT_CHARS,\n    )\n    chunks = retrieval.get(\"chunks\", [])\n    context_text = retrieval.get(\"context_text\", \"\")\n\n    # Self-RAG\n    final_answer, draft_answer = self_rag_answer(\n        question=question,\n        context_text=context_text,\n        max_new_tokens=max_new_tokens,\n        temperature_draft=temperature_draft,\n        temperature_verify=temperature_verify,\n        top_p=top_p,\n    )\n\n    # Собираем refs_dict\n    refs = {\"chunks\": []}\n    for ch in chunks:\n        cid = int(ch.get(\"chunk_id\"))\n        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n\n        ref_item = {\n            \"chunk_id\": cid,\n            \"doc_id\": int(ch.get(\"doc_id\", row.get(\"doc_id\", 0))),\n            \"path\": str(row.get(\"path\", \"\")),\n            \"score_faiss\": float(ch.get(\"score_faiss\", 0.0)),\n            \"score_bm25\": float(ch.get(\"score_bm25\", 0.0)),\n            \"score_hybrid\": float(ch.get(\"score_hybrid\", 0.0)),\n        }\n        if \"score_ce\" in ch:\n            ref_item[\"score_ce\"] = float(ch[\"score_ce\"])\n\n        # если у тебя в chunks_df есть 'page' или 'page_start' — можно добавить сюда:\n        if \"page\" in row.index:\n            ref_item[\"page\"] = int(row[\"page\"])\n\n        refs[\"chunks\"].append(ref_item)\n\n    refs_json = json.dumps(refs, ensure_ascii=False)\n\n    return final_answer.strip(), context_text, refs_json, refs\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Block 12A\n\nBlock 12B\n\nBlock 12C\n\nи дальше:\n\nBlock 8 — для проверки качества ответов\n\nBlock 9B — для нового сабмита","metadata":{}},{"cell_type":"markdown","source":"## к слову о параметрах","metadata":{}},{"cell_type":"code","source":"'''1. Retrieval Parameters (FAISS + BM25 + Multi-Query + Reranker)\n\nЭти параметры находятся в:\n\nBlock 10B — Cross-encoder reranker\n\nBlock 11A — Multi-query generation\n\nBlock 11B — Candidate merging\n\nBlock 11C — Final retrieve_relevant_chunks\n\n🔹 1.1. Adaptive K (Block 11C)\nADAPTIVE_THRESHOLD = 0.0\nMIN_CHUNKS = 3\nMAX_CHUNKS = 8\n\n\nADAPTIVE_THRESHOLD: порог по cross-encoder score.\n\n↑ больше (0.2–0.5) → меньше мусора\n\n↓ меньше (0.0) → шире покрытие\n\nMIN_CHUNKS: минимум чанков.\n\nувеличить до 4 → если LLM слишком поверхностна\n\nуменьшить до 2 → если слишком многословно\n\nMAX_CHUNKS: максимум чанков.\n\nувеличить до 10–12 → лучше покрытие\n\nуменьшить до 6 → меньше шума\n\n🔹 1.2. Hybrid Search Parameters (Block 11B)\ntop_k_faiss = 50\ntop_k_bm25 = 50\nalpha_faiss = 0.6\n\n\ntop_k_faiss, top_k_bm25:\n\n↑ до 80–100 → больше кандидатов для reranker\n\n↓ до 30 → быстрее, но хуже качество\n\nalpha_faiss:\n\nближе к 0.7–0.8 → лучше семантический поиск\n\nближе к 0.4–0.5 → лучше по ключевым словам (если вопросы прямые)\n\n🔹 1.3. Reranker Parameters (Block 10B)\nCE_TOP_K_CANDIDATES = 50\nCE_TOP_K_FINAL = 8\n\n\nCE_TOP_K_CANDIDATES: сколько кандидатов проверяет cross-encoder\n\n↑ до 80–100 → выше качество поиска\n\n↓ до 30 → быстрее\n\nCE_TOP_K_FINAL: сколько берём после reranker до adaptive K\n\nобычно от 8 до 16.\n\n🔹 1.4. Multi-Query Generation Parameters (Block 11A)\nnum_paraphrases = 3\ntemperature (paraphrases) = 0.7\ntemperature (step-back) = 0.5\n\n\nnum_paraphrases:\n\n↑ до 4–5 → более устойчивый поиск\n\n↓ до 2 → быстрее\n\nТемпературы:\n\n↑ → разнообразные формулировки\n\n↓ → более точные формулировки\n\n#️⃣ 2. Self-RAG Parameters (Block 12A / 12B / 12C)\n\nЭти параметры управляют качеством финального ответа.\n\n🔹 2.1. Draft generation\ntemperature_draft = 0.4\ntop_p = 0.9\n\n\n↑ до 0.5–0.6 → ответы богаче и разнообразнее\n\n↓ до 0.2–0.3 → меньше галлюцинаций, строже привязка к контексту\n\n🔹 2.2. Verification stage\ntemperature_verify = 0.2\n\n\nне стоит сильно поднимать — это «строгая» стадия\n\n↑ до 0.3–0.4 → чуть плавнее формулировки\n\n↓ до 0.1 → максимально строгий ответ «по контексту»\n\n🔹 2.3. Token Limits\nmax_new_tokens = 256\n\n\n↑ до 320–384 → если ответы обрезаются\n\n↓ до 192 → если модель многословна\n\n#️⃣ 3. Prompting Parameters (Block 7B + Block 12A)\n🔹 3.1. Output Structure Strictness\n\nМожно усилить:\n\nболее строгие инструкции\n\nуказание «не использовать информацию вне контекста»\n\nтребование ссылок (pages / sections)\n\n🔹 3.2. Chain-of-Thought\n\nМожно включить короткий CoT:\n\nThink step-by-step, but keep reasoning concise (no more than 3–4 steps).\n\n\nв draft-промпт.\n\n#️⃣ 4. Chunking & Context Parameters (Block 3A/3B)\n🔹 4.1. Overlap\n\n↑ overlap до 40–50% → больше связности\n\n↓ до 20% → меньше шума, короче индекс\n\n🔹 4.2. Max context length\n\nВ блоке retrieve_relevant_chunks:\n\nmax_context_chars = MAX_CONTEXT_CHARS\n\n\n↑ до 50k → больше контекста (если модель тянет)\n\n↓ до 20k → если ответы слишком загромождены\n\n#️⃣ 5. Что влияет сильнее всего (приоритет)\n🥇 ТОП-5 ПАРАМЕТРОВ, которые дают реальный прирост:\n\nADAPTIVE_THRESHOLD\n\nMAX_CHUNKS / MIN_CHUNKS\n\nnum_paraphrases (multi-query)\n\ntop_k_faiss / top_k_bm25\n\ntemperature_draft\n\n🥈 Средней важности:\n\nalpha_faiss\n\ntemperatures при multi-query\n\nchunk overlap\n\nCE_TOP_K_CANDIDATES\n\n🥉 Наименее критичные:\n\ntemperature_verify\n\ntop_p\n\nmax_new_tokens'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## новая версия build_verify_prompt с указанием источников","metadata":{}},{"cell_type":"code","source":"# === Block 13A. Обновлённый build_verify_prompt с указанием источников ===\n\ndef build_verify_prompt(question: str, context_text: str, draft_answer: str) -> str:\n    \"\"\"\n    Промпт для второго шага Self-RAG:\n    проверка и переписывание ответа строго по контексту + указание источников.\n    \"\"\"\n\n    prompt = (\n        \"You are a verification and refinement module for a RAG system.\\n\"\n        \"Your task is to check whether the DRAFT ANSWER is fully supported by the CONTEXT.\\n\"\n        \"If any part of the answer is not supported, you must remove it or rewrite it so that it is strictly grounded in the context.\\n\"\n        \"If the context does not contain enough information to answer the question, you must explicitly say that in the final answer.\\n\\n\"\n        \"=== CONTEXT ===\\n\"\n        f\"{context_text}\\n\\n\"\n        \"=== QUESTION ===\\n\"\n        f\"{question}\\n\\n\"\n        \"=== DRAFT ANSWER ===\\n\"\n        f\"{draft_answer}\\n\\n\"\n        \"=== TASK ===\\n\"\n        \"1) Identify unsupported, speculative, or hallucinated parts of the draft answer.\\n\"\n        \"2) Rewrite the answer so that EVERY statement is supported by the context.\\n\"\n        \"3) The final answer MUST be clear, concise and in ENGLISH.\\n\"\n        \"4) After the main answer, add a short 'SOURCES:' section where you briefly list which parts of the context support the key points of the answer.\\n\"\n        \"   For example: 'SOURCES: paragraph about Pavlov's experiments; section describing behaviorism; part about Taylor's management theory'.\\n\"\n        \"5) Do NOT invent page numbers; use only descriptions that can be inferred from the context.\\n\\n\"\n        \"=== OUTPUT FORMAT (STRICT) ===\\n\"\n        \"FINAL ANSWER:\\n\"\n        \"- <your final, verified answer here>\\n\\n\"\n        \"SOURCES:\\n\"\n        \"- <short description of the main supporting fragment 1>\\n\"\n        \"- <short description of the main supporting fragment 2>\\n\"\n        \"- ...\\n\\n\"\n        \"Return ONLY the 'FINAL ANSWER' and 'SOURCES' sections, without any extra commentary.\\n\"\n    )\n    return prompt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Запусти Block 13A.\n\nЗапусти Block 12A не нужно заново (если ядро не перезапускалось) — мы просто переопределили функцию, она уже в памяти.\n\nЗапусти Block 12B и 12C, если ты их менял после этого (не обязательно, но безопасно).\n\nЗапусти Block 8\n\nя уже засыпаю мм","metadata":{}},{"cell_type":"markdown","source":"## лёгкий sentence-level trimming","metadata":{}},{"cell_type":"code","source":"# === Block 14. Sentence-level trimming контекста под вопрос ===\n# ИДЕЯ:\n#   - каждый выбранный чанк мы режем на предложения;\n#   - для каждого предложения считаем \"похожесть\" на вопрос по пересечению ключевых слов;\n#   - оставляем N самых релевантных предложений, сохраняя исходный порядок;\n#   - если текст и так короткий — оставляем как есть.\n#\n# ПЛЮС:\n#   - меньше шума в контексте → LLM проще держаться фактов;\n#   - self-RAG видит более сфокусированный текст.\n#\n# ВАЖНО:\n#   - мы НЕ трогаем FAISS/BM25/reranker, только финальный context_text.\n\nimport re\nfrom collections import Counter\n\n# Небольшой набор английских стоп-слов, чтобы не учитывать \"the, and, of, to, ...\"\n_TRIM_STOPWORDS = {\n    # русские\n    \"и\", \"в\", \"во\", \"не\", \"что\", \"он\", \"она\", \"оно\", \"они\",\n    \"как\", \"а\", \"но\", \"yet\", \"да\", \"или\", \"либо\",\n    \"к\", \"ко\", \"от\", \"до\", \"из\", \"за\", \"над\", \"под\", \"при\", \"по\", \"о\", \"об\", \"обо\",\n    \"на\", \"с\", \"со\", \"у\", \"про\", \"для\", \"без\", \"через\", \"между\",\n    \"это\", \"этот\", \"эта\", \"это\", \"эти\", \"того\", \"той\", \"тех\",\n    \"тот\", \"та\", \"те\", \"там\", \"тут\", \"здесь\",\n    \"же\", \"уж\", \"ли\", \"бы\", \"то\", \"же\", \"уже\", \"ещё\", \"ещё\", \"тоже\",\n    \"быть\", \"есть\", \"был\", \"была\", \"были\", \"будет\", \"будут\",\n    \"мы\", \"вы\", \"ты\", \"они\", \"он\", \"она\",\n    \"мой\", \"моя\", \"мои\", \"твой\", \"твоя\", \"твои\", \"наш\", \"наша\", \"наши\",\n    \"их\", \"его\", \"ее\", \"её\", \"сам\", \"сама\", \"сами\",\n    \"там\", \"здесь\", \"сюда\", \"туда\",\n    # английские (на всякий случай — вдруг смешанный текст)\n    \"the\", \"a\", \"an\", \"and\", \"or\", \"for\", \"of\", \"to\", \"in\", \"on\", \"at\",\n    \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\",\n    \"this\", \"that\", \"these\", \"those\",\n    \"by\", \"with\", \"from\", \"as\", \"it\", \"its\",\n    \"about\", \"into\", \"over\", \"under\", \"between\", \"through\",\n    \"he\", \"she\", \"they\", \"them\", \"his\", \"her\", \"their\",\n}\n\ndef _simple_tokenize(text: str):\n    \"\"\"\n    Очень простой токенайзер:\n    - приводим к нижнему регистру\n    - выкидываем все, что не буква/цифра/пробел\n    - сплитим по пробелам\n    \"\"\"\n    text = text.lower()\n    text = re.sub(r\"[^a-z0-9\\s]\", \" \", text)\n    tokens = [t for t in text.split() if t]\n    return tokens\n\ndef _keywords(text: str):\n    \"\"\"\n    Ключевые слова: токены без стоп-слов и длиной >= 3.\n    \"\"\"\n    toks = _simple_tokenize(text)\n    return [t for t in toks if len(t) >= 3 and t not in _TRIM_STOPWORDS]\n\ndef split_into_sentences(text: str):\n    \"\"\"\n    Очень простой сплиттер на предложения:\n    режем по .!? с сохранением базовой структуры.\n    \"\"\"\n    # Заменяем перевод строки на пробел — часто в pdf они стоят внутри предложений\n    text = text.replace(\"\\n\", \" \")\n    # Грубый сплит по конечным знакам. Это не идеально, но для учебника ок.\n    parts = re.split(r\"(?<=[.!?])\\s+\", text)\n    sentences = [s.strip() for s in parts if s.strip()]\n    return sentences\n\ndef score_sentence_relevance(sentence: str, question: str) -> float:\n    \"\"\"\n    Оценка релевантности предложения к вопросу.\n    Очень простой скор:\n      score = |пересечение ключевых слов| / (1 + |ключевые слова предложения|)\n    \"\"\"\n    q_kw = set(_keywords(question))\n    s_kw = _keywords(sentence)\n    if not s_kw or not q_kw:\n        return 0.0\n    inter = q_kw.intersection(s_kw)\n    score = len(inter) / (1.0 + len(s_kw))\n    return score\n\ndef trim_chunk_for_question(\n    chunk_text: str,\n    question: str,\n    max_sentences: int = 3,\n    min_chars: int = 300,\n):\n    \"\"\"\n    Основная функция тримминга чанка:\n    - если чанк короткий (<= min_chars) — возвращаем как есть;\n    - иначе:\n        * режем на предложения\n        * если предложений мало (<= max_sentences) — тоже возвращаем как есть\n        * считаем score_sentence_relevance для каждого предложения\n        * выбираем top-k предложений по score\n        * сортируем их в исходном порядке и склеиваем обратно\n        * если по каким-то причинам всё пусто — fallback к исходному тексту\n    \"\"\"\n    text = chunk_text.strip()\n    if len(text) <= min_chars:\n        return text\n\n    sentences = split_into_sentences(text)\n    if len(sentences) <= max_sentences:\n        return text\n\n    # считаем скор для каждой фразы\n    scored = []\n    for idx, sent in enumerate(sentences):\n        sc = score_sentence_relevance(sent, question)\n        scored.append((idx, sent, sc))\n\n    # если все скоры нулевые — лучше ничего не трогать\n    if all(sc == 0.0 for _, _, sc in scored):\n        return text\n\n    # сортируем по score (по убыванию), берём top-k,\n    # затем сортируем по исходному индексу, чтобы сохранить порядок\n    scored_sorted = sorted(scored, key=lambda x: x[2], reverse=True)\n    top = scored_sorted[:max_sentences]\n    top_sorted_by_idx = sorted(top, key=lambda x: x[0])\n\n    trimmed_sentences = [s for _, s, _ in top_sorted_by_idx]\n    trimmed_text = \" \".join(trimmed_sentences).strip()\n\n    # на всякий случай fallback\n    if len(trimmed_text) < 1:\n        return text\n\n    return trimmed_text\n\n\n# --- Переопределяем retrieve_relevant_chunks, чтобы использовать тримминг внутри ---\n# ВАЖНО:\n#   - предполагается, что уже определены:\n#       * get_candidates_multiquery(...)\n#       * rerank_chunks_with_ce(...)\n#       * chunks_df\n#       * MAX_CONTEXT_CHARS\n#   - то есть этот блок надо запускать ПОСЛЕ Block 11A/11B/11C и 10A/10B.\n\nADAPTIVE_THRESHOLD = 0.0     # можно тюнить\nMIN_CHUNKS = 3\nMAX_CHUNKS = 8\n\ndef retrieve_relevant_chunks(\n    query: str,\n    top_k_faiss: int = 50,\n    top_k_bm25: int = 50,\n    alpha_faiss: float = 0.6,\n    max_context_chars: int = MAX_CONTEXT_CHARS,\n    trim_max_sentences: int = 3,\n    trim_min_chars: int = 300,\n):\n    \"\"\"\n    Финальная версия:\n    1) multi-query (paraphrases + step-back)\n    2) hybrid search (FAISS + BM25)\n    3) cross-encoder rerank\n    4) Adaptive K (по score_ce)\n    5) sentence-level trimming внутри каждого чанка\n    6) склейка контекста\n    \"\"\"\n\n    # 1. Multi-query candidates\n    multi_candidates = get_candidates_multiquery(\n        question=query,\n        top_k_faiss=top_k_faiss,\n        top_k_bm25=top_k_bm25,\n        alpha_faiss=alpha_faiss,\n    )\n\n    if not multi_candidates:\n        return {\"chunks\": [], \"context_text\": \"\"}\n\n    # 2. Cross-encoder rerank\n    reranked = rerank_chunks_with_ce(\n        query=query,\n        candidates=multi_candidates,\n        top_k_final=MAX_CHUNKS * 3,   # сначала берём побольше, потом фильтруем\n    )\n\n    # 3. Adaptive K\n    confident = [ch for ch in reranked if ch.get(\"score_ce\", 0) >= ADAPTIVE_THRESHOLD]\n\n    if len(confident) < MIN_CHUNKS:\n        selected = reranked[:MIN_CHUNKS]\n    else:\n        selected = confident[:MAX_CHUNKS]\n\n    # 4. Формируем context_text с триммингом предложений\n    seen = set()\n    parts = []\n    final_chunks = []\n\n    for ch in selected:\n        cid = int(ch[\"chunk_id\"])\n        if cid in seen:\n            continue\n        seen.add(cid)\n\n        row = chunks_df.loc[chunks_df[\"chunk_id\"] == cid].iloc[0]\n        raw_text = str(row[\"chunk_text\"])\n\n        trimmed = trim_chunk_for_question(\n            chunk_text=raw_text,\n            question=query,\n            max_sentences=trim_max_sentences,\n            min_chars=trim_min_chars,\n        )\n\n        final_chunks.append(ch)\n        parts.append(trimmed)\n\n    context_text = \"\\n\\n\".join(parts)\n    if len(context_text) > max_context_chars:\n        context_text = context_text[:max_context_chars]\n\n    return {\n        \"chunks\": final_chunks,\n        \"context_text\": context_text,\n    }\n\nprint(\"Sentence-level trimming для retrieve_relevant_chunks() активирован.\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}