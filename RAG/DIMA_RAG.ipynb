{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89475,"databundleVersionId":10464219,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pip install","metadata":{}},{"cell_type":"code","source":"!pip install langchain_community\n!pip install faiss-cpu\n!pip install -q rank-bm25\n!pip install langchain-experimental\n!pip -q install pypdf langchain-text-splitters","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:12.240619Z","iopub.execute_input":"2025-11-12T19:40:12.241503Z","iopub.status.idle":"2025-11-12T19:40:28.905492Z","shell.execute_reply.started":"2025-11-12T19:40:12.241466Z","shell.execute_reply":"2025-11-12T19:40:28.904533Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.4.1)\nRequirement already satisfied: langchain-core<2.0.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.0.4)\nRequirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.0.0)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.12.4)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (1.33)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (25.0)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.1->langchain_community) (4.15.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain_community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (0.16.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.1->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-classic<2.0.0,>=1.0.0->langchain_community) (2.41.5)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain_community) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain_community) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain_community) (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.12.0)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.25.0->faiss-cpu) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.25.0->faiss-cpu) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: langchain-experimental in /usr/local/lib/python3.11/dist-packages (0.4.0)\nRequirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (1.0.4)\nRequirement already satisfied: langchain-community<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-experimental) (0.4.1)\nRequirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.0.41)\nRequirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.32.5)\nRequirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.0.3)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.13.2)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (9.1.2)\nRequirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.11.0)\nRequirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.8)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.3)\nRequirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.26.4)\nRequirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (1.33)\nRequirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (25.0)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.12.4)\nRequirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain-experimental) (4.15.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.22.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.9.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (3.0.0)\nRequirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11.0)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.0)\nRequirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.23.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (2.41.5)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.0.0->langchain-experimental) (0.4.2)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.2.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2025.10.5)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (3.2.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (0.16.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.1.0)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.26.2->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community<1.0.0,>=0.4.0->langchain-experimental) (1.3.1)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":52},{"cell_type":"markdown","source":"# Seeds","metadata":{}},{"cell_type":"code","source":"import torch \nimport numpy as np\nimport random\nimport os\n\nseed=824\n\nos.environ['PYTHONHASHSEED']=str(seed)\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ntorch.manual_seed(seed)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.907389Z","iopub.execute_input":"2025-11-12T19:40:28.907730Z","iopub.status.idle":"2025-11-12T19:40:28.918354Z","shell.execute_reply.started":"2025-11-12T19:40:28.907695Z","shell.execute_reply":"2025-11-12T19:40:28.917609Z"}},"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7dd82c10aeb0>"},"metadata":{}}],"execution_count":53},{"cell_type":"code","source":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.919145Z","iopub.execute_input":"2025-11-12T19:40:28.919357Z","iopub.status.idle":"2025-11-12T19:40:28.931301Z","shell.execute_reply.started":"2025-11-12T19:40:28.919338Z","shell.execute_reply":"2025-11-12T19:40:28.930625Z"}},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":"# Imports ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport transformers\nfrom transformers import pipeline\nimport json \nimport faiss\nfrom rank_bm25 import BM25Okapi\nfrom sentence_transformers import SentenceTransformer, CrossEncoder\n\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom tqdm import tqdm \n\n\nimport re \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.933011Z","iopub.execute_input":"2025-11-12T19:40:28.933533Z","iopub.status.idle":"2025-11-12T19:40:28.948568Z","shell.execute_reply.started":"2025-11-12T19:40:28.933511Z","shell.execute_reply":"2025-11-12T19:40:28.947781Z"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":"# ЕСЛИ ЕСТЬ СТРУКТУРА НАДО ПОЛЬЗОВАТЬСЯ!","metadata":{}},{"cell_type":"markdown","source":"# Тут будет расписаны все возможные варианты действий взависимости от данных:\n\n\nЕсли есть структура: \n\n\nЕсли нету структуры:\n\n","metadata":{}},{"cell_type":"code","source":"def enrich_structure_heuristic(d: Document) -> dict:\n    # Простейшие эвристики заголовков: Markdown, нумерация, ALL-CAPS короткая строка\n    text = d.page_content\n    heading = None; level = None\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if lines:\n        hline = lines[0]\n        if re.match(r\"^#{1,6}\\s+\\S\", hline):\n            level = len(hline) - len(hline.lstrip(\"#\"))\n            heading = hline.lstrip(\"#\").strip()\n        elif re.match(r\"^\\d+(\\.\\d+){0,3}\\s+\\S\", hline):\n            level = 1 + hline.count(\".\")\n            heading = hline\n        elif len(hline) < 80 and hline.isupper():\n            level = 1; heading = hline\n    return {\"type\": \"text\", \"heading\": heading, \"heading_level\": level}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.949324Z","iopub.execute_input":"2025-11-12T19:40:28.949574Z","iopub.status.idle":"2025-11-12T19:40:28.964255Z","shell.execute_reply.started":"2025-11-12T19:40:28.949557Z","shell.execute_reply":"2025-11-12T19:40:28.963497Z"}},"outputs":[],"execution_count":56},{"cell_type":"code","source":"import re\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Optional\nfrom langchain_core.documents import Document\n\n# ——— эвристика для строки-заголовка ———\n_HEADING_MD = re.compile(r\"^(#{1,6})\\s+(.+)\")\n_HEADING_NUM = re.compile(r\"^(\\d+(?:\\.\\d+){0,5})\\s+(.+)\")\ndef is_mostly_upper(s: str, min_len=3, ratio=0.75):\n    letters = [c for c in s if c.isalpha()]\n    return len(letters) >= min_len and sum(c.isupper() for c in letters)/len(letters) >= ratio\n\ndef detect_heading_line(line: str) -> Optional[dict]:\n    line = line.strip()\n    if not line: return None\n    m = _HEADING_MD.match(line)\n    if m:\n        level = len(m.group(1))\n        text = m.group(2).strip()\n        return {\"heading\": text, \"level\": level, \"score\": 0.95, \"kind\":\"md\"}\n    m = _HEADING_NUM.match(line)\n    if m:\n        depth = m.group(1).count(\".\") + 1\n        text = m.group(2).strip()\n        return {\"heading\": f\"{m.group(1)} {text}\", \"level\": min(6, depth), \"score\": 0.9, \"kind\":\"num\"}\n    if len(line) < 80 and is_mostly_upper(line):\n        return {\"heading\": line.title(), \"level\": 1, \"score\": 0.7, \"kind\":\"caps\"}\n    if line.endswith(\":\") and len(line) < 120:\n        return {\"heading\": line[:-1].strip(), \"level\": 2, \"score\": 0.55, \"kind\":\"colon\"}\n    return None\n\n@dataclass\nclass OutlineNode:\n    level: int\n    title: str\n    page: Optional[int]\n    path: List[str]   # хлебные крошки\n    node_id: str\n\ndef build_outline_from_corpus(corpus: List[Document]) -> Dict[str, List[OutlineNode]]:\n    \"\"\"\n    Строим набор заголовков по doc_id (по страницам). Предполагаем, что corpus — постраничные Documents.\n    \"\"\"\n    outlines: Dict[str, List[OutlineNode]] = {}\n    stacks: Dict[str, List[OutlineNode]] = {}\n    counters: Dict[str, int] = {}\n\n    for d in corpus:\n        doc_id = d.metadata.get(\"doc_id\",\"_\")\n        page = d.metadata.get(\"page\") or d.metadata.get(\"page_number\")\n        text = d.page_content or \"\"\n        lines = [ln for ln in (text.splitlines()) if ln.strip()]\n\n        if doc_id not in stacks:\n            stacks[doc_id] = []\n            outlines[doc_id] = []\n            counters[doc_id] = 0\n\n        for ln in lines:\n            h = detect_heading_line(ln)\n            if not h: \n                continue\n            level = h[\"level\"]\n            title = h[\"heading\"]\n            # срежем стек до parent уровня\n            while stacks[doc_id] and stacks[doc_id][-1].level >= level:\n                stacks[doc_id].pop()\n            path = [n.title for n in stacks[doc_id]] + [title]\n            counters[doc_id] += 1\n            nid = f\"{doc_id}::h{counters[doc_id]}\"\n            node = OutlineNode(level=level, title=title, page=page, path=path, node_id=nid)\n            outlines[doc_id].append(node)\n            stacks[doc_id].append(node)\n\n    return outlines\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.965240Z","iopub.execute_input":"2025-11-12T19:40:28.966010Z","iopub.status.idle":"2025-11-12T19:40:28.988312Z","shell.execute_reply.started":"2025-11-12T19:40:28.965987Z","shell.execute_reply":"2025-11-12T19:40:28.987495Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def attach_structure_to_chunks(chunks: List[Document], outlines: Dict[str, List[OutlineNode]]) -> List[Document]:\n    \"\"\"\n    Для каждого чанка найдём \"ближайший выше\" заголовок (по странице/порядку)\n    и добавим: heading, heading_level, section_path, parent_id.\n    \"\"\"\n    by_doc: Dict[str, List[OutlineNode]] = outlines\n    for d in chunks:\n        doc_id = d.metadata.get(\"doc_id\",\"_\")\n        page = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or -1\n        cand = None\n        if doc_id in by_doc:\n            # берём последний заголовок на странице <= page, если нет — предыдущих страниц\n            cands = [n for n in by_doc[doc_id] if (n.page is None or page is None or int(n.page) <= int(page))]\n            if cands:\n                cand = cands[-1]\n        if cand:\n            d.metadata[\"heading\"] = cand.title\n            d.metadata[\"heading_level\"] = cand.level\n            d.metadata[\"section_path\"] = cand.path\n            d.metadata[\"parent_id\"] = cand.node_id\n        else:\n            # нет явного заголовка — пустые поля\n            d.metadata.setdefault(\"heading\", None)\n            d.metadata.setdefault(\"heading_level\", None)\n            d.metadata.setdefault(\"section_path\", [])\n            d.metadata.setdefault(\"parent_id\", None)\n    return chunks\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:28.989088Z","iopub.execute_input":"2025-11-12T19:40:28.989286Z","iopub.status.idle":"2025-11-12T19:40:29.002977Z","shell.execute_reply.started":"2025-11-12T19:40:28.989268Z","shell.execute_reply":"2025-11-12T19:40:29.002192Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"_TABLE_HINTS = re.compile(r\"\\b(table|таблица|колонк|column|row|строк|столб|csv|tsv)\\b\", re.I)\ndef is_tabular_like(txt: str) -> bool:\n    lines = [ln for ln in txt.splitlines() if ln.strip()]\n    if not lines: return False\n    bars = sum(ln.count(\"|\") for ln in lines)\n    tabs = sum(ln.count(\"\\t\") for ln in lines)\n    commas = sum(ln.count(\",\") for ln in lines[:10])\n    headerish = any(len(ln.split()) <= len(ln.split(\",\"))+1 for ln in lines[:3])\n    hint = bool(_TABLE_HINTS.search(txt))\n    return (bars >= 3) or (tabs >= 3) or (hint and headerish) or (commas >= 6)\n\ndef label_block_type(d: Document):\n    txt = d.page_content[:2000]\n    if is_tabular_like(txt):\n        d.metadata[\"type\"] = \"table\"\n    elif txt.strip().lower().startswith((\"figure\", \"рис.\", \"рисунок\", \"рис \")):\n        d.metadata[\"type\"] = \"figure\"\n    else:\n        d.metadata[\"type\"] = \"text\"\n    return d\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.003856Z","iopub.execute_input":"2025-11-12T19:40:29.004094Z","iopub.status.idle":"2025-11-12T19:40:29.024518Z","shell.execute_reply.started":"2025-11-12T19:40:29.004061Z","shell.execute_reply":"2025-11-12T19:40:29.023671Z"}},"outputs":[],"execution_count":59},{"cell_type":"code","source":"from collections import Counter\n\ndef structure_report(chunks: List[Document]):\n    lvls = Counter(d.metadata.get(\"heading_level\") for d in chunks)\n    types = Counter(d.metadata.get(\"type\") for d in chunks)\n    paths_nonempty = sum(1 for d in chunks if d.metadata.get(\"section_path\"))\n    print(\"Heading levels:\", dict(lvls))\n    print(\"Types:\", dict(types))\n    print(\"Sections with path:\", paths_nonempty, \"/\", len(chunks))\n\ndef print_outline_sample(outlines: Dict[str, List[OutlineNode]], k=20):\n    for doc_id, nodes in outlines.items():\n        print(f\"== {doc_id} ==\")\n        for n in nodes[:k]:\n            indent = \"  \"*(n.level-1)\n            print(f\"{indent}- p.{n.page} L{n.level}: {n.title}\")\n        break  # один документ для примера\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.025551Z","iopub.execute_input":"2025-11-12T19:40:29.025851Z","iopub.status.idle":"2025-11-12T19:40:29.043475Z","shell.execute_reply.started":"2025-11-12T19:40:29.025826Z","shell.execute_reply":"2025-11-12T19:40:29.042644Z"}},"outputs":[],"execution_count":60},{"cell_type":"code","source":"#Надо сделать ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.046298Z","iopub.execute_input":"2025-11-12T19:40:29.046544Z","iopub.status.idle":"2025-11-12T19:40:29.059969Z","shell.execute_reply.started":"2025-11-12T19:40:29.046525Z","shell.execute_reply":"2025-11-12T19:40:29.059259Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def composite_text(d: Document) -> str:\n    path = \" / \".join(d.metadata.get(\"section_path\", [])) if d.metadata.get(\"section_path\") else \"\"\n    head = d.metadata.get(\"heading\") or \"\"\n    typ  = d.metadata.get(\"type\") or \"text\"\n    base = d.page_content\n    summ = d.metadata.get(\"_summary_text\")\n    head_block = []\n    head_block.append(f\"[TYPE] {typ}\")\n    if path: head_block.append(f\"[PATH] {path}\")\n    if head: head_block.append(f\"[HEAD] {head}\")\n    if summ: head_block.append(f\"[SUMMARY] {summ}\")\n    return \"\\n\".join(head_block + [\"[TEXT] \" + base])\n\ndef _tok_with_heading(d: Document) -> List[str]:\n    base = _tok(d.page_content)\n    head = _tok(d.metadata.get(\"heading\",\"\") or \"\")\n    path = _tok(\" / \".join(d.metadata.get(\"section_path\", [])) if d.metadata.get(\"section_path\") else \"\")\n    summ = _tok(d.metadata.get(\"_summary_text\",\"\") or \"\")\n    return base + head*3 + path*2 + summ*2\n\ndef dedup_by_chunk_id(docs: List[Document]) -> List[Document]:\n    seen, out = set(), []\n    for d in docs:\n        cid = d.metadata.get(\"_chunk_id\")\n        if cid in seen: \n            continue\n        seen.add(cid); out.append(d)\n    return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.060787Z","iopub.execute_input":"2025-11-12T19:40:29.061050Z","iopub.status.idle":"2025-11-12T19:40:29.076657Z","shell.execute_reply.started":"2025-11-12T19:40:29.061022Z","shell.execute_reply":"2025-11-12T19:40:29.075881Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cleaning data","metadata":{}},{"cell_type":"code","source":"import re\nimport unicodedata\nimport html\n\n# precompiled bits\n_WS_IN_LINE = re.compile(r'[ \\t]+')\n_URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n_EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n\ndef cleaning(\n    text: str,\n    *,\n    lower: bool = False,\n    keep_newlines: bool = True,\n    remove_urls: bool = False,\n    remove_emails: bool = False\n) -> str:\n    \"\"\"\n    Light text normalization (safe defaults).\n    - Unicode normalize (NFKC), unescape HTML entities\n    - Drop zero-width & control chars (keeps \\\\n, \\\\t)\n    - Normalize curly quotes/dashes/ellipsis\n    - Collapse excess whitespace; optionally preserve newlines\n    - (Optional) strip URLs/emails\n    - (Optional) lowercase\n    \"\"\"\n    if text is None:\n        return \"\"\n\n    s = html.unescape(str(text))\n    s = unicodedata.normalize(\"NFKC\", s)\n\n    # remove zero-width & most control chars (keep newlines/tabs)\n    s = \"\".join(\n        ch for ch in s\n        if not (\n            unicodedata.category(ch) in {\"Cf\", \"Cc\"} and ch not in (\"\\n\", \"\\t\")\n        )\n    )\n\n    # typographic normalization\n    s = s.translate({\n        ord(\"“\"): '\"', ord(\"”\"): '\"', ord(\"„\"): '\"', ord(\"‟\"): '\"',\n        ord(\"’\"): \"'\", ord(\"‘\"): \"'\", ord(\"‚\"): \"'\", ord(\"′\"): \"'\",\n        ord(\"–\"): \"-\", ord(\"—\"): \"-\", ord(\"−\"): \"-\", ord(\"‐\"): \"-\",\n        ord(\"…\"): \"...\",\n    })\n\n    # optional removals\n    if remove_urls:\n        s = _URL_RE.sub(\" \", s)\n    if remove_emails:\n        s = _EMAIL_RE.sub(\" \", s)\n\n    # whitespace normalization\n    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    if keep_newlines:\n        # collapse spaces/tabs within lines\n        s = \"\\n\".join(_WS_IN_LINE.sub(\" \", line).strip() for line in s.split(\"\\n\"))\n        # collapse 3+ newlines -> 2\n        s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n    else:\n        s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    # tidy spaces before punctuation\n    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n\n    if lower:\n        s = s.lower()\n\n    return s","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.077522Z","iopub.execute_input":"2025-11-12T19:40:29.077751Z","iopub.status.idle":"2025-11-12T19:40:29.097326Z","shell.execute_reply.started":"2025-11-12T19:40:29.077731Z","shell.execute_reply":"2025-11-12T19:40:29.096488Z"}},"outputs":[],"execution_count":63},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nfrom typing import List\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain_core.documents import Document\n\ndef load_any(path: str) -> List[Document]:\n    p = Path(path)\n    suf = p.suffix.lower()\n    if suf == \".pdf\":\n        docs = PyPDFLoader(str(p)).load()\n    elif suf in {\".txt\", \".md\"}:\n        docs = TextLoader(str(p), encoding=\"utf-8\").load()\n    else:\n        raise ValueError(f\"Unsupported format: {suf}\")\n\n    # normalize + enrich metadata\n    out = []\n    for i, d in enumerate(docs):\n        cleaned = cleaning(d.page_content, remove_urls=True, remove_emails=True)\n        meta = {**(d.metadata or {}), \"source\": str(p), \"doc_id\": str(p.stem)}\n        out.append(Document(page_content=cleaned, metadata=meta))\n    return out\n\n# Пример: собрать корпус\n# corpus = []\n# for p in [\"report.pdf\", \"notes.txt\"]:\n#     corpus += load_any(p)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.098245Z","iopub.execute_input":"2025-11-12T19:40:29.098561Z","iopub.status.idle":"2025-11-12T19:40:29.116598Z","shell.execute_reply.started":"2025-11-12T19:40:29.098544Z","shell.execute_reply":"2025-11-12T19:40:29.115844Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"#corpus = []\n#for p in [\"report.pdf\", \"notes.txt\"]:\n#    corpus += load_any(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.117383Z","iopub.execute_input":"2025-11-12T19:40:29.117686Z","iopub.status.idle":"2025-11-12T19:40:29.136219Z","shell.execute_reply.started":"2025-11-12T19:40:29.117661Z","shell.execute_reply":"2025-11-12T19:40:29.135335Z"}},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":"## PDF ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## txt ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## anything else?","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## general cleaning","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chunking","metadata":{}},{"cell_type":"markdown","source":"## recursive ","metadata":{}},{"cell_type":"code","source":"recursive_text_splitter=RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.136983Z","iopub.execute_input":"2025-11-12T19:40:29.137186Z","iopub.status.idle":"2025-11-12T19:40:29.150339Z","shell.execute_reply.started":"2025-11-12T19:40:29.137167Z","shell.execute_reply":"2025-11-12T19:40:29.149658Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"#texts = text_splitter.split_documents(documents)\n#chunks = recursive_text_splitter.split_documents(corpus)\n#for i, d in enumerate(chunks):\n#    d.metadata[\"_chunk_id\"] = i  # критично для фьюжна/логов\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.151359Z","iopub.execute_input":"2025-11-12T19:40:29.151604Z","iopub.status.idle":"2025-11-12T19:40:29.165170Z","shell.execute_reply.started":"2025-11-12T19:40:29.151584Z","shell.execute_reply":"2025-11-12T19:40:29.164477Z"}},"outputs":[],"execution_count":67},{"cell_type":"markdown","source":"## semantic ","metadata":{}},{"cell_type":"code","source":"hf = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n\nsemantic_text_splitter = SemanticChunker(hf,\n                                breakpoint_threshold_type='percentile',\n                                breakpoint_threshold_amount=90) # chose which embeddings and breakpoint type and threshold to use\n\n\n#ПОНЯТЬ ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:29.166040Z","iopub.execute_input":"2025-11-12T19:40:29.166333Z","iopub.status.idle":"2025-11-12T19:40:31.554862Z","shell.execute_reply.started":"2025-11-12T19:40:29.166310Z","shell.execute_reply":"2025-11-12T19:40:31.554028Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"#chunks_sem = semantic_text_splitter.split_documents(corpus)\n#for i, d in enumerate(chunks_sem):\n#    d.metadata[\"_chunk_id\"] = i","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:31.555703Z","iopub.execute_input":"2025-11-12T19:40:31.555927Z","iopub.status.idle":"2025-11-12T19:40:31.559461Z","shell.execute_reply.started":"2025-11-12T19:40:31.555910Z","shell.execute_reply":"2025-11-12T19:40:31.558509Z"}},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":"# Retrival ","metadata":{}},{"cell_type":"code","source":"'''\nRetrieval\n1)\tBM25 + FIASS\n2)\tАнсамбль энкодеров\n3)\tДвуступенчатый ретривал + использование cross-encoder (next sentence prediction) \t\n4)\tДополнительная мета, как в задании на кинопоиск\n5)\tMaximal Marginal Relevance (есть в langchain) помогает разнообразную инфу добавлять в контекст\n6)\tHierarchical retrieval + родительские куски текста\n7)\tAdaptive K - выбираем порог по уверенности а не по количеству \n1)\tMulti-hop/ Self-ask - разбивает запрос на части + Self-RAG + ReAct\n2)\tChain-of-Thought prompting\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:31.560342Z","iopub.execute_input":"2025-11-12T19:40:31.560626Z","iopub.status.idle":"2025-11-12T19:40:31.607744Z","shell.execute_reply.started":"2025-11-12T19:40:31.560603Z","shell.execute_reply":"2025-11-12T19:40:31.607024Z"}},"outputs":[{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"'\\nRetrieval\\n1)\\tBM25 + FIASS\\n2)\\tАнсамбль энкодеров\\n3)\\tДвуступенчатый ретривал + использование cross-encoder (next sentence prediction) \\t\\n4)\\tДополнительная мета, как в задании на кинопоиск\\n5)\\tMaximal Marginal Relevance (есть в langchain) помогает разнообразную инфу добавлять в контекст\\n6)\\tHierarchical retrieval + родительские куски текста\\n7)\\tAdaptive K - выбираем порог по уверенности а не по количеству \\n1)\\tMulti-hop/ Self-ask - разбивает запрос на части + Self-RAG + ReAct\\n2)\\tChain-of-Thought prompting\\n'"},"metadata":{}}],"execution_count":70},{"cell_type":"markdown","source":"# Making candidates ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## bm25","metadata":{}},{"cell_type":"code","source":"#4)\tЛематизация ДЛЯ BM25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:31.608532Z","iopub.execute_input":"2025-11-12T19:40:31.608775Z","iopub.status.idle":"2025-11-12T19:40:31.626919Z","shell.execute_reply.started":"2025-11-12T19:40:31.608754Z","shell.execute_reply":"2025-11-12T19:40:31.626208Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"try:\n    import spacy\n    nlp = spacy.load(\"en_core_web_sm\")  # для RU: ru_core_news_sm\nexcept Exception:\n    nlp = None\n\ndef _tok(s: str):\n    if nlp:\n        doc = nlp(s)\n        return [t.lemma_.lower() for t in doc if not t.is_space and not t.is_punct]\n    return re.findall(r\"\\b\\w+\\b\", s.lower())\n\n#bm25_corpus_tokens = [_tok(d.page_content) for d in chunks]\n#bm25 = BM25Okapi(bm25_corpus_tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:31.628065Z","iopub.execute_input":"2025-11-12T19:40:31.628321Z","iopub.status.idle":"2025-11-12T19:40:32.230914Z","shell.execute_reply.started":"2025-11-12T19:40:31.628300Z","shell.execute_reply":"2025-11-12T19:40:32.229960Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## faiss","metadata":{}},{"cell_type":"code","source":"#5)\tIVF / HNSW / PQ (Product Quantization) + использовать батч (fiass gpu)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.231867Z","iopub.execute_input":"2025-11-12T19:40:32.232113Z","iopub.status.idle":"2025-11-12T19:40:32.235665Z","shell.execute_reply.started":"2025-11-12T19:40:32.232097Z","shell.execute_reply":"2025-11-12T19:40:32.234758Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"#embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\") # FIX\n#vs = FAISS.from_documents(chunks, embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.236488Z","iopub.execute_input":"2025-11-12T19:40:32.236728Z","iopub.status.idle":"2025-11-12T19:40:32.251298Z","shell.execute_reply.started":"2025-11-12T19:40:32.236709Z","shell.execute_reply":"2025-11-12T19:40:32.250480Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"#vs = FAISS.from_documents(chunks, hf)\n\n#faiss_path = \"faiss_index\"\n#vs.save_local(faiss_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.252069Z","iopub.execute_input":"2025-11-12T19:40:32.252385Z","iopub.status.idle":"2025-11-12T19:40:32.265543Z","shell.execute_reply.started":"2025-11-12T19:40:32.252359Z","shell.execute_reply":"2025-11-12T19:40:32.264845Z"}},"outputs":[],"execution_count":75},{"cell_type":"code","source":"def build_faiss_augmented(chunks: List[Document], emb: HuggingFaceEmbeddings) -> FAISS:\n    \"\"\"\n    Индексируем composite_text(d), но оригинал храним в metadata[\"_raw\"].\n    \"\"\"\n    aug_docs = []\n    for d in chunks:\n        aug_text = composite_text(d)\n        md = {**d.metadata, \"_raw\": d.page_content}\n        aug_docs.append(Document(page_content=aug_text, metadata=md))\n    return FAISS.from_documents(aug_docs, emb)\n\ndef to_raw(doc: Document) -> Document:\n    \"\"\"\n    Возвращаем документ с исходным текстом из metadata[\"_raw\"], если он есть.\n    \"\"\"\n    raw = doc.metadata.get(\"_raw\")\n    if raw:\n        return Document(page_content=raw, metadata=doc.metadata)\n    return doc\n\ndef to_raw_many(docs: List[Document]) -> List[Document]:\n    return [to_raw(d) for d in docs]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.266351Z","iopub.execute_input":"2025-11-12T19:40:32.266720Z","iopub.status.idle":"2025-11-12T19:40:32.281580Z","shell.execute_reply.started":"2025-11-12T19:40:32.266698Z","shell.execute_reply":"2025-11-12T19:40:32.280795Z"}},"outputs":[],"execution_count":76},{"cell_type":"code","source":"#emb = HuggingFaceEmbeddings(\n#    model_name=embeddings_model,\n#    encode_kwargs={\"normalize_embeddings\": True}  # косинус лучше себя ведёт с нормировкой\n#)\n#vs = build_faiss_augmented(chunks, emb)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.282303Z","iopub.execute_input":"2025-11-12T19:40:32.282605Z","iopub.status.idle":"2025-11-12T19:40:32.301686Z","shell.execute_reply.started":"2025-11-12T19:40:32.282581Z","shell.execute_reply":"2025-11-12T19:40:32.300944Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"### Ансамбль энкодеров","metadata":{}},{"cell_type":"code","source":"#hf1 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n#hf2 = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n#vs1 = FAISS.from_documents(chunks, hf1)\n#vs2 = FAISS.from_documents(chunks, hf2)\n\ndef vec_rrf(query, stores, base_docs, k_each=200, C=60):\n    rrf = {}\n    for store in stores:\n        res = store.similarity_search_with_score(query, k=k_each)\n        for r, (d, _) in enumerate(res):\n            cid = d.metadata[\"_chunk_id\"]\n            rrf[cid] = rrf.get(cid, 0.0) + 1.0/(C + r + 1)\n    top = sorted(rrf, key=rrf.get, reverse=True)[:50]\n    cid2doc = {d.metadata[\"_chunk_id\"]: d for d in base_docs}\n    return [cid2doc[i] for i in top]\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.302482Z","iopub.execute_input":"2025-11-12T19:40:32.302731Z","iopub.status.idle":"2025-11-12T19:40:32.318122Z","shell.execute_reply.started":"2025-11-12T19:40:32.302710Z","shell.execute_reply":"2025-11-12T19:40:32.317412Z"}},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"## fuze them","metadata":{}},{"cell_type":"code","source":"'''\ndef rrf_fusion(query: str, vs: FAISS, bm25: BM25Okapi, *,\n               bm25_k: int = 200, vec_k: int = 200, final_k: int = 10, C: int = 60):\n    # BM25 топы\n    q_tokens = _tok(query)\n    scores = bm25.get_scores(q_tokens)\n    order = np.argsort(scores)[::-1][:bm25_k]\n\n    # FAISS топы\n    vec = vs.similarity_search_with_score(query, k=vec_k)\n\n    # индексируем по _chunk_id\n    rrf = {}\n    # BM25\n    for r, idx in enumerate(order):\n        chunk_id = chunks[idx].metadata[\"_chunk_id\"]\n        rrf[chunk_id] = rrf.get(chunk_id, 0.0) + 1.0 / (C + r + 1)\n    # FAISS\n    for r, (doc, _) in enumerate(vec):\n        cid = doc.metadata.get(\"_chunk_id\")\n        if cid is not None:\n            rrf[cid] = rrf.get(cid, 0.0) + 1.0 / (C + r + 1)\n\n    top_ids = sorted(rrf, key=rrf.get, reverse=True)[:final_k]\n    cid2doc = {d.metadata[\"_chunk_id\"]: d for d in chunks}\n    return [cid2doc[i] for i in top_ids]\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.321902Z","iopub.execute_input":"2025-11-12T19:40:32.322189Z","iopub.status.idle":"2025-11-12T19:40:32.334171Z","shell.execute_reply.started":"2025-11-12T19:40:32.322162Z","shell.execute_reply":"2025-11-12T19:40:32.333488Z"}},"outputs":[{"execution_count":79,"output_type":"execute_result","data":{"text/plain":"'\\ndef rrf_fusion(query: str, vs: FAISS, bm25: BM25Okapi, *,\\n               bm25_k: int = 200, vec_k: int = 200, final_k: int = 10, C: int = 60):\\n    # BM25 топы\\n    q_tokens = _tok(query)\\n    scores = bm25.get_scores(q_tokens)\\n    order = np.argsort(scores)[::-1][:bm25_k]\\n\\n    # FAISS топы\\n    vec = vs.similarity_search_with_score(query, k=vec_k)\\n\\n    # индексируем по _chunk_id\\n    rrf = {}\\n    # BM25\\n    for r, idx in enumerate(order):\\n        chunk_id = chunks[idx].metadata[\"_chunk_id\"]\\n        rrf[chunk_id] = rrf.get(chunk_id, 0.0) + 1.0 / (C + r + 1)\\n    # FAISS\\n    for r, (doc, _) in enumerate(vec):\\n        cid = doc.metadata.get(\"_chunk_id\")\\n        if cid is not None:\\n            rrf[cid] = rrf.get(cid, 0.0) + 1.0 / (C + r + 1)\\n\\n    top_ids = sorted(rrf, key=rrf.get, reverse=True)[:final_k]\\n    cid2doc = {d.metadata[\"_chunk_id\"]: d for d in chunks}\\n    return [cid2doc[i] for i in top_ids]\\n\\n'"},"metadata":{}}],"execution_count":79},{"cell_type":"code","source":"def rrf_fusion(query: str, vs: FAISS, bm25: BM25Okapi, base_docs: List[Document],\n               bm25_k: int = 200, vec_k: int = 200, final_k: int = 10, C: int = 60):\n    q_tokens = _tok(query)\n    scores = bm25.get_scores(q_tokens)\n    order = np.argsort(scores)[::-1][:bm25_k]\n\n    vec = vs.similarity_search_with_score(query, k=vec_k)\n\n    rrf = {}\n    for r, idx in enumerate(order):\n        chunk_id = base_docs[idx].metadata[\"_chunk_id\"]\n        rrf[chunk_id] = rrf.get(chunk_id, 0.0) + 1.0/(C + r + 1)\n    for r, (doc, _) in enumerate(vec):\n        cid = doc.metadata.get(\"_chunk_id\")\n        if cid is not None:\n            rrf[cid] = rrf.get(cid, 0.0) + 1.0/(C + r + 1)\n\n    top_ids = sorted(rrf, key=rrf.get, reverse=True)[:final_k]\n    cid2doc = {d.metadata[\"_chunk_id\"]: d for d in base_docs}\n    return [cid2doc[i] for i in top_ids]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.334917Z","iopub.execute_input":"2025-11-12T19:40:32.335190Z","iopub.status.idle":"2025-11-12T19:40:32.353806Z","shell.execute_reply.started":"2025-11-12T19:40:32.335137Z","shell.execute_reply":"2025-11-12T19:40:32.353149Z"}},"outputs":[],"execution_count":80},{"cell_type":"markdown","source":"### ParentDocumentRetriever","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cross encoder","metadata":{}},{"cell_type":"markdown","source":"## Обучение кросс энкодера ","metadata":{}},{"cell_type":"code","source":"'''\nНа чем?\n1)\tОдна фраза из чанка, остальное в чанке позитив, любой другой чанк из другого документа негатив\n2)\tЗаголовок - запрос, содержащиеся в абзаце/параграфе этого заголовка - ответ, если есть структура\n3)\tЧерез Spacy получить ключевое слово в абзаце, абзац - ответ \n4)\tПрпросить ллм задать вопрос\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.354535Z","iopub.execute_input":"2025-11-12T19:40:32.354759Z","iopub.status.idle":"2025-11-12T19:40:32.370628Z","shell.execute_reply.started":"2025-11-12T19:40:32.354737Z","shell.execute_reply":"2025-11-12T19:40:32.369900Z"}},"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"'\\nНа чем?\\n1)\\tОдна фраза из чанка, остальное в чанке позитив, любой другой чанк из другого документа негатив\\n2)\\tЗаголовок - запрос, содержащиеся в абзаце/параграфе этого заголовка - ответ, если есть структура\\n3)\\tЧерез Spacy получить ключевое слово в абзаце, абзац - ответ \\n4)\\tПрпросить ллм задать вопрос\\n'"},"metadata":{}}],"execution_count":81},{"cell_type":"markdown","source":"### training cross encoder on Одна фраза из чанка, остальное в чанке позитив, любой другой чанк из другого документа негатив","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import InputExample\nfrom torch.utils.data import DataLoader\n\ndef make_training_pairs_variant1(docs: List[Document], negatives_per_pos=1):\n    # 1) \"Одна фраза из чанка\" → позитив — сам чанк; негатив — из других доков\n    import random, itertools\n    examples = []\n    by_docid = {}\n    for d in docs:\n        by_docid.setdefault(d.metadata.get(\"doc_id\",\"_\"), []).append(d)\n\n    all_docs = docs[:]\n    for d in docs:\n        sentences = re.split(r'(?<=[.!?])\\s+', d.page_content)\n        if not sentences: \n            continue\n        q = sentences[0][:200]\n        pos = d.page_content\n        # негативы из других doc_id\n        other = [x for x in all_docs if x.metadata.get(\"doc_id\") != d.metadata.get(\"doc_id\")]\n        negs = random.sample(other, k=min(negatives_per_pos, len(other)))\n        examples.append(InputExample(texts=[q, pos], label=1.0))\n        for n in negs:\n            examples.append(InputExample(texts=[q, n.page_content], label=0.0))\n    return examples\n\n#train_ex = make_training_pairs_variant1(chunks, negatives_per_pos=2)\n#dl = DataLoader(train_ex, shuffle=True, batch_size=16)\n#ce = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', num_labels=1)\n#ce.fit(train_dataloader=dl, epochs=1, warmup_steps=100, output_path=\"ce_finetuned\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.371420Z","iopub.execute_input":"2025-11-12T19:40:32.371675Z","iopub.status.idle":"2025-11-12T19:40:32.389571Z","shell.execute_reply.started":"2025-11-12T19:40:32.371652Z","shell.execute_reply":"2025-11-12T19:40:32.388881Z"}},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":"### training cross encoder on Заголовок - запрос, содержащиеся в абзаце/параграфе этого заголовка - ответ, если есть структура","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### training cross encoder on Через Spacy получить ключевое слово в абзаце, абзац - ответ ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using cross encoder","metadata":{}},{"cell_type":"code","source":"from typing import Any, List\nfrom pydantic import BaseModel, Field  # pydantic v2\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom sentence_transformers import CrossEncoder\n\n#cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device='cuda' if torch.cuda.is_available() else 'cpu')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.390314Z","iopub.execute_input":"2025-11-12T19:40:32.390930Z","iopub.status.idle":"2025-11-12T19:40:32.404583Z","shell.execute_reply.started":"2025-11-12T19:40:32.390908Z","shell.execute_reply":"2025-11-12T19:40:32.403672Z"}},"outputs":[],"execution_count":83},{"cell_type":"markdown","source":"### adaptive k + MMR(?)","metadata":{}},{"cell_type":"code","source":"# MMR (diversity): сначала — кандидаты\n#mmr_docs = vs.max_marginal_relevance_search(query, k=10, fetch_k=60, lambda_mult=0.5)\ndef retrieve_mmr(vs, query, k=10, fetch_k=60, lambda_mult=0.5):\n    return vs.max_marginal_relevance_search(query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult)\n\n\n# Adaptive-K: возьми побольше кандидатов → пропусти через cross-encoder → оставь все с score >= τ\ndef adaptive_k(query, vs, ce, fetch_k=60, tau=0.35):\n    cands = vs.similarity_search(query, k=fetch_k)\n    scores = ce.predict([[query, d.page_content] for d in cands])\n    picked = [d for d, s in sorted(zip(cands, scores), key=lambda x: x[1], reverse=True) if s >= tau]\n    return picked or cands[:5]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.405301Z","iopub.execute_input":"2025-11-12T19:40:32.405597Z","iopub.status.idle":"2025-11-12T19:40:32.425677Z","shell.execute_reply.started":"2025-11-12T19:40:32.405572Z","shell.execute_reply":"2025-11-12T19:40:32.424982Z"}},"outputs":[],"execution_count":84},{"cell_type":"code","source":"def retrieve_then_rerank(query: str, base_docs: List[Document], vs: FAISS, bm25: BM25Okapi,\n                         ce: CrossEncoder, pre_k: int = 60, final_k: int = 10,\n                         mmr_fetch_k: int = 180, mmr_lambda: float = 0.5) -> List[Document]:\n    # 1) RRF (BM25 + FAISS)\n    cands = rrf_fusion(query, vs, bm25, base_docs=base_docs, final_k=pre_k)\n\n    # 1b) MMR-альтернатива из FAISS → привести к base_docs и дедуп\n    try:\n        mmr_alt_raw = retrieve_mmr(vs, query, k=min(pre_k, 50),\n                                   fetch_k=min(mmr_fetch_k, 3*pre_k), lambda_mult=mmr_lambda)\n        mmr_alt = map_to_base(mmr_alt_raw, base_docs)\n        cands = dedup_by_chunk_id(cands + mmr_alt)[:pre_k]\n    except Exception:\n        pass\n\n    # 2) CE-rerank\n    pairs  = [[query, d.page_content] for d in cands]\n    scores = ce.predict(pairs)\n    ranked = [d for d, s in sorted(zip(cands, scores), key=lambda x: float(x[1]), reverse=True)]\n    return ranked[:final_k]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.426408Z","iopub.execute_input":"2025-11-12T19:40:32.426681Z","iopub.status.idle":"2025-11-12T19:40:32.442505Z","shell.execute_reply.started":"2025-11-12T19:40:32.426666Z","shell.execute_reply":"2025-11-12T19:40:32.441635Z"}},"outputs":[],"execution_count":85},{"cell_type":"markdown","source":"# Generation ","metadata":{}},{"cell_type":"code","source":"'''\n1)\tРолевая инструкция (“Ты эксперт…”) + добавить конкретики\n2)\tЧёткая структура вывода\n3)\tКонтекст всегда подаётся раньше вопроса\n4)\tАнсамбль промптов + переписать промпт ллм НАЙТИ ПРОМПТЫ \n5)\t\n6)\tПросить оценить свой ответ перед выводом\n7)\tFew-shot\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.443566Z","iopub.execute_input":"2025-11-12T19:40:32.443805Z","iopub.status.idle":"2025-11-12T19:40:32.461510Z","shell.execute_reply.started":"2025-11-12T19:40:32.443788Z","shell.execute_reply":"2025-11-12T19:40:32.460745Z"}},"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"'\\n1)\\tРолевая инструкция (“Ты эксперт…”) + добавить конкретики\\n2)\\tЧёткая структура вывода\\n3)\\tКонтекст всегда подаётся раньше вопроса\\n4)\\tАнсамбль промптов + переписать промпт ллм НАЙТИ ПРОМПТЫ \\n5)\\t\\n6)\\tПросить оценить свой ответ перед выводом\\n7)\\tFew-shot\\n'"},"metadata":{}}],"execution_count":86},{"cell_type":"code","source":"'''\nGeneration\n1)\tРазбить важные куски по краям\n2)\tСуммаризация контекста\n3)\tПросить указывать источник каждого факта\n4)\tSelf-consistency sampling - посемплить несколько ответов с разным сидом и температурой, разным порядком, выбрать самый часто встречающийся ллмкой\n5)\tОтдельная LLM проверяет соответствие ответ-контекст\n\nи на финальную модель скоринга тоже ллм\nтипа даешь запрос даешь ответ и оцениваешь на сколько хорошо раз справился с задачей\n\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.462213Z","iopub.execute_input":"2025-11-12T19:40:32.462413Z","iopub.status.idle":"2025-11-12T19:40:32.476886Z","shell.execute_reply.started":"2025-11-12T19:40:32.462399Z","shell.execute_reply":"2025-11-12T19:40:32.476251Z"}},"outputs":[{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"'\\nGeneration\\n1)\\tРазбить важные куски по краям\\n2)\\tСуммаризация контекста\\n3)\\tПросить указывать источник каждого факта\\n4)\\tSelf-consistency sampling - посемплить несколько ответов с разным сидом и температурой, разным порядком, выбрать самый часто встречающийся ллмкой\\n5)\\tОтдельная LLM проверяет соответствие ответ-контекст\\n\\nи на финальную модель скоринга тоже ллм\\nтипа даешь запрос даешь ответ и оцениваешь на сколько хорошо раз справился с задачей\\n\\n'"},"metadata":{}}],"execution_count":87},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n\ndef build_prompt(q: str, ctx_docs: List[Document]):\n    ctx = \"\\n\\n\".join([f\"[S{i+1}] {d.page_content}\" for i, d in enumerate(ctx_docs)])\n    return f\"\"\"You are a careful, citation-first QA assistant.\nUse ONLY the snippets [S1..S{len(ctx_docs)}]. Cite as [S#] after each sentence.\nIf not supported, say \"I don't know\".\n\nCONTEXT:\n{ctx}\n\nQUESTION: {q}\n\nReturn JSON with keys: answer (string), citations (list of [S#]), confidence (0..1).\"\"\"\n\n'''\n# HF pipeline (надёжно на CPU/GPU)\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\nGEN_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\ntok = AutoTokenizer.from_pretrained(GEN_MODEL, use_fast=True)\nmdl = AutoModelForCausalLM.from_pretrained(GEN_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\ngen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n\ndef answer(query, docs):\n    prompt, _ = build_prompt(query, docs, tok)\n    out = gen(prompt, max_new_tokens=300, temperature=0.2, top_p=0.9)[0][\"generated_text\"]\n    return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:32.477698Z","iopub.execute_input":"2025-11-12T19:40:32.477939Z","iopub.status.idle":"2025-11-12T19:40:35.309240Z","shell.execute_reply.started":"2025-11-12T19:40:32.477921Z","shell.execute_reply":"2025-11-12T19:40:35.308445Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":88},{"cell_type":"code","source":"import re, json, math, random\nfrom typing import List, Dict, Any\nfrom langchain_core.documents import Document\n\n# --- токены и обрезка ---\ndef token_len(text: str, tok) -> int:\n    return len(tok(text, add_special_tokens=False).input_ids)\n\ndef truncate_by_tokens(text: str, tok, max_tokens: int) -> str:\n    ids = tok(text, add_special_tokens=False, truncation=True, max_length=max_tokens).input_ids\n    return tok.decode(ids, skip_special_tokens=True)\n\n# --- упаковка контекста в бюджет по токенам ---\ndef pack_context(docs: List[Document], tok, max_ctx_tokens: int = 2500, max_per_snippet: int = 400):\n    packed, used = [], 0\n    for i, d in enumerate(docs, 1):\n        # если есть сжатие — используем его, иначе сырой текст\n        body_raw = d.metadata.get(\"_summary_text\") or d.page_content\n        body = body_raw.strip()\n        body = truncate_by_tokens(body, tok, max_per_snippet)\n        tag = f\"[S{i}]\"\n        src = d.metadata.get(\"source\", \"?\")\n        pg  = d.metadata.get(\"page\", d.metadata.get(\"page_number\", d.metadata.get(\"_page\", \"?\")))\n        header = f\"{tag} ({src} p.{pg})\"\n        chunk = f\"{header}\\n{body}\"\n        t = token_len(chunk, tok)\n        if used + t > max_ctx_tokens:\n            break\n        packed.append(chunk); used += t\n    return \"\\n\\n\".join(packed), len(packed)\n\n\n# --- перестановка: «важные по краям» ---\ndef reorder_edge(docs: List[Document]) -> List[Document]:\n    # если есть score в metadata — предварительно отсортируем\n    ranked = sorted(docs, key=lambda d: d.metadata.get(\"score\", 0), reverse=True)\n    left, right, out = 0, len(ranked)-1, []\n    while left <= right:\n        out.append(ranked[left]); left += 1\n        if left <= right:\n            out.append(ranked[right]); right -= 1\n    return out\n\n# --- парс JSON из вывода модели (робастно) ---\ndef extract_json(text: str) -> Dict[str, Any]:\n    # находим последний JSON-блок\n    m = list(re.finditer(r\"\\{[\\s\\S]*\\}\", text))\n    if not m: return {}\n    block = m[-1].group(0)\n    try:\n        return json.loads(block)\n    except Exception:\n        # лёгкий фикс запятых/кавычек не делаем агрессивно — лучше упасть в безопасный режим\n        return {}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.310068Z","iopub.execute_input":"2025-11-12T19:40:35.310280Z","iopub.status.idle":"2025-11-12T19:40:35.320207Z","shell.execute_reply.started":"2025-11-12T19:40:35.310263Z","shell.execute_reply":"2025-11-12T19:40:35.319472Z"}},"outputs":[],"execution_count":89},{"cell_type":"code","source":"# Базовая ролевая инструкция (варианты стиля)\nPROMPT_STYLES = {\n    \"concise\": \"Answer concisely, directly, with exact citations after each sentence.\",\n    \"teacher\": \"Explain clearly as to an advanced student, but keep each sentence short and cite precisely.\",\n    \"lawyer\":  \"Be rigorous and conservative; include a citation [S#] after any factual claim.\",\n    \"scientist\":\"Be cautious, quantify uncertainty, and avoid claims not supported by the snippets.\"\n}\n\n# Few-shot примеры (минимальные, citation-first, JSON)\nFEW_SHOTS = [\n    {\n        \"ctx\": \"[S1] The sky appears blue due to Rayleigh scattering.\\n[S2] Rayleigh scattering affects shorter wavelengths.\",\n        \"q\":   \"Why does the sky look blue?\",\n        \"a_json\": {\"answer\": \"The sky appears blue because shorter wavelengths are scattered more strongly by air molecules [S1][S2].\",\n                   \"citations\": [\"S1\",\"S2\"], \"confidence\": 0.84}\n    }\n]\n\ndef render_few_shots():\n    parts = []\n    for ex in FEW_SHOTS:\n        parts.append(\nf\"\"\"EXAMPLE\nCONTEXT:\n{ex['ctx']}\n\nQUESTION: {ex['q']}\n\nOUTPUT (JSON):\n{json.dumps(ex['a_json'], ensure_ascii=False)}\n\"\"\")\n    return \"\\n\".join(parts)\n\ndef build_prompt(q: str, ctx_docs: List[Document], tok,\n                 style: str = \"concise\",\n                 use_few_shot: bool = True,\n                 max_ctx_tokens: int = 2500,\n                 max_per_snippet: int = 400):\n    ctx_docs = reorder_edge(ctx_docs)\n    ctx_block, used_n = pack_context(ctx_docs, tok, max_ctx_tokens, max_per_snippet)\n    style_txt = PROMPT_STYLES.get(style, PROMPT_STYLES[\"concise\"])\n    few = render_few_shots() if use_few_shot and FEW_SHOTS else \"\"\n\n    prompt = f\"\"\"You are a careful, citation-first QA assistant.\n{style_txt}\nUse ONLY the snippets [S1..S{used_n}]. Every factual sentence MUST end with one or more citations like [S#].\nIf the answer is not fully supported, say \"I don't know\".\nReturn your output STRICTLY as a single JSON object wrapped in <JSON>...</JSON> tags. \nDo not include code fences, explanations, or any extra text.\n\n{few}\nCONTEXT:\n{ctx_block}\n\nQUESTION: {q}\n\nFormat exactly:\n<JSON>{{\"answer\": string, \"citations\": [\"S#\"...], \"confidence\": number between 0 and 1}}</JSON>\"\"\"\n    return prompt, used_n\n\n\ndef llm_generate_text(prompt: str, *, max_new: int = 512, temp: float = 0.0, top_p: float = 0.9) -> str:\n    # Предпочтительно — chat template (Qwen-Instruct слушается лучше)\n    if hasattr(tok, \"apply_chat_template\"):\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a precise assistant that outputs ONLY JSON wrapped in <JSON> tags.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ]\n        input_ids = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(mdl.device)\n        # ✅ явная маска\n        import torch\n        attention_mask = torch.ones_like(input_ids)\n\n        gen_kwargs = {\"max_new_tokens\": max_new, \"do_sample\": (temp > 0.0)}\n        if temp > 0.0:\n            gen_kwargs[\"temperature\"] = temp\n            gen_kwargs[\"top_p\"] = top_p\n\n        with torch.no_grad():\n            out_ids = mdl.generate(input_ids=input_ids, attention_mask=attention_mask, **gen_kwargs)\n        return tok.decode(out_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n\n    # Fallback: pipeline, но без temperature при temp=0.0\n    kwargs = {\"max_new_tokens\": max_new, \"do_sample\": (temp > 0.0), \"return_full_text\": False}\n    if temp > 0.0:\n        kwargs[\"temperature\"] = temp\n        kwargs[\"top_p\"] = top_p\n    return gen(prompt, **kwargs)[0][\"generated_text\"]\n\n\ndef extract_json(text: str) -> dict:\n    import json, re\n    # 1) <JSON>{...}</JSON>\n    m = re.findall(r\"<JSON>\\s*(\\{[\\s\\S]*?\\})\\s*</JSON>\", text)\n    if m:\n        try:\n            return json.loads(m[-1])\n        except Exception:\n            pass\n    # 2) ```json ... ```\n    m = re.findall(r\"```json\\s*(\\{[\\s\\S]*?\\})\\s*```\", text, flags=re.IGNORECASE)\n    if m:\n        try:\n            return json.loads(m[-1])\n        except Exception:\n            pass\n    # 3) last {...} block (rough)\n    m = list(re.finditer(r\"\\{[\\s\\S]*\\}\", text))\n    if m:\n        block = m[-1].group(0)\n        try:\n            return json.loads(block)\n        except Exception:\n            return {}\n    return {}\n\n\ndef generate_one(query: str, docs: List[Document], gen, tok,\n                 style=\"concise\", temp=0.0, top_p=0.9, max_new=512, seed=None):\n    if seed is not None:\n        torch.manual_seed(seed)\n        if torch.cuda.is_available():\n            torch.cuda.manual_seed_all(seed)\n    prompt, n_ctx = build_prompt(query, docs, tok, style=style)\n    out_text = llm_generate_text(prompt, max_new=max_new, temp=temp, top_p=top_p)\n    data = extract_json(out_text)\n    return data, out_text, n_ctx\n\n\ndef naive_fallback_answer(query: str, docs: List[Document]) -> dict:\n    # минимально осмысленный ответ из первого сниппета, чтобы не было пустых строк\n    if not docs:\n        return {\"answer\": \"I don't know.\", \"citations\": [], \"confidence\": 0.0}\n    body = (docs[0].metadata.get(\"_summary_text\") or docs[0].page_content or \"\").strip()\n    sents = re.split(r'(?<=[.!?])\\s+', body)\n    ans = \" \".join(sents[:2]).strip()\n    ans = (ans[:500] + \"…\") if len(ans) > 500 else ans\n    if not ans:\n        ans = \"I don't know.\"\n    return {\"answer\": f\"{ans} [S1]\", \"citations\": [\"S1\"], \"confidence\": 0.35}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.321190Z","iopub.execute_input":"2025-11-12T19:40:35.321650Z","iopub.status.idle":"2025-11-12T19:40:35.377251Z","shell.execute_reply.started":"2025-11-12T19:40:35.321628Z","shell.execute_reply":"2025-11-12T19:40:35.376490Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def citation_stats(ans_json: Dict[str, Any], n_ctx: int) -> Dict[str, Any]:\n    txt = (ans_json or {}).get(\"answer\", \"\") or \"\"\n    # найдём все [S#]\n    refs = re.findall(r\"\\[S(\\d+)\\]\", txt)\n    refs = [int(x) for x in refs if x.isdigit()]\n    valid = [r for r in refs if 1 <= r <= n_ctx]\n    coverage = 1.0 if txt.strip()==\"\" else (1.0 if len(valid)>0 else 0.0)  # минимум: есть ли цитаты\n    distinct = sorted(set(valid))\n    return {\n        \"has_json\": bool(ans_json),\n        \"has_answer\": bool(txt.strip()),\n        \"refs_total\": len(refs),\n        \"refs_valid\": len(valid),\n        \"refs_distinct\": distinct,\n        \"coverage\": coverage,\n    }\n\n# Опционально: скор поддержки через CrossEncoder (если есть)\ndef support_score_ce(query: str, ans_json: Dict[str, Any], docs: List[Document], cross_encoder=None) -> float:\n    if not cross_encoder or not ans_json: \n        return 0.0\n    txt = ans_json.get(\"answer\", \"\") or \"\"\n    if not txt.strip():\n        return 0.0\n    # возьмём процитированные сниппеты, склеим\n    cited_ids = set(int(m) for m in re.findall(r\"\\[S(\\d+)\\]\", txt) if m.isdigit())\n    if not cited_ids:\n        return 0.0\n    ctx_text = \"\\n\".join([docs[i-1].page_content for i in sorted(cited_ids) if 1 <= i <= len(docs)])\n    # пара: (вопрос + ответ) vs (объединённый контекст)\n    pair = [[f\"{query}\\n\\nANSWER:\\n{txt}\", ctx_text]]\n    sc = float(cross_encoder.predict(pair)[0])  # 0..1-ish\n    return sc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.378361Z","iopub.execute_input":"2025-11-12T19:40:35.378663Z","iopub.status.idle":"2025-11-12T19:40:35.399792Z","shell.execute_reply.started":"2025-11-12T19:40:35.378639Z","shell.execute_reply":"2025-11-12T19:40:35.398927Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"def score_candidate(query, docs, ans_json, n_ctx, cross_encoder=None, w_conf=0.3, w_cit=0.3, w_ce=0.4):\n    cs = citation_stats(ans_json, n_ctx)\n    conf = float(ans_json.get(\"confidence\", 0.0)) if ans_json else 0.0\n    ce  = support_score_ce(query, ans_json, docs, cross_encoder) if cross_encoder else 0.0\n    # простая агрегация\n    s = w_conf*conf + w_cit*(1.0 if cs[\"refs_valid\"]>0 else 0.0) + w_ce*ce\n    return s, {\"conf\": conf, \"cit_ok\": cs[\"refs_valid\"]>0, \"ce\": ce, \"stats\": cs}\n\ndef self_consistency_generate(query: str, docs: List[Document], gen, tok, cross_encoder=None,\n                              rounds: int = 5, styles=(\"concise\",\"teacher\",\"lawyer\",\"scientist\"),\n                              temps=(0.2, 0.35)):\n    cands = []\n    for r in range(rounds):\n        style = random.choice(styles)\n        temp  = random.choice(temps)\n        seed  = random.randrange(10**9)\n        ret = generate_one(query, docs, gen, tok, style=style, temp=temp, seed=seed)\n\n        # Универсальный разбор результата\n        if isinstance(ret, tuple):\n            if len(ret) >= 3:\n                ans_json, raw, n_ctx = ret[0], ret[1], ret[2]\n            else:\n                raise ValueError(f\"generate_one returned tuple of len {len(ret)}\")\n        elif isinstance(ret, dict):\n            ans_json, raw, n_ctx = ret.get(\"json\", {}), ret.get(\"raw\", \"\"), int(ret.get(\"n_ctx\", 0))\n        else:\n            # совсем неожиданный случай\n            ans_json, raw, n_ctx = {}, str(ret), 0\n\n        score, info = score_candidate(query, docs, ans_json, n_ctx, cross_encoder=cross_encoder)\n        cands.append({\n            \"json\": ans_json, \"raw\": raw, \"style\": style, \"temp\": temp, \"seed\": seed,\n            \"score\": score, \"info\": info, \"n_ctx\": n_ctx\n        })\n    cands.sort(key=lambda x: x[\"score\"], reverse=True)\n    best = cands[0] if cands else {\"json\": {}, \"raw\": \"\", \"score\": 0.0}\n    return best, cands\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.400675Z","iopub.execute_input":"2025-11-12T19:40:35.401004Z","iopub.status.idle":"2025-11-12T19:40:35.421390Z","shell.execute_reply.started":"2025-11-12T19:40:35.400976Z","shell.execute_reply":"2025-11-12T19:40:35.420607Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"def build_judge_prompt(query: str, ctx_docs: List[Document], ans_json: Dict[str,Any], tok):\n    ctx_block, used_n = pack_context(ctx_docs, tok, max_ctx_tokens=1800, max_per_snippet=300)\n    ans = json.dumps(ans_json, ensure_ascii=False)\n    return f\"\"\"You are a strict judge of factual alignment.\nRead CONTEXT and the proposed ANSWER (JSON). \nScore alignment from 0.0 to 1.0 where 1.0 = fully supported, 0.0 = unsupported or hallucinated.\nRespond with ONLY a JSON: {{\"score\": number, \"reason\": string}}.\n\nCONTEXT:\n{ctx_block}\n\nANSWER:\n{ans}\n\"\"\"\n\ndef judge_score(query, docs, ans_json, gen, tok):\n    jp = build_judge_prompt(query, docs, ans_json, tok)\n    # ✅ temp=0.0 -> do_sample=False, без ошибки\n    out = llm_generate_text(jp, max_new=200, temp=0.0, top_p=1.0)\n    jd = extract_json(out)\n    sc = float(jd.get(\"score\", 0.0)) if isinstance(jd, dict) else 0.0\n    return sc, jd\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.422271Z","iopub.execute_input":"2025-11-12T19:40:35.422561Z","iopub.status.idle":"2025-11-12T19:40:35.438678Z","shell.execute_reply.started":"2025-11-12T19:40:35.422535Z","shell.execute_reply":"2025-11-12T19:40:35.437900Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"def generate_answer(query: str, ctx_docs: List[Document], gen, tok,\n                    cross_encoder=None, use_judge=False):\n    best, all_cands = self_consistency_generate(\n        query, ctx_docs, gen, tok,\n        cross_encoder=cross_encoder,\n        rounds=3,                    # можно 2–3 для скорости/стабильности\n        styles=(\"concise\",\"teacher\"),\n        temps=(0.0, 0.2)\n    )\n    ans_json = best.get(\"json\") or {}\n\n    # --- Fallback if model failed or answer empty ---\n    if not isinstance(ans_json, dict) or not ans_json.get(\"answer\"):\n        ans_json = naive_fallback_answer(query, ctx_docs)\n\n    judge = {}\n    if use_judge and ans_json:\n        sc, judge = judge_score(query, ctx_docs, ans_json, gen, tok)\n        if \"confidence\" in ans_json:\n            ans_json[\"confidence\"] = float(max(0.0, min(1.0, 0.5*ans_json[\"confidence\"] + 0.5*sc)))\n\n    # Финальная чистка ссылок\n    n_ctx = best.get(\"n_ctx\", len(ctx_docs))\n    if \"citations\" in ans_json and isinstance(ans_json[\"citations\"], list):\n        ans_json[\"citations\"] = [c for c in ans_json[\"citations\"]\n                                 if isinstance(c, str) and re.fullmatch(r\"S([1-9]\\d*)\", c) and 1 <= int(c[1:]) <= n_ctx]\n        ans_json[\"citations\"] = sorted(set(ans_json[\"citations\"]), key=lambda x: int(x[1:]))\n\n    return {\n        \"answer_json\": ans_json,\n        \"best_raw\": best.get(\"raw\",\"\"),\n        \"score_internal\": best.get(\"score\", 0.0),\n        \"meta\": {\"style\": best.get(\"style\"), \"temp\": best.get(\"temp\"), \"seed\": best.get(\"seed\")},\n        \"judge\": judge,\n        \"candidates\": all_cands,\n    }\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.439594Z","iopub.execute_input":"2025-11-12T19:40:35.439928Z","iopub.status.idle":"2025-11-12T19:40:35.458584Z","shell.execute_reply.started":"2025-11-12T19:40:35.439904Z","shell.execute_reply":"2025-11-12T19:40:35.457711Z"}},"outputs":[],"execution_count":94},{"cell_type":"markdown","source":"### Summarization context","metadata":{}},{"cell_type":"code","source":"def build_chunk_summary_prompt(text: str, *, target_sentences: int = 4,\n                               mode: str = \"neutral\", query: str | None = None):\n    \"\"\"\n    mode: \"neutral\" — сжать главное; \"query\" — выделить релевантное запросу\n    Возвращаем JSON: {\"summary\": \"...\", \"bullets\": [\"...\"], \"salient_terms\": [\"...\"]}\n    \"\"\"\n    if mode == \"query\" and query:\n        return f\"\"\"You compress passages for retrieval-augmented QA.\n\nRULES:\n- Use ONLY the given PASSAGE. No external knowledge.\n- Extract the {target_sentences} most important facts RELEVANT to the QUERY.\n- Quote exact numbers/names; avoid speculation.\n- Keep neutral and factual.\n- Return ONLY JSON: {{\"summary\": string, \"bullets\": [string,...], \"salient_terms\": [string,...]}}\n\nQUERY:\n{query}\n\nPASSAGE:\n{text}\n\"\"\"\n    else:\n        return f\"\"\"You compress passages for retrieval-augmented QA.\n\nRULES:\n- Use ONLY the given PASSAGE. No external knowledge.\n- Extract the {target_sentences} most important facts.\n- Quote exact numbers/names; avoid speculation.\n- Keep neutral and factual.\n- Return ONLY JSON: {{\"summary\": string, \"bullets\": [string,...], \"salient_terms\": [string,...]}}\n\nPASSAGE:\n{text}\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.459483Z","iopub.execute_input":"2025-11-12T19:40:35.459776Z","iopub.status.idle":"2025-11-12T19:40:35.477279Z","shell.execute_reply.started":"2025-11-12T19:40:35.459754Z","shell.execute_reply":"2025-11-12T19:40:35.476611Z"}},"outputs":[],"execution_count":95},{"cell_type":"code","source":"from typing import List, Dict, Any\n\ndef summarize_docs(docs: List[Document], gen, tok, *,\n                   mode: str = \"neutral\", query: str | None = None,\n                   max_input_tokens: int = 1500,      # обрежем слишком длинные входы\n                   target_summary_tokens: int = 160,   # ~ длина сжатия\n                   target_sentences: int = 4,\n                   batch_size: int = 6,\n                   only_if_longer_than: int = 500      # сжимать только длинные чанки (по токенам)\n                   ) -> List[Document]:\n    \"\"\"\n    Генерит сжатия и кладёт их в d.metadata[\"_summary_text\"], d.metadata[\"_summary_tokens\"].\n    Ничего не затирает — исходный текст остаётся в page_content.\n    \"\"\"\n    # 1) подготовим промпты\n    prompts, ix = [], []\n    for i, d in enumerate(docs):\n        tlen = token_len(d.page_content, tok)\n        if tlen < only_if_longer_than:\n            # короткие оставляем как есть\n            continue\n        passage = truncate_by_tokens(d.page_content, tok, max_input_tokens)\n        p = build_chunk_summary_prompt(passage, target_sentences=target_sentences,\n                                       mode=(\"query\" if (mode==\"query\" and query) else \"neutral\"),\n                                       query=query)\n        prompts.append(p); ix.append(i)\n\n    # 2) батчевой прогон\n    outputs: List[str] = []\n    for b in range(0, len(prompts), batch_size):\n        batch = prompts[b:b+batch_size]\n        if not batch: break\n        #gen_out = gen(batch, max_new_tokens=220, temperature=0.1, top_p=0.9)\n        gen_out = gen(batch, max_new_tokens=220, temperature=0.1, top_p=0.9)\n        for item in gen_out:\n            item0 = item[0] if isinstance(item, list) else item\n            outputs.append(item0[\"generated_text\"])\n        # HF pipeline вернёт список; достаём тексты по порядку\n        \n\n    # 3) распарсим JSON, соберём финальное сжатие и положим в metadata\n    for j, raw in enumerate(outputs):\n        i_doc = ix[j]\n        d = docs[i_doc]\n        jd = extract_json(raw)\n        # fallback, если модель не вернула JSON\n        summary = (jd.get(\"summary\") if isinstance(jd, dict) else \"\") or \"\"\n        bullets = (jd.get(\"bullets\") if isinstance(jd, dict) else None)\n        if isinstance(bullets, list):\n            bullets = [str(x).strip() for x in bullets if str(x).strip()]\n        else:\n            bullets = []\n\n        # сборка компактного текста-сжатия\n        #  - короткий summary\n        #  - 3–5 буллетов\n        #  - без фантазий: всё только из PASSAGE\n        summ_text = summary.strip()\n        if bullets:\n            bl = \"\\n\".join(f\"• {b}\" for b in bullets[:6])\n            summ_text = (summ_text + (\"\\n\" if summ_text else \"\") + bl).strip()\n\n        # ограничим итог по токенам\n        if summ_text:\n            summ_text = truncate_by_tokens(summ_text, tok, target_summary_tokens)\n            d.metadata[\"_summary_text\"] = summ_text\n            d.metadata[\"_summary_tokens\"] = token_len(summ_text, tok)\n        else:\n            # если не получилось — не пишем summary, будет использован оригинал\n            pass\n\n    return docs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.478234Z","iopub.execute_input":"2025-11-12T19:40:35.478565Z","iopub.status.idle":"2025-11-12T19:40:35.496325Z","shell.execute_reply.started":"2025-11-12T19:40:35.478541Z","shell.execute_reply":"2025-11-12T19:40:35.495587Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using everything ","metadata":{}},{"cell_type":"code","source":"def map_to_base(docs_vs: List[Document], base_docs: List[Document]) -> List[Document]:\n    pool = {d.metadata.get(\"_chunk_id\"): d for d in base_docs}\n    out = []\n    for d in docs_vs:\n        cid = d.metadata.get(\"_chunk_id\")\n        if cid in pool:\n            out.append(pool[cid])\n    return dedup_by_chunk_id(out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.497848Z","iopub.execute_input":"2025-11-12T19:40:35.498102Z","iopub.status.idle":"2025-11-12T19:40:35.513973Z","shell.execute_reply.started":"2025-11-12T19:40:35.498085Z","shell.execute_reply":"2025-11-12T19:40:35.513224Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"# ===== CE training pairs (вариант 1) =====\nfrom sentence_transformers import InputExample, CrossEncoder\nfrom torch.utils.data import DataLoader\n\ndef make_training_pairs_variant1(docs: List[Document], negatives_per_pos=1):\n    import random\n    examples = []\n    all_docs = docs[:]\n    for d in docs:\n        sentences = re.split(r'(?<=[.!?])\\s+', d.page_content)\n        if not sentences:\n            continue\n        q = sentences[0][:200]\n        pos = d.page_content\n        other = [x for x in all_docs if x.metadata.get(\"doc_id\") != d.metadata.get(\"doc_id\")]\n        negs = random.sample(other, k=min(negatives_per_pos, len(other))) if other else []\n        examples.append(InputExample(texts=[q, pos], label=1.0))\n        for n in negs:\n            examples.append(InputExample(texts=[q, n.page_content], label=0.0))\n    return examples\n\n\n# ===== CONFIG-DRIVEN ORCHESTRATOR (patched) =====\nfrom dataclasses import dataclass, asdict\nfrom typing import List, Dict, Any, Optional\nfrom pathlib import Path\nimport time\nimport numpy as np\n\n@dataclass\nclass RAGConfig:\n    files: List[str]\n    query: str\n\n    # Структура/чанкинг\n    use_outline: bool = True\n    use_semantic_chunking: bool = False\n    chunk_size: int = 800\n    chunk_overlap: int = 120\n\n    # Индексы/ретривал\n    use_bm25: bool = True\n    use_faiss: bool = True\n    use_rrf: bool = True\n    use_mmr: bool = True\n    pre_k: int = 60\n    final_k: int = 10\n\n    # Сжатие\n    use_summarize: bool = True\n    summarize_min_tokens: int = 500\n\n    # Cross-Encoder: инференс\n    use_ce: bool = True\n    ce_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n    adaptive_tau: Optional[float] = None  # порог для CE, если задан — включается адаптивный отбор\n\n    # Cross-Encoder: тренировка (необязательно)\n    train_ce: bool = False\n    ce_train_output_path: str = \"ce_finetuned\"\n    ce_train_epochs: int = 1\n    ce_train_batch_size: int = 16\n    ce_train_warmup_steps: int = 100\n    ce_negatives_per_pos: int = 2\n\n    # Эмбеддинги\n    embeddings_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    normalize_embeddings: bool = True\n\n    # Генерация/судья\n    use_judge: bool = True\n\ndef _log(msg: str):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")\n\ndef _rank_with_ce(query: str, cands: List[Document], ce: CrossEncoder, final_k: int, adaptive_tau: Optional[float]):\n    \"\"\"Единый шаблон ранжирования с CE + опциональный adaptive_tau.\"\"\"\n    if not cands:\n        return []\n    pairs  = [[query, d.page_content] for d in cands]\n    scores = ce.predict(pairs, batch_size=64)\n    ranked_pairs = sorted(zip(cands, map(float, scores)), key=lambda x: x[1], reverse=True)\n    if adaptive_tau is not None:\n        picked = [(d, s) for d, s in ranked_pairs if s >= adaptive_tau]\n        chosen = picked if picked else ranked_pairs[:final_k]\n    else:\n        chosen = ranked_pairs[:final_k]\n    return [d for d, _ in chosen]\n\ndef run_with_config(cfg: RAGConfig) -> Dict[str, Any]:\n    _log(\"Start RAG run\")\n    _log(\"Config: \" + json.dumps({k:v for k,v in asdict(cfg).items() if k not in {'query'}}, ensure_ascii=False))\n\n    # === 1) Load corpus ===\n    _log(\"Loading corpus...\")\n    corpus: List[Document] = []\n    for p in cfg.files:\n        if not Path(p).exists():\n            raise FileNotFoundError(f\"File not found: {p}\")\n        corpus += load_any(p)\n    assert corpus, \"Corpus is empty — проверь пути к файлам.\"\n\n    # === 2) Chunk ===\n    _log(f\"Chunking: {'Semantic' if cfg.use_semantic_chunking else 'Recursive'}\")\n    if cfg.use_semantic_chunking:\n        _emb_tmp = HuggingFaceEmbeddings(model_name=cfg.embeddings_model)\n        semantic_text_splitter = SemanticChunker(_emb_tmp,\n                           breakpoint_threshold_type='percentile',\n                           breakpoint_threshold_amount=90)\n        chunks: List[Document] = semantic_text_splitter.split_documents(corpus)\n    else:\n        rc = RecursiveCharacterTextSplitter(\n            chunk_size=cfg.chunk_size, chunk_overlap=cfg.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        chunks: List[Document] = rc.split_documents(corpus)\n    for i, d in enumerate(chunks):\n        d.metadata[\"_chunk_id\"] = i\n\n    # === 3) Outline & structure ===\n    if cfg.use_outline:\n        _log(\"Building outline & attaching structure...\")\n        outlines = build_outline_from_corpus(corpus)\n        chunks = attach_structure_to_chunks(chunks, outlines)\n        chunks = [label_block_type(d) for d in chunks]\n        structure_report(chunks)\n    else:\n        _log(\"Outline skipped\")\n        chunks = [label_block_type(d) for d in chunks]\n\n    # === 4) Dedup ===\n    _log(\"Dedup chunks by text...\")\n    seen, uniq = set(), []\n    for d in chunks:\n        key = re.sub(r\"\\s+\", \" \", d.page_content.strip().lower())[:2000]\n        if key in seen:\n            continue\n        seen.add(key); uniq.append(d)\n    chunks = uniq\n    _log(f\"Chunks after dedup: {len(chunks)}\")\n\n    # === 5) Build indices ===\n    bm25_inst = None\n    vs = None\n    if cfg.use_bm25:\n        _log(\"Building BM25...\")\n        bm25_inst = BM25Okapi([_tok_with_heading(d) for d in chunks])\n    if cfg.use_faiss:\n        _log(\"Building FAISS...\")\n        emb = HuggingFaceEmbeddings(\n            model_name=cfg.embeddings_model,\n            encode_kwargs={\"normalize_embeddings\": cfg.normalize_embeddings}\n        )\n        vs = build_faiss_augmented(chunks, emb) \n\n    # === 6) (Опционально) Query-aware summarization для широкой витрины ===\n    if cfg.use_summarize and (cfg.use_bm25 or cfg.use_faiss):\n        _log(\"Wide candidate set for query-aware summarization...\")\n        if cfg.use_rrf and (bm25_inst is not None) and (vs is not None):\n            wide = rrf_fusion(cfg.query, vs, bm25_inst, base_docs=chunks, final_k=cfg.pre_k)\n        elif bm25_inst is not None:\n            q_tokens = _tok(cfg.query)\n            scores = bm25_inst.get_scores(q_tokens)\n            order = np.argsort(scores)[::-1][:cfg.pre_k]\n            wide = [chunks[i] for i in order]\n        else:\n            wide_raw = vs.similarity_search(cfg.query, k=cfg.pre_k)\n            wide = map_to_base(wide_raw, chunks)\n        summarize_docs(wide, gen, tok, mode=\"query\", query=cfg.query, only_if_longer_than=cfg.summarize_min_tokens)\n        _log(\"Summarization done\")\n        #if cfg.use_faiss:\n        #    _log(\"Rebuilding FAISS to include summaries...\")\n        #    emb = HuggingFaceEmbeddings(\n        #    model_name=cfg.embeddings_model,\n        #    encode_kwargs={\"normalize_embeddings\": cfg.normalize_embeddings}\n        #    )\n        #    vs = build_faiss_augmented(chunks, emb)\n    else:\n        _log(\"Summarization skipped\")\n\n    # === 7) Cross-Encoder (optional training) ===\n    ce_infer = None\n    if cfg.use_ce:\n        if cfg.train_ce:\n            _log(\"Training CrossEncoder on-the-fly...\")\n            train_ex = make_training_pairs_variant1(chunks, negatives_per_pos=cfg.ce_negatives_per_pos)\n            dl = DataLoader(train_ex, shuffle=True, batch_size=cfg.ce_train_batch_size)\n            ce_train = CrossEncoder(cfg.ce_model, num_labels=1, max_length=512)\n            ce_train.fit(\n                train_dataloader=dl,\n                epochs=cfg.ce_train_epochs,\n                warmup_steps=cfg.ce_train_warmup_steps,\n                output_path=cfg.ce_train_output_path\n            )\n            _log(f\"CE saved to: {cfg.ce_train_output_path}\")\n            ce_infer = CrossEncoder(cfg.ce_train_output_path, device='cuda' if torch.cuda.is_available() else 'cpu', max_length=512)\n        else:\n            ce_infer = CrossEncoder(cfg.ce_model, device='cuda' if torch.cuda.is_available() else 'cpu', max_length=512)\n\n    # === 8) Retrieval (final context) ===\n    _log(\"Retrieval for final context...\")\n    ctx_docs: List[Document] = []\n    if cfg.use_ce:\n        # RRF → CE\n        if cfg.use_rrf and (bm25_inst is not None) and (vs is not None):\n            cands = rrf_fusion(cfg.query, vs, bm25_inst, base_docs=chunks, final_k=cfg.pre_k)\n            # + MMR-альтернатива из FAISS (RAW) и дедуп\n            if cfg.use_mmr and vs is not None:\n                try:\n                    mmr_alt_raw = retrieve_mmr(vs, cfg.query, k=min(cfg.pre_k, 50),\n                           fetch_k=min(3*cfg.pre_k, 180), lambda_mult=0.5)\n                    mmr_alt = map_to_base(mmr_alt_raw, chunks)\n                    cands   = dedup_by_chunk_id(cands + mmr_alt)[:cfg.pre_k]\n                except Exception:\n                    pass\n            ctx_docs = _rank_with_ce(cfg.query, cands, ce_infer, cfg.final_k, cfg.adaptive_tau)\n\n        # Только BM25 → CE\n        elif (bm25_inst is not None) and (vs is None):\n            q_tokens = _tok(cfg.query)\n            scores = bm25_inst.get_scores(q_tokens)\n            order = np.argsort(scores)[::-1][:cfg.pre_k]\n            cands = [chunks[i] for i in order]\n            ctx_docs = _rank_with_ce(cfg.query, cands, ce_infer, cfg.final_k, cfg.adaptive_tau)\n\n        # Только FAISS → CE (обязательно to_raw)\n        elif (vs is not None) and (bm25_inst is None):\n            cands_raw = vs.similarity_search(cfg.query, k=cfg.pre_k)\n            cands = map_to_base(cands_raw, chunks)\n            ctx_docs = _rank_with_ce(cfg.query, cands, ce_infer, cfg.final_k, cfg.adaptive_tau)\n\n        else:\n            raise RuntimeError(\"Нет индексов для ретривала (включи BM25 и/или FAISS).\")\n\n    else:\n        # Без CE: RRF или одиночный индекс (+опц. MMR)\n        if cfg.use_rrf and (bm25_inst is not None) and (vs is not None):\n            cands = rrf_fusion(cfg.query, vs, bm25_inst, base_docs=chunks, final_k=max(cfg.pre_k, cfg.final_k))\n            if cfg.use_mmr and vs is not None:\n                try:\n                    mmr_alt_raw = retrieve_mmr(vs, cfg.query, k=min(cfg.pre_k, 50), fetch_k=min(3*cfg.pre_k, 180))\n                    mmr_alt     = map_to_base(mmr_alt_raw, chunks)\n                    cands       = dedup_by_chunk_id(cands + mmr_alt)[:cfg.pre_k]\n                except Exception:\n                    pass\n        elif bm25_inst is not None:\n            q_tokens = _tok(cfg.query)\n            scores = bm25_inst.get_scores(q_tokens)\n            order = np.argsort(scores)[::-1][:cfg.pre_k]\n            cands = [chunks[i] for i in order]\n        else:\n            cands = to_raw_many(vs.similarity_search(cfg.query, k=cfg.pre_k))\n        if cfg.adaptive_tau is not None:\n            _log(\"adaptive_tau задан, но CE отключён — возьмём обычный top-K.\")\n        ctx_docs = cands[:cfg.final_k]\n\n    _log(f\"Context docs: {len(ctx_docs)}\")\n\n    # === 9) Generation ===\n    _log(\"Generation...\")\n    result = generate_answer(cfg.query, ctx_docs, gen, tok,\n                             cross_encoder=(ce_infer if cfg.use_ce else None),\n                             use_judge=cfg.use_judge)\n\n    # === 10) Diagnostics ===\n    _log(\"Done.\")\n    diag = {\n        \"files\": cfg.files,\n        \"query\": cfg.query,\n        \"ctx_ids\": [d.metadata.get(\"_chunk_id\") for d in ctx_docs],\n        \"ctx_heads\": [d.metadata.get(\"heading\") for d in ctx_docs],\n        \"ctx_paths\": [d.metadata.get(\"section_path\") for d in ctx_docs],\n        \"types\": [d.metadata.get(\"type\") for d in ctx_docs],\n        \"config\": asdict(cfg)\n    }\n    return {\"result\": result, \"context_docs\": ctx_docs, \"diagnostics\": diag}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.514998Z","iopub.execute_input":"2025-11-12T19:40:35.515330Z","iopub.status.idle":"2025-11-12T19:40:35.551163Z","shell.execute_reply.started":"2025-11-12T19:40:35.515306Z","shell.execute_reply":"2025-11-12T19:40:35.550332Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"import json, re, unicodedata\n\ndef _slugify(s: str) -> str:\n    s = s.lower()\n    s = re.sub(r\"[^a-z0-9/_\\-\\s]\", \"\", s)\n    s = s.replace(\" \", \"_\")\n    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n    return s\n\ndef _section_id_from_doc(d: Document) -> str:\n    # приоритет: section_path → heading → doc_id\n    path = d.metadata.get(\"section_path\") or []\n    if isinstance(path, list) and path:\n        return _slugify(\"/\".join(path))\n    head = d.metadata.get(\"heading\")\n    if head:\n        return _slugify(head)\n    return _slugify(d.metadata.get(\"doc_id\", \"book\"))\n\ndef _page_str_from_doc(d: Document, page_shift: int = 1) -> str:\n    # PyPDFLoader обычно даёт 0-based page; в датасете могут быть «нативные» номера\n    for key in (\"true_page\",\"page\",\"page_number\",\"_page\"):\n        if key in d.metadata and d.metadata[key] is not None:\n            try:\n                p = int(d.metadata[key])\n                return str(p + page_shift)\n            except Exception:\n                return str(d.metadata[key])\n    return \"?\"\n\ndef build_references(ans_json: dict, ordered_docs: List[Document]) -> dict:\n    # берём только процитированные сниппеты\n    cited_ids = []\n    for c in (ans_json or {}).get(\"citations\", []):\n        m = re.fullmatch(r\"S(\\d+)\", c.strip())\n        if m:\n            cited_ids.append(int(m.group(1)))\n    cited_ids = sorted(set(i for i in cited_ids if 1 <= i <= len(ordered_docs)))\n\n    if not cited_ids:\n        # fallback: тогда считаем процитированными топ-3\n        cited_ids = list(range(1, min(3, len(ordered_docs)) + 1))\n\n    sections, pages = [], []\n    for i in cited_ids:\n        d = ordered_docs[i-1]\n        sections.append(_section_id_from_doc(d))\n        pages.append(_page_str_from_doc(d))\n    # уникальность с сохранением порядка\n    def _uniq_keep(seq): \n        seen=set(); out=[]\n        for x in seq:\n            if x not in seen:\n                seen.add(x); out.append(x)\n        return out\n    return {\"sections\": _uniq_keep(sections), \"pages\": _uniq_keep(pages)}\n\ndef build_context_for_submission(ordered_docs: List[Document], max_chars: int = 8000) -> str:\n    # оставим те же [S#], это помогает валидации и читаемости\n    parts = []\n    used = 0\n    for i, d in enumerate(ordered_docs, 1):\n        block = f\"[S{i}] {d.page_content.strip()}\"\n        if used + len(block) + 2 > max_chars:\n            break\n        parts.append(block); used += len(block) + 2\n    return \"\\n\\n\".join(parts)\n\ndef finalize_answer_text(ans_json: dict) -> str:\n    # из JSON берём «answer», fallback — пусто\n    return (ans_json or {}).get(\"answer\", \"\") or \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.552106Z","iopub.execute_input":"2025-11-12T19:40:35.552564Z","iopub.status.idle":"2025-11-12T19:40:35.571750Z","shell.execute_reply.started":"2025-11-12T19:40:35.552540Z","shell.execute_reply":"2025-11-12T19:40:35.571009Z"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":"# Если gen/tok ещё не определены:\n# from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n# GEN_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n# tok = AutoTokenizer.from_pretrained(GEN_MODEL, use_fast=True)\n# mdl = AutoModelForCausalLM.from_pretrained(GEN_MODEL, torch_dtype=\"auto\", device_map=\"auto\").eval()\n# gen = pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n\nCFG = RAGConfig(\n    files=[\"report.pdf\", \"notes.txt\"],\n    query=\"Кратко: как устроена модель доходов в отчёте и чем она отличается от прошлой версии?\",\n\n    use_outline=True,\n    use_semantic_chunking=False,\n    use_bm25=True,\n    use_faiss=True,\n    use_rrf=True,\n    use_mmr=True,\n    pre_k=60,\n    final_k=10,\n\n    use_summarize=True,\n    summarize_min_tokens=500,\n\n    use_ce=True,\n    ce_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    adaptive_tau=None,          # например, 0.35 для порогового отбора\n\n    # онлайновое обучение CE (по желанию)\n    train_ce=False,             # True — обучит CE на текущих чанках\n    ce_train_output_path=\"ce_finetuned\",\n    ce_train_epochs=1,\n    ce_train_batch_size=16,\n    ce_train_warmup_steps=100,\n    ce_negatives_per_pos=2,\n\n    embeddings_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n    normalize_embeddings=True,\n    use_judge=True\n)\n\n#run = run_with_config(CFG)\n#print(json.dumps(run[\"result\"][\"answer_json\"], ensure_ascii=False, indent=2))\n#print(\"\\nDiagnostics:\", json.dumps(run[\"diagnostics\"], ensure_ascii=False, indent=2))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.572647Z","iopub.execute_input":"2025-11-12T19:40:35.572887Z","iopub.status.idle":"2025-11-12T19:40:35.589037Z","shell.execute_reply.started":"2025-11-12T19:40:35.572866Z","shell.execute_reply":"2025-11-12T19:40:35.588246Z"}},"outputs":[],"execution_count":100},{"cell_type":"code","source":"# ===== CACHED STATE =====\nfrom dataclasses import dataclass\nfrom typing import Optional, List, Dict, Any\n\n@dataclass\nclass RAGState:\n    chunks: List[Document]\n    bm25: Optional[BM25Okapi]\n    vs: Optional[FAISS]\n    ce: Optional[CrossEncoder]\n    cfg_base: RAGConfig  # базовый конфиг без query\n\ndef prepare_state(files: List[str], cfg_base: RAGConfig) -> RAGState:\n    _log(\"Preparing state (ONE TIME)...\")\n    # 1) Load corpus once\n    corpus: List[Document] = []\n    for p in files:\n        if not Path(p).exists():\n            raise FileNotFoundError(f\"File not found: {p}\")\n        corpus += load_any(p)\n    assert corpus, \"Corpus is empty.\"\n\n    # 2) Chunk once\n    if cfg_base.use_semantic_chunking:\n        _emb_tmp = HuggingFaceEmbeddings(model_name=cfg_base.embeddings_model)\n        semantic_text_splitter = SemanticChunker(_emb_tmp,\n                           breakpoint_threshold_type='percentile',\n                           breakpoint_threshold_amount=90)\n        chunks: List[Document] = semantic_text_splitter.split_documents(corpus)\n    else:\n        rc = RecursiveCharacterTextSplitter(\n            chunk_size=cfg_base.chunk_size, chunk_overlap=cfg_base.chunk_overlap,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n        chunks: List[Document] = rc.split_documents(corpus)\n    for i, d in enumerate(chunks):\n        d.metadata[\"_chunk_id\"] = i\n\n    # 3) Outline/structure once\n    if cfg_base.use_outline:\n        outlines = build_outline_from_corpus(corpus)\n        chunks = attach_structure_to_chunks(chunks, outlines)\n        chunks = [label_block_type(d) for d in chunks]\n        structure_report(chunks)\n    else:\n        chunks = [label_block_type(d) for d in chunks]\n\n    # 4) Dedup once\n    seen, uniq = set(), []\n    for d in chunks:\n        key = re.sub(r\"\\s+\", \" \", d.page_content.strip().lower())[:2000]\n        if key in seen: \n            continue\n        seen.add(key); uniq.append(d)\n    chunks = uniq\n    _log(f\"Chunks after dedup: {len(chunks)}\")\n\n    # 5) Build indices once\n    bm25_inst = None\n    vs = None\n    if cfg_base.use_bm25:\n        _log(\"Building BM25 (ONE TIME)...\")\n        bm25_inst = BM25Okapi([_tok_with_heading(d) for d in chunks])\n    if cfg_base.use_faiss:\n        _log(\"Building FAISS (ONE TIME)...\")\n        emb = HuggingFaceEmbeddings(\n            model_name=cfg_base.embeddings_model,\n            encode_kwargs={\"normalize_embeddings\": cfg_base.normalize_embeddings}\n        )\n        # ВНИМАНИЕ: НЕ ПЕРЕСТРАИВАЕМ ПОСЛЕ САММАРИ, индекс по сырому/композитному тексту строим один раз\n        vs = build_faiss_augmented(chunks, emb)\n\n    # 6) Cross-Encoder once\n    ce_infer = None\n    if cfg_base.use_ce:\n        ce_infer = CrossEncoder(cfg_base.ce_model, device='cuda' if torch.cuda.is_available() else 'cpu', max_length=512)\n\n    _log(\"State ready.\")\n    return RAGState(chunks=chunks, bm25=bm25_inst, vs=vs, ce=ce_infer, cfg_base=cfg_base)\n\n# === Retrieval per-query using prepared state ===\ndef retrieve_context_for_query(query: str, state: RAGState, pre_k: int, final_k: int, use_rrf=True, use_mmr=True):\n    chunks, bm25_inst, vs, ce = state.chunks, state.bm25, state.vs, state.ce\n\n    # Широкая витрина (без перестройки индексов!)\n    if use_rrf and (bm25_inst is not None) and (vs is not None):\n        cands = rrf_fusion(query, vs, bm25_inst, base_docs=chunks, final_k=pre_k)\n        if use_mmr and vs is not None:\n            try:\n                mmr_alt = retrieve_mmr(vs, query, k=min(pre_k, 50), fetch_k=min(3*pre_k, 180), lambda_mult=0.5)\n                mmr_alt = map_to_base(mmr_alt, chunks)   # ВНИМАНИЕ: map_to_base, без mmr_alt_raw\n                cands = dedup_by_chunk_id(cands + mmr_alt)[:pre_k]\n            except Exception:\n                pass\n    elif bm25_inst is not None:\n        q_tokens = _tok(query)\n        scores = bm25_inst.get_scores(q_tokens)\n        order = np.argsort(scores)[::-1][:pre_k]\n        cands = [chunks[i] for i in order]\n    elif vs is not None:\n        cands_raw = vs.similarity_search(query, k=pre_k)\n        cands = map_to_base(cands_raw, chunks)\n    else:\n        raise RuntimeError(\"No indices available.\")\n\n    # CE rerank (если есть)\n    if ce is not None:\n        pairs  = [[query, d.page_content] for d in cands]\n        scores = ce.predict(pairs, batch_size=64)\n        ranked = [d for d,_ in sorted(zip(cands, map(float, scores)), key=lambda x: x[1], reverse=True)]\n        ctx_docs = ranked[:final_k]\n    else:\n        ctx_docs = cands[:final_k]\n    return ctx_docs\n\ndef run_for_query_with_state(question: str, state: RAGState,\n                             pre_k: int = 60, final_k: int = 10,\n                             summarize_min_tokens: int = 500,\n                             use_judge: bool = True,\n                             self_consistency_rounds: int = 2):\n    _log(\"Retrieval...\")\n    ctx_docs = retrieve_context_for_query(question, state, pre_k=pre_k, final_k=final_k,\n                                          use_rrf=state.cfg_base.use_rrf, use_mmr=state.cfg_base.use_mmr)\n    # Query-aware summarization ТОЛЬКО для упаковки (НЕ перестраиваем FAISS!)\n    summarize_docs(ctx_docs, gen, tok, mode=\"query\", query=question,\n                   only_if_longer_than=summarize_min_tokens)\n\n    _log(\"Generation...\")\n    # Вариант ускорения: меньше раундов self-consistency\n    best, all_cands = self_consistency_generate(\n        question, ctx_docs, gen, tok,\n        cross_encoder=(state.ce if state.cfg_base.use_ce else None),\n        rounds=self_consistency_rounds,\n        styles=(\"concise\",\"teacher\")  # можно 2 стиля для скорости\n    )\n    ans_json = best[\"json\"] or {}\n    judge = {}\n    if use_judge and ans_json:\n        sc, judge = judge_score(question, ctx_docs, ans_json, gen, tok)\n        if \"confidence\" in ans_json:\n            ans_json[\"confidence\"] = float(max(0.0, min(1.0, 0.5*ans_json[\"confidence\"] + 0.5*sc)))\n\n    # Ссылки на секции/страницы\n    sections = []\n    pages    = []\n    for d in ctx_docs:\n        sp = d.metadata.get(\"section_path\") or []\n        if sp: sections.append(\"/\".join(sp))\n        pg = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or d.metadata.get(\"_page\")\n        if pg is not None: pages.append(str(pg))\n    refs_json = json.dumps({\"sections\": sorted(set(sections))[:6], \"pages\": sorted(set(pages))[:10]}, ensure_ascii=False)\n\n    # Контекст для submission (склеим разумно)\n    ctx_text = \"\\n\\n\".join(\n        f\"[S{i+1}] {(d.metadata.get('_summary_text') or d.page_content).strip()}\"\n        for i, d in enumerate(ctx_docs)\n    )[:12000]  # чтобы не раздуть CSV\n\n    return {\n        \"answer_json\": ans_json,\n        \"context\": ctx_text,\n        \"references\": refs_json,\n        \"ctx_docs\": ctx_docs,\n        \"judge\": judge\n    }\n\n# === FAST SUBMISSION ===\ndef make_submission_fast(queries_json_path: str, files: List[str], out_csv: str,\n                         cfg_base: Optional[RAGConfig] = None):\n    # базовый конфиг без query\n    cfg_base = cfg_base or RAGConfig(\n        files=files, query=\"\",\n        use_outline=True,\n        use_semantic_chunking=False,\n        use_bm25=True, use_faiss=True, use_rrf=True, use_mmr=True,\n        pre_k=60, final_k=10,\n        use_summarize=True, summarize_min_tokens=500,\n        use_ce=True, ce_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n        adaptive_tau=None,\n        train_ce=False,\n        embeddings_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        normalize_embeddings=True,\n        use_judge=True\n    )\n\n    # 1) Считываем вопросы\n    data = json.loads(Path(queries_json_path).read_text(encoding=\"utf-8\"))\n    items = data[\"queries\"] if isinstance(data, dict) and \"queries\" in data else data\n    assert isinstance(items, list) and items, \"queries.json must be a list or have a 'queries' list\"\n\n    # 2) Готовим состояние один раз\n    state = prepare_state(files, cfg_base)\n\n    # 3) Пробегаем вопросы\n    rows = []\n    for it in tqdm(items, desc=\"Answering (fast)\"):\n        qid = it.get(\"id\") or it.get(\"query_id\") or it.get(\"ID\") or it.get(\"qid\") or len(rows)\n        qtext = it.get(\"question\") or it.get(\"query\") or it.get(\"text\") or \"\"\n        qid_int = int(\"\".join(ch for ch in str(qid) if ch.isdigit()) or 0)\n\n        out = run_for_query_with_state(\n            qtext, state,\n            pre_k=cfg_base.pre_k, final_k=cfg_base.final_k,\n            summarize_min_tokens=cfg_base.summarize_min_tokens,\n            use_judge=cfg_base.use_judge,\n            self_consistency_rounds=2  # ускоряем\n        )\n        ans = out[\"answer_json\"].get(\"answer\", \"\")\n        # На всякий случай чистим переносы в answer, чтобы CSV был аккуратнее\n        ans = ans.replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n\n        rows.append({\n            \"ID\": qid_int,\n            \"context\": out[\"context\"],\n            \"answer\": ans,\n            \"references\": out[\"references\"]\n        })\n\n    df = pd.DataFrame(rows, columns=[\"ID\", \"context\", \"answer\", \"references\"])\n    df.to_csv(out_csv, index=False)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.589984Z","iopub.execute_input":"2025-11-12T19:40:35.590286Z","iopub.status.idle":"2025-11-12T19:40:35.618799Z","shell.execute_reply.started":"2025-11-12T19:40:35.590265Z","shell.execute_reply":"2025-11-12T19:40:35.617881Z"}},"outputs":[],"execution_count":101},{"cell_type":"code","source":"import pandas as pd\nimport json\nfrom pathlib import Path\n\ndef run_for_query(files, question: str) -> dict:\n    cfg = RAGConfig(\n        files=files,\n        query=question,\n        use_outline=True,\n        use_semantic_chunking=False,\n        use_bm25=True,\n        use_faiss=True,\n        use_rrf=True,\n        use_mmr=True,\n        pre_k=60,\n        final_k=10,\n        use_summarize=True,\n        summarize_min_tokens=500,\n        use_ce=True,\n        ce_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n        adaptive_tau=None,\n        train_ce=False,\n        embeddings_model=\"sentence-transformers/all-MiniLM-L6-v2\",\n        normalize_embeddings=True,\n        use_judge=True\n    )\n    out = run_with_config(cfg)\n    res = out[\"result\"]\n    ordered = res.get(\"ordered_docs\", out.get(\"context_docs\", []))  # на всякий\n    ctx = build_context_for_submission(ordered)\n    refs = build_references(res.get(\"answer_json\", {}), ordered)\n    ans = finalize_answer_text(res.get(\"answer_json\", {}))\n    return {\"context\": ctx, \"answer\": ans, \"references\": json.dumps(refs, ensure_ascii=False)}\n\ndef make_submission(queries_json_path: str, files: list, out_csv: str = \"submission.csv\"):\n    data = json.loads(Path(queries_json_path).read_text(encoding=\"utf-8\"))\n\n    # Унифицированое извлечение списка запросов\n    if isinstance(data, dict):\n        items = data.get(\"queries\") or data.get(\"data\") or data.get(\"items\") or data.get(\"questions\") or []\n    elif isinstance(data, list):\n        items = data\n    else:\n        raise ValueError(\"Unsupported queries.json format\")\n\n    rows = []\n    for it in tqdm(items, desc=\"Answering\"):\n        qid = it.get(\"ID\") or it.get(\"query_id\") or it.get(\"id\")\n        question = it.get(\"question\") or it.get(\"query\") or it.get(\"text\")\n        if qid is None or not question:\n            continue\n        # безопасно к int (обычно query_id числовой; если строка-число — тоже ок)\n        try:\n            qid_int = int(qid)\n        except Exception:\n            qid_int = int(\"\".join(ch for ch in str(qid) if ch.isdigit()) or 0)\n\n        r = run_for_query(files, question)\n        rows.append({\n            \"ID\": qid_int,\n            \"context\": r[\"context\"],\n            \"answer\": r[\"answer\"],\n            \"references\": r[\"references\"]\n        })\n\n    df = pd.DataFrame(rows).sort_values(\"ID\")\n    df.to_csv(out_csv, index=False)\n    return df\n\n# пример вызова:\nfiles = [\"/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/book.pdf\"]\n\ndf_sub = make_submission_fast(\n    \"/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/queries.json\",\n    files,\n    \"submission.csv\"\n)\ndisplay(df_sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:40:35.619681Z","iopub.execute_input":"2025-11-12T19:40:35.620120Z","iopub.status.idle":"2025-11-12T19:52:21.662193Z","shell.execute_reply.started":"2025-11-12T19:40:35.620093Z","shell.execute_reply":"2025-11-12T19:52:21.661609Z"}},"outputs":[{"name":"stdout","text":"[19:40:35] Preparing state (ONE TIME)...\nHeading levels: {None: 1, 1: 2274, 2: 1386}\nTypes: {'text': 1284, 'table': 2317, 'figure': 60}\nSections with path: 3660 / 3661\n[19:40:52] Chunks after dedup: 3661\n[19:40:52] Building BM25 (ONE TIME)...\n[19:42:49] Building FAISS (ONE TIME)...\n[19:43:00] State ready.\n","output_type":"stream"},{"name":"stderr","text":"Answering (fast):   0%|          | 0/50 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"[19:43:00] Retrieval...\n[19:43:00] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):   2%|▏         | 1/50 [00:11<09:07, 11.17s/it]","output_type":"stream"},{"name":"stdout","text":"[19:43:11] Retrieval...\n[19:43:11] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):   4%|▍         | 2/50 [00:20<08:15, 10.33s/it]","output_type":"stream"},{"name":"stdout","text":"[19:43:21] Retrieval...\n[19:43:21] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):   6%|▌         | 3/50 [00:34<09:10, 11.70s/it]","output_type":"stream"},{"name":"stdout","text":"[19:43:34] Retrieval...\n[19:43:34] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):   8%|▊         | 4/50 [00:45<08:54, 11.62s/it]","output_type":"stream"},{"name":"stdout","text":"[19:43:45] Retrieval...\n[19:43:46] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  10%|█         | 5/50 [00:54<07:48, 10.42s/it]","output_type":"stream"},{"name":"stdout","text":"[19:43:54] Retrieval...\n[19:43:54] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  12%|█▏        | 6/50 [01:03<07:28, 10.18s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:03] Retrieval...\n[19:44:04] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  14%|█▍        | 7/50 [01:13<07:09,  9.98s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:13] Retrieval...\n[19:44:13] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  16%|█▌        | 8/50 [01:25<07:29, 10.71s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:25] Retrieval...\n[19:44:25] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  18%|█▊        | 9/50 [01:34<06:59, 10.22s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:34] Retrieval...\n[19:44:35] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  20%|██        | 10/50 [01:45<06:54, 10.37s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:45] Retrieval...\n[19:44:45] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  22%|██▏       | 11/50 [01:57<07:01, 10.82s/it]","output_type":"stream"},{"name":"stdout","text":"[19:44:57] Retrieval...\n[19:44:57] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  24%|██▍       | 12/50 [02:14<08:02, 12.70s/it]","output_type":"stream"},{"name":"stdout","text":"[19:45:14] Retrieval...\n[19:45:14] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  26%|██▌       | 13/50 [02:22<07:05, 11.49s/it]","output_type":"stream"},{"name":"stdout","text":"[19:45:23] Retrieval...\n[19:45:23] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  28%|██▊       | 14/50 [02:34<06:57, 11.61s/it]","output_type":"stream"},{"name":"stdout","text":"[19:45:35] Retrieval...\n[19:45:35] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  30%|███       | 15/50 [02:46<06:43, 11.52s/it]","output_type":"stream"},{"name":"stdout","text":"[19:45:46] Retrieval...\n[19:45:46] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  32%|███▏      | 16/50 [02:59<06:52, 12.13s/it]","output_type":"stream"},{"name":"stdout","text":"[19:45:59] Retrieval...\n[19:46:00] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  34%|███▍      | 17/50 [03:12<06:47, 12.34s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:12] Retrieval...\n[19:46:12] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  36%|███▌      | 18/50 [03:23<06:17, 11.81s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:23] Retrieval...\n[19:46:23] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  38%|███▊      | 19/50 [03:33<05:56, 11.49s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:34] Retrieval...\n[19:46:34] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  40%|████      | 20/50 [03:43<05:28, 10.96s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:43] Retrieval...\n[19:46:43] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  42%|████▏     | 21/50 [03:52<04:57, 10.27s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:52] Retrieval...\n[19:46:52] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  44%|████▍     | 22/50 [03:59<04:22,  9.37s/it]","output_type":"stream"},{"name":"stdout","text":"[19:46:59] Retrieval...\n[19:46:59] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  46%|████▌     | 23/50 [04:10<04:22,  9.74s/it]","output_type":"stream"},{"name":"stdout","text":"[19:47:10] Retrieval...\n[19:47:10] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  48%|████▊     | 24/50 [04:21<04:23, 10.15s/it]","output_type":"stream"},{"name":"stdout","text":"[19:47:21] Retrieval...\n[19:47:21] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  50%|█████     | 25/50 [04:30<04:08,  9.96s/it]","output_type":"stream"},{"name":"stdout","text":"[19:47:30] Retrieval...\n[19:47:31] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  52%|█████▏    | 26/50 [04:41<04:07, 10.31s/it]","output_type":"stream"},{"name":"stdout","text":"[19:47:42] Retrieval...\n[19:47:42] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  54%|█████▍    | 27/50 [04:50<03:48,  9.92s/it]","output_type":"stream"},{"name":"stdout","text":"[19:47:51] Retrieval...\n[19:47:51] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  56%|█████▌    | 28/50 [05:00<03:38,  9.93s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:01] Retrieval...\n[19:48:01] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  58%|█████▊    | 29/50 [05:13<03:46, 10.77s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:13] Retrieval...\n[19:48:13] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  60%|██████    | 30/50 [05:21<03:20, 10.04s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:22] Retrieval...\n[19:48:22] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  62%|██████▏   | 31/50 [05:34<03:24, 10.75s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:34] Retrieval...\n[19:48:34] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  64%|██████▍   | 32/50 [05:44<03:10, 10.57s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:44] Retrieval...\n[19:48:44] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  66%|██████▌   | 33/50 [05:57<03:10, 11.23s/it]","output_type":"stream"},{"name":"stdout","text":"[19:48:57] Retrieval...\n[19:48:57] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  68%|██████▊   | 34/50 [06:13<03:22, 12.68s/it]","output_type":"stream"},{"name":"stdout","text":"[19:49:13] Retrieval...\n[19:49:13] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  70%|███████   | 35/50 [06:22<02:53, 11.59s/it]","output_type":"stream"},{"name":"stdout","text":"[19:49:22] Retrieval...\n[19:49:22] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  72%|███████▏  | 36/50 [06:33<02:41, 11.56s/it]","output_type":"stream"},{"name":"stdout","text":"[19:49:33] Retrieval...\n[19:49:34] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  74%|███████▍  | 37/50 [06:44<02:27, 11.32s/it]","output_type":"stream"},{"name":"stdout","text":"[19:49:44] Retrieval...\n[19:49:44] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  76%|███████▌  | 38/50 [06:59<02:27, 12.29s/it]","output_type":"stream"},{"name":"stdout","text":"[19:49:59] Retrieval...\n[19:49:59] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  78%|███████▊  | 39/50 [07:11<02:16, 12.37s/it]","output_type":"stream"},{"name":"stdout","text":"[19:50:11] Retrieval...\n[19:50:12] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  80%|████████  | 40/50 [07:21<01:55, 11.55s/it]","output_type":"stream"},{"name":"stdout","text":"[19:50:21] Retrieval...\n[19:50:21] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  82%|████████▏ | 41/50 [07:33<01:44, 11.63s/it]","output_type":"stream"},{"name":"stdout","text":"[19:50:33] Retrieval...\n[19:50:33] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  84%|████████▍ | 42/50 [07:45<01:35, 11.94s/it]","output_type":"stream"},{"name":"stdout","text":"[19:50:45] Retrieval...\n[19:50:46] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  86%|████████▌ | 43/50 [07:56<01:20, 11.48s/it]","output_type":"stream"},{"name":"stdout","text":"[19:50:56] Retrieval...\n[19:50:56] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  88%|████████▊ | 44/50 [08:06<01:07, 11.22s/it]","output_type":"stream"},{"name":"stdout","text":"[19:51:06] Retrieval...\n[19:51:07] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  90%|█████████ | 45/50 [08:20<00:59, 11.85s/it]","output_type":"stream"},{"name":"stdout","text":"[19:51:20] Retrieval...\n[19:51:20] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  92%|█████████▏| 46/50 [08:35<00:52, 13.04s/it]","output_type":"stream"},{"name":"stdout","text":"[19:51:36] Retrieval...\n[19:51:36] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  94%|█████████▍| 47/50 [08:45<00:35, 11.98s/it]","output_type":"stream"},{"name":"stdout","text":"[19:51:45] Retrieval...\n[19:51:45] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  96%|█████████▌| 48/50 [08:59<00:25, 12.65s/it]","output_type":"stream"},{"name":"stdout","text":"[19:51:59] Retrieval...\n[19:52:00] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast):  98%|█████████▊| 49/50 [09:10<00:12, 12.22s/it]","output_type":"stream"},{"name":"stdout","text":"[19:52:11] Retrieval...\n[19:52:11] Generation...\n","output_type":"stream"},{"name":"stderr","text":"The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nAnswering (fast): 100%|██████████| 50/50 [09:21<00:00, 11.23s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   ID                                            context  \\\n0   1  [S1] explores questions like these. Psychology...   \n1   2  [S1] the other hand, serve as interconnected i...   \n2   3  [S1] modification of work by Ryan Vaarsi)\\nNRE...   \n3   4  [S1] Operant conditioning is based on the work...   \n4   5  [S1] sequence for individuals around the world...   \n\n                                              answer  \\\n0  The scientific method in psychology involves d...   \n1  A neuron consists of several different parts, ...   \n2  The stages of sleep include:  1. **Stage 1**: ...   \n3  Operant conditioning is a form of learning whe...   \n4  Problem-solving in psychology involves identif...   \n\n                                          references  \n0  {\"sections\": [\"2 • Critical Thinking Questions...  \n1  {\"sections\": [\"102 3 • Summary\", \"3 • Review Q...  \n2  {\"sections\": [\"118 4 • States of Consciousness...  \n3  {\"sections\": [\"16 • Summary 629\", \"192 6 • Lea...  \n4  {\"sections\": [\"214 7 • Thinking and Intelligen...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>context</th>\n      <th>answer</th>\n      <th>references</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[S1] explores questions like these. Psychology...</td>\n      <td>The scientific method in psychology involves d...</td>\n      <td>{\"sections\": [\"2 • Critical Thinking Questions...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[S1] the other hand, serve as interconnected i...</td>\n      <td>A neuron consists of several different parts, ...</td>\n      <td>{\"sections\": [\"102 3 • Summary\", \"3 • Review Q...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[S1] modification of work by Ryan Vaarsi)\\nNRE...</td>\n      <td>The stages of sleep include:  1. **Stage 1**: ...</td>\n      <td>{\"sections\": [\"118 4 • States of Consciousness...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[S1] Operant conditioning is based on the work...</td>\n      <td>Operant conditioning is a form of learning whe...</td>\n      <td>{\"sections\": [\"16 • Summary 629\", \"192 6 • Lea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[S1] sequence for individuals around the world...</td>\n      <td>Problem-solving in psychology involves identif...</td>\n      <td>{\"sections\": [\"214 7 • Thinking and Intelligen...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":102},{"cell_type":"code","source":"df_sub['answer']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T19:56:57.512765Z","iopub.execute_input":"2025-11-12T19:56:57.513409Z","iopub.status.idle":"2025-11-12T19:56:57.521300Z","shell.execute_reply.started":"2025-11-12T19:56:57.513385Z","shell.execute_reply":"2025-11-12T19:56:57.520639Z"}},"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"0     The scientific method in psychology involves d...\n1     A neuron consists of several different parts, ...\n2     The stages of sleep include:  1. **Stage 1**: ...\n3     Operant conditioning is a form of learning whe...\n4     Problem-solving in psychology involves identif...\n5     According to the Atkinson-Shiffrin model, ther...\n6     Key components of emotion include physiologica...\n7     The major personality traits in the Five Facto...\n8     Social psychology is the study of how people i...\n9     The sociocultural model in therapy considers t...\n10    The history of psychology began in the late 18...\n11    Wilhelm Wundt was a German scientist who found...\n12    Functionalism is a psychological perspective t...\n13    Freud's contributions to psychology include de...\n14    Gestalt principles are rules or guidelines tha...\n15    Classical conditioning is a form of learning i...\n16    Skinner's contributions to behaviorism include...\n17    Maslow's hierarchy of needs consists of five l...\n18    Client-centered therapy, also known as Rogeria...\n19    Cognitive psychology is the study of cognition...\n20    Developmental psychology is the scientific stu...\n21    Biopsychology is the study of how biology infl...\n22    Evolutionary psychologists are interested in p...\n23    Sensation is the detection of sensory stimuli ...\n24    Sensation refers to the detection of sensory s...\n25    Jean Piaget's theories of cognitive developmen...\n26    Object permanence is the understanding that ph...\n27    Neurotransmitters play a crucial role in psych...\n28    The Big Five personality trait model consists ...\n29    Psychological disorders are conditions charact...\n30    The DSM-5 is the fifth edition of the Diagnost...\n31    Ethics in psychological research are crucial b...\n32    The cognitive revolution in psychology refers ...\n33    Feminist psychology emerged as a response to t...\n34    Multicultural psychology is the study of psych...\n35    Cross-cultural psychology faces several challe...\n36    In psychology, stress refers to a process wher...\n37    The stages of psychosexual development accordi...\n38    Replication in psychological research is cruci...\n39    Different types of psychological therapies inc...\n40    Neurotransmitters play crucial roles in mental...\n41    Major themes in industrial-organizational psyc...\n42    Social norms dictate the behavior that is appr...\n43    Prejudice involves negative feelings and evalu...\n44    The key takeaways from the Brown v. Board of E...\n45    The scientific process of hypothesis testing i...\n46    Psychologists play roles in education, includi...\n47    Mood disorders involve severe disturbances in ...\n48    Cultural diversity positively impacts psycholo...\n49    Critical thinking plays a crucial role in psyc...\nName: answer, dtype: object"},"metadata":{}}],"execution_count":103}]}