{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pip install ","metadata":{}},{"cell_type":"code","source":"!pip install -q langchain_community langchain-experimental faiss-cpu rank-bm25 pypdf langchain-text-splitters\n!pip install -q sentence-transformers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"\nimport os, random, re, json, html, unicodedata, time\nfrom pathlib import Path\nfrom typing import List, Dict, Any, Optional\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\n\nfrom dataclasses import dataclass\nfrom collections import Counter\n\nfrom rank_bm25 import BM25Okapi\nfrom sentence_transformers import SentenceTransformer, CrossEncoder, InputExample\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline as hf_pipeline\n\nfrom langchain_core.documents import Document\nfrom langchain_community.document_loaders import PyPDFLoader, TextLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_experimental.text_splitter import SemanticChunker\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS as LCFAISS","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Seeds","metadata":{}},{"cell_type":"code","source":"seed = 824\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\nimport random\nrandom.seed(seed)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"def _log(msg: str):\n    print(f\"[{time.strftime('%H:%M:%S')}] {msg}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cleaning","metadata":{}},{"cell_type":"code","source":"_WS_IN_LINE = re.compile(r'[ \\t]+')\n_URL_RE = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n_EMAIL_RE = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n\ndef cleaning(\n    text: str,\n    *,\n    lower: bool = False,\n    keep_newlines: bool = True,\n    remove_urls: bool = True,\n    remove_emails: bool = True,\n) -> str:\n    \"\"\"Аккуратная нормализация текста (без убийства структуры).\"\"\"\n    if text is None:\n        return \"\"\n    s = html.unescape(str(text))\n    s = unicodedata.normalize(\"NFKC\", s)\n\n    # убираем zero-width и control chars (но не \\n, \\t)\n    s = \"\".join(\n        ch for ch in s\n        if not (\n            unicodedata.category(ch) in {\"Cf\", \"Cc\"} and ch not in (\"\\n\", \"\\t\")\n        )\n    )\n\n    # типографика: кавычки/тире/многоточия\n    s = s.translate({\n        ord(\"“\"): '\"', ord(\"”\"): '\"', ord(\"„\"): '\"', ord(\"‟\"): '\"',\n        ord(\"’\"): \"'\", ord(\"‘\"): \"'\", ord(\"‚\"): \"'\", ord(\"′\"): \"'\",\n        ord(\"–\"): \"-\", ord(\"—\"): \"-\", ord(\"−\"): \"-\", ord(\"‐\"): \"-\",\n        ord(\"…\"): \"...\",\n    })\n\n    if remove_urls:\n        s = _URL_RE.sub(\" \", s)\n    if remove_emails:\n        s = _EMAIL_RE.sub(\" \", s)\n\n    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n    if keep_newlines:\n        s = \"\\n\".join(_WS_IN_LINE.sub(\" \", line).strip() for line in s.split(\"\\n\"))\n        s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n    else:\n        s = re.sub(r\"\\s+\", \" \", s).strip()\n\n    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n\n    if lower:\n        s = s.lower()\n    return s","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"def load_any(path: str) -> List[Document]:\n    \"\"\"Загрузка PDF / txt / md в список Document с базовыми метаданными.\"\"\"\n    p = Path(path)\n    suf = p.suffix.lower()\n    if suf == \".pdf\":\n        docs = PyPDFLoader(str(p)).load()\n    elif suf in {\".txt\", \".md\"}:\n        docs = TextLoader(str(p), encoding=\"utf-8\").load()\n    else:\n        raise ValueError(f\"Unsupported format: {suf}\")\n\n    out = []\n    for d in docs:\n        cleaned = cleaning(d.page_content)\n        meta = {**(d.metadata or {}), \"source\": str(p), \"doc_id\": p.stem}\n        out.append(Document(page_content=cleaned, metadata=meta))\n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_corpus(paths: List[str]) -> List[Document]:\n    corpus = []\n    for p in paths:\n        p = Path(p)\n        if p.is_dir():\n            for f in p.rglob(\"*\"):\n                if f.suffix.lower() in {\".pdf\", \".txt\", \".md\"}:\n                    corpus += load_any(str(f))\n        else:\n            corpus += load_any(str(p))\n    return corpus\n\n# использование\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Structure","metadata":{}},{"cell_type":"code","source":"# 2. STRUCTURE HEURISTICS\n# =====================\n_HEADING_MD  = re.compile(r\"^(#{1,6})\\s+(.+)\")\n_HEADING_NUM = re.compile(r\"^(\\d+(?:\\.\\d+){0,5})\\s+(.+)\")\n\ndef _is_mostly_upper(s: str, min_len=3, ratio=0.75) -> bool:\n    letters = [c for c in s if c.isalpha()]\n    return len(letters) >= min_len and sum(c.isupper() for c in letters)/len(letters) >= ratio\n\ndef detect_heading_line(line: str) -> Optional[dict]:\n    line = line.strip()\n    if not line:\n        return None\n    m = _HEADING_MD.match(line)\n    if m:\n        level = len(m.group(1))\n        text = m.group(2).strip()\n        return {\"heading\": text, \"level\": level}\n    m = _HEADING_NUM.match(line)\n    if m:\n        depth = m.group(1).count(\".\") + 1\n        text = m.group(2).strip()\n        return {\"heading\": f\"{m.group(1)} {text}\", \"level\": min(6, depth)}\n    if len(line) < 80 and _is_mostly_upper(line):\n        return {\"heading\": line.title(), \"level\": 1}\n    if line.endswith(\":\") and len(line) < 120:\n        return {\"heading\": line[:-1].strip(), \"level\": 2}\n    return None\n\n@dataclass\nclass OutlineNode:\n    level: int\n    title: str\n    page: Optional[int]\n    path: List[str]\n    node_id: str\n\ndef build_outline_from_corpus(corpus: List[Document]) -> Dict[str, List[OutlineNode]]:\n    outlines: Dict[str, List[OutlineNode]] = {}\n    stacks: Dict[str, List[OutlineNode]] = {}\n    counters: Dict[str, int] = {}\n\n    for d in corpus:\n        doc_id = d.metadata.get(\"doc_id\", \"_\")\n        page = d.metadata.get(\"page\") or d.metadata.get(\"page_number\")\n        text = d.page_content or \"\"\n        lines = [ln for ln in text.splitlines() if ln.strip()]\n\n        if doc_id not in stacks:\n            stacks[doc_id] = []\n            outlines[doc_id] = []\n            counters[doc_id] = 0\n\n        for ln in lines:\n            h = detect_heading_line(ln)\n            if not h:\n                continue\n            level = h[\"level\"]\n            title = h[\"heading\"]\n\n            while stacks[doc_id] and stacks[doc_id][-1].level >= level:\n                stacks[doc_id].pop()\n\n            path = [n.title for n in stacks[doc_id]] + [title]\n            counters[doc_id] += 1\n            nid = f\"{doc_id}::h{counters[doc_id]}\"\n            node = OutlineNode(level=level, title=title, page=page, path=path, node_id=nid)\n            outlines[doc_id].append(node)\n            stacks[doc_id].append(node)\n    return outlines\n\ndef attach_structure_to_chunks(chunks: List[Document], outlines: Dict[str, List[OutlineNode]]) -> List[Document]:\n    by_doc = outlines\n    for d in chunks:\n        doc_id = d.metadata.get(\"doc_id\", \"_\")\n        page = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or -1\n        cand = None\n        if doc_id in by_doc:\n            cands = [n for n in by_doc[doc_id] if (n.page is None or page is None or int(n.page) <= int(page))]\n            if cands:\n                cand = cands[-1]\n        if cand:\n            d.metadata[\"heading\"] = cand.title\n            d.metadata[\"heading_level\"] = cand.level\n            d.metadata[\"section_path\"] = cand.path\n            d.metadata[\"parent_id\"] = cand.node_id\n        else:\n            d.metadata.setdefault(\"heading\", None)\n            d.metadata.setdefault(\"heading_level\", None)\n            d.metadata.setdefault(\"section_path\", [])\n            d.metadata.setdefault(\"parent_id\", None)\n    return chunks\n\n# тип блока: текст/таблица/рисунок (очень грубо)\n_TABLE_HINTS = re.compile(r\"\\b(table|таблица|column|row|строк|столб|csv|tsv)\\b\", re.I)\n\ndef is_tabular_like(txt: str) -> bool:\n    lines = [ln for ln in txt.splitlines() if ln.strip()]\n    if not lines:\n        return False\n    bars = sum(ln.count(\"|\") for ln in lines)\n    tabs = sum(ln.count(\"\\t\") for ln in lines)\n    hint = bool(_TABLE_HINTS.search(txt[:1000]))\n    return (bars >= 3) or (tabs >= 3) or hint\n\ndef label_block_type(d: Document) -> Document:\n    txt = d.page_content[:2000]\n    if is_tabular_like(txt):\n        d.metadata[\"type\"] = \"table\"\n    else:\n        d.metadata[\"type\"] = \"text\"\n    return d\n\ndef structure_report(chunks: List[Document]):\n    lvls = Counter(d.metadata.get(\"heading_level\") for d in chunks)\n    types = Counter(d.metadata.get(\"type\") for d in chunks)\n    paths_nonempty = sum(1 for d in chunks if d.metadata.get(\"section_path\"))\n    print(\"Heading levels:\", dict(lvls))\n    print(\"Types:\", dict(types))\n    print(\"Sections with path:\", paths_nonempty, \"/\", len(chunks))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Using","metadata":{}},{"cell_type":"code","source":"files = [\n    \"/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)\",\n    # можно потом любую папку/несколько файлов добавить\n]\ncorpus = load_corpus(files)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#corpus: List[Document] = []\n#for p in files:\n#    print(\"Loading:\", p)\n#    corpus += load_any(p)\nprint(f\"Loaded {len(corpus)} raw docs/pages\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2) Структура (outline) по страницам\noutlines = build_outline_from_corpus(corpus)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## eda","metadata":{}},{"cell_type":"code","source":"import statistics\nfrom collections import defaultdict\n\ndef corpus_eda(corpus: List[Document], outlines: Dict[str, List[OutlineNode]]):\n    print(\"\\n=== CORPUS EDA ===\")\n    # 1) какие doc_id и сколько страниц у каждого\n    pages_per_doc = defaultdict(int)\n    lens_chars = []\n    for d in corpus:\n        doc_id = d.metadata.get(\"doc_id\", \"_\")\n        pages_per_doc[doc_id] += 1\n        lens_chars.append(len(d.page_content))\n\n    print(\"Документы и кол-во страниц:\")\n    for doc_id, n in pages_per_doc.items():\n        print(f\"  - {doc_id}: {n} pages\")\n\n    print(f\"\\nВсего страниц: {len(corpus)}\")\n    print(f\"Длина страницы (символы): min={min(lens_chars)}, \"\n          f\"median={int(statistics.median(lens_chars))}, \"\n          f\"mean={int(statistics.mean(lens_chars))}, \"\n          f\"max={max(lens_chars)}\")\n\n    # 2) EDA по outline\n    print(\"\\nOutline по документам:\")\n    for doc_id, nodes in outlines.items():\n        levels = [n.level for n in nodes]\n        titles = [n.title for n in nodes]\n        print(f\"  - {doc_id}: {len(nodes)} заголовков,\"\n              f\" уровни={sorted(set(levels))}\")\n        print(\"    Примеры заголовков:\")\n        for t in titles[:5]:\n            print(\"      •\", t)\n        print()\n        # один-два документа достаточно\n        # break  # можно раскомментировать, если outline очень большой\n\n# Вызов:\ncorpus_eda(corpus, outlines)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Chunking","metadata":{}},{"cell_type":"code","source":"# 3. CHUNKING\n# =====================\n\ndef make_recursive_chunks(corpus: List[Document],\n                          chunk_size: int = 800,\n                          chunk_overlap: int = 120) -> List[Document]:\n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n    )\n    chunks = splitter.split_documents(corpus)\n    for i, d in enumerate(chunks):\n        d.metadata[\"_chunk_id\"] = i\n    return chunks\n\n\ndef make_semantic_chunks(corpus: List[Document],\n                         emb_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n                         breakpoint_percentile: int = 90) -> List[Document]:\n    emb = HuggingFaceEmbeddings(model_name=emb_model_name)\n    splitter = SemanticChunker(\n        emb,\n        breakpoint_threshold_type=\"percentile\",\n        breakpoint_threshold_amount=breakpoint_percentile,\n    )\n    chunks = splitter.split_documents(corpus)\n    for i, d in enumerate(chunks):\n        d.metadata[\"_chunk_id\"] = i\n    return chunks","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using","metadata":{}},{"cell_type":"code","source":"chunks = make_recursive_chunks(corpus, chunk_size=800, chunk_overlap=120)\nfor i, d in enumerate(chunks):\n    d.metadata[\"_chunk_id\"] = i","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4) Привязка структуры и тип блока\nchunks = attach_structure_to_chunks(chunks, outlines)\nchunks = [label_block_type(d) for d in chunks]\nstructure_report(chunks)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5) Dedup по тексту\nseen, uniq = set(), []\nfor d in chunks:\n    key = re.sub(r\"\\s+\", \" \", d.page_content.strip().lower())[:2000]\n    if key in seen:\n        continue\n    seen.add(key)\n    uniq.append(d)\nchunks = uniq\nprint(\"Chunks after dedup:\", len(chunks))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### иерархия","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef build_section_corpus_from_chunks(\n    chunks: List[Document],\n    min_chunks_per_section: int = 1,\n    max_section_chars: int = 4000,\n) -> (List[Document], Dict[int, List[int]]):\n    \"\"\"\n    Группируем чанки по (doc_id, section_path) => \"секции\".\n    Если section_path пустой — группируем по странице.\n    Возвращаем:\n      - список секций как Document'ов,\n      - mapping: section_id (== _chunk_id секции) -> список chunk_id внутри неё.\n    \"\"\"\n    groups: Dict[tuple, List[Document]] = defaultdict(list)\n\n    for d in chunks:\n        doc_id = d.metadata.get(\"doc_id\", \"_\")\n        path   = d.metadata.get(\"section_path\") or []\n        if path:\n            key = (doc_id, tuple(path))\n        else:\n            pg = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or d.metadata.get(\"_page\") or 0\n            key = (doc_id, (f\"page_{pg}\",))\n        groups[key].append(d)\n\n    section_docs: List[Document] = []\n    section_to_chunk_ids: Dict[int, List[int]] = {}\n\n    sec_id = 0\n    for (doc_id, path), docs in groups.items():\n        if len(docs) < min_chunks_per_section:\n            continue\n\n        # текст секции = конкатенация текстов чанков с обрезкой\n        text = \"\\n\\n\".join(dd.page_content for dd in docs)\n        if len(text) > max_section_chars:\n            text = text[:max_section_chars]\n\n        meta = {\n            \"doc_id\": doc_id,\n            \"section_path\": list(path),\n            \"heading\": path[-1],\n            \"type\": \"section\",\n            \"section_doc\": True,\n            \"section_idx\": sec_id,\n        }\n        sec_doc = Document(page_content=text, metadata=meta)\n        sec_doc.metadata[\"_chunk_id\"] = sec_id  # пусть секции тоже имеют _chunk_id\n\n        section_docs.append(sec_doc)\n        section_to_chunk_ids[sec_id] = [dd.metadata.get(\"_chunk_id\") for dd in docs]\n        sec_id += 1\n\n    print(f\"Built {len(section_docs)} section docs from {len(chunks)} chunks\")\n    return section_docs, section_to_chunk_ids\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"section_docs, section_to_chunk_ids = build_section_corpus_from_chunks(\n    chunks,\n    min_chunks_per_section=1,\n    max_section_chars=4000,\n)\n\n# EDA по секциям (опционально)\nprint(f\"\\nBuilt {len(section_docs)} sections\")\nfor s in section_docs[:5]:\n    print(\"SECTION:\", \" / \".join(s.metadata.get(\"section_path\", [])))\n    print(\"  len:\", len(s.page_content))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## eda","metadata":{}},{"cell_type":"code","source":"import statistics\n\ndef chunks_eda_basic(chunks: List[Document]):\n    print(\"\\n=== CHUNKS EDA (BASIC) ===\")\n    n = len(chunks)\n    print(f\"Всего чанков: {n}\")\n\n    lens_chars = [len(d.page_content) for d in chunks]\n    print(f\"Длина чанка (символы): min={min(lens_chars)}, \"\n          f\"median={int(statistics.median(lens_chars))}, \"\n          f\"mean={int(statistics.mean(lens_chars))}, \"\n          f\"max={max(lens_chars)}\")\n\n    # распределение по doc_id\n    from collections import Counter\n    doc_counts = Counter(d.metadata.get(\"doc_id\", \"_\") for d in chunks)\n    print(\"\\nЧанков по doc_id (топ 10):\")\n    for doc_id, c in doc_counts.most_common(10):\n        print(f\"  - {doc_id}: {c}\")\n\n    # распределение по heading_level\n    lvl_counts = Counter(d.metadata.get(\"heading_level\") for d in chunks)\n    print(\"\\nРаспределение по heading_level:\")\n    print(dict(lvl_counts))\n\n    # распределение по type\n    type_counts = Counter(d.metadata.get(\"type\") for d in chunks)\n    print(\"\\nРаспределение по type:\")\n    print(dict(type_counts))\n\ndef chunks_eda_tokens(chunks: List[Document], sample_size: int = 200):\n    \"\"\"\n    Грубая оценка длины чанков в словах (без LLM-токенизатора,\n    чтобы не зависеть от модели).\n    \"\"\"\n    print(\"\\n=== CHUNKS EDA (TOKENS ROUGH) ===\")\n    import re, random\n    sample = chunks if len(chunks) <= sample_size else random.sample(chunks, sample_size)\n    lens_words = [len(re.findall(r\"\\w+\", d.page_content)) for d in sample]\n    print(f\"По сэмплу из {len(sample)} чанков:\")\n    print(f\"Слова: min={min(lens_words)}, \"\n          f\"median={int(statistics.median(lens_words))}, \"\n          f\"mean={int(statistics.mean(lens_words))}, \"\n          f\"max={max(lens_words)}\")\n\n# Вызов:\nchunks_eda_basic(chunks)\nchunks_eda_tokens(chunks, sample_size=300)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_chunk_length_hist(chunks: List[Document], max_chars: int = 3000):\n    lens = [len(d.page_content) for d in chunks]\n    lens = [min(x, max_chars) for x in lens]  # обрезаем хвост\n    plt.figure(figsize=(6,4))\n    plt.hist(lens, bins=40)\n    plt.xlabel(\"Chunk length (chars, clipped)\")\n    plt.ylabel(\"Count\")\n    plt.title(\"Распределение длин чанков\")\n    plt.show()\n\n# Вызов:\nplot_chunk_length_hist(chunks)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Making candidates ","metadata":{}},{"cell_type":"markdown","source":"## bm25","metadata":{}},{"cell_type":"code","source":"try:\n    import spacy\n    _NLP = spacy.load(\"en_core_web_sm\")   # для RU можно ru_core_news_sm\nexcept Exception:\n    _NLP = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. TOKENIZATION / COMPOSITE TEXT\n# =====================\n\ndef _tok(s: str) -> List[str]:\n    if _NLP is not None:\n        doc = _NLP(s)\n        return [t.lemma_.lower() for t in doc if not t.is_space and not t.is_punct]\n    return re.findall(r\"\\b\\w+\\b\", s.lower())\n\ndef _tok_with_heading(d: Document) -> List[str]:\n    base = _tok(d.page_content)\n    head = _tok(d.metadata.get(\"heading\") or \"\")\n    path = _tok(\" / \".join(d.metadata.get(\"section_path\", [])))\n    return base + head*2 + path\n\n\ndef composite_text(d: Document) -> str:\n    path = \" / \".join(d.metadata.get(\"section_path\", [])) if d.metadata.get(\"section_path\") else \"\"\n    head = d.metadata.get(\"heading\") or \"\"\n    typ  = d.metadata.get(\"type\") or \"text\"\n    head_block = [f\"[TYPE] {typ}\"]\n    if path:\n        head_block.append(f\"[PATH] {path}\")\n    if head:\n        head_block.append(f\"[HEAD] {head}\")\n    return \"\\n\".join(head_block + [\"[TEXT] \" + d.page_content])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def dedup_by_chunk_id(docs: List[Document]) -> List[Document]:\n    seen, out = set(), []\n    for d in docs:\n        cid = d.metadata.get(\"_chunk_id\")\n        if cid in seen:\n            continue\n        seen.add(cid)\n        out.append(d)\n    return out\n\ndef build_bm25(chunks: List[Document]) -> BM25Okapi:\n    tokenized = []\n    for d in tqdm(chunks, desc=\"BM25 tokenization\"):\n        tokenized.append(_tok_with_heading(d))\n    bm25 = BM25Okapi(tokenized)\n    return bm25","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## FAISS","metadata":{}},{"cell_type":"code","source":"def build_faiss_index(chunks: List[Document],\n                      emb_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> LCFAISS:\n    emb = HuggingFaceEmbeddings(\n        model_name=emb_model_name,\n        encode_kwargs={\"normalize_embeddings\": True}\n    )\n    aug_docs = []\n    for d in chunks:\n        aug = composite_text(d)\n        md = {**d.metadata, \"_raw\": d.page_content}\n        aug_docs.append(Document(page_content=aug, metadata=md))\n    vs = LCFAISS.from_documents(aug_docs, emb)\n    return vs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def to_raw(doc: Document) -> Document:\n    raw = doc.metadata.get(\"_raw\")\n    if raw:\n        return Document(page_content=raw, metadata={k:v for k,v in doc.metadata.items() if k != \"_raw\"})\n    return doc\n\ndef map_to_base(docs_vs: List[Document], base_docs: List[Document]) -> List[Document]:\n    pool = {d.metadata.get(\"_chunk_id\"): d for d in base_docs}\n    out, seen = [], set()\n    for d in docs_vs:\n        cid = d.metadata.get(\"_chunk_id\")\n        if cid in pool and cid not in seen:\n            seen.add(cid)\n            out.append(pool[cid])\n    return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using","metadata":{}},{"cell_type":"code","source":"bm25 = build_bm25(chunks)\nvs   = build_faiss_index(chunks, emb_model_name=\"sentence-transformers/all-MiniLM-L6-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## for иерархия","metadata":{}},{"cell_type":"code","source":"bm25_sec = build_bm25(section_docs)\nvs_sec   = build_faiss_index(section_docs, emb_model_name=\"sentence-transformers/all-MiniLM-L6-v2\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fuze them (RRF)","metadata":{}},{"cell_type":"code","source":"def rrf_fusion(query: str,\n               vs: LCFAISS,\n               bm25: BM25Okapi,\n               base_docs: List[Document],\n               bm25_k: int = 200,\n               vec_k: int = 200,\n               final_k: int = 60,\n               C: int = 60) -> List[Document]:\n    q_tokens = _tok(query)\n    scores = bm25.get_scores(q_tokens)\n    order = np.argsort(scores)[::-1][:bm25_k]\n\n    vec = vs.similarity_search_with_score(query, k=vec_k)\n\n    rrf = {}\n    for r, idx in enumerate(order):\n        cid = base_docs[idx].metadata[\"_chunk_id\"]\n        rrf[cid] = rrf.get(cid, 0.0) + 1.0/(C + r + 1)\n    for r, (doc, _) in enumerate(vec):\n        cid = doc.metadata.get(\"_chunk_id\")\n        if cid is not None:\n            rrf[cid] = rrf.get(cid, 0.0) + 1.0/(C + r + 1)\n\n    top_ids = sorted(rrf, key=rrf.get, reverse=True)[:final_k]\n    id2doc = {d.metadata[\"_chunk_id\"]: d for d in base_docs}\n    return [id2doc[i] for i in top_ids]\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## MMR","metadata":{}},{"cell_type":"code","source":"def retrieve_mmr(vs: LCFAISS, query: str,\n                 k: int = 10,\n                 fetch_k: int = 60,\n                 lambda_mult: float = 0.5) -> List[Document]:\n    return vs.max_marginal_relevance_search(query, k=k, fetch_k=fetch_k, lambda_mult=lambda_mult)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using v2","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Cross encoder","metadata":{}},{"cell_type":"markdown","source":"### If want to do meta data in cross encoder also ","metadata":{}},{"cell_type":"code","source":"def as_ce_text(d: Document) -> str:\n    head = d.metadata.get(\"heading\") or \"\"\n    path = \" / \".join(d.metadata.get(\"section_path\", [])) or \"\"\n    prefix = \"\"\n    if path:\n        prefix += f\"SECTION: {path}\\n\"\n    if head:\n        prefix += f\"HEADING: {head}\\n\"\n    return (prefix + d.page_content).strip()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def retrieve_with_rerank(\n    query: str,\n    chunks: List[Document],\n    bm25: Optional[BM25Okapi],\n    vs: Optional[LCFAISS],\n    ce: Optional[CrossEncoder],\n    pre_k: int = 60,\n    final_k: int = 10,\n    use_rrf: bool = True,\n    use_mmr: bool = True,\n    adaptive_tau: Optional[float] = None,  # порог CE для adaptive k, если None → обычный top-k   0.35 good start  но сначала руками глянуть \n    min_k: int = 3,\n    max_k: int = 15,\n) -> List[Document]:\n    \"\"\"\n    1) BM25 + FAISS (RRF) -> кандидаты.\n    2) (опц.) MMR для разнообразия.\n    3) CrossEncoder:\n        - если adaptive_tau is None -> берём top-final_k;\n        - если adaptive_tau задан:\n            * берём все кандидаты со score >= tau,\n            * если их < min_k -> добираем до min_k top'ами,\n            * если их > max_k -> режем до max_k.\n    \"\"\"\n    # 1) Кандидаты\n    if bm25 is not None and vs is not None and use_rrf:\n        cands = rrf_fusion(query, vs, bm25, base_docs=chunks, final_k=pre_k)\n        if use_mmr:\n            try:\n                mmr_raw = retrieve_mmr(vs, query, k=min(pre_k, 50), fetch_k=min(3*pre_k, 180), lambda_mult=0.5)\n                mmr = map_to_base(mmr_raw, chunks)\n                cands = dedup_by_chunk_id(cands + mmr)[:pre_k]\n            except Exception:\n                pass\n    elif bm25 is not None:\n        q_tokens = _tok(query)\n        scores = bm25.get_scores(q_tokens)\n        order = np.argsort(scores)[::-1][:pre_k]\n        cands = [chunks[i] for i in order]\n    elif vs is not None:\n        raw = vs.similarity_search(query, k=pre_k)\n        cands = map_to_base(raw, chunks)\n    else:\n        raise RuntimeError(\"No indices available (BM25/FAISS).\")\n\n    # 2) CrossEncoder-rerank + adaptive k\n    if ce is not None and cands:\n        pairs = [[query, d.page_content] for d in cands]\n        #pairs = [[query, as_ce_text(d)] for d in cands] ________ Тут если мета дата \n        scores = ce.predict(pairs, batch_size=64)\n        # сортируем по score по убыванию\n        ranked = list(sorted(zip(cands, map(float, scores)), key=lambda x: x[1], reverse=True))\n\n        if adaptive_tau is not None:\n            # все кандидаты, у которых score >= tau\n            selected = [d for d, s in ranked if s >= adaptive_tau]\n\n            # если слишком мало — доберём top'ами, чтобы контекст не был пустой\n            if len(selected) < min_k:\n                selected = [d for d, _ in ranked[:min_k]]\n\n            # если слишком много — ограничим max_k, чтобы не взорвать окно LLM\n            if len(selected) > max_k:\n                selected = selected[:max_k]\n\n            return selected\n\n        # classic fixed top-k\n        return [d for d, _ in ranked[:final_k]]\n\n    # 3) Без CE — обычный top-k\n    return cands[:final_k]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## иерархия","metadata":{}},{"cell_type":"code","source":"def hierarchical_retrieve(\n    question: str,\n    section_docs: List[Document],\n    section_to_chunk_ids: Dict[int, List[int]],\n    all_chunks: List[Document],\n    bm25_sec: BM25Okapi,\n    vs_sec: LCFAISS,\n    ce: Optional[CrossEncoder],\n    top_sections: int = 5,\n    final_k_chunks: int = 10,\n    adaptive_tau: Optional[float] = 0.35,\n    min_k: int = 3,\n    max_k: int = 15,\n) -> List[Document]:\n    \"\"\"\n    1) На уровне секций: BM25+FAISS(+CE) -> top_sections.\n    2) Собираем все дочерние чанки из этих секций.\n    3) Переранкаем дочерние чанки CE и выбираем final_k_chunks.\n    \"\"\"\n\n    # --- 1) Ретрив по секциям (тот же retrieve_with_rerank, но на section_docs) ---\n    sec_cands = retrieve_with_rerank(\n        query=question,\n        chunks=section_docs,\n        bm25=bm25_sec,\n        vs=vs_sec,\n        ce=ce,\n        pre_k=30,\n        final_k=top_sections,\n        use_rrf=True,\n        use_mmr=True,\n        adaptive_tau=None,  # на уровне секций можно просто top-K\n    )\n\n    # --- 2) Собираем дочерние чанки ---\n    cand_chunk_ids = set()\n    for sd in sec_cands:\n        sec_idx = sd.metadata.get(\"section_idx\")\n        for cid in section_to_chunk_ids.get(sec_idx, []):\n            if cid is not None:\n                cand_chunk_ids.add(cid)\n\n    cand_chunks = [d for d in all_chunks if d.metadata.get(\"_chunk_id\") in cand_chunk_ids]\n\n    if not cand_chunks:\n        # fallback: обычный flat retriever\n        return retrieve_with_rerank(\n            question, all_chunks, bm25=None, vs=None, ce=ce,\n            pre_k=final_k_chunks, final_k=final_k_chunks,\n            adaptive_tau=adaptive_tau, min_k=min_k, max_k=max_k\n        )\n\n    # --- 3) CE-реранк только по этим дочерним чанкам ---\n    if ce is not None:\n        pairs = [[question, d.page_content] for d in cand_chunks]\n        scores = ce.predict(pairs, batch_size=64)\n        ranked = list(sorted(zip(cand_chunks, map(float, scores)), key=lambda x: x[1], reverse=True))\n\n        if adaptive_tau is not None:\n            selected = [d for d, s in ranked if s >= adaptive_tau]\n            if len(selected) < min_k:\n                selected = [d for d, _ in ranked[:min_k]]\n            if len(selected) > max_k:\n                selected = selected[:max_k]\n        else:\n            selected = [d for d, _ in ranked[:final_k_chunks]]\n\n        return selected\n\n    # без CE — просто top-N по BM25/FAISS на уровне секций уже ограничили пространство\n    return cand_chunks[:final_k_chunks]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## training cross encoder","metadata":{}},{"cell_type":"markdown","source":"### 1)\tОдна фраза из чанка, остальное в чанке позитив, любой другой чанк из другого документа негатив","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import InputExample\n\ndef make_training_pairs_sentence_query(\n    chunks: List[Document],\n    negatives_per_pos: int = 1,\n    max_q_len: int = 200,\n    max_pairs: int = 20000,\n    use_metadata: bool = False,\n) -> List[InputExample]:\n    \"\"\"\n    Первая фраза чанка = \"псевдо-запрос\".\n    Позитив — сам чанк.\n    Негативы:\n      * если документов > 1 → из других doc_id;\n      * если документ один → из других чанков того же doc_id.\n    \"\"\"\n    examples: List[InputExample] = []\n    all_docs = chunks[:]\n\n    # есть ли вообще несколько документов?\n    doc_ids = {d.metadata.get(\"doc_id\") for d in chunks}\n    multi_doc = len(doc_ids) > 1\n\n    def ce_text(d: Document) -> str:\n        if not use_metadata:\n            return d.page_content\n        head = d.metadata.get(\"heading\") or \"\"\n        path = \" / \".join(d.metadata.get(\"section_path\", [])) or \"\"\n        prefix = \"\"\n        if path:\n            prefix += f\"SECTION: {path}\\n\"\n        if head:\n            prefix += f\"HEADING: {head}\\n\"\n        return (prefix + d.page_content).strip()\n\n    for d in tqdm(chunks, desc=\"CE training pairs\"):\n        if len(examples) >= max_pairs:\n            break\n\n        sentences = re.split(r'(?<=[.!?])\\s+', d.page_content)\n        sentences = [s.strip() for s in sentences if s.strip()]\n        if not sentences:\n            continue\n\n        q = sentences[0][:max_q_len]\n        pos = ce_text(d)\n\n        # ---- выбираем негативы ----\n        if multi_doc:\n            this_doc_id = d.metadata.get(\"doc_id\")\n            neg_pool = [x for x in all_docs if x.metadata.get(\"doc_id\") != this_doc_id]\n        else:\n            this_cid = d.metadata.get(\"_chunk_id\")\n            neg_pool = [x for x in all_docs if x.metadata.get(\"_chunk_id\") != this_cid]\n\n        if not neg_pool:\n            continue\n\n        negs = random.sample(neg_pool, k=min(negatives_per_pos, len(neg_pool)))\n\n        # ---- добавляем пары ----\n        examples.append(InputExample(texts=[q, pos], label=1.0))\n        for n in negs:\n            examples.append(InputExample(texts=[q, ce_text(n)], label=0.0))\n\n        if len(examples) >= max_pairs:\n            break\n\n    return examples\n\nfrom torch.utils.data import Dataset\n\nclass CETrainDataset(Dataset):\n    def __init__(self, examples: List[InputExample], tokenizer, max_len: int = 512):\n        self.examples = examples\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        ex = self.examples[idx]\n        # ex.texts = [query, passage]\n        q, p = ex.texts\n        enc = self.tokenizer(\n            q,\n            p,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\",\n        )\n        item = {k: v.squeeze(0) for k, v in enc.items()}\n        item[\"labels\"] = torch.tensor(float(ex.label), dtype=torch.float32)\n        return item\n\n\n\ndef train_cross_encoder_from_chunks(\n    chunks: List[Document],\n    model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    epochs: int = 1,\n    batch_size: int = 16,\n    negatives_per_pos: int = 2,\n    output_path: str = \"ce_finetuned\",\n    max_pairs: int = 8000,\n    max_len: int = 512,\n    lr: float = 2e-5,\n) -> CrossEncoder:\n    \"\"\"\n    Ручной тренинг cross-encoder'а через HF, потом оборачиваем в CrossEncoder.\n\n    - собираем self-supervised пары (query, positive/negative chunk);\n    - режем до max_pairs, чтобы не улететь по времени;\n    - тренируем AutoModelForSequenceClassification на GPU;\n    - сохраняем и перезагружаем через CrossEncoder(output_path, device=...).\n    \"\"\"\n\n    # 1) собрали тренировочные примеры\n    train_ex = make_training_pairs_sentence_query(\n        chunks, negatives_per_pos=negatives_per_pos\n    )\n    if not train_ex:\n        raise ValueError(\"Нет тренировочных примеров для CE\")\n\n    # чуть ограничим датасет, чтобы не страдать\n    random.shuffle(train_ex)\n    if max_pairs is not None and len(train_ex) > max_pairs:\n        train_ex = train_ex[:max_pairs]\n\n    print(f\"[CE] Всего training examples: {len(train_ex)}\")\n\n    # 2) токенайзер и модель\n    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=1,\n    )\n    model.to(device)\n\n    # 3) датасет и dataloader\n    train_ds = CETrainDataset(train_ex, tokenizer, max_len=max_len)\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n    num_training_steps = epochs * len(train_dl)\n    warmup_steps = max(10, int(0.1 * num_training_steps))\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=warmup_steps,\n        num_training_steps=num_training_steps,\n    )\n    loss_fct = torch.nn.BCEWithLogitsLoss()\n\n    print(f\"[CE] Dataloader batches: {len(train_dl)}, warmup_steps: {warmup_steps}\")\n    print(f\"[CE] device: {device}\")\n\n    # 4) training loop\n    model.train()\n    for epoch in range(epochs):\n        pbar = tqdm(train_dl, desc=f\"CE epoch {epoch+1}/{epochs}\")\n        epoch_loss = 0.0\n        for step, batch in enumerate(pbar):\n            labels = batch.pop(\"labels\").to(device)\n            batch = {k: v.to(device) for k, v in batch.items()}\n\n            outputs = model(**batch)\n            logits = outputs.logits.view(-1)\n            loss = loss_fct(logits, labels)\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n            epoch_loss += loss.item()\n            pbar.set_postfix(loss=loss.item())\n\n        print(f\"[CE] Epoch {epoch+1} finished, mean loss = {epoch_loss/len(train_dl):.4f}\")\n\n    # 5) сохраняем и оборачиваем в CrossEncoder\n    if output_path:\n        model.save_pretrained(output_path)\n        tokenizer.save_pretrained(output_path)\n        print(f\"[CE] Saved finetuned CE to: {output_path}\")\n        ce = CrossEncoder(output_path, device=device, max_length=max_len)\n    else:\n        # fallback: можно обернуть напрямую, но надёжнее через сохранение\n        ce = CrossEncoder(output_path, device=device, max_length=max_len)\n\n    return ce","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## using","metadata":{}},{"cell_type":"code","source":"# 7) CrossEncoder (можно просто загрузить готовый, а можно дообучить)\nce = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n                  device=device, max_length=512)\n# или так, если хочешь дообучить:\n# ce = train_cross_encoder_from_chunks(chunks,\n#                                      model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n#                                      epochs=1,\n#                                      batch_size=16,\n#                                      negatives_per_pos=2,\n#                                      output_path=\"ce_finetuned\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\nce = train_cross_encoder_from_chunks(\n    chunks,\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    epochs=1,\n    batch_size=16,\n    negatives_per_pos=2,\n    output_path=\"ce_finetuned\",   # можно None, если не надо сохранять на диск\n    max_pairs=20000,\n    use_metadata=False,           # или True, если будешь в inference использовать as_ce_text\n)\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ce = train_cross_encoder_from_chunks(\n    chunks,\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    epochs=1,\n    batch_size=16,\n    negatives_per_pos=2,\n    output_path=\"ce_finetuned\",\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"GEN_MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\ntok = AutoTokenizer.from_pretrained(GEN_MODEL, use_fast=True)\nmdl = AutoModelForCausalLM.from_pretrained(\n    GEN_MODEL,\n    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n    device_map=\"auto\"\n).eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def token_len(text: str) -> int:\n    return len(tok(text, add_special_tokens=False).input_ids)\n\ndef truncate_by_tokens(text: str, max_tokens: int) -> str:\n    ids = tok(text, add_special_tokens=False, truncation=True, max_length=max_tokens).input_ids\n    return tok.decode(ids, skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build prompt","metadata":{}},{"cell_type":"code","source":"def build_prompt(question: str,\n                 ctx_docs: List[Document],\n                 max_ctx_tokens: int = 2500,\n                 max_per_snippet: int = 400) -> str:\n    \"\"\"\n    Формируем prompt:\n    - каждый чанк как [S#] ...;\n    - просим отвечать кратко, с цитатами вида [S1], [S2].\n    \"\"\"\n    chunks_text = []\n    used = 0\n    for i, d in enumerate(ctx_docs, 1):\n        body = d.page_content.strip()\n        body = truncate_by_tokens(body, max_per_snippet)\n        tag  = f\"[S{i}]\"\n        block = f\"{tag} {body}\"\n        t = token_len(block)\n        if used + t > max_ctx_tokens:\n            break\n        chunks_text.append(block)\n        used += t\n\n    ctx = \"\\n\\n\".join(chunks_text)\n    prompt = f\"\"\"You are a careful QA assistant.\n\nUse ONLY the context snippets [S1..S{len(chunks_text)}] below.\nEvery factual sentence in your answer MUST end with one or more citations like [S1], [S2].\nIf the answer is not supported by the context, say \"I don't know\".\n\nCONTEXT:\n{ctx}\n\nQUESTION:\n{question}\n\nAnswer in the same language as the question. Be concise (3–6 sentences).\"\"\"\n    return prompt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## LLM generate","metadata":{}},{"cell_type":"code","source":"def llm_generate(prompt: str,\n                 max_new_tokens: int = 300,\n                 temperature: float = 0.2,\n                 top_p: float = 0.9) -> str:\n    if hasattr(tok, \"apply_chat_template\"):\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a precise assistant. Follow instructions exactly.\"},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        input_ids = tok.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(mdl.device)\n        attention_mask = torch.ones_like(input_ids)\n        with torch.no_grad():\n            out_ids = mdl.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_new_tokens=max_new_tokens,\n                do_sample=(temperature > 0.0),\n                temperature=temperature if temperature > 0.0 else None,\n                top_p=top_p if temperature > 0.0 else None,\n            )\n        text = tok.decode(out_ids[0][input_ids.shape[-1]:], skip_special_tokens=True)\n        return text\n    else:\n        pipe = hf_pipeline(\"text-generation\", model=mdl, tokenizer=tok)\n        out = pipe(prompt, max_new_tokens=max_new_tokens, temperature=temperature, top_p=top_p,\n                   return_full_text=False)[0][\"generated_text\"]\n        return out","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## generate answer","metadata":{}},{"cell_type":"code","source":"def generate_answer(question: str,\n                    ctx_docs: List[Document],\n                    temperature: float = 0.2) -> str:\n    prompt = build_prompt(question, ctx_docs)\n    answer = llm_generate(prompt, max_new_tokens=300, temperature=temperature)\n    return answer.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"def build_context_for_submission(ctx_docs: List[Document], max_chars: int = 8000) -> str:\n    parts, used = [], 0\n    for i, d in enumerate(ctx_docs, 1):\n        block = f\"[S{i}] {d.page_content.strip()}\"\n        #block = f\"[S{i}] (section: {' / '.join(path)})\\n{body}\" ___________ Если хочется мета данные в контекст\n\n        if used + len(block) + 2 > max_chars:\n            break\n        parts.append(block)\n        used += len(block) + 2\n    return \"\\n\\n\".join(parts)\n\ndef build_references_simple(ctx_docs: List[Document]) -> Dict[str, List[str]]:\n    \"\"\"Простая ссылка: секции + страницы из метаданных.\"\"\"\n    sections, pages = [], []\n    for d in ctx_docs:\n        path = d.metadata.get(\"section_path\") or []\n        if path:\n            sections.append(\"/\".join(path))\n        pg = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or d.metadata.get(\"_page\")\n        if pg is not None:\n            pages.append(str(pg))\n    # dedup с сохранением порядка\n    def uniq(seq):\n        seen, out = set(), []\n        for x in seq:\n            if x not in seen:\n                seen.add(x); out.append(x)\n        return out\n    return {\n        \"sections\": uniq(sections)[:6],\n        \"pages\": uniq(pages)[:10],\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Проверка на одном запросе","metadata":{}},{"cell_type":"code","source":"USE_HIER = True\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def answer_one_question(\n    question: str,\n    chunks: List[Document],\n    bm25: BM25Okapi,\n    vs: LCFAISS,\n    ce: CrossEncoder,\n    adaptive_tau: float = 0.35,\n    use_hierarchical: bool = False,\n) -> Dict[str, Any]:\n    if use_hierarchical:\n        ctx_docs = hierarchical_retrieve(\n            question=question,\n            section_docs=section_docs,\n            section_to_chunk_ids=section_to_chunk_ids,\n            all_chunks=chunks,\n            bm25_sec=bm25_sec,\n            vs_sec=vs_sec,\n            ce=ce,\n            top_sections=5,\n            final_k_chunks=10,\n            adaptive_tau=adaptive_tau,\n        )\n    else:\n        ctx_docs = retrieve_with_rerank(\n            question=question,\n            chunks=chunks,\n            bm25=bm25,\n            vs=vs,\n            ce=ce,\n            pre_k=60,\n            final_k=10,\n            use_rrf=True,\n            use_mmr=True,\n            adaptive_tau=adaptive_tau,\n            min_k=3,\n            max_k=15,\n        )\n\n    # 2) Answer\n    answer_text = generate_answer(question, ctx_docs)\n\n    # 3) Context + references\n    context_text = build_context_for_submission(ctx_docs)\n    refs = build_references_simple(ctx_docs)\n\n    return {\n        \"answer\": answer_text,\n        \"context\": context_text,\n        \"references\": json.dumps(refs, ensure_ascii=False),\n        \"ctx_docs\": ctx_docs,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"q = \"О чём раздел по моделям доходов и чем они отличаются от прошлой версии?\"\nres = answer_one_question(q, chunks, bm25, vs, ce, adaptive_tau=0.35, use_hierarchical=True, )\n\nprint(\"ANSWER:\\n\", res[\"answer\"])\nprint(\"\\nCONTEXT SNIPPETS:\\n\", res[\"context\"][:2000])\nprint(\"\\nREFERENCES:\", res[\"references\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA for like Danya said ","metadata":{}},{"cell_type":"code","source":"def _short(text: str, n: int = 200) -> str:\n    \"\"\"Обрезаем текст для выдачи в логах.\"\"\"\n    text = (text or \"\").replace(\"\\n\", \" \").strip()\n    return (text[:n] + \"…\") if len(text) > n else text\n\ndef _print_docs_block(title: str,\n                      docs: List[Document],\n                      scores: Optional[List[float]] = None,\n                      max_n: int = 5):\n    print(f\"\\n=== {title} (top {min(max_n, len(docs))}) ===\")\n    for i, d in enumerate(docs[:max_n], 1):\n        cid   = d.metadata.get(\"_chunk_id\")\n        docid = d.metadata.get(\"doc_id\")\n        pg    = d.metadata.get(\"page\") or d.metadata.get(\"page_number\") or d.metadata.get(\"_page\")\n        head  = d.metadata.get(\"heading\")\n        typ   = d.metadata.get(\"type\")\n        sc    = f\"score={scores[i-1]:.4f}\" if scores is not None and i-1 < len(scores) else \"\"\n        print(f\"[{i}] cid={cid} {sc}\")\n        print(f\"    doc_id={docid}, page={pg}, type={typ}, heading={repr(head)}\")\n        print(f\"    text: {_short(d.page_content)}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def top_bm25_docs(query: str,\n                  bm25: BM25Okapi,\n                  chunks: List[Document],\n                  k: int = 20):\n    \"\"\"Топ-чunks по BM25 + их score.\"\"\"\n    q_tokens = _tok(query)\n    scores = bm25.get_scores(q_tokens)\n    order = np.argsort(scores)[::-1][:k]\n    docs  = [chunks[i] for i in order]\n    top_scores = [scores[i] for i in order]\n    return docs, top_scores\n\ndef top_faiss_docs(query: str,\n                   vs: LCFAISS,\n                   chunks: List[Document],\n                   k: int = 20):\n    \"\"\"Топ-chunks по FAISS (vector search) + расстояние/score из similarity_search_with_score.\"\"\"\n    res = vs.similarity_search_with_score(query, k=k)\n    docs = []\n    scores = []\n    for d, s in res:\n        # s — дистанция (для FAISS) или «очень какая-то метрика», чем меньше, тем ближе;\n        # для удобства можно инвертировать, но для EDA достаточно видеть относительный порядок.\n        cid = d.metadata.get(\"_chunk_id\")\n        # мапнем к base-чункам, чтобы метадата совпадала\n        for base in chunks:\n            if base.metadata.get(\"_chunk_id\") == cid:\n                docs.append(base)\n                scores.append(float(-s))  # сделаем «чем больше, тем лучше»\n                break\n    return docs, scores\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def debug_question(\n    question: str,\n    chunks: List[Document],\n    bm25: Optional[BM25Okapi],\n    vs: Optional[LCFAISS],\n    ce: Optional[CrossEncoder],\n    pre_k: int = 60,\n    final_k: int = 10,\n    adaptive_tau: Optional[float] = 0.35,\n    min_k: int = 3,\n    max_k: int = 15,\n    show_n: int = 5,\n) -> Dict[str, Any]:\n    \"\"\"\n    Полный EDA по одному запросу:\n    - BM25 top-k\n    - FAISS top-k\n    - RRF + (опц.) MMR\n    - CE-rerank (+ adaptive k)\n    - итоговый контекст\n    - ответ LLM\n    \"\"\"\n\n    print(\"\\n\" + \"=\"*80)\n    print(f\"DEBUG QUESTION: {question}\")\n    print(\"=\"*80)\n\n    # 1) BM25\n    if bm25 is not None:\n        bm25_docs, bm25_scores = top_bm25_docs(question, bm25, chunks, k=pre_k)\n        _print_docs_block(\"BM25 TOP\", bm25_docs, bm25_scores, max_n=show_n)\n    else:\n        print(\"\\n[BM25] Нет индекса.\")\n\n    # 2) FAISS\n    if vs is not None:\n        faiss_docs, faiss_scores = top_faiss_docs(question, vs, chunks, k=pre_k)\n        _print_docs_block(\"FAISS TOP\", faiss_docs, faiss_scores, max_n=show_n)\n    else:\n        print(\"\\n[FAISS] Нет индекса.\")\n\n    # 3) Кандидаты RRF + MMR (как в retrieve_with_rerank, но без CE)\n    print(\"\\n--- CANDIDATES (RRF + MMR) ---\")\n    if bm25 is not None and vs is not None:\n        cands = rrf_fusion(question, vs, bm25, base_docs=chunks, final_k=pre_k)\n        try:\n            mmr_raw = retrieve_mmr(vs, question, k=min(pre_k, 50),\n                                   fetch_k=min(3*pre_k, 180), lambda_mult=0.5)\n            mmr = map_to_base(mmr_raw, chunks)\n            cands = dedup_by_chunk_id(cands + mmr)[:pre_k]\n        except Exception as e:\n            print(\"MMR error:\", e)\n    elif bm25 is not None:\n        q_tokens = _tok(question)\n        scores = bm25.get_scores(q_tokens)\n        order = np.argsort(scores)[::-1][:pre_k]\n        cands = [chunks[i] for i in order]\n    elif vs is not None:\n        raw = vs.similarity_search(question, k=pre_k)\n        cands = map_to_base(raw, chunks)\n    else:\n        raise RuntimeError(\"No indices available (BM25/FAISS).\")\n\n    _print_docs_block(\"RRF+MMR CANDIDATES\", cands, scores=None, max_n=show_n)\n\n    # 4) CE-rerank + adaptive k\n    print(\"\\n--- CE RERANK (+ adaptive k) ---\")\n    if ce is not None and cands:\n        pairs = [[question, d.page_content] for d in cands]\n        scores = ce.predict(pairs, batch_size=64)\n        ranked = list(sorted(zip(cands, map(float, scores)), key=lambda x: x[1], reverse=True))\n\n        if adaptive_tau is not None:\n            selected = [d for d, s in ranked if s >= adaptive_tau]\n            if len(selected) < min_k:\n                selected = [d for d, _ in ranked[:min_k]]\n            if len(selected) > max_k:\n                selected = selected[:max_k]\n        else:\n            selected = [d for d, _ in ranked[:final_k]]\n\n        ctx_docs = selected\n        _print_docs_block(\"CE TOP (FINAL CONTEXT CANDIDATES)\",\n                          [d for d, _ in ranked],\n                          [s for _, s in ranked],\n                          max_n=show_n)\n        print(f\"\\n[INFO] CE adaptive_tau={adaptive_tau}, выбрано в контекст: {len(ctx_docs)} чанков\")\n    else:\n        print(\"[CE] Нет модели, используем просто кандидатов без rerank.\")\n        ctx_docs = cands[:final_k]\n\n    # 5) Генерация ответа\n    print(\"\\n--- LLM ANSWER ---\")\n    answer_text = generate_answer(question, ctx_docs)\n    print(\"ANSWER:\\n\", answer_text)\n\n    # 6) Контекст и простые references\n    context_text = build_context_for_submission(ctx_docs)\n    refs = build_references_simple(ctx_docs)\n\n    print(\"\\n--- FINAL CONTEXT (TRUNCATED) ---\")\n    print(context_text[:2000])\n\n    print(\"\\n--- REFERENCES ---\")\n    print(json.dumps(refs, ensure_ascii=False, indent=2))\n\n    return {\n        \"answer\": answer_text,\n        \"context\": context_text,\n        \"references\": refs,\n        \"ctx_docs\": ctx_docs,\n        \"bm25_top\": bm25_docs if bm25 is not None else [],\n        \"faiss_top\": faiss_docs if vs is not None else [],\n        \"candidates\": cands,\n    }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_q = \"What is the scientific method in psychology?\"\ndbg = debug_question(\n    test_q,\n    chunks=chunks,\n    bm25=bm25,\n    vs=vs,\n    ce=ce,\n    pre_k=60,\n    final_k=10,\n    adaptive_tau=0.35,\n    min_k=3,\n    max_k=15,\n    show_n=5,   # сколько показывать на каждом шаге\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit ","metadata":{}},{"cell_type":"code","source":"def make_submission(\n    queries_json_path: str,\n    out_csv: str = \"submission.csv\",\n    adaptive_tau: float = 0.35,\n) -> pd.DataFrame:\n    \"\"\"\n    Делает submission:\n    - читает queries.json,\n    - для каждого вопроса вызывает answer_one_question,\n    - собирает DataFrame с колонками [ID, context, answer, references],\n    - сохраняет в CSV.\n    ОПИРАЕТСЯ на уже подготовленные: chunks, bm25, vs, ce.\n    \"\"\"\n    # 1) читаем queries.json\n    data = json.loads(Path(queries_json_path).read_text(encoding=\"utf-8\"))\n\n    # допускаем, что формат может быть либо {\"queries\": [...]}, либо просто [...]\n    if isinstance(data, dict):\n        items = data.get(\"queries\") or data.get(\"data\") or data.get(\"items\") or data.get(\"questions\") or []\n    elif isinstance(data, list):\n        items = data\n    else:\n        raise ValueError(\"Unsupported queries.json format\")\n\n    assert items, \"Не нашли список запросов в queries.json\"\n\n    rows = []\n    for it in tqdm(items, desc=\"Answering\"):\n        # вытаскиваем ID и сам текст запроса\n        qid = it.get(\"ID\") or it.get(\"id\") or it.get(\"query_id\")\n        question = it.get(\"question\") or it.get(\"query\") or it.get(\"text\")\n\n        if qid is None or not question:\n            continue\n\n        # аккуратно приводим ID к int\n        try:\n            qid_int = int(qid)\n        except Exception:\n            qid_int = int(\"\".join(ch for ch in str(qid) if ch.isdigit()) or 0)\n\n        # получаем ответ с помощью нашего пайплайна\n        out = answer_one_question(\n            question=question,\n            chunks=chunks,\n            bm25=bm25,\n            vs=vs,\n            ce=ce,\n            adaptive_tau=adaptive_tau,\n            use_hierarchical=True, # ____________________ \n        )\n\n        ans = out[\"answer\"].replace(\"\\r\", \" \").replace(\"\\n\", \" \").strip()\n\n        rows.append({\n            \"ID\": qid_int,\n            \"context\": out[\"context\"],\n            \"answer\": ans,\n            \"references\": out[\"references\"],\n        })\n\n    df = pd.DataFrame(rows).sort_values(\"ID\")\n    df.to_csv(out_csv, index=False)\n    _log(f\"Saved submission to: {out_csv}\")\n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## queries eda","metadata":{}},{"cell_type":"code","source":"def queries_eda(queries_json_path: str):\n    print(\"\\n=== QUERIES EDA ===\")\n    data = json.loads(Path(queries_json_path).read_text(encoding=\"utf-8\"))\n    if isinstance(data, dict):\n        items = data.get(\"queries\") or data.get(\"data\") or data.get(\"items\") or data.get(\"questions\") or []\n    elif isinstance(data, list):\n        items = data\n    else:\n        raise ValueError(\"Unsupported queries.json format\")\n\n    texts = []\n    for it in items:\n        q = it.get(\"question\") or it.get(\"query\") or it.get(\"text\")\n        if q:\n            texts.append(q.strip())\n\n    print(f\"Всего запросов: {len(texts)}\")\n    lens_chars = [len(t) for t in texts]\n    lens_words = [len(t.split()) for t in texts]\n\n    import statistics\n    print(f\"Длина запроса (символы): min={min(lens_chars)}, \"\n          f\"median={int(statistics.median(lens_chars))}, \"\n          f\"mean={int(statistics.mean(lens_chars))}, \"\n          f\"max={max(lens_chars)}\")\n    print(f\"Длина запроса (слова):   min={min(lens_words)}, \"\n          f\"median={int(statistics.median(lens_words))}, \"\n          f\"mean={int(statistics.mean(lens_words))}, \"\n          f\"max={max(lens_words)}\")\n\n    print(\"\\nПримеры коротких запросов:\")\n    for t in sorted(texts, key=len)[:5]:\n        print(\"  -\", t)\n\n    print(\"\\nПримеры длинных запросов:\")\n    for t in sorted(texts, key=len, reverse=True)[:5]:\n        print(\"  -\", t)\n\n# Вызов:\nqueries_eda(\"/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/queries.json\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"queries_path = \"/kaggle/input/casml-generative-ai-hackathon/Dataset_RAG (1)/queries.json\"\n\ndf_sub = make_submission(\n    queries_json_path=queries_path,\n    out_csv=\"submission.csv\",\n    adaptive_tau=0.35,\n)\n\ndf_sub.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}