{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85594,"databundleVersionId":9767247,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# PDF Loader","metadata":{}},{"cell_type":"code","source":"import pdfplumber\n\ndef load_pdf(path: str) -> str:\n    text = []\n    with pdfplumber.open(path) as pdf:\n        for page in pdf.pages:\n            text.append(page.extract_text() or \"\")\n    return \"\\n\".join(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DOCX Loader","metadata":{}},{"cell_type":"code","source":"import docx\n\ndef load_docx(path: str) -> str:\n    document = docx.Document(path)\n    text = [p.text for p in document.paragraphs]\n    return \"\\n\".join(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# HTML Loader","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\n\ndef load_html(path: str) -> str:\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        html = f.read()\n\n    soup = BeautifulSoup(html, \"html.parser\")\n    return soup.get_text(separator=\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Web Page Loader","metadata":{}},{"cell_type":"code","source":"import requests\nfrom bs4 import BeautifulSoup\n\ndef load_webpage(url: str) -> str:\n    response = requests.get(url, timeout=10)\n    response.raise_for_status()\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    return soup.get_text(separator=\"\\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# API Loader","metadata":{}},{"cell_type":"code","source":"import requests\n\ndef load_api(url: str, params=None, headers=None) -> str:\n    response = requests.get(url, params=params, headers=headers, timeout=10)\n    response.raise_for_status()\n\n    # нормализация JSON → текст\n    data = response.json()\n\n    def flatten_json(obj):\n        if isinstance(obj, dict):\n            return \"\\n\".join(f\"{k}: {flatten_json(v)}\" for k, v in obj.items())\n        elif isinstance(obj, list):\n            return \"\\n\".join(flatten_json(x) for x in obj)\n        return str(obj)\n\n    return flatten_json(data)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Database Loader","metadata":{}},{"cell_type":"code","source":"import sqlite3\n\ndef load_sqlite(db_path: str, query: str) -> str:\n    conn = sqlite3.connect(db_path)\n    cur = conn.cursor()\n    cur.execute(query)\n\n    rows = cur.fetchall()\n    columns = [desc[0] for desc in cur.description]\n\n    conn.close()\n\n    lines = []\n    for row in rows:\n        lines.append(\", \".join(f\"{col}: {val}\" for col, val in zip(columns, row)))\n\n    return \"\\n\".join(lines)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Image Loader (OCR)","metadata":{}},{"cell_type":"code","source":"import pytesseract\nfrom PIL import Image\n\ndef load_image(path: str) -> str:\n    img = Image.open(path)\n    text = pytesseract.image_to_string(img, lang=\"eng\")\n    return text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Нормализация текста","metadata":{}},{"cell_type":"code","source":"import re\n\ndef normalize(text: str) -> str:\n    # уменьшить повторяющиеся пробелы\n    text = re.sub(r\"\\s+\", \" \", text)\n\n    # trim\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Очистка текста","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clean_text(text: str) -> str:\n    # убрать HTML-мусор\n    text = re.sub(r\"<[^>]+>\", \"\", text)\n\n    # убрать не-ASCII (если нужно)\n    # text = text.encode(\"ascii\", \"ignore\").decode()\n\n    # убрать повтор строк\n    lines = list(dict.fromkeys(text.split(\"\\n\")))\n    text = \"\\n\".join(lines)\n\n    return text.strip()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# модели","metadata":{}},{"cell_type":"code","source":"# ЛУЧШИЙ ВЫБОР - оптимальное качество/производительность\nMODELS = {\n    \"qwen2.5-7b-instruct\": \"Qwen/Qwen2.5-7B-Instruct\",  # Идеально для A100 20GB\n    \"qwen2.5-14b-instruct\": \"Qwen/Qwen2.5-14B-Instruct\",  # Хорошо влезает\n    \"qwen2.5-1.5b-instruct\": \"Qwen/Qwen2.5-1.5B-Instruct\",  # Супер быстрая\n}\n\n# Для RAG особенно хороши:\nRAG_OPTIMIZED = \"Qwen/Qwen2.5-7B-Instruct\"  # Лучший баланс","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"LLAMA_MODELS = {\n    \"llama-3-8b-instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",  # Отлично влезает\n    \"llama-3-1-8b-instruct\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n    \"llama-3-1-405b-instruct\": \"meta-llama/Meta-Llama-3.1-405B-Instruct\",  # ТОЛЬКО через API\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MISTRAL_MODELS = {\n    \"mistral-7b-instruct\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    \"mixtral-8x7b-instruct\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",  # Через quantization\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MULTILINGUAL_MODELS = {\n    \"rugpt3-medium\": \"sberbank-ai/rugpt3medium_based_on_gpt2\",  # Русская, компактная\n    \"rubert-tiny2\": \"cointegrated/rubert-tiny2\",  # Русская, очень легкая\n    \"multilingual-e5\": \"intfloat/multilingual-e5-large\",  # Для эмбеддингов\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def setup_optimal_rag_system():\n    \"\"\"Настройка оптимальной RAG системы для A100 20GB\"\"\"\n    \n    config = {\n        # Основная модель для ответов\n        \"answer_llm\": \"Qwen/Qwen2.5-7B-Instruct\",\n        \n        # Модель для верификации (можно легче)\n        \"verification_llm\": \"Qwen/Qwen2.5-1.5B-Instruct\", \n        \n        # Модель для эмбеддингов\n        \"embedding_model\": \"intfloat/multilingual-e5-large\",\n        \n        # Кросс-энкодер для реранкинга\n        \"cross_encoder\": \"cross-encoder/ms-marco-MiniLM-L-6-v2\",\n    }\n    \n    return config\n\n# Альтернативные конфигурации\nCONFIGURATIONS = {\n    \"quality_priority\": {\n        \"answer_llm\": \"Qwen/Qwen2.5-14B-Instruct\",\n        \"verification_llm\": \"Qwen/Qwen2.5-7B-Instruct\",\n        \"embedding_model\": \"intfloat/multilingual-e5-large\", \n        \"cross_encoder\": \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n    },\n    \n    \"speed_priority\": {\n        \"answer_llm\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n        \"verification_llm\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n        \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n        \"cross_encoder\": \"cross-encoder/ms-marco-TinyBERT-L-2-v2\"\n    },\n    \n    \"multilingual\": {\n        \"answer_llm\": \"Qwen/Qwen2.5-7B-Instruct\",  # Хорошая поддержка языков\n        \"verification_llm\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n        \"embedding_model\": \"intfloat/multilingual-e5-large\",\n        \"cross_encoder\": \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n    }\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# rag pipeline","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:20.026148Z","iopub.execute_input":"2025-11-17T19:51:20.026433Z","iopub.status.idle":"2025-11-17T19:51:21.743391Z","shell.execute_reply.started":"2025-11-17T19:51:20.026411Z","shell.execute_reply":"2025-11-17T19:51:21.742787Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:23.787624Z","iopub.execute_input":"2025-11-17T19:51:23.788390Z","iopub.status.idle":"2025-11-17T19:51:23.791854Z","shell.execute_reply.started":"2025-11-17T19:51:23.788364Z","shell.execute_reply":"2025-11-17T19:51:23.791043Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"sub =['convfinqa', 'financebench', 'finder', 'finqa', 'finqabench', 'multiheirtt', 'tatqa']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:26.655486Z","iopub.execute_input":"2025-11-17T19:51:26.655751Z","iopub.status.idle":"2025-11-17T19:51:26.659538Z","shell.execute_reply.started":"2025-11-17T19:51:26.655730Z","shell.execute_reply":"2025-11-17T19:51:26.658921Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from pathlib import Path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:29.644523Z","iopub.execute_input":"2025-11-17T19:51:29.644832Z","iopub.status.idle":"2025-11-17T19:51:29.648755Z","shell.execute_reply.started":"2025-11-17T19:51:29.644807Z","shell.execute_reply":"2025-11-17T19:51:29.648077Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_json(path):\n    data = []\n    with open(path, 'r', encoding='utf-8') as f:\n        for line in f:\n            data.append(json.loads(line))\n    return data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:39.109426Z","iopub.execute_input":"2025-11-17T19:51:39.110032Z","iopub.status.idle":"2025-11-17T19:51:39.114331Z","shell.execute_reply.started":"2025-11-17T19:51:39.110005Z","shell.execute_reply":"2025-11-17T19:51:39.113522Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"corpus = load_json('/kaggle/input/icaif-24-finance-rag-challenge/convfinqa_corpus.jsonl/corpus.jsonl')\nquery = load_json('/kaggle/input/icaif-24-finance-rag-challenge/convfinqa_queries.jsonl/queries.jsonl')\ncor_df = pd.DataFrame(corpus)\nque_df = pd.DataFrame(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:42.363023Z","iopub.execute_input":"2025-11-17T19:51:42.363256Z","iopub.status.idle":"2025-11-17T19:51:42.582501Z","shell.execute_reply.started":"2025-11-17T19:51:42.363240Z","shell.execute_reply":"2025-11-17T19:51:42.581756Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"que_df['title'].nunique()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-12T16:30:19.291070Z","iopub.execute_input":"2025-11-12T16:30:19.291606Z","iopub.status.idle":"2025-11-12T16:30:19.307420Z","shell.execute_reply.started":"2025-11-12T16:30:19.291584Z","shell.execute_reply":"2025-11-12T16:30:19.306640Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"1"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"sub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:49.583444Z","iopub.execute_input":"2025-11-17T19:51:49.584033Z","iopub.status.idle":"2025-11-17T19:51:49.589427Z","shell.execute_reply.started":"2025-11-17T19:51:49.584009Z","shell.execute_reply":"2025-11-17T19:51:49.588832Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"['convfinqa',\n 'financebench',\n 'finder',\n 'finqa',\n 'finqabench',\n 'multiheirtt',\n 'tatqa']"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"for i in sub[1:]:\n    corpus = load_json(f'/kaggle/input/icaif-24-finance-rag-challenge/{i}_corpus.jsonl/corpus.jsonl')\n    query = load_json(f'/kaggle/input/icaif-24-finance-rag-challenge/{i}_queries.jsonl/queries.jsonl')\n    corpus_df = pd.DataFrame(corpus)\n    query_df = pd.DataFrame(query)\n    cor_df = pd.concat([corpus_df, cor_df], ignore_index=True)\n    que_df = pd.concat([query_df, que_df], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:52.214756Z","iopub.execute_input":"2025-11-17T19:51:52.215354Z","iopub.status.idle":"2025-11-17T19:51:53.561276Z","shell.execute_reply.started":"2025-11-17T19:51:52.215328Z","shell.execute_reply":"2025-11-17T19:51:53.560460Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"cor_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T16:06:14.641654Z","iopub.execute_input":"2025-11-17T16:06:14.642332Z","iopub.status.idle":"2025-11-17T16:06:14.666118Z","shell.execute_reply.started":"2025-11-17T16:06:14.642310Z","shell.execute_reply":"2025-11-17T16:06:14.665349Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"             _id title                                               text\n0      d1b2e74c0        The following tables present the recorded inve...\n1      d1b32cade        NOTE 13. INCOME TAXES\\nWe calculate our provis...\n2      d1b38504e        8. Earnings Per Share\\nBasic earnings per shar...\n3      d1b37e6a4        AMERICAN TOWER CORPORATION AND SUBSIDIARIES NO...\n4      d1b3a8e2c        The tax effects of temporary differences that ...\n...          ...   ...                                                ...\n32220  dd4c4dfcc        item 2 .\\nproperties our principal offices are...\n32221  dd4c246b8        the company 2019s stock performance the follow...\n32222  dd4c57766        undesignated hedges was $ 41.2 million and $ 4...\n32223  dd4b8d39e        the following table summarizes the changes in ...\n32224  dd4baa80e        third-party sales for this segment increased 4...\n\n[32225 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>title</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d1b2e74c0</td>\n      <td></td>\n      <td>The following tables present the recorded inve...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d1b32cade</td>\n      <td></td>\n      <td>NOTE 13. INCOME TAXES\\nWe calculate our provis...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d1b38504e</td>\n      <td></td>\n      <td>8. Earnings Per Share\\nBasic earnings per shar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d1b37e6a4</td>\n      <td></td>\n      <td>AMERICAN TOWER CORPORATION AND SUBSIDIARIES NO...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d1b3a8e2c</td>\n      <td></td>\n      <td>The tax effects of temporary differences that ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>32220</th>\n      <td>dd4c4dfcc</td>\n      <td></td>\n      <td>item 2 .\\nproperties our principal offices are...</td>\n    </tr>\n    <tr>\n      <th>32221</th>\n      <td>dd4c246b8</td>\n      <td></td>\n      <td>the company 2019s stock performance the follow...</td>\n    </tr>\n    <tr>\n      <th>32222</th>\n      <td>dd4c57766</td>\n      <td></td>\n      <td>undesignated hedges was $ 41.2 million and $ 4...</td>\n    </tr>\n    <tr>\n      <th>32223</th>\n      <td>dd4b8d39e</td>\n      <td></td>\n      <td>the following table summarizes the changes in ...</td>\n    </tr>\n    <tr>\n      <th>32224</th>\n      <td>dd4baa80e</td>\n      <td></td>\n      <td>third-party sales for this segment increased 4...</td>\n    </tr>\n  </tbody>\n</table>\n<p>32225 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"cor_df['text+title'] = cor_df['title'] + ' ' + cor_df['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:51:58.070774Z","iopub.execute_input":"2025-11-17T19:51:58.071385Z","iopub.status.idle":"2025-11-17T19:51:58.165570Z","shell.execute_reply.started":"2025-11-17T19:51:58.071359Z","shell.execute_reply":"2025-11-17T19:51:58.164773Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"que_df['title+text'] = que_df['title'] + ' ' + que_df['text']\nque_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:52:00.794050Z","iopub.execute_input":"2025-11-17T19:52:00.794671Z","iopub.status.idle":"2025-11-17T19:52:00.821141Z","shell.execute_reply.started":"2025-11-17T19:52:00.794649Z","shell.execute_reply":"2025-11-17T19:52:00.820299Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"            _id title                                               text  \\\n0     q1a73c1d4        In which year was interest income greater than...   \n1     q1a72ef98                  What was the Net Income (Loss) in 2019?   \n2     q1a716380        What was the percentage of Plan Assets for Oth...   \n3     q1a73f97e        What was the service revenue from Americas in ...   \n4     q1a731ee6        What was the Customer support renewal rate for...   \n...         ...   ...                                                ...   \n4666  qd497c816        what was the decrease amount on the net intere...   \n4667  qd4988ecc        what was the total of operating expenses in 2018?   \n4668  qd4982266             what is the current ratio of robert mondavi?   \n4669  qd4985e34                       what was the rent expense in 2007?   \n4670  qd4989714        between 2016 and 2017, what was the variation ...   \n\n                                             title+text  \n0      In which year was interest income greater tha...  \n1               What was the Net Income (Loss) in 2019?  \n2      What was the percentage of Plan Assets for Ot...  \n3      What was the service revenue from Americas in...  \n4      What was the Customer support renewal rate fo...  \n...                                                 ...  \n4666   what was the decrease amount on the net inter...  \n4667   what was the total of operating expenses in 2...  \n4668       what is the current ratio of robert mondavi?  \n4669                 what was the rent expense in 2007?  \n4670   between 2016 and 2017, what was the variation...  \n\n[4671 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>title+text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td></td>\n      <td>In which year was interest income greater than...</td>\n      <td>In which year was interest income greater tha...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a72ef98</td>\n      <td></td>\n      <td>What was the Net Income (Loss) in 2019?</td>\n      <td>What was the Net Income (Loss) in 2019?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a716380</td>\n      <td></td>\n      <td>What was the percentage of Plan Assets for Oth...</td>\n      <td>What was the percentage of Plan Assets for Ot...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73f97e</td>\n      <td></td>\n      <td>What was the service revenue from Americas in ...</td>\n      <td>What was the service revenue from Americas in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a731ee6</td>\n      <td></td>\n      <td>What was the Customer support renewal rate for...</td>\n      <td>What was the Customer support renewal rate fo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4666</th>\n      <td>qd497c816</td>\n      <td></td>\n      <td>what was the decrease amount on the net intere...</td>\n      <td>what was the decrease amount on the net inter...</td>\n    </tr>\n    <tr>\n      <th>4667</th>\n      <td>qd4988ecc</td>\n      <td></td>\n      <td>what was the total of operating expenses in 2018?</td>\n      <td>what was the total of operating expenses in 2...</td>\n    </tr>\n    <tr>\n      <th>4668</th>\n      <td>qd4982266</td>\n      <td></td>\n      <td>what is the current ratio of robert mondavi?</td>\n      <td>what is the current ratio of robert mondavi?</td>\n    </tr>\n    <tr>\n      <th>4669</th>\n      <td>qd4985e34</td>\n      <td></td>\n      <td>what was the rent expense in 2007?</td>\n      <td>what was the rent expense in 2007?</td>\n    </tr>\n    <tr>\n      <th>4670</th>\n      <td>qd4989714</td>\n      <td></td>\n      <td>between 2016 and 2017, what was the variation ...</td>\n      <td>between 2016 and 2017, what was the variation...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4671 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:52:04.371234Z","iopub.execute_input":"2025-11-17T19:52:04.371867Z","iopub.status.idle":"2025-11-17T19:52:04.379183Z","shell.execute_reply.started":"2025-11-17T19:52:04.371845Z","shell.execute_reply":"2025-11-17T19:52:04.378467Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"!pip install qdrant_client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:52:26.804139Z","iopub.execute_input":"2025-11-17T19:52:26.804736Z","iopub.status.idle":"2025-11-17T19:52:31.979454Z","shell.execute_reply.started":"2025-11-17T19:52:26.804715Z","shell.execute_reply":"2025-11-17T19:52:31.978531Z"}},"outputs":[{"name":"stdout","text":"Collecting qdrant_client\n  Downloading qdrant_client-1.16.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (1.74.0)\nRequirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (0.28.1)\nRequirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (1.26.4)\nCollecting portalocker<4.0,>=2.7.0 (from qdrant_client)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (6.33.0)\nRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (2.12.4)\nRequirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant_client) (2.5.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (4.11.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (2025.10.5)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (0.16.0)\nRequirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant_client) (4.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21->qdrant_client) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant_client) (0.4.2)\nRequirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (6.1.0)\nRequirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant_client) (4.1.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant_client) (1.3.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21->qdrant_client) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21->qdrant_client) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21->qdrant_client) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21->qdrant_client) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21->qdrant_client) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21->qdrant_client) (2024.2.0)\nDownloading qdrant_client-1.16.0-py3-none-any.whl (328 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.6/328.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, qdrant_client\nSuccessfully installed portalocker-3.2.0 qdrant_client-1.16.0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"#построим базу данных\nfrom qdrant_client import QdrantClient, models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:53:08.463374Z","iopub.execute_input":"2025-11-17T19:53:08.463682Z","iopub.status.idle":"2025-11-17T19:53:10.078559Z","shell.execute_reply.started":"2025-11-17T19:53:08.463655Z","shell.execute_reply":"2025-11-17T19:53:10.077949Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"import uuid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:53:13.510545Z","iopub.execute_input":"2025-11-17T19:53:13.510994Z","iopub.status.idle":"2025-11-17T19:53:13.515025Z","shell.execute_reply.started":"2025-11-17T19:53:13.510969Z","shell.execute_reply":"2025-11-17T19:53:13.514270Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from transformers import pipeline\nimport torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(device)\n\ngeneration_pipeline = pipeline(\n    \"text-generation\",\n    model=\"RefalMachine/ruadapt_qwen2.5_3B_ext_u48_instruct_v4\",\n    device=device,\n    torch_dtype=torch.float16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:53:47.858538Z","iopub.execute_input":"2025-11-17T19:53:47.859106Z","iopub.status.idle":"2025-11-17T19:54:53.527941Z","shell.execute_reply.started":"2025-11-17T19:53:47.859079Z","shell.execute_reply":"2025-11-17T19:54:53.527150Z"}},"outputs":[{"name":"stderr","text":"2025-11-17 19:54:00.937398: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763409241.164480      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763409241.229052      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5751b206a59b4a25a6f73f72bc0de3d8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfb31796d96f44c5a3e70908a7be882a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf4e52f18a84d81ab0b8714e8128feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49a102f46a5d4e4c9e4405ada0193cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7da0d5646d534edbabdb4d58884dcb73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b242e43fd3f42068caccc200d534eea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25e453caca9f4e93bad70ad0aea38332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b68cb383a68488f8f5b206e3a68397e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe82e999f7414cafa8a9e42e3f044260"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de6025bea1bb4c9ab093950f7f14bbda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/12.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a302b4fd05c84b8ab933971111e1d6ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36ad4718f7f0441e8165ee3648783c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/759 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96a2140ac8354f668ee4fcf216a5b245"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"messages = [\n    {\"role\": \"system\", \"content\": \"Ты полезный и дружелюбный помощник.\"},\n    {\"role\": \"user\", \"content\": \"Привет, напиши анекдот про домработниц\"},\n]\n\nprint(generation_pipeline(messages, max_new_tokens=256, do_sample=True, temperature=0.5, top_p=0.9)[0]['generated_text'][-1]['content'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T19:55:01.828758Z","iopub.execute_input":"2025-11-17T19:55:01.829387Z","iopub.status.idle":"2025-11-17T19:55:05.701158Z","shell.execute_reply.started":"2025-11-17T19:55:01.829361Z","shell.execute_reply":"2025-11-17T19:55:05.700412Z"}},"outputs":[{"name":"stdout","text":"Здравствуйте! Вот вам забавный анекдот про домработницу:\n\nЖена: \"Дорогой, ты знаешь, почему я тебя люблю?\"\nМуж: \"Потому что ты всегда меня кормишь?\"\nЖена: \"Ну да, но еще и мою домашнюю уборщицу!\"\nМуж: \"Ой, не надо было это говорить!\"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"что за пизд","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:01:23.359915Z","iopub.execute_input":"2025-11-17T20:01:23.360512Z","iopub.status.idle":"2025-11-17T20:01:26.060421Z","shell.execute_reply.started":"2025-11-17T20:01:23.360487Z","shell.execute_reply":"2025-11-17T20:01:26.059823Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"#эмбеддинги модели \n\nembedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\", model_kwargs={'torch_dtype': torch.float16})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:01:30.937032Z","iopub.execute_input":"2025-11-17T20:01:30.938212Z","iopub.status.idle":"2025-11-17T20:01:43.317002Z","shell.execute_reply.started":"2025-11-17T20:01:30.938183Z","shell.execute_reply":"2025-11-17T20:01:43.316369Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edfcd3b19fd04851b8417247f732bd6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05ec9f6e22a549fb81833ed34172844b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f70f1c2399942c9ac716b7f70d2ae99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"477349f8227f480bad9b849dc6ee77ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b06a6403f3794c58b4b05058a2dc3acd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/418 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2273ecce23154a198e8538bf3479346b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aef51234c4f048a09b00530eb3dd8cd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb3ea896c3f49589ef53fce8e8c4dca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0cfc75329f645e3898a257bf116ac52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/201 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c5d67ea70a34a35b8f4363948d1adce"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"import qdrant_client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:01:56.316865Z","iopub.execute_input":"2025-11-17T20:01:56.317185Z","iopub.status.idle":"2025-11-17T20:01:56.321326Z","shell.execute_reply.started":"2025-11-17T20:01:56.317160Z","shell.execute_reply":"2025-11-17T20:01:56.320382Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# создаем базу данных, в которой представляем схожесть векторов через косинусное расстояние \nfrom qdrant_client import QdrantClient, models\n\nclient = QdrantClient(\":memory:\")\n\nclient.create_collection(\n    collection_name=\"icaif\",\n    on_disk_payload=True,\n    vectors_config=models.VectorParams(\n        size=1024,\n        distance=models.Distance.COSINE,\n        on_disk=True\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:09.337140Z","iopub.execute_input":"2025-11-17T20:02:09.337767Z","iopub.status.idle":"2025-11-17T20:02:09.353874Z","shell.execute_reply.started":"2025-11-17T20:02:09.337743Z","shell.execute_reply":"2025-11-17T20:02:09.353136Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"#чанкинг\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:13.995857Z","iopub.execute_input":"2025-11-17T20:02:13.996151Z","iopub.status.idle":"2025-11-17T20:02:14.253129Z","shell.execute_reply.started":"2025-11-17T20:02:13.996131Z","shell.execute_reply":"2025-11-17T20:02:14.252381Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"cor_df['text+title'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:18.295205Z","iopub.execute_input":"2025-11-17T20:02:18.295735Z","iopub.status.idle":"2025-11-17T20:02:18.301274Z","shell.execute_reply.started":"2025-11-17T20:02:18.295712Z","shell.execute_reply":"2025-11-17T20:02:18.300430Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"' NOTE 13. INCOME TAXES\\nWe calculate our provision for federal and state income taxes based on current tax law. U.S. federal tax reform (Tax Act) was enacted on December 22, 2017, and has several key provisions impacting the accounting for and reporting of income taxes. The most significant provision reduced the U.S. corporate statutory tax rate from 35% to 21% beginning on January 1, 2018. We remeasured the applicable deferred tax assets and liabilities based on the rates at which they are expected to reverse. As a result, the gross deferred tax assets and liabilities were adjusted which resulted in an expense for income taxes of $7.1 million which was fully offset by a corresponding change to our valuation allowance in 2017. The Tax Act contains several base broadening provisions that became effective on January 1, 2018, that did not have a material impact on 2018 and 2019 earnings.\\nDeferred tax asset (liability) is comprised of the following (in thousands):\\nWe have determined it is more likely than not that our deferred tax assets will not be realized. Accordingly, we have provided a valuation allowance for deferred tax assets.\\n\\nDecember 31                              |                 |                \\n---------------------------------------- | --------------- | ---------------\\n                                         | 2019            | 2018           \\nNet operating loss carryforwards         | $7,672          | $4,541         \\nStock options and warrants               | 420             | 214            \\nProperty                                 | 138             | 299            \\nIntangible assets                        | 66              | 94             \\nCapitalized expenses                     | 54              | 86             \\nOther                                    | 210             | 164            \\nOperating right-of-use lease assets      | (667)           |                \\nOperating right-of-use lease liabilities | 794             |                \\nNet deferred tax assets                  | 8,687           | 5,398          \\nLess: Valuation allowance                | (8,687)         | (5,398)        \\nDeferred tax asset (liability)           | $             - | $             -'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:30.275883Z","iopub.execute_input":"2025-11-17T20:02:30.276386Z","iopub.status.idle":"2025-11-17T20:02:30.279863Z","shell.execute_reply.started":"2025-11-17T20:02:30.276363Z","shell.execute_reply":"2025-11-17T20:02:30.279095Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:33.442083Z","iopub.execute_input":"2025-11-17T20:02:33.442820Z","iopub.status.idle":"2025-11-17T20:02:33.446513Z","shell.execute_reply.started":"2025-11-17T20:02:33.442795Z","shell.execute_reply":"2025-11-17T20:02:33.445693Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"text_chunks = [\n    {cor_df['_id'].iloc[i]: text_splitter.split_text(cor_df['text+title'][i])}\n    for i in tqdm(tqdm((range(len(cor_df)))))]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:38.046327Z","iopub.execute_input":"2025-11-17T20:02:38.047043Z","iopub.status.idle":"2025-11-17T20:02:41.041056Z","shell.execute_reply.started":"2025-11-17T20:02:38.047021Z","shell.execute_reply":"2025-11-17T20:02:41.040297Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/32225 [00:00<?, ?it/s]\n  2%|▏         | 684/32225 [00:00<00:04, 6830.87it/s]\n  4%|▍         | 1394/32225 [00:00<00:04, 6971.89it/s][A\n  7%|▋         | 2124/32225 [00:00<00:04, 7115.95it/s]\u001b[A\n  9%|▉         | 2885/32225 [00:00<00:04, 7308.97it/s]\u001b[A\n 14%|█▎        | 4378/32225 [00:00<00:02, 10053.98it/s][A\n 18%|█▊        | 5807/32225 [00:00<00:02, 11491.38it/s]\u001b[A\n 22%|██▏       | 7213/32225 [00:00<00:02, 12329.89it/s]\u001b[A\n 27%|██▋       | 8615/32225 [00:00<00:01, 12866.29it/s]\u001b[A\n 31%|███       | 10002/32225 [00:00<00:01, 13177.43it/s][A\n 36%|███▌      | 11452/32225 [00:01<00:01, 13575.09it/s]\u001b[A\n 40%|███▉      | 12870/32225 [00:01<00:01, 13756.71it/s]\u001b[A\n 44%|████▍     | 14246/32225 [00:01<00:01, 12037.90it/s]\u001b[A\n 48%|████▊     | 15488/32225 [00:01<00:01, 10687.41it/s]\u001b[A\n 52%|█████▏    | 16606/32225 [00:01<00:01, 10544.80it/s]\u001b[A\n 55%|█████▍    | 17694/32225 [00:01<00:01, 10554.62it/s]\u001b[A\n 59%|█████▊    | 18859/32225 [00:01<00:01, 10845.94it/s]\u001b[A\n 62%|██████▏   | 19963/32225 [00:01<00:01, 10781.96it/s]\u001b[A\n 65%|██████▌   | 21055/32225 [00:01<00:01, 10690.81it/s]\u001b[A\n 69%|██████▊   | 22133/32225 [00:02<00:00, 10467.71it/s]\u001b[A\n 72%|███████▏  | 23221/32225 [00:02<00:00, 10584.03it/s]\u001b[A\n 76%|███████▌  | 24414/32225 [00:02<00:00, 10971.83it/s]\u001b[A\n 79%|███████▉  | 25517/32225 [00:02<00:00, 10942.30it/s]\u001b[A\n 83%|████████▎ | 26781/32225 [00:02<00:00, 11440.42it/s]\u001b[A\n 87%|████████▋ | 28015/32225 [00:02<00:00, 11704.98it/s]\u001b[A\n 91%|█████████ | 29189/32225 [00:02<00:00, 11135.94it/s]\u001b[A\n 94%|█████████▍| 30310/32225 [00:02<00:00, 10964.18it/s]\u001b[A\n 94%|█████████▍| 30322/32225 [00:02<00:00, 10837.42it/s]\u001b[A\n100%|██████████| 32225/32225 [00:02<00:00, 10791.41it/s]\u001b[A\n100%|██████████| 32225/32225 [00:02<00:00, 10797.74it/s]\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"text_chunks[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:02:46.375646Z","iopub.execute_input":"2025-11-17T20:02:46.375941Z","iopub.status.idle":"2025-11-17T20:02:46.381126Z","shell.execute_reply.started":"2025-11-17T20:02:46.375919Z","shell.execute_reply":"2025-11-17T20:02:46.380349Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'d1b2e74c0': ['The following tables present the recorded investment by portfolio segment and by class, excluding commercial financing receivables and other miscellaneous financing receivables at December 31, 2019 and 2018. Commercial financing receivables are excluded from the presentation of financing receivables by portfolio segment, as they are short term in nature and the current estimated risk of loss and resulting impact to the company’s financing results are not material.',\n  'Write-offs of lease receivables and loan receivables were $16 million and $47 million, respectively, for the year ended December 31, 2019. Provisions for credit losses recorded for lease receivables and loan receivables were a release of $6 million and an addition of $2 million, respectively, for the year ended December 31, 2019.',\n  'The average recorded investment of impaired leases and loans for Americas, EMEA and Asia Pacific was $138 million, $49 million and $45 million, respectively, for the year ended December 31, 2019. Both interest income recognized, and interest income recognized on a cash basis on impaired leases and loans were immaterial for the year ended December 31, 2019.',\n  '($ in millions)                                            |          |        |              |         \\n---------------------------------------------------------- | -------- | ------ | ------------ | --------\\nAt December 31, 2019:                                      | Americas | EMEA   | Asia Pacific | Total   \\nRecorded investment:                                       |          |        |              |',\n  'Lease receivables                                          | $ 3,419  | $1,186 | $  963       | $ 5,567 \\nLoan receivables                                           | 6,726    | 3,901  | 2,395        | 13,022  \\nEnding balance                                             | $10,144  | $5,087 | $3,359       | $18,590 \\nRecorded investment, collectively evaluated for impairment | $10,032  | $5,040 | $3,326       | $18,399',\n  'Recorded investment, individually evaluated for impairment | $   112  | $   47 | $   32       | $   191 \\nAllowance for credit losses                                |          |        |              |         \\nBeginning balance at January 1, 2019                       |          |        |              |         \\nLease receivables                                          | $    53  | $   22 | $   24       | $     99',\n  'Loan receivables                                           | 105      | 43     | 32           | 179     \\nTotal                                                      | $   158  | $   65 | $   56       | $   279 \\nWrite-offs                                                 | (42)     | (3)    | (18)         | (63)    \\nRecoveries                                                 | 1        | 0      | 1            | 2',\n  'Provision                                                  | 5        | (7)    | (3)          | (5)     \\nOther*                                                     | (1)      | 0      | (1)          | (2)     \\nEnding balance at December 31, 2019                        | $   120  | $   54 | $   36       | $   210 \\nLease receivables                                          | $    33  | $   23 | $   16       | $    72',\n  'Loan receivables                                           | $    88  | $   31 | $   20       | $   138 \\nRelated allowance, collectively evaluated for impairment   | $    25  | $   11 | $    4       | $    39 \\nRelated allowance, individually evaluated for impairment   | $    96  | $   43 | $   32       | $   171']}"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"text_chunks_id = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:03:02.244987Z","iopub.execute_input":"2025-11-17T20:03:02.245537Z","iopub.status.idle":"2025-11-17T20:03:02.249061Z","shell.execute_reply.started":"2025-11-17T20:03:02.245513Z","shell.execute_reply":"2025-11-17T20:03:02.248235Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"for chunk_dict in text_chunks:\n    key, values = next(iter(chunk_dict.items()))\n\n    for chunk in values:\n        text_chunks_id.append((key, chunk))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:03:08.623556Z","iopub.execute_input":"2025-11-17T20:03:08.623838Z","iopub.status.idle":"2025-11-17T20:03:08.696943Z","shell.execute_reply.started":"2025-11-17T20:03:08.623818Z","shell.execute_reply":"2025-11-17T20:03:08.696366Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"text_chunks_id[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:03:11.175049Z","iopub.execute_input":"2025-11-17T20:03:11.175726Z","iopub.status.idle":"2025-11-17T20:03:11.180374Z","shell.execute_reply.started":"2025-11-17T20:03:11.175700Z","shell.execute_reply":"2025-11-17T20:03:11.179738Z"}},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"('d1b2e74c0',\n 'The following tables present the recorded investment by portfolio segment and by class, excluding commercial financing receivables and other miscellaneous financing receivables at December 31, 2019 and 2018. Commercial financing receivables are excluded from the presentation of financing receivables by portfolio segment, as they are short term in nature and the current estimated risk of loss and resulting impact to the company’s financing results are not material.')"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"idx, text_chunks = zip(*text_chunks_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:03:17.200313Z","iopub.execute_input":"2025-11-17T20:03:17.200583Z","iopub.status.idle":"2025-11-17T20:03:17.801082Z","shell.execute_reply.started":"2025-11-17T20:03:17.200562Z","shell.execute_reply":"2025-11-17T20:03:17.800417Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"len(text_chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:32:47.795806Z","iopub.execute_input":"2025-11-17T20:32:47.796392Z","iopub.status.idle":"2025-11-17T20:32:47.801012Z","shell.execute_reply.started":"2025-11-17T20:32:47.796368Z","shell.execute_reply":"2025-11-17T20:32:47.800290Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"182416"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"vectors = embedding_model.encode(text_chunks, batch_size=32, device=device, normalize_embeddings=True, show_progress_bar=True).tolist()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport torch\n\n# Инициализация модели\nembedding_model = SentenceTransformer(\"intfloat/multilingual-e5-large\", device=\"cuda\")\n\n# Получение размерности\nembedding_dim = embedding_model.get_sentence_embedding_dimension()\nprint(f\"Размерность векторов: {embedding_dim}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:32:01.100313Z","iopub.execute_input":"2025-11-17T20:32:01.100668Z","iopub.status.idle":"2025-11-17T20:32:06.715087Z","shell.execute_reply.started":"2025-11-17T20:32:01.100634Z","shell.execute_reply":"2025-11-17T20:32:06.714353Z"}},"outputs":[{"name":"stdout","text":"Размерность векторов: 1024\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"#чтобы не грузить vectors, просто не использовать \nimport numpy as np\nimport random\nfrom typing import List\n\ndef generate_random_vectors(num_vectors: int = 182416, vector_dim: int = 1024) -> List[List[float]]:\n    \"\"\"\n    Генерация случайных векторов с нормальным распределением\n    \"\"\"\n    print(f\"Генерация {num_vectors:,} векторов размерности {vector_dim}...\")\n    \n    # Используем нормальное распределение (более реалистично для эмбеддингов)\n    vectors = np.random.normal(0, 1, (num_vectors, vector_dim))\n    \n    # Нормализуем векторы (как при normalize_embeddings=True)\n    vectors = vectors / np.linalg.norm(vectors, axis=1, keepdims=True)\n    \n    # Конвертируем в список списков\n    vectors_list = vectors.tolist()\n    \n    print(f\"Сгенерировано {len(vectors_list):,} векторов\")\n    print(f\"Размерность каждого вектора: {len(vectors_list[0])}\")\n    \n    return vectors_list\n\n# Использование\nvectors = generate_random_vectors(182416, 1024)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:39:52.479493Z","iopub.execute_input":"2025-11-17T20:39:52.480207Z","iopub.status.idle":"2025-11-17T20:40:17.980930Z","shell.execute_reply.started":"2025-11-17T20:39:52.480183Z","shell.execute_reply":"2025-11-17T20:40:17.980127Z"}},"outputs":[{"name":"stdout","text":"Генерация 182,416 векторов размерности 1024...\nСгенерировано 182,416 векторов\nРазмерность каждого вектора: 1024\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"import uuid","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:40:47.973144Z","iopub.execute_input":"2025-11-17T20:40:47.973721Z","iopub.status.idle":"2025-11-17T20:40:47.977380Z","shell.execute_reply.started":"2025-11-17T20:40:47.973696Z","shell.execute_reply":"2025-11-17T20:40:47.976353Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"# может будет полезно: https://habr.com/ru/articles/966966/","metadata":{}},{"cell_type":"code","source":"for i in tqdm(range(len(vectors))):\n    client.upsert(\n        collection_name='icaif',\n        points=[\n            models.PointStruct(\n                id=str(uuid.uuid4()),\n                vector=vectors[i],\n                payload={\n                    'text': text_chunks[i],\n                }\n            )\n        ]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T20:40:56.013881Z","iopub.execute_input":"2025-11-17T20:40:56.014664Z"}},"outputs":[{"name":"stderr","text":" 11%|█         | 19987/182416 [00:18<02:31, 1074.92it/s]/tmp/ipykernel_48/289549586.py:2: UserWarning: Local mode is not recommended for collections with more than 20,000 points. Current collection contains 20001 points. Consider using Qdrant in Docker or Qdrant Cloud for better performance with large datasets.\n  client.upsert(\n 97%|█████████▋| 176546/182416 [03:05<00:06, 852.73it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"que_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:43:00.392602Z","iopub.execute_input":"2025-11-16T08:43:00.392905Z","iopub.status.idle":"2025-11-16T08:43:00.404514Z","shell.execute_reply.started":"2025-11-16T08:43:00.392884Z","shell.execute_reply":"2025-11-16T08:43:00.403731Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"            _id title                                               text  \\\n0     q1a73c1d4        In which year was interest income greater than...   \n1     q1a72ef98                  What was the Net Income (Loss) in 2019?   \n2     q1a716380        What was the percentage of Plan Assets for Oth...   \n3     q1a73f97e        What was the service revenue from Americas in ...   \n4     q1a731ee6        What was the Customer support renewal rate for...   \n...         ...   ...                                                ...   \n4666  qd497c816        what was the decrease amount on the net intere...   \n4667  qd4988ecc        what was the total of operating expenses in 2018?   \n4668  qd4982266             what is the current ratio of robert mondavi?   \n4669  qd4985e34                       what was the rent expense in 2007?   \n4670  qd4989714        between 2016 and 2017, what was the variation ...   \n\n                                             title+text  \n0      In which year was interest income greater tha...  \n1               What was the Net Income (Loss) in 2019?  \n2      What was the percentage of Plan Assets for Ot...  \n3      What was the service revenue from Americas in...  \n4      What was the Customer support renewal rate fo...  \n...                                                 ...  \n4666   what was the decrease amount on the net inter...  \n4667   what was the total of operating expenses in 2...  \n4668       what is the current ratio of robert mondavi?  \n4669                 what was the rent expense in 2007?  \n4670   between 2016 and 2017, what was the variation...  \n\n[4671 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>title</th>\n      <th>text</th>\n      <th>title+text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td></td>\n      <td>In which year was interest income greater than...</td>\n      <td>In which year was interest income greater tha...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a72ef98</td>\n      <td></td>\n      <td>What was the Net Income (Loss) in 2019?</td>\n      <td>What was the Net Income (Loss) in 2019?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a716380</td>\n      <td></td>\n      <td>What was the percentage of Plan Assets for Oth...</td>\n      <td>What was the percentage of Plan Assets for Ot...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73f97e</td>\n      <td></td>\n      <td>What was the service revenue from Americas in ...</td>\n      <td>What was the service revenue from Americas in...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a731ee6</td>\n      <td></td>\n      <td>What was the Customer support renewal rate for...</td>\n      <td>What was the Customer support renewal rate fo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4666</th>\n      <td>qd497c816</td>\n      <td></td>\n      <td>what was the decrease amount on the net intere...</td>\n      <td>what was the decrease amount on the net inter...</td>\n    </tr>\n    <tr>\n      <th>4667</th>\n      <td>qd4988ecc</td>\n      <td></td>\n      <td>what was the total of operating expenses in 2018?</td>\n      <td>what was the total of operating expenses in 2...</td>\n    </tr>\n    <tr>\n      <th>4668</th>\n      <td>qd4982266</td>\n      <td></td>\n      <td>what is the current ratio of robert mondavi?</td>\n      <td>what is the current ratio of robert mondavi?</td>\n    </tr>\n    <tr>\n      <th>4669</th>\n      <td>qd4985e34</td>\n      <td></td>\n      <td>what was the rent expense in 2007?</td>\n      <td>what was the rent expense in 2007?</td>\n    </tr>\n    <tr>\n      <th>4670</th>\n      <td>qd4989714</td>\n      <td></td>\n      <td>between 2016 and 2017, what was the variation ...</td>\n      <td>between 2016 and 2017, what was the variation...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4671 rows × 4 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"query_vector = embedding_model.encode(que_df['title+text'], normalize_embeddings=True, device=device).tolist()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:43:07.250145Z","iopub.execute_input":"2025-11-16T08:43:07.250486Z","iopub.status.idle":"2025-11-16T08:43:12.432001Z","shell.execute_reply.started":"2025-11-16T08:43:07.250464Z","shell.execute_reply":"2025-11-16T08:43:12.431427Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"hits = client.search(\n    collection_name=\"icaif\",\n    query_vector=query_vector[0],\n    limit=10)\n[hit.payload for hit in hits]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:55:41.026284Z","iopub.execute_input":"2025-11-16T08:55:41.026818Z","iopub.status.idle":"2025-11-16T08:55:41.466793Z","shell.execute_reply.started":"2025-11-16T08:55:41.026777Z","shell.execute_reply":"2025-11-16T08:55:41.465974Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_48/1072691417.py:1: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  hits = client.search(\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"[{'text': 'Investment Income and Interest Expense 2007 compared to 2006 The increase in investment income of $13 million was primarily due to higher realized gains on sales of investments, but partially offset by a lower average balance of interest-earning investments and increased equitymethod losses on investment partnerships.\\nInterest expense increased $35 million in 2007, primarily due to higher average debt balances outstanding, largely related to commercial paper.\\nOur commercial paper balances increased in the fourth quarter of 2007, causing a corresponding increase in interest expense, as a result of the payment made to withdraw from the Central States Pension Fund.'},\n {'text': '|  | December 31, 2008 | December 31, 2007 |\\n| (in thousands) | Interest Expense | Amortization | Interest Expense | Amortization |\\n| May 1, 2006 term loan | $1,219 | $952 | $6,023 | $504 |\\n| July 31, 2008 term loan | 6,524 | 573 | — | — |\\n| Total | $7,743 | $1,525 | $6,023 | $504 |\\nThe increased interest costs shown above for the 2008 period are primarily a result of a higher average outstanding debt balance, partially offset by a lower weighted-average effective interest rate of 4.72% as compared to 5.85% in the 2007 period.\\nThe increased amortization costs shown above for 2008 are primarily a result of the early prepayment of the 2006 term loan as well as the additional amortization related to the 2008 term loan.\\nInterest Income (Expense), net: Interest income for the year ended December 31, 2008 was $5.6 million as compared to $4.9 million for the year ended December 31, 2007.'},\n {'text': 'Total interest expense was $451 million in 2007, $651 million in 2006 and $681 million in 2005, net of a $46 million credit related to the settlement of the tax audits discussed above.\\nInterest income was $154 million, $130 million and $86 million in 2007, 2006 and 2005, respectively.\\nInterest expense and interest income in 2007 both exclude approximately $340 million related to variable interest entities for which the Company has a legal right of offset (see Note 8).\\nThe components of the $1.2 billion and $24 million increase related to U. S.  plans and non-U.\\nS.  plans, respectively, in the amounts recognized in OCI during 2012 consisted of:'},\n {'text': 'In addition, net gains from public equities were significantly higher, as global equity prices increased during the year.\\nNet Interest Income.\\nNet interest income in the consolidated statements of earnings was $2.93 billion for 2017, 13% higher than 2016, reflecting an increase in interest income primarily due to the impact of higher interest rates on collateralized agreements, higher interest income from loans receivable due to higher yields and an increase in total average loans receivable, an increase in total average financial instruments owned, and the impact of higher interest rates on other interest-earning assets and deposits with banks.\\nThe increase in interest income was partially offset by higher interest expense primarily due to the impact of higher interest rates on other interest-bearing liabilities, an increase in total average long-term borrowings, and the impact of higher interest rates on interest-bearing deposits, short-term borrowings and collateralized financings.'},\n {'text': 'ITEM 6 \\x80\\x93 SELECTED FINANCIAL DATA\\n|  | Year ended December 31 |\\n| Dollars in millions, except per share data | 2009 (a) | 2008 | 2007 | 2006 | 2005 |\\n| SUMMARYOFOPERATIONS |  |  |  |  |  |\\n| Interest income | $12,086 | $6,301 | $6,144 | $4,592 | $3,720 |\\n| Interest expense | 3,003 | 2,447 | 3,197 | 2,309 | 1,533 |\\n| Net interest income | 9,083 | 3,854 | 2,947 | 2,283 | 2,187 |\\n| Noninterest income (b) | 7,145 | 2,442 | 2,944 | 5,422 | 3,297 |\\n| Total revenue | 16,228 | 6,296 | 5,891 | 7,705 | 5,484 |\\n| Provision for credit losses (c) | 3,930 | 1,517 | 315 | 124 | 21 |\\n| Noninterest expense | 9,073 | 3,685 | 3,652 | 3,795 | 3,662 |\\n| Income from continuing operations before income taxes and noncontrolling interests | 3,225 | 1,094 | 1,924 | 3,786 | 1,801 |\\n| Income taxes | 867 | 298 | 561 | 1,311 | 547 |\\n| Income from continuing operations before noncontrolling interests | 2,358 | 796 | 1,363 | 2,475 | 1,254 |'},\n {'text': '| 2008       | 2007       | 2006      \\n------------------------------ | ---------- | ---------- | ----------\\ninterest income                | $ 653      | $ 647      | $ 394     \\nother income ( expense ) net   | -33 ( 33 ) | -48 ( 48 ) | -29 ( 29 )\\ntotal other income and expense | $ 620      | $ 599      | $ 365'},\n {'text': '|  | 2008 | 2007 | 2006 |\\n| Interest income | $653 | $647 | $394 |\\n| Other income (expense), net | -33 | -48 | -29 |\\n| Total other income and expense | $620 | $599 | $365 |\\nTotal other income and expense increased $21 million to $620 million during 2008 as compared to $599 million and $365 million in 2007 and 2006, respectively.\\nWhile the Company\\x80\\x99s cash, cash equivalents and short-term investment balances increased by 59% in 2008, other income and expense increased only 4% due to the decline in the weighted average interest rate earned of 3.44%.\\nThe overall increase in other income and expense is attributable to the Company\\x80\\x99s higher cash and short-term investment balances, which more than offset the decline in interest rates during 2008 as compared to 2007.\\nThe weighted average interest rate earned by the Company on its cash, cash equivalents, and short-term investments was 5.27% and 4.58% during 2007 and 2006, respectively.'},\n {'text': '( millions )     | 2007         | 2006       | 2005      \\n---------------- | ------------ | ---------- | ----------\\ninterest expense | $ 210        | $ 122      | $ 82      \\ninterest income  | -132 ( 132 ) | -51 ( 51 ) | -56 ( 56 )\\ntotal            | $ 78         | $ 71       | $ 26      \\n\\ninterest expense : interest expense increased year-on-year in both 2007 and 2006 , primarily due to higher average debt balances and higher interest rates .\\ninterest income : interest income increased in 2007 due to higher average cash , cash equivalent and marketable securities balances and higher interest rates .\\ninterest income was lower in 2006 , with lower average cash , cash equivalent and marketable securities balances partially offset by higher interest rates.'},\n {'text': 'Other Income, Net Other income (expense), net, for the years ended Dec.  31 consisted of the following:\\n|  | 2009 | 2008 | 2007 |\\n|  | (Thousands of Dollars) |\\n| Interest income | $14,928 | $29,753 | $24,093 |\\n| Other nonoperating income | 3,650 | 6,320 | 6,510 |\\n| Insurance policy (expenses) income | -8,646 | 4,337 | -21,548 |\\n| Other nonoperating expenses | -161 | -4 | -7 |\\n| Other income, net | $9,771 | $40,406 | $9,048 |\\n13.'},\n {'text': 'Income taxes paid\\n $\\n27,658 $\\n50,573 $\\n7,465\\nInterest paid\\n \\n111,761 \\n41,085 \\n19,114\\nInvesting activities included in liabilities\\n \\n18,824 \\n23,802 \\n11,508\\nSee accompanying notes to consolidated financial statements.\\n40'}]"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"text_chunks_id[1][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T08:58:42.187336Z","iopub.execute_input":"2025-11-16T08:58:42.187905Z","iopub.status.idle":"2025-11-16T08:58:42.192496Z","shell.execute_reply.started":"2025-11-16T08:58:42.187882Z","shell.execute_reply":"2025-11-16T08:58:42.191820Z"}},"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"'d1b2e74c0'"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"data = pd.DataFrame(text_chunks_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:03:53.366098Z","iopub.execute_input":"2025-11-16T09:03:53.366424Z","iopub.status.idle":"2025-11-16T09:03:53.388455Z","shell.execute_reply.started":"2025-11-16T09:03:53.366378Z","shell.execute_reply":"2025-11-16T09:03:53.387793Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:03:58.754371Z","iopub.execute_input":"2025-11-16T09:03:58.755341Z","iopub.status.idle":"2025-11-16T09:03:58.765688Z","shell.execute_reply.started":"2025-11-16T09:03:58.755308Z","shell.execute_reply":"2025-11-16T09:03:58.764850Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"               0                                                  1\n0      d1b2e74c0  The following tables present the recorded inve...\n1      d1b2e74c0  The average recorded investment of impaired le...\n2      d1b2e74c0  ($ in millions)                               ...\n3      d1b2e74c0  Allowance for credit losses                   ...\n4      d1b2e74c0  Ending balance at December 31, 2019           ...\n...          ...                                                ...\n95147  dd4baa80e  atoi for this segment climbed $ 10 in 2014 com...\n95148  dd4baa80e  | 2015   | 2014   | 2013  \\n----------------- ...\n95149  dd4baa80e  this segment represents a portion of alcoa 201...\n95150  dd4baa80e  dollar , the euro , and the brazilian real .\\n...\n95151  dd4baa80e  these negative impacts were mostly offset by n...\n\n[95152 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>d1b2e74c0</td>\n      <td>The following tables present the recorded inve...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>d1b2e74c0</td>\n      <td>The average recorded investment of impaired le...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>d1b2e74c0</td>\n      <td>($ in millions)                               ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>d1b2e74c0</td>\n      <td>Allowance for credit losses                   ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>d1b2e74c0</td>\n      <td>Ending balance at December 31, 2019           ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95147</th>\n      <td>dd4baa80e</td>\n      <td>atoi for this segment climbed $ 10 in 2014 com...</td>\n    </tr>\n    <tr>\n      <th>95148</th>\n      <td>dd4baa80e</td>\n      <td>| 2015   | 2014   | 2013  \\n----------------- ...</td>\n    </tr>\n    <tr>\n      <th>95149</th>\n      <td>dd4baa80e</td>\n      <td>this segment represents a portion of alcoa 201...</td>\n    </tr>\n    <tr>\n      <th>95150</th>\n      <td>dd4baa80e</td>\n      <td>dollar , the euro , and the brazilian real .\\n...</td>\n    </tr>\n    <tr>\n      <th>95151</th>\n      <td>dd4baa80e</td>\n      <td>these negative impacts were mostly offset by n...</td>\n    </tr>\n  </tbody>\n</table>\n<p>95152 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"[hit.payload for hit in hits][0]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:05:40.929052Z","iopub.execute_input":"2025-11-16T09:05:40.929901Z","iopub.status.idle":"2025-11-16T09:05:40.935010Z","shell.execute_reply.started":"2025-11-16T09:05:40.929870Z","shell.execute_reply":"2025-11-16T09:05:40.934366Z"}},"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'Investment Income and Interest Expense 2007 compared to 2006 The increase in investment income of $13 million was primarily due to higher realized gains on sales of investments, but partially offset by a lower average balance of interest-earning investments and increased equitymethod losses on investment partnerships.\\nInterest expense increased $35 million in 2007, primarily due to higher average debt balances outstanding, largely related to commercial paper.\\nOur commercial paper balances increased in the fourth quarter of 2007, causing a corresponding increase in interest expense, as a result of the payment made to withdraw from the Central States Pension Fund.'"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"value = [hit.payload for hit in hits][0]['text']\n\nidx = data.index[data[1] == value].tolist()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-16T20:46:16.332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val = data[0].iloc[idx]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:18:00.532596Z","iopub.execute_input":"2025-11-16T10:18:00.533289Z","iopub.status.idle":"2025-11-16T10:18:00.538082Z","shell.execute_reply.started":"2025-11-16T10:18:00.533263Z","shell.execute_reply":"2025-11-16T10:18:00.537168Z"}},"outputs":[],"execution_count":72},{"cell_type":"code","source":"val","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:18:05.755596Z","iopub.execute_input":"2025-11-16T10:18:05.755902Z","iopub.status.idle":"2025-11-16T10:18:05.762193Z","shell.execute_reply.started":"2025-11-16T10:18:05.755849Z","shell.execute_reply":"2025-11-16T10:18:05.761199Z"}},"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"732    d1a73fa6e\nName: 0, dtype: object"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"import pandas as pd\n\nrows = []\n\nfor i in range(100):\n    row = {\"col1\": i, \"col2\": i * 2}\n    rows.append(row)\n\ndf = pd.DataFrame(rows)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:24:05.165625Z","iopub.execute_input":"2025-11-16T09:24:05.165909Z","iopub.status.idle":"2025-11-16T09:24:05.171200Z","shell.execute_reply.started":"2025-11-16T09:24:05.165889Z","shell.execute_reply":"2025-11-16T09:24:05.170649Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"que_df['_id'].iloc[1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:18:49.700905Z","iopub.execute_input":"2025-11-16T10:18:49.701601Z","iopub.status.idle":"2025-11-16T10:18:49.707134Z","shell.execute_reply.started":"2025-11-16T10:18:49.701577Z","shell.execute_reply":"2025-11-16T10:18:49.706272Z"}},"outputs":[{"execution_count":75,"output_type":"execute_result","data":{"text/plain":"'q1a72ef98'"},"metadata":{}}],"execution_count":75},{"cell_type":"code","source":"len(query_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T11:19:06.922087Z","iopub.execute_input":"2025-11-16T11:19:06.922352Z","iopub.status.idle":"2025-11-16T11:19:06.927493Z","shell.execute_reply.started":"2025-11-16T11:19:06.922335Z","shell.execute_reply":"2025-11-16T11:19:06.926709Z"}},"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"1663"},"metadata":{}}],"execution_count":104},{"cell_type":"code","source":"len(query_vector)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T10:59:37.794322Z","iopub.execute_input":"2025-11-16T10:59:37.794971Z","iopub.status.idle":"2025-11-16T10:59:37.799708Z","shell.execute_reply.started":"2025-11-16T10:59:37.794949Z","shell.execute_reply":"2025-11-16T10:59:37.798876Z"}},"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"4671"},"metadata":{}}],"execution_count":101},{"cell_type":"code","source":"rows = []\nfor i in tqdm(range(len(que_df))):\n    hits = client.search(\n    collection_name=\"icaif\",\n    query_vector=query_vector[i],\n    limit=10)\n    for j in range(10):\n        value = [hit.payload for hit in hits][j]['text']\n        idx = data.index[data[1] == value].tolist()\n        val = data[0].iloc[idx]\n        row = {'query_id': que_df['_id'].iloc[i], 'corpus_id': val}\n        rows.append(row)\nresult = pd.DataFrame(rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:14:07.843808Z","iopub.execute_input":"2025-11-16T12:14:07.844553Z","iopub.status.idle":"2025-11-16T12:53:28.305125Z","shell.execute_reply.started":"2025-11-16T12:14:07.844530Z","shell.execute_reply":"2025-11-16T12:53:28.304343Z"}},"outputs":[{"name":"stderr","text":"  0%|          | 0/4671 [00:00<?, ?it/s]/tmp/ipykernel_48/1270308095.py:3: DeprecationWarning: `search` method is deprecated and will be removed in the future. Use `query_points` instead.\n  hits = client.search(\n100%|██████████| 4671/4671 [39:20<00:00,  1.98it/s]\n","output_type":"stream"}],"execution_count":119},{"cell_type":"code","source":"result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:55:17.109381Z","iopub.execute_input":"2025-11-16T12:55:17.109716Z","iopub.status.idle":"2025-11-16T12:55:17.123649Z","shell.execute_reply.started":"2025-11-16T12:55:17.109696Z","shell.execute_reply":"2025-11-16T12:55:17.123022Z"}},"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"        query_id                                  corpus_id\n0      q1a73c1d4  32936    d87de6e90\nName: 0, dtype: object\n1      q1a73c1d4  33678    d8c3f3a28\nName: 0, dtype: object\n2      q1a73c1d4  17065    d87e41d36\nName: 0, dtype: object\n3      q1a73c1d4  47379    d8b39b2a2\nName: 0, dtype: object\n4      q1a73c1d4  25782    d8e99e52a\nName: 0, dtype: object\n...          ...                                        ...\n46705  qd4989714  11195    d8e4cbf8e\nName: 0, dtype: object\n46706  qd4989714  37837    d8c2c6ef2\nName: 0, dtype: object\n46707  qd4989714  21149    d8166161c\nName: 0, dtype: object\n46708  qd4989714  42558    d8ac57680\nName: 0, dtype: object\n46709  qd4989714  16383    d874ac4ec\nName: 0, dtype: object\n\n[46710 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>corpus_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td>32936    d87de6e90\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a73c1d4</td>\n      <td>33678    d8c3f3a28\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a73c1d4</td>\n      <td>17065    d87e41d36\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73c1d4</td>\n      <td>47379    d8b39b2a2\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a73c1d4</td>\n      <td>25782    d8e99e52a\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46705</th>\n      <td>qd4989714</td>\n      <td>11195    d8e4cbf8e\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>46706</th>\n      <td>qd4989714</td>\n      <td>37837    d8c2c6ef2\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>46707</th>\n      <td>qd4989714</td>\n      <td>21149    d8166161c\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>46708</th>\n      <td>qd4989714</td>\n      <td>42558    d8ac57680\nName: 0, dtype: object</td>\n    </tr>\n    <tr>\n      <th>46709</th>\n      <td>qd4989714</td>\n      <td>16383    d874ac4ec\nName: 0, dtype: object</td>\n    </tr>\n  </tbody>\n</table>\n<p>46710 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":120},{"cell_type":"code","source":"result['corpus_id_clean'] = result['corpus_id'].apply(lambda x: list(x)[-1] if hasattr(x, \"__iter__\") else x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:55:30.439452Z","iopub.execute_input":"2025-11-16T12:55:30.439729Z","iopub.status.idle":"2025-11-16T12:55:30.599476Z","shell.execute_reply.started":"2025-11-16T12:55:30.439711Z","shell.execute_reply":"2025-11-16T12:55:30.598861Z"}},"outputs":[],"execution_count":121},{"cell_type":"code","source":"result = result.drop('corpus_id', axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:55:44.220305Z","iopub.execute_input":"2025-11-16T12:55:44.220627Z","iopub.status.idle":"2025-11-16T12:55:44.227119Z","shell.execute_reply.started":"2025-11-16T12:55:44.220605Z","shell.execute_reply":"2025-11-16T12:55:44.226405Z"}},"outputs":[],"execution_count":122},{"cell_type":"code","source":"result.rename(columns={'corpus_id_clean': 'corpus_id'}, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:55:48.728778Z","iopub.execute_input":"2025-11-16T12:55:48.729063Z","iopub.status.idle":"2025-11-16T12:55:48.733721Z","shell.execute_reply.started":"2025-11-16T12:55:48.729044Z","shell.execute_reply":"2025-11-16T12:55:48.732945Z"}},"outputs":[],"execution_count":123},{"cell_type":"code","source":"result.to_csv('/kaggle/working/xz3.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:56:17.362908Z","iopub.execute_input":"2025-11-16T12:56:17.363243Z","iopub.status.idle":"2025-11-16T12:56:17.415568Z","shell.execute_reply.started":"2025-11-16T12:56:17.363222Z","shell.execute_reply":"2025-11-16T12:56:17.414878Z"}},"outputs":[],"execution_count":126},{"cell_type":"code","source":"result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:56:03.008381Z","iopub.execute_input":"2025-11-16T12:56:03.009141Z","iopub.status.idle":"2025-11-16T12:56:03.018038Z","shell.execute_reply.started":"2025-11-16T12:56:03.009115Z","shell.execute_reply":"2025-11-16T12:56:03.017273Z"}},"outputs":[{"execution_count":125,"output_type":"execute_result","data":{"text/plain":"        query_id  corpus_id\n0      q1a73c1d4  d87de6e90\n1      q1a73c1d4  d8c3f3a28\n2      q1a73c1d4  d87e41d36\n3      q1a73c1d4  d8b39b2a2\n4      q1a73c1d4  d8e99e52a\n...          ...        ...\n46705  qd4989714  d8e4cbf8e\n46706  qd4989714  d8c2c6ef2\n46707  qd4989714  d8166161c\n46708  qd4989714  d8ac57680\n46709  qd4989714  d874ac4ec\n\n[46710 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>corpus_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td>d87de6e90</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a73c1d4</td>\n      <td>d8c3f3a28</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a73c1d4</td>\n      <td>d87e41d36</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73c1d4</td>\n      <td>d8b39b2a2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a73c1d4</td>\n      <td>d8e99e52a</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46705</th>\n      <td>qd4989714</td>\n      <td>d8e4cbf8e</td>\n    </tr>\n    <tr>\n      <th>46706</th>\n      <td>qd4989714</td>\n      <td>d8c2c6ef2</td>\n    </tr>\n    <tr>\n      <th>46707</th>\n      <td>qd4989714</td>\n      <td>d8166161c</td>\n    </tr>\n    <tr>\n      <th>46708</th>\n      <td>qd4989714</td>\n      <td>d8ac57680</td>\n    </tr>\n    <tr>\n      <th>46709</th>\n      <td>qd4989714</td>\n      <td>d874ac4ec</td>\n    </tr>\n  </tbody>\n</table>\n<p>46710 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":125},{"cell_type":"markdown","source":"скор: 0.20620","metadata":{}},{"cell_type":"markdown","source":"BM25 + Dense","metadata":{}},{"cell_type":"code","source":"pip install rank_bm25","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T12:58:14.515085Z","iopub.execute_input":"2025-11-16T12:58:14.515611Z","iopub.status.idle":"2025-11-16T12:58:18.374779Z","shell.execute_reply.started":"2025-11-16T12:58:14.515589Z","shell.execute_reply":"2025-11-16T12:58:18.373866Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rank_bm25) (2.4.1)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rank_bm25) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rank_bm25) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rank_bm25) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rank_bm25) (2024.2.0)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank_bm25\nSuccessfully installed rank_bm25-0.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":129},{"cell_type":"code","source":"from rank_bm25 import BM25Okapi\nimport numpy as np\n\nclass BM25Retriever:\n    def __init__(self, documents):\n        self.docs = documents['text+title'].tolist()\n        self.ids = documents['_id'].tolist()\n\n        tokenized = [d.split() for d in self.docs]\n        self.bm25 = BM25Okapi(tokenized)\n\n    def retrieve(self, query, top_k=5):\n        scores = self.bm25.get_scores(query.split())\n        idx = np.argsort(scores)[::-1][:top_k]\n\n        return [\n            {\n                \"id\": self.ids[i],\n                \"text\": self.docs[i],\n                \"score\": float(scores[i])\n            }\n            for i in idx\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:18:47.006092Z","iopub.execute_input":"2025-11-16T13:18:47.006442Z","iopub.status.idle":"2025-11-16T13:18:47.012635Z","shell.execute_reply.started":"2025-11-16T13:18:47.006418Z","shell.execute_reply":"2025-11-16T13:18:47.011764Z"}},"outputs":[],"execution_count":156},{"cell_type":"code","source":"class DenseRetriever:\n    def __init__(self, embedder, client, collection):\n        self.embedder = embedder\n        self.client = client\n        self.collection = collection\n\n    def retrieve(self, query, top_k=5, metadata_filter=None):\n        q_emb = self.embedder.encode(query)\n\n        res = self.client.query_points(\n            collection_name=self.collection,\n            query=q_emb,           \n            query_filter=metadata_filter,   \n            limit=top_k\n        ).points\n\n        return [\n            {\n                \"id\": r.id,\n                \"text\": r.payload[\"text\"],\n                \"score\": r.score\n            }\n            for r in res\n        ]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:21:34.543973Z","iopub.execute_input":"2025-11-16T13:21:34.544383Z","iopub.status.idle":"2025-11-16T13:21:34.549902Z","shell.execute_reply.started":"2025-11-16T13:21:34.544355Z","shell.execute_reply":"2025-11-16T13:21:34.549304Z"}},"outputs":[],"execution_count":163},{"cell_type":"code","source":"dense = DenseRetriever(embedding_model, client, \"icaif\")\nbm25 = BM25Retriever(cor_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:21:38.107825Z","iopub.execute_input":"2025-11-16T13:21:38.108414Z","iopub.status.idle":"2025-11-16T13:21:42.226269Z","shell.execute_reply.started":"2025-11-16T13:21:38.108377Z","shell.execute_reply":"2025-11-16T13:21:42.225407Z"}},"outputs":[],"execution_count":164},{"cell_type":"code","source":"class HybridRetriever:\n    def __init__(self, dense_retriever, bm25_retriever, alpha=0.5):\n        self.dense = dense_retriever\n        self.bm25 = bm25_retriever\n        self.alpha = alpha\n\n    def retrieve(self, query, top_k=10, metadata_filter=None):\n        bm25 = self.bm25.retrieve(query, top_k=top_k)\n        dense = self.dense.retrieve(query, top_k=top_k, metadata_filter=metadata_filter)\n\n        combined = {}\n\n        # Dense\n        for d in dense:\n            combined[d[\"id\"]] = {\n                \"source\": d,\n                \"dense\": d[\"score\"],\n                \"bm25\": 0\n            }\n\n        # BM25\n        for b in bm25:\n            doc_id = b[\"id\"]\n            if doc_id not in combined:\n                combined[doc_id] = {\"source\": b, \"dense\": 0, \"bm25\": b[\"score\"]}\n            else:\n                combined[doc_id][\"bm25\"] = b[\"score\"]\n\n        # финальный скор\n        for c in combined.values():\n            c[\"final\"] = self.alpha * c[\"dense\"] + (1 - self.alpha) * c[\"bm25\"]\n\n        final = sorted(combined.values(), key=lambda x: x[\"final\"], reverse=True)[:top_k]\n\n        return [f[\"source\"] for f in final]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:19:52.094831Z","iopub.execute_input":"2025-11-16T13:19:52.095516Z","iopub.status.idle":"2025-11-16T13:19:52.101773Z","shell.execute_reply.started":"2025-11-16T13:19:52.095492Z","shell.execute_reply":"2025-11-16T13:19:52.101054Z"}},"outputs":[],"execution_count":160},{"cell_type":"code","source":"que_df['title+text'][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:05:20.791464Z","iopub.execute_input":"2025-11-16T13:05:20.792219Z","iopub.status.idle":"2025-11-16T13:05:20.797163Z","shell.execute_reply.started":"2025-11-16T13:05:20.792196Z","shell.execute_reply":"2025-11-16T13:05:20.796536Z"}},"outputs":[{"execution_count":141,"output_type":"execute_result","data":{"text/plain":"' In which year was interest income greater than 7,000 thousands?'"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"hybrid = HybridRetriever(dense, bm25, alpha=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:21:52.574956Z","iopub.execute_input":"2025-11-16T13:21:52.575470Z","iopub.status.idle":"2025-11-16T13:21:52.578937Z","shell.execute_reply.started":"2025-11-16T13:21:52.575447Z","shell.execute_reply":"2025-11-16T13:21:52.578351Z"}},"outputs":[],"execution_count":165},{"cell_type":"code","source":"results = hybrid.retrieve(\" In which year was interest income greater than 7,000 thousands?\", top_k=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:21:55.512463Z","iopub.execute_input":"2025-11-16T13:21:55.513132Z","iopub.status.idle":"2025-11-16T13:21:56.533820Z","shell.execute_reply.started":"2025-11-16T13:21:55.513103Z","shell.execute_reply":"2025-11-16T13:21:56.533163Z"}},"outputs":[],"execution_count":166},{"cell_type":"code","source":"results[0]['id']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:23:08.426164Z","iopub.execute_input":"2025-11-16T13:23:08.426915Z","iopub.status.idle":"2025-11-16T13:23:08.431739Z","shell.execute_reply.started":"2025-11-16T13:23:08.426891Z","shell.execute_reply":"2025-11-16T13:23:08.430959Z"}},"outputs":[{"execution_count":168,"output_type":"execute_result","data":{"text/plain":"'d869ea586'"},"metadata":{}}],"execution_count":168},{"cell_type":"code","source":"results[0]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:23:20.871770Z","iopub.execute_input":"2025-11-16T13:23:20.872063Z","iopub.status.idle":"2025-11-16T13:23:20.877150Z","shell.execute_reply.started":"2025-11-16T13:23:20.872041Z","shell.execute_reply":"2025-11-16T13:23:20.876275Z"}},"outputs":[{"execution_count":169,"output_type":"execute_result","data":{"text/plain":"\" | Named Exeutive Officer | Stock Options |\\n| J. Wayne Leonard | 175,000 |\\n| Leo P. Denault | 50,000 |\\n| Richard J. Smith | 35,000 |\\n| E. Renae Conley | 15,600 |\\n| Hugh T. McDonald | 7,000 |\\n| Haley Fisackerly | 5,000 |\\n| Joseph F. Domino | 7,000 |\\n| Roderick K. West | 8,000 |\\n| Theodore H. Bunting, Jr. | 18,000 |\\n| Carolyn Shanks | 7,000 |\\nThe option grants awarded to the Named Executive Officers (other than Mr.  Leonard and Mr.  Lewis) ranged in amount between 5,000 and 50,000 shares.\\nMr.  Lewis did not receive any stock option awards in 2008.\\nIn the case of Mr.  Leonard, who received 175,000 stock options, the Committee took special note of his performance as Entergy Corporation's Chief Executive Officer.\\nAmong other things, the Committee noted that\""},"metadata":{}}],"execution_count":169},{"cell_type":"code","source":"len(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:28:02.159637Z","iopub.execute_input":"2025-11-16T13:28:02.160416Z","iopub.status.idle":"2025-11-16T13:28:02.164690Z","shell.execute_reply.started":"2025-11-16T13:28:02.160365Z","shell.execute_reply":"2025-11-16T13:28:02.163997Z"}},"outputs":[{"execution_count":172,"output_type":"execute_result","data":{"text/plain":"10"},"metadata":{}}],"execution_count":172},{"cell_type":"code","source":"que_df['title+text'][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:26:42.661889Z","iopub.execute_input":"2025-11-16T13:26:42.662224Z","iopub.status.idle":"2025-11-16T13:26:42.667731Z","shell.execute_reply.started":"2025-11-16T13:26:42.662203Z","shell.execute_reply":"2025-11-16T13:26:42.666970Z"}},"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"' What was the Net Income (Loss) in 2019?'"},"metadata":{}}],"execution_count":170},{"cell_type":"code","source":"rows = []\nfor i in tqdm(range(len(que_df))):\n    results = []\n    results = hybrid.retrieve(que_df['title+text'][i], top_k=10)\n    for j in range(len(results)):\n        row = {'query_id': que_df['_id'][i], 'corpus_id': results[j]['id']}\n        rows.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T13:29:44.815886Z","iopub.execute_input":"2025-11-16T13:29:44.816496Z","iopub.status.idle":"2025-11-16T14:18:45.857680Z","shell.execute_reply.started":"2025-11-16T13:29:44.816473Z","shell.execute_reply":"2025-11-16T14:18:45.856894Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4671/4671 [49:01<00:00,  1.59it/s]\n","output_type":"stream"}],"execution_count":175},{"cell_type":"code","source":"df = pd.DataFrame(rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:21:08.888053Z","iopub.execute_input":"2025-11-16T14:21:08.888372Z","iopub.status.idle":"2025-11-16T14:21:08.926474Z","shell.execute_reply.started":"2025-11-16T14:21:08.888351Z","shell.execute_reply":"2025-11-16T14:21:08.925761Z"}},"outputs":[],"execution_count":176},{"cell_type":"code","source":"df.to_csv('/kaggle/working/xz4.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:22:01.382022Z","iopub.execute_input":"2025-11-16T14:22:01.382291Z","iopub.status.idle":"2025-11-16T14:22:01.435087Z","shell.execute_reply.started":"2025-11-16T14:22:01.382273Z","shell.execute_reply":"2025-11-16T14:22:01.434300Z"}},"outputs":[],"execution_count":178},{"cell_type":"code","source":"df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:21:18.516371Z","iopub.execute_input":"2025-11-16T14:21:18.516716Z","iopub.status.idle":"2025-11-16T14:21:18.526647Z","shell.execute_reply.started":"2025-11-16T14:21:18.516693Z","shell.execute_reply":"2025-11-16T14:21:18.525938Z"}},"outputs":[{"execution_count":177,"output_type":"execute_result","data":{"text/plain":"        query_id      corpus_id\n0      q1a73c1d4      d869ea586\n1      q1a73c1d4      d863d98cc\n2      q1a73c1d4    JNJ20231807\n3      q1a73c1d4  GOOGL20231108\n4      q1a73c1d4      d8dc0635e\n...          ...            ...\n46705  qd4989714      d8ea8bcd0\n46706  qd4989714      d8d2c9b56\n46707  qd4989714      d88000c08\n46708  qd4989714      d8e42d0aa\n46709  qd4989714      d8d3a9c9c\n\n[46710 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>corpus_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td>d869ea586</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a73c1d4</td>\n      <td>d863d98cc</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a73c1d4</td>\n      <td>JNJ20231807</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73c1d4</td>\n      <td>GOOGL20231108</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a73c1d4</td>\n      <td>d8dc0635e</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46705</th>\n      <td>qd4989714</td>\n      <td>d8ea8bcd0</td>\n    </tr>\n    <tr>\n      <th>46706</th>\n      <td>qd4989714</td>\n      <td>d8d2c9b56</td>\n    </tr>\n    <tr>\n      <th>46707</th>\n      <td>qd4989714</td>\n      <td>d88000c08</td>\n    </tr>\n    <tr>\n      <th>46708</th>\n      <td>qd4989714</td>\n      <td>d8e42d0aa</td>\n    </tr>\n    <tr>\n      <th>46709</th>\n      <td>qd4989714</td>\n      <td>d8d3a9c9c</td>\n    </tr>\n  </tbody>\n</table>\n<p>46710 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":177},{"cell_type":"markdown","source":"скор 0.22276","metadata":{}},{"cell_type":"markdown","source":"# Подключим LLM","metadata":{}},{"cell_type":"code","source":"def llm_answer(query, context):\n    prompt = f\"\"\"Information from the book:\n{context}\n\nQuestion:\n{query}\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": prompt}, \n    ]\n    output = generation_pipeline(messages, max_new_tokens=512, do_sample=True, temperature=TEMP, top_p=0.9)\n\n    return output[0]['generated_text'][-1]['content'], context","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict(query):\n    selected_chunks = semantic_search(client, query)\n    context = ' ; '.join([f\"Book content piece: {chunk['text']}\" for chunk in selected_chunks])\n    pages = [chunk['page'] for chunk in selected_chunks]\n\n    return (llm_answer(query, context), pages)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, query in enumerate(queries):\n    print(i, query)\n    ans = predict(query)\n    submission_data.append({\n        'ID': i+1,\n        'context': str(ans[0][1]),\n        'answer': str(ans[0][0]),\n        'references': json.dumps({\"sections\": [\"section_1\"], \"pages\": ans[1]})\n    })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Добавим кросс-энкодер","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, CrossEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:24:27.779730Z","iopub.execute_input":"2025-11-16T14:24:27.780273Z","iopub.status.idle":"2025-11-16T14:24:27.783730Z","shell.execute_reply.started":"2025-11-16T14:24:27.780247Z","shell.execute_reply":"2025-11-16T14:24:27.782968Z"}},"outputs":[],"execution_count":181},{"cell_type":"code","source":"#или \ncross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n\n# --- Cross-Encoder Reranking Example ---\n# This part can be used later during retrieval to refine similarity ranking\ndef rerank_with_cross_encoder(query, passages, top_k=5):\n    \"\"\"\n    Given a query and a list of passage dicts (each with 'text' or 'content'),\n    returns the top_k highest-ranked passages.\n    \"\"\"\n    # Extract plain text for scoring\n    texts = [p[\"text\"] if \"text\" in p else p[\"content\"] for p in passages]\n\n    # Form query-passage pairs for the cross-encoder\n    pairs = [(query, text) for text in texts]\n\n    # Get relevance scores\n    scores = cross_encoder.predict(pairs)\n\n    # Sort by score\n    ranked_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n\n    # Return the top_k *original* passage dicts\n    return [passages[i] for i in ranked_indices[:top_k]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#или\nclass CrossEncoderReranker:\n    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n        self.model = CrossEncoder(model_name)\n\n    def rerank(self, query, candidates, top_k=5):\n        \"\"\"\n        candidates — список dict: {\"id\", \"text\", \"score\"}\n        \"\"\"\n        pairs = [(query, c[\"text\"]) for c in candidates]\n\n        scores = self.model.predict(pairs)\n\n        reranked = sorted(\n            [\n                {**c, \"rerank_score\": float(s)}\n                for c, s in zip(candidates, scores)\n            ],\n            key=lambda x: x[\"rerank_score\"],\n            reverse=True\n        )\n\n        return reranked[:top_k]\n\n\n# -----------------------------\n# 5) Общая система — двухступенчатый ретривал\n# -----------------------------\nclass TwoStageRetriever:\n    def __init__(self, hybrid, reranker):\n        self.hybrid = hybrid\n        self.reranker = reranker\n\n    def retrieve(self, query, n_candidates=20, final_top_k=5):\n        # Этап 1 — быстрый поиск\n        stage1 = self.hybrid.retrieve(query, top_k=n_candidates)\n\n        # Этап 2 — rerank через cross-encoder\n        stage2 = self.reranker.rerank(query, stage1, top_k=final_top_k)\n\n        return stage2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:23:49.448518Z","iopub.execute_input":"2025-11-16T14:23:49.448795Z","iopub.status.idle":"2025-11-16T14:23:49.455847Z","shell.execute_reply.started":"2025-11-16T14:23:49.448775Z","shell.execute_reply":"2025-11-16T14:23:49.455029Z"}},"outputs":[],"execution_count":179},{"cell_type":"code","source":"reranker = CrossEncoderReranker()\n\nretriever = TwoStageRetriever(hybrid, reranker)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:24:30.611371Z","iopub.execute_input":"2025-11-16T14:24:30.611666Z","iopub.status.idle":"2025-11-16T14:24:33.666332Z","shell.execute_reply.started":"2025-11-16T14:24:30.611642Z","shell.execute_reply":"2025-11-16T14:24:33.665590Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84dfedd54cb4e64bd08b1ec944cc8ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd755c90ac574c3d92895d89e5b938d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c201cd3223b144b3983d09b12a524487"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d981d926138428f882396c99eb9da3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de2f3cc35b944a12967819e2af45101e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"898bed1e39814fbbadb96c5998db5b6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9330022ef23d476fa02315e940c6856a"}},"metadata":{}}],"execution_count":182},{"cell_type":"code","source":"results = retriever.retrieve(' What was the Net Income (Loss) in 2019?', n_candidates=20, final_top_k=3)\n\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:24:51.470669Z","iopub.execute_input":"2025-11-16T14:24:51.471275Z","iopub.status.idle":"2025-11-16T14:24:52.305179Z","shell.execute_reply.started":"2025-11-16T14:24:51.471251Z","shell.execute_reply":"2025-11-16T14:24:52.304558Z"}},"outputs":[{"execution_count":183,"output_type":"execute_result","data":{"text/plain":"[{'id': 'd1b35c0d6',\n  'text': ' Net Income (Loss) per Common Share\\nBasic net income per share is based on the weighted average common and Class A shares outstanding. Diluted net income per share includes any dilutive effects of stock options outstanding and unvested restricted shares.\\nBasic net income per share was calculated by dividing net income by the weighted-average number of common and Class A shares outstanding during the period. Diluted net income per share was calculated by dividing net income by the weighted-average number of common shares outstanding during the period plus the dilutive effects of stock options and unvested restricted shares. Due to the net loss in the year ended June 3, 2017 restricted shares in the amount of 131,292 were excluded from the calculation of diluted earnings per share because their inclusion would have been\\nantidilutive. The computations of basic net income per share and diluted net income per share are as follows (in thousands):\\n\\n                                                         | June 1, 2019 | June 2, 2018 | June 3, 2017\\n-------------------------------------------------------- | ------------ | ------------ | ------------\\nNet income (loss) attributable to Cal-Maine Foods, Inc.  | $54,229      | $125,932     | $(74,278)   \\nBasic weighted-average common shares (including Class A) | 48,467       | 48,353       | 48,362      \\nEffect of dilutive securities:                           |              |              |             \\nCommon stock options and restricted stock                | 122          | 115          | —           \\nDilutive potential common shares                         | 48,589       | 48,468       | 48,362      \\nNet income (loss) per common share:                      |              |              |             \\nBasic                                                    | $1.12        | $2.60        | $(1.54)     \\nDiluted                                                  | $1.12        | $2.60        | $(1.54)     ',\n  'score': 21.71707957217168,\n  'rerank_score': 4.604833602905273},\n {'id': 'd1b2eb41c',\n  'text': \" Changes in Accumulated Other Comprehensive Income (Loss)\\nThe following table summarizes the changes in accumulated other comprehensive income (loss), which is reported as a component of shareholders’ equity, for the years ended December 31, 2019 and 2018:\\nExpressed in US $000's except share and per share amounts\\n\\n                                                                                                                      | Accumulated Other Comprehensive Income (Loss) |                  \\n--------------------------------------------------------------------------------------------------------------------- | --------------------------------------------- | -----------------\\n                                                                                                                      | Years ended                                   |                  \\n                                                                                                                      | December 31, 2019                             | December 31, 2018\\n                                                                                                                      | $                                             | $                \\nBalance, beginning of the year                                                                                        | (12,216)                                      | 3,435            \\n                                                                                                                      |                                               |                  \\nOther comprehensive income (loss) before reclassifications                                                            | 12,865                                        | (19,821)         \\nLoss on cash flow hedges reclassified from accumulated other comprehensive income (loss) to earnings were as follows: |                                               |                  \\nCost of revenues                                                                                                      | 279                                           | 255              \\nSales and marketing                                                                                                   | 1,538                                         | 1,224            \\nResearch and development                                                                                              | 2,620                                         | 2,063            \\nGeneral and administrative                                                                                            | 744                                           | 628              \\nTax effect on unrealized gain (loss) on cash flow hedges                                                              | (4,784)                                       | —                \\nOther comprehensive income (loss), net of tax                                                                         | 13,262                                        | (15,651)         \\nBalance, end of the year                                                                                              | 1,046                                         | (12,216)         \",\n  'score': 21.84980644464455,\n  'rerank_score': 2.6263465881347656},\n {'id': 'd1b3164f0',\n  'text': ' A. Operating Results\\nYEAR ENDED DECEMBER 31, 2019 COMPARED TO YEAR ENDED DECEMBER 31, 2018\\nManagement believes that net voyage revenue, a non-GAAP financial measure, provides additional meaningful information because it enables us to compare the profitability of our vessels which are employed under bareboat charters, spot related time charters and spot charters. Net voyage revenues divided by the number of days on the charter provides the Time Charter Equivalent (TCE) Rate. Net voyage revenues and TCE rates are widely used by investors and analysts in the tanker shipping industry for comparing the financial performance of companies and for preparing industry averages. We believe that our method of calculating net voyage revenue is consistent with industry standards. The table below reconciles our net voyage revenues to voyage revenues.\\n\\n                                      |           | Years Ended December 31,     |          \\n------------------------------------- | --------- | ---------------------------- | ---------\\nAll figures in USD ‘000               | 2019      | 2018                         | Variance \\nVoyage Revenue                        | 317,220   | 289,016                      | 9.8%     \\nVoyage Expenses                       | (141,770) | (165,012)                    | (14.1%)  \\nVessel Operating Expenses             | (66,033)  | (80,411)                     | (17.9%)  \\nImpairment Loss on Vessels            | -         | (2,168)                      | N/A      \\nImpairment Loss on Goodwill           | -         | -                            | N/A      \\nLoss from Disposal of Vessels         | -         | (6,619)                      | N/A      \\nGeneral and Administrative Expenses   | (13,481)  | (12,727)                     | 5.9%     \\nDepreciation Expenses                 | (63,965)  | (60,695)                     | 5.4%     \\nNet Operating (Loss) Income           | 31,971    | (38,616)                     | (182.8%) \\nInterest Income                       | 298       | 334                          | (10.8%)  \\nInterest Expenses                     | (38,390)  | (34,549)                     | 11.1%    \\nOther Financial Expenses              | (4,231)   | (14,808)                     | (71.4%)  \\nEquity Loss from Associate            | -         | (7,667)                      | N/A      \\nNet (Loss) Income                     | (10,352)  | (95,306)                     | (89.1%)  ',\n  'score': 23.876003938580183,\n  'rerank_score': 0.5696696043014526}]"},"metadata":{}}],"execution_count":183},{"cell_type":"code","source":"rows = []\nfor i in tqdm(range(len(que_df))):\n    results = []\n    results = retriever.retrieve(que_df['title+text'][i], n_candidates=20, final_top_k=10)\n    for j in range(len(results)):\n        row = {'query_id': que_df['_id'][i], 'corpus_id': results[j]['id']}\n        rows.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T14:38:05.620126Z","iopub.execute_input":"2025-11-16T14:38:05.620433Z","iopub.status.idle":"2025-11-16T15:38:03.851504Z","shell.execute_reply.started":"2025-11-16T14:38:05.620379Z","shell.execute_reply":"2025-11-16T15:38:03.850414Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4671/4671 [59:58<00:00,  1.30it/s] \n","output_type":"stream"}],"execution_count":184},{"cell_type":"code","source":"res = pd.DataFrame(rows)\nres","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:39:01.486953Z","iopub.execute_input":"2025-11-16T15:39:01.487236Z","iopub.status.idle":"2025-11-16T15:39:01.532957Z","shell.execute_reply.started":"2025-11-16T15:39:01.487217Z","shell.execute_reply":"2025-11-16T15:39:01.532189Z"}},"outputs":[{"execution_count":185,"output_type":"execute_result","data":{"text/plain":"        query_id  corpus_id\n0      q1a73c1d4  d893a246e\n1      q1a73c1d4  d8dc0635e\n2      q1a73c1d4  d8788d7c8\n3      q1a73c1d4  dd2abb3e0\n4      q1a73c1d4  d867936a2\n...          ...        ...\n46705  qd4989714  d8e42d0aa\n46706  qd4989714  d882fa0a8\n46707  qd4989714  d8b024a4c\n46708  qd4989714  d8ea8bcd0\n46709  qd4989714  d88a2a9b8\n\n[46710 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>query_id</th>\n      <th>corpus_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>q1a73c1d4</td>\n      <td>d893a246e</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>q1a73c1d4</td>\n      <td>d8dc0635e</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>q1a73c1d4</td>\n      <td>d8788d7c8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>q1a73c1d4</td>\n      <td>dd2abb3e0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>q1a73c1d4</td>\n      <td>d867936a2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>46705</th>\n      <td>qd4989714</td>\n      <td>d8e42d0aa</td>\n    </tr>\n    <tr>\n      <th>46706</th>\n      <td>qd4989714</td>\n      <td>d882fa0a8</td>\n    </tr>\n    <tr>\n      <th>46707</th>\n      <td>qd4989714</td>\n      <td>d8b024a4c</td>\n    </tr>\n    <tr>\n      <th>46708</th>\n      <td>qd4989714</td>\n      <td>d8ea8bcd0</td>\n    </tr>\n    <tr>\n      <th>46709</th>\n      <td>qd4989714</td>\n      <td>d88a2a9b8</td>\n    </tr>\n  </tbody>\n</table>\n<p>46710 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":185},{"cell_type":"code","source":"res.to_csv('/kaggle/working/xz5.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:39:54.296603Z","iopub.execute_input":"2025-11-16T15:39:54.297315Z","iopub.status.idle":"2025-11-16T15:39:54.349117Z","shell.execute_reply.started":"2025-11-16T15:39:54.297289Z","shell.execute_reply":"2025-11-16T15:39:54.348570Z"}},"outputs":[],"execution_count":186},{"cell_type":"markdown","source":"скор: 0.22292","metadata":{}},{"cell_type":"markdown","source":"# Adaptive-K для Cross-Encoder","metadata":{}},{"cell_type":"code","source":"class AdaptiveKReranker:\n    def __init__(self, model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", threshold=0.3):\n        \"\"\"\n        threshold — минимальный порог уверенности cross-encoder\n        \"\"\"\n        self.model = CrossEncoder(model_name)\n        self.threshold = threshold\n\n    def rerank(self, query, candidates):\n        \"\"\"\n        Возвращает все кандидаты с оценкой выше порога.\n        \"\"\"\n        pairs = [(query, c[\"text\"]) for c in candidates]\n        scores = self.model.predict(pairs)\n\n        reranked = [\n            {**c, \"score\": float(s)}\n            for c, s in zip(candidates, scores)\n            if float(s) >= self.threshold\n        ]\n\n        # сортируем по score\n        reranked = sorted(reranked, key=lambda x: x[\"score\"], reverse=True)\n\n        return reranked\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:44:31.646719Z","iopub.execute_input":"2025-11-16T15:44:31.647341Z","iopub.status.idle":"2025-11-16T15:44:31.652879Z","shell.execute_reply.started":"2025-11-16T15:44:31.647306Z","shell.execute_reply":"2025-11-16T15:44:31.652276Z"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":"class TwoStageAdaptiveRetriever:\n    def __init__(self, retriever_stage1, adaptive_reranker):\n        self.retriever_stage1 = retriever_stage1\n        self.reranker = adaptive_reranker\n\n    def retrieve(self, query, n_candidates=30):\n        # Этап 1 — быстрый грубый поиск\n        stage1 = self.retriever_stage1.retrieve(query, top_k=n_candidates)\n\n        # Этап 2 — Adaptive K reranking\n        stage2 = self.reranker.rerank(query, stage1)\n\n        return stage2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:44:35.318295Z","iopub.execute_input":"2025-11-16T15:44:35.318608Z","iopub.status.idle":"2025-11-16T15:44:35.323111Z","shell.execute_reply.started":"2025-11-16T15:44:35.318586Z","shell.execute_reply":"2025-11-16T15:44:35.322346Z"}},"outputs":[],"execution_count":188},{"cell_type":"markdown","source":"# Модели Cross-encoder\nанглийские модели\nTOP-1 рекомендация\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\nИли\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\nДля максимального качества\nCROSS_ENCODER_MODEL = \"cross-encoder/ms-marco-electra-base\"\n\nмногоязычные модели \nДля мультиязычных данных\nCROSS_ENCODER_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\nИли\nCROSS_ENCODER_MODEL = \"sentence-transformers/distiluse-base-multilingual-cased-v2\"","metadata":{}},{"cell_type":"code","source":"adaptive_reranker = AdaptiveKReranker(\n    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2,\n    threshold=0.45   # выбираем порог\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:45:52.253772Z","iopub.execute_input":"2025-11-16T15:45:52.254039Z","iopub.status.idle":"2025-11-16T15:45:58.057911Z","shell.execute_reply.started":"2025-11-16T15:45:52.254018Z","shell.execute_reply":"2025-11-16T15:45:58.057121Z"}},"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":190},{"cell_type":"code","source":"retriever = TwoStageAdaptiveRetriever(hybrid, adaptive_reranker)\n\nquery = ' What was the Net Income (Loss) in 2019?'\n\nresults = retriever.retrieve(query, n_candidates=10)\n\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:46:56.467331Z","iopub.execute_input":"2025-11-16T15:46:56.468153Z","iopub.status.idle":"2025-11-16T15:46:58.008706Z","shell.execute_reply.started":"2025-11-16T15:46:56.468126Z","shell.execute_reply":"2025-11-16T15:46:58.007901Z"}},"outputs":[{"execution_count":192,"output_type":"execute_result","data":{"text/plain":"[{'id': 'd873e2002',\n  'text': ' CONSOLIDATED RESULTS Net sales and revenue and operating income numbers reported in our consolidated results do not include the activity of our discontinued operations: ?Containerboard, Packaging and Recycling operations (sold in August 2008); ?Australian operations (sold in July 2008); ?Trus Joist?Commercial division (held for sale as of December 2008); ?Fine Paper operations (divested in March 2007); ?Irish composite panel operations (sold in November 2006); and ?North American composite panel operations (sold in July 2006).\\nWe report these activities and results as discontinued operations in our Consolidated Statement of Earnings.\\nHowever, we include the results of these operations in the segment discussions that follow.\\nSee Note 3: Discontinued Operations and Assets Held For Sale in the Notes to Consolidated Financial Statements for more information about our discontinued operations.\\nHOW WE DID IN 2008 Net Sales and Revenues, Operating Income (Loss), Earnings From Discontinued Operations and Net Earnings (Loss)',\n  'score': 0.5738213062286377},\n {'id': 'd8ca47eba',\n  'text': ' IRON MOUNTAIN INCORPORATED NOTES TO CONSOLIDATED FINANCIAL STATEMENTS (Continued) DECEMBER 31, 2012 (In thousands, except share and per share data) 5.\\nSelected Consolidated Financial Statements of Parent, Guarantors, Canada Company and Non-Guarantors (Continued)\\n|  | Year Ended December 31, 2012 |\\n|  | Parent | Guarantors | Canada Company | Non- Guarantors | Eliminations | Consolidated |\\n| Revenues: |  |  |  |  |  |  |\\n| Storage Rental | $— | $1,156,681 | $124,370 | $452,087 | $— | $1,733,138 |\\n| Service | — | 784,068 | 115,746 | 372,303 | — | 1,272,117 |\\n| Total Revenues | — | 1,940,749 | 240,116 | 824,390 | — | 3,005,255 |\\n| Operating Expenses: |  |  |  |  |  |  |\\n| Cost of Sales (Excluding Depreciation and Amortization) | — | 761,092 | 97,436 | 418,585 | — | 1,277,113 |\\n| Selling, General and Administrative | 220 | 591,092 | 35,554 | 223,505 | — | 850,371 |\\n| Depreciation and Amortization | 320 | 192,304 | 18,601 | 105,119 | — | 316,344 |\\n| (Gain) Loss on Disposal/Write-down of Property, Plant and Equipment, Net | — | -966 | -122 | 5,488 | — | 4,400 |\\n| Total Operating Expenses | 540 | 1,543,522 | 151,469 | 752,697 | — | 2,448,228 |\\n| Operating (Loss) Income | -540 | 397,227 | 88,647 | 71,693 | — | 557,027 |\\n| Interest Expense (Income), Net | 196,423 | -17,117 | 45,826 | 17,467 | — | 242,599 |\\n| Other Expense (Income), Net | 32,161 | -3,842 | -53 | -12,204 | — | 16,062 |\\n| (Loss) Income from Continuing Operations Before Provision (Benefit) for Income Taxes | -229,124 | 418,186 | 42,874 | 66,430 | — | 298,366 |\\n| Provision (Benefit) for Income Taxes | — | 86,549 | 14,715 | 13,609 | — | 114,873 |\\n| Equity in the (Earnings) Losses of Subsidiaries, Net of Tax | -400,832 | -73,625 | -591 | — | 475,048 | — |\\n| Income (Loss) from Continuing Operations | 171,708 | 405,262 | 28,750 | 52,821 | -475,048 | 183,493 |\\n| Income (Loss) from Discontinued Operations, Net of Tax | — | 430 | — | -7,204 | — | -6,774 |\\n| Gain (Loss) on Sale of Discontinued Operations, Net of Tax | — | — | — | -1,885 | — | -1,885 |\\n| Net Income (Loss) | 171,708 | 405,692 | 28,750 | 43,732 | -475,048 | 174,834 |\\n| Less: Net Income (Loss) Attributable to Noncontrolling Interests | — | — | — | 3,126 | — | 3,126 |\\n| Net Income (Loss) Attributable to Iron Mountain Incorporated | $171,708 | $405,692 | $28,750 | $40,606 | $-475,048 | $171,708 |\\n| Net Income (Loss) | $171,708 | $405,692 | $28,750 | $43,732 | $-475,048 | $174,834 |\\n| Other Comprehensive Income (Loss): |  |  |  |  |  |  |\\n| Foreign Currency Translation Adjustments | -2,668 | -212 | 7,578 | 18,488 | — | 23,186 |\\n| Equity in Other Comprehensive Income (Loss) of Subsidiaries | 25,185 | 25,421 | 434 | — | -51,040 | — |\\n| Total Other Comprehensive Income (Loss) | 22,517 | 25,209 | 8,012 | 18,488 | -51,040 | 23,186 |\\n| Comprehensive Income (Loss) | 194,225 | 430,901 | 36,762 | 62,220 | -526,088 | 198,020 |\\n| Comprehensive Income (Loss) Attributable to Noncontrolling Interests | — | — | — | 3,795 | — | 3,795 |\\n| Comprehensive Income (Loss) Attributable to Iron Mountain Incorporated | $194,225 | $430,901 | $36,762 | $58,425 | $-526,088 | $194,225 |\\nFinancial Instruments and Debt Financial instruments that potentially subject us to credit risk consist principally of cash and cash equivalents (including money market funds and time deposits) and accounts receivable.\\nThe only significant concentrations of liquid investments as of December 31, 2015 related to cash and cash equivalents held in time deposits with four global banks, all of which we consider to be large, highly-rated investment-grade institutions.\\nAs per our risk management investment policy, we limit exposure to concentration of credit risk by limiting the amount invested in any one mutual fund to a maximum of $50.0 million or in any one financial institution to a maximum of $75.0 million.\\nAs of December 31, 2015, our cash and cash equivalents balance was $128.4 million, which included time deposits amounting to $18.6 million.',\n  'score': 0.5630667209625244},\n {'id': 'd1b3164f0',\n  'text': ' A. Operating Results\\nYEAR ENDED DECEMBER 31, 2019 COMPARED TO YEAR ENDED DECEMBER 31, 2018\\nManagement believes that net voyage revenue, a non-GAAP financial measure, provides additional meaningful information because it enables us to compare the profitability of our vessels which are employed under bareboat charters, spot related time charters and spot charters. Net voyage revenues divided by the number of days on the charter provides the Time Charter Equivalent (TCE) Rate. Net voyage revenues and TCE rates are widely used by investors and analysts in the tanker shipping industry for comparing the financial performance of companies and for preparing industry averages. We believe that our method of calculating net voyage revenue is consistent with industry standards. The table below reconciles our net voyage revenues to voyage revenues.\\n\\n                                      |           | Years Ended December 31,     |          \\n------------------------------------- | --------- | ---------------------------- | ---------\\nAll figures in USD ‘000               | 2019      | 2018                         | Variance \\nVoyage Revenue                        | 317,220   | 289,016                      | 9.8%     \\nVoyage Expenses                       | (141,770) | (165,012)                    | (14.1%)  \\nVessel Operating Expenses             | (66,033)  | (80,411)                     | (17.9%)  \\nImpairment Loss on Vessels            | -         | (2,168)                      | N/A      \\nImpairment Loss on Goodwill           | -         | -                            | N/A      \\nLoss from Disposal of Vessels         | -         | (6,619)                      | N/A      \\nGeneral and Administrative Expenses   | (13,481)  | (12,727)                     | 5.9%     \\nDepreciation Expenses                 | (63,965)  | (60,695)                     | 5.4%     \\nNet Operating (Loss) Income           | 31,971    | (38,616)                     | (182.8%) \\nInterest Income                       | 298       | 334                          | (10.8%)  \\nInterest Expenses                     | (38,390)  | (34,549)                     | 11.1%    \\nOther Financial Expenses              | (4,231)   | (14,808)                     | (71.4%)  \\nEquity Loss from Associate            | -         | (7,667)                      | N/A      \\nNet (Loss) Income                     | (10,352)  | (95,306)                     | (89.1%)  ',\n  'score': 0.5497161746025085},\n {'id': 'd88755b0c',\n  'text': ' |  | Year Ended December 31, |\\n|  | 2011 | 2010 | 2009 |\\n| (millions, except per share) |  |  |  |\\n| Total Revenues | $3,763 | $3,022 | $2,313 |\\n| Total Operating Expenses | 3,023 | 2,070 | 2,371 |\\n| Operating Income (Loss) | 740 | 952 | -58 |\\n| Total Other (Income) Expense | 25 | -79 | 206 |\\n| Income (Loss) Before Income Taxes | 715 | 1,031 | -264 |\\n| Net Income (Loss) | 453 | 725 | -131 |\\n| Earnings (Loss) Per Share |  |  |  |\\n| Basic | $2.57 | $4.15 | $-0.75 |\\n| Diluted | 2.54 | 4.10 | -0.75 |\\nIf the realized gains and losses on commodity derivative instruments, which are included in (gain) loss on commodity derivative instruments in our consolidated statements of operations, had been included in oil and gas revenues, the effect on average realized prices would have been as follows:',\n  'score': 0.546572208404541},\n {'id': 'JNJ20231480',\n  'text': 'JNJ 13. Accumulated other comprehensive income (loss) Components of other comprehensive income (loss) consist of the following: (Dollars in Millions)##Foreign Currency Translation####Gain/ (Loss) On Securities##Employee Benefit Plans####Gain/ (Loss) On Derivatives & Hedges##Total Accumulated Other Comprehensive Income (Loss) January 3, 2021####$(8,938)##1##(6,957)####652##(15,242) Net 2021 changes####(1,079)##(4)##4,255####(988)##2,184 January 2, 2022####(10,017)##(3)##(2,702)####(336)##(13,058) Net 2022 changes####(1,796)##(24)##1,805####106##91 January 1, 2023####(11,813)##(27)##(897)####(230)##(12,967) Net 2023 changes####(3,221)##26##(1,399)####(147)##(4,741) Kenvue Separation/IPO####4,885####296##*####5,181 December 31, 2023####$(10,149)##(1)##(2,000)####(377)##(12,527)',\n  'score': 0.5341396331787109},\n {'id': 'd8619f854',\n  'text': ' |  | Accumulated Other Comprehensive Income (Loss) |\\n|  | Foreign Currency Translation Adjustments | Net Unrealized Investment Gains (Losses)(1) | Pension Liability Adjustment | Total Accumulated Other Comprehensive Income (Loss) |\\n|  | (in millions) |\\n| Balance, December 31, 2002 | $-154 | $2,834 | $-95 | $2,585 |\\n| Change in component during year | 153 | -259 | -33 | -139 |\\n| Balance, December 31, 2003 | -1 | 2,575 | -128 | 2,446 |\\n| Change in component during year-2 | 327 | -554 | -28 | -255 |\\n| Balance, December 31, 2004 | 326 | 2,021 | -156 | 2,191 |\\n| Change in component during year | -401 | -445 | -111 | -957 |\\n| Balance, December 31, 2005 | $-75 | $1,576 | $-267 | $1,234 |\\n(1) Includes cash flow hedges.\\nSee Note 19 for information on cash flow hedges.\\n(2) Net unrealized investment gains (losses) for 2004 includes the impact of cumulative effect of accounting change of $73 million.',\n  'score': 0.523111879825592},\n {'id': 'd86cac9c2',\n  'text': ' CMS ENERGY CORPORATION CONSUMERS ENERGY COMPANY NOTES TO CONSOLIDATED FINANCIAL STATEMENTS (CONTINUED) In connection with the sale of CMS Energy’s Argentine and Michigan assets to Lucid Energy in March 2007, CMS Energy entered into agreements that granted MEI, an affiliate of Lucid Energy, the right to any proceeds from an assignment of the ICSID award associated with TGN.\\nThe agreements also granted MEI an option to purchase CMS Gas Transmission’s ownership interests in TGN, and the rights to any proceeds CMS Enterprises will receive if it sells its stock interest in CMS Generation San Nicolas Company.\\nIn June 2008, CMS Energy executed an agreement with MEI and a third party to assign the ICSID award and to sell its interests in TGN directly to the third party.\\nIn accordance with the agreements executed in March 2007, the proceeds from the assignment of the ICSID award and the sale of TGN were passed on to MEI and CMS Energy recognized an $8 million gain on the assignment of the ICSID award in Gain on asset sales, net on CMS Energy’s Consolidated Statements of Income (Loss).\\nCMS Energy also recognized a $197 million cumulative net foreign currency translation loss related to TGN, which had been deferred as a Foreign currency translation component of stockholders’ equity.\\nThis charge was fully offset by the elimination of a $197 million Argentine currency impairment reserve on CMS Energy’s Consolidated Balance Sheets, created when it impaired its investment in TGN in March 2007.\\nFor additional details, see “Impairment Charges” within this Note.\\nAs of December 31, 2009, $7 million remained as a deferred credit on CMS Energy’s Consolidated Balance Sheets related to MEI’s right to proceeds that CMS Enterprises will receive if it sells its stock interest in CMS Generation San Nicolas Company.\\nDISCONTINUED OPERATIONS Discontinued operations are a component of CMS Energy’s enterprises business segment.\\nCMS Energy included the following amounts in the Income (Loss) From Discontinued Operations line on its Consolidated Statements of Income (Loss):\\n| Years Ended December 31 | 2009 | 2008 | 2007 |\\n|  | In Millions |\\n| Revenues | $7 | $14 | $248 |\\n| Discontinued operations: |  |  |  |\\n| Pretax income (loss) from discontinued operations | $33 | $2 | $-111 |\\n| Income tax expense (benefit) | 13 | 1 | -1 |\\n| Income (Loss) From Discontinued Operations, Net of Tax Expense (Benefit) | $20(a) | $1 | $-110(b) |\\n(a) Includes an operating loss of $11 million ($7 million after tax) at Exeter, whose assets and liabilities were reclassified as held for sale in 2009, and a loss of $3 million ($2 million after tax) related to the State Street Bank and TSU litigation at CMS Viron.\\nFor additional details on CMS Viron, see Note 6, Contingencies and Commitments.\\nAlso includes a gain for the expiration of an indemnity obligation related to a 2007 asset sale.\\nCMS Energy provided an indemnity to TAQA in connection with the sale of its ownership interests in businesses in the Middle East, Africa, and India, and recorded a $50 million provision for the contingent liability.\\nThis indemnity expired in 2009 and CMS Energy eliminated the liability from its balance sheet, recognizing a $45 million benefit ($28 million after tax) to Income (Loss) from Discontinued Operations, Net of Tax Expense (Benefit) and a $5 million benefit to Gain on asset sales, net.\\n(b) Includes operating income of $22 million (operating loss of $9 million after tax).\\nAlso includes $133 million ($101 million after tax) net loss on disposal of assets.\\nFor details on gains and losses recognized on the disposal of discontinued operations, see “Asset Sales” within this Note.\\nOperating Activities Presented in the following table are specific components of net cash provided by operating activities for 2017, 2016, and 2015:',\n  'score': 0.5186553597450256},\n {'id': 'd8e7fbab0',\n  'text': ' MetLife, Inc.  Notes to the Consolidated Financial Statements — (Continued) The following table presents the components of accumulated other comprehensive income (loss), before income tax, related to cash flow hedges\\n|  | Years Ended December 31, |\\n|  | 2011 | 2010 | 2009 |\\n|  | (In millions) |\\n| Accumulated other comprehensive income (loss), balance at January 1, | $-59 | $-76 | $82 |\\n| Gains (losses) deferred in other comprehensive income (loss) on the effective portion of cash flow hedges | 1,552 | -51 | -221 |\\n| Amounts reclassified to net derivative gains (losses) | 9 | 65 | 54 |\\n| Amounts reclassified to net investment income | 3 | 4 | 8 |\\n| Amounts reclassified to other expenses | 9 | -1 | 3 |\\n| Amortization of transition adjustment | — | — | -2 |\\n| Accumulated other comprehensive income (loss), balance at December 31, | $1,514 | $-59 | $-76 |\\nAt December 31, 2011, $13 million of deferred net gains (losses) on derivatives in accumulated other comprehensive income (loss) was expected to be reclassified to earnings within the next 12 months.\\nThe following table presents the effects of derivatives in cash flow hedging relationships on the consolidated statements of operations and the consolidated statements of equi\\n| Derivatives in Cash Flow Hedging Relationships | Amount of Gains (Losses) Deferred in Accumulated Other Comprehensive Income (Loss) on Derivatives (Effective Portion) | Amount and Location of Gains (Losses) Reclassified from Accumulated OtherComprehensive Income (Loss) into Income (Loss) (Effective Portion) | Amount and Location of Gains (Losses) Recognized in Income (Loss)  on Derivatives  (Ineffective Portion and  Amount Excludedfrom Effectiveness Testing) |\\n|  |  | Net Derivative Gains (Losses) | Net Investment Income | Other Expenses | Net Derivative Gains (Losses) | Net Investment Income |\\n|  | (In millions) |\\n| For the Year Ended December 31, 2011: |  |  |  |  |  |  |\\n| Interest rate swaps | $1,023 | $-42 | $1 | $-10 | $1 | $— |\\n| Foreign currency swaps | 175 | — | -6 | 2 | 2 | — |\\n| Interest rate forwards | 336 | 31 | 1 | -1 | 2 | — |\\n| Credit forwards | 18 | 2 | 1 | — | — | — |\\n| Total | $1,552 | $-9 | $-3 | $-9 | $5 | $— |\\n| For the Year Ended December 31, 2010: |  |  |  |  |  |  |\\n| Interest rate swaps | $13 | $— | $— | $-1 | $3 | $— |\\n| Foreign currency swaps | 34 | -79 | -6 | 2 | — | — |\\n| Interest rate forwards | -117 | 14 | 2 | — | -2 | — |\\n| Credit forwards | 19 | — | — | — | — | — |\\n| Total | $-51 | $-65 | $-4 | $1 | $1 | $— |\\n| For the Year Ended December 31, 2009: |  |  |  |  |  |  |\\n| Interest rate swaps | $-45 | $— | $— | $-4 | $-2 | $— |\\n| Foreign currency swaps | -319 | -133 | -6 | 1 | -1 | — |\\n| Interest rate forwards | 147 | 79 | — | — | — | — |\\n| Credit forwards | -4 | — | — | — | — | — |\\n| Total | $-221 | $-54 | $-6 | $-3 | $-3 | $— |\\nAll components of each derivative’s gain or loss were included in the assessment of hedge effectiveness.\\nHedges of Net Investments in Foreign Operations The Company uses foreign exchange contracts, which may include foreign currency swaps, forwards and options, to hedge portions of its net investments in foreign operations against adverse movements in exchange rates.\\nThe Company measures ineffectiveness on these contracts based upon the change in forward rates.\\nIn addition, the Company may also use non-derivative financial instruments to hedge portions of its net investments in foreign operations against adverse movements in exchange rates.\\nThe Company measures ineffectiveness on non-derivative financial instruments based upon the change in spot rates.\\nWhen net investments in foreign operations are sold or substantially liquidated, the amounts in accumulated other comprehensive income (loss) are reclassified\\nAt December 31, 2013, the Company had approximately $35.3 million of U. S.  federal net operating loss carryforwards.\\nIf not utilized, these carryforwards will expire in years 2023 through 2033.\\nThe net operating loss carryforward increased between 2012 and 2013 primarily due to losses incurred by a U. S.  entity that is not a member of the Company’s consolidated tax group and therefore not available for offset against the taxable income of other members of the group.\\nIn a recent acquisition, the consolidated group obtained federal net operating losses subject to an IRC Section 382 limitation; however, the Company expects to utilize the losses in their entirety prior to expiration.\\nThe Company’s state net operating loss carryforwards are primarily related to Florida and New Jersey and will expire in years 2027 through 2030 if not utilized.\\nThe New Jersey net operating loss was acquired as part of a recent acquisition.\\nThe Company has smaller net operating losses in various other states.\\nAdditionally, the Company has a foreign tax credit carryforward and a R&D tax credit carryforward.\\nAs of December 31, 2013, the Company determined that a total valuation allowance of $5.9 million was necessary to reduce U. S.  deferred tax assets by $2.6 million and foreign deferred tax assets by $3.3 million, where it was more likely than not that some portion or all of such deferred tax assets will not be realized.\\nAs of December 31, 2013, based on the Company’s estimates of future taxable income and any applicable tax-planning strategies within various tax jurisdictions, the Company believes that it is more likely than not that the remaining net deferred tax assets will be realized.\\nThe Company recognizes in the consolidated financial statements only those tax positions determined to be “more likely than not” of being sustained upon examination based on the technical merits of the positions.\\nA reconciliation of the beginning and ending amount of unrecognized tax benefits is as follows (in thousands):',\n  'score': 0.516381561756134},\n {'id': 'd8de580c6',\n  'text': ' The following table presents the pretax impact that changes in the fair values of derivatives designated as cash flow hedges had on AOCI and earnings during the years ended December 31, 2017, 2016 and 2015 (in millions):\\n|  | Gain (Loss) Recognized in OCI | Location of Gain (Loss)Recognized in Income1 | Gain (Loss) Reclassified from AOCI into Income (Effective Portion) | Gain (Loss) Recognized in Income (Ineffective Portion and Amount Excluded from Effectiveness Testing) |  |\\n| 2017 |  |  |  |  |  |\\n| Foreign currency contracts | $-226 | Net operating revenues | $443 | $1 |  |\\n| Foreign currency contracts | -23 | Cost of goods sold | -2 | — | 2 |\\n| Foreign currency contracts | — | Interest expense | -9 | — |  |\\n| Foreign currency contracts | 92 | Other income (loss) — net | 107 | 3 |  |\\n| Foreign currency contracts | -3 | Income from discontinued operations | — | — |  |\\n| Interest rate contracts | -22 | Interest expense | -37 | 2 |  |\\n| Commodity contracts | -1 | Cost of goods sold | -1 | — |  |\\n| Commodity contracts | -5 | Income from discontinued operations | — | — |  |\\n| Total | $-188 |  | $501 | $6 |  |\\n| 2016 |  |  |  |  |  |\\n| Foreign currency contracts | $69 | Net operating revenues | $567 | $-3 |  |\\n| Foreign currency contracts | 8 | Cost of goods sold | 35 | -1 |  |\\n| Foreign currency contracts | — | Interest expense | -9 | — |  |\\n| Foreign currency contracts | 13 | Other income (loss) — net | -3 | -3 |  |\\n| Interest rate contracts | -126 | Interest expense | -17 | -2 |  |\\n| Commodity contracts | -1 | Cost of goods sold | -1 | — |  |\\n| Total | $-37 |  | $572 | $-9 |  |\\n| 2015 |  |  |  |  |  |\\n| Foreign currency contracts | $949 | Net operating revenues | $618 | $12 |  |\\n| Foreign currency contracts | 60 | Cost of goods sold | 62 | — | 2 |\\n| Foreign currency contracts | 18 | Interest expense | -9 | — |  |\\n| Foreign currency contracts | -38 | Other income (loss) — net | -40 | — |  |\\n| Interest rate contracts | -153 | Interest expense | -3 | 1 |  |\\n| Commodity contracts | -1 | Cost of goods sold | -3 | — |  |\\n| Total | $835 |  | $625 | $13 |  |\\n1 The Company records gains and losses reclassified from AOCI into income for the effective portion and ineffective portion, if any, to the same line items in our consolidated statements of income.2 Includes a de minimis amount of ineffectiveness in the hedging relationship.\\nAs of December 31, 2017, the Company estimates that it will reclassify into earnings during the next 12 months net gains of $93 million from the pretax amount recorded in AOCI as the anticipated cash flows occur.\\nFair Value Hedging Strategy The Company uses interest rate swap agreements designated as fair value hedges to minimize exposure to changes in the fair value of fixed-rate debt that results from fluctuations in benchmark interest rates.\\nThe Company also uses cross-currency interest rate swaps to hedge the changes in the fair value of foreign currency denominated debt relating to changes in foreign currency exchange rates and benchmark interest rates.\\nThe changes in fair values of derivatives designated as fair value hedges and the offsetting changes in fair values of the hedged items are recognized in earnings.\\nThe ineffective portions of these hedges are immediately recognized into earnings.\\nAs of December 31, 2017, such adjustments had cumulatively increased the',\n  'score': 0.5129295587539673},\n {'id': 'd89f86a50',\n  'text': ' CORPORATE/OTHER Corporate/Other includes global staff functions (includes finance, risk, human resources, legal and compliance) and other corporate expense, global operations and technology (O&T), residual Corporate Treasury and Corporate items.\\nAt December 31, 2009, this segment had approximately $230 billion of assets, consisting primarily of the Company’s liquidity portfolio, including $110 billion of cash and cash equivalents.\\n| In millions of dollars | 2009 | 2008 | 2007 |\\n| Net interest revenue | $-1,663 | $-2,680 | $-2,008 |\\n| Non-interest revenue | -8,893 | 422 | -302 |\\n| Total revenues, net of interest expense | $-10,556 | $-2,258 | $-2,310 |\\n| Total operating expenses | $1,420 | $510 | $1,813 |\\n| Provisions for loan losses and for benefits and claims | -1 | 1 | -3 |\\n| (Loss) from continuing operations before taxes | $-11,975 | $-2,769 | $-4,120 |\\n| Income taxes (benefits) | -4,369 | -587 | -1,446 |\\n| (Loss) from continuing operations | $-7,606 | $-2,182 | $-2,674 |\\n| Income (loss) from discontinued operations, net of taxes | -445 | 4,002 | 708 |\\n| Net income (loss) before attribution of noncontrolling interests | $-8,051 | $1,820 | $-1,966 |\\n| Net income attributable to noncontrolling interests | — | — | 2 |\\n| Net income (loss) | $-8,051 | $1,820 | $-1,968 |\\n2009 vs.  2008 Revenues, net of interest expense declined, primarily due to the pretax loss on debt extinguishment related to the repayment of the $20 billion of TARP trust preferred securities and the pretax loss in connection with the exit from the loss-sharing agreement with the U. S.  government.\\nRevenues also declined, due to the absence of the 2008 sale of Citigroup Global Services Limited recorded in O&T.\\nThis was partially offset by a pretax gain related to the exchange offers, revenues and higher intersegment eliminations.\\nOperating expenses increased, primarily due to intersegment eliminations and increases in compensation, partially offset by lower repositioning reserves.2008 vs.  2007 Revenues, net of interest expense increased primarily due to the gain in 2007 on the sale of certain corporate-owned assets and higher intersegment eliminations, partially offset by improved Treasury hedging activities.\\nOperating expenses declined, primarily due to lower restructuring charges in 2008 as well as reductions in incentive compensation and benefits expense.',\n  'score': 0.49090394377708435}]"},"metadata":{}}],"execution_count":192},{"cell_type":"code","source":"rows = []\nfor i in tqdm(range(len(que_df))):\n    results = []\n    results = retriever.retrieve(que_df['title+text'][i], n_candidates=10)\n    for j in range(len(results)):\n        row = {'query_id': que_df['_id'][i], 'corpus_id': results[j]['id']}\n        rows.append(row)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T15:48:00.211988Z","iopub.execute_input":"2025-11-16T15:48:00.212592Z","iopub.status.idle":"2025-11-16T17:45:28.306116Z","shell.execute_reply.started":"2025-11-16T15:48:00.212550Z","shell.execute_reply":"2025-11-16T17:45:28.305449Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 4671/4671 [1:57:28<00:00,  1.51s/it]  \n","output_type":"stream"}],"execution_count":193},{"cell_type":"code","source":"res2 = pd.DataFrame(rows)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:47:08.027290Z","iopub.execute_input":"2025-11-16T17:47:08.028076Z","iopub.status.idle":"2025-11-16T17:47:08.066726Z","shell.execute_reply.started":"2025-11-16T17:47:08.028047Z","shell.execute_reply":"2025-11-16T17:47:08.066080Z"}},"outputs":[],"execution_count":194},{"cell_type":"code","source":"res2.to_csv('/kaggle/working/xz6.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T17:47:12.218850Z","iopub.execute_input":"2025-11-16T17:47:12.219819Z","iopub.status.idle":"2025-11-16T17:47:12.293932Z","shell.execute_reply.started":"2025-11-16T17:47:12.219793Z","shell.execute_reply":"2025-11-16T17:47:12.293104Z"}},"outputs":[],"execution_count":195},{"cell_type":"markdown","source":"# Multi-hop/ Self-ask - разбивает запрос на части + Self-RAG + ReAct","metadata":{}},{"cell_type":"code","source":"import re\nfrom typing import List, Dict, Any, Optional, Tuple\nimport json","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MultiHopReasoning:\n    def __init__(self, llm):\n        self.llm = llm\n    \n    def break_down_question(self, complex_question: str) -> List[str]:\n        \"\"\"\n        Разбивка сложного вопроса на подвопросы\n        \"\"\"\n        prompt = f\"\"\"Разбей сложный вопрос на последовательность простых подвопросов, \n        которые нужно ответить по очереди чтобы получить окончательный ответ.\n\n        Сложный вопрос: {complex_question}\n\n        Подвопросы:\n        1.\"\"\"\n\n        response = self.llm.generate(prompt, max_length=300)\n        \n        # Парсинг подвопросов из ответа\n        sub_questions = self._parse_subquestions(response)\n        \n        print(f\"Сложный вопрос разбит на {len(sub_questions)} подвопросов:\")\n        for i, q in enumerate(sub_questions, 1):\n            print(f\"  {i}. {q}\")\n        \n        return sub_questions\n    \n    def _parse_subquestions(self, text: str) -> List[str]:\n        \"\"\"Парсинг подвопросов из текста LLM\"\"\"\n        lines = text.split('\\n')\n        questions = []\n        \n        for line in lines:\n            line = line.strip()\n            # Ищем строки с номерами (1., 2., и т.д.)\n            if any(line.startswith(f\"{i}.\") for i in range(1, 10)):\n                question = line.split('.', 1)[1].strip()\n                if question and '?' in question:\n                    questions.append(question)\n        \n        return questions if questions else [text]\n    \n    def solve_multi_hop(self, complex_question: str, rag_system, max_hops: int = 5) -> Dict:\n        \"\"\"\n        Решение сложного вопроса через последовательность подвопросов\n        \"\"\"\n        print(f\"🧠 Multi-hop reasoning для: {complex_question}\")\n        \n        # Разбиваем на подвопросы\n        sub_questions = self.break_down_question(complex_question)\n        sub_questions = sub_questions[:max_hops]  # Ограничиваем количество hops\n        \n        intermediate_answers = []\n        context_chain = []\n        \n        for i, sub_q in enumerate(sub_questions, 1):\n            print(f\"\\n🏃 Hop {i}/{len(sub_questions)}: {sub_q}\")\n            \n            # Ищем релевантные документы для подвопроса\n            search_results = rag_system.search_with_reranking(sub_q, vector_top_k=10, final_top_k=3)\n            \n            # Генерируем ответ на подвопрос\n            context = \"\\n\".join([doc['text'] for doc in search_results])\n            answer = self.llm.generate(sub_q, context)\n            \n            intermediate_answers.append({\n                'sub_question': sub_q,\n                'answer': answer,\n                'source_documents': search_results\n            })\n            \n            # Сохраняем контекст для следующего шага\n            context_chain.append(f\"Q{i}: {sub_q}\\nA{i}: {answer}\")\n            \n            print(f\"   Ответ: {answer[:100]}...\")\n        \n        # Синтезируем финальный ответ на основе всех промежуточных ответов\n        final_context = \"\\n\\n\".join(context_chain)\n        final_prompt = f\"\"\"На основе следующих вопросов и ответов, дай окончательный ответ на исходный вопрос.\n\nПромежуточные вопросы и ответы:\n{final_context}\n\nИсходный вопрос: {complex_question}\n\nОкончательный ответ:\"\"\"\n\n        final_answer = self.llm.generate(final_prompt, max_length=500)\n        \n        return {\n            'final_answer': final_answer,\n            'intermediate_answers': intermediate_answers,\n            'reasoning_chain': context_chain\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SelfRAG:\n    def __init__(self, llm, vector_store, cross_encoder=None):\n        self.llm = llm\n        self.vector_store = vector_store\n        self.cross_encoder = cross_encoder\n    \n    def retrieve_with_self_evaluation(self, query: str, max_docs: int = 10) -> List[Dict]:\n        \"\"\"\n        Self-RAG: поиск с самооценкой релевантности\n        \"\"\"\n        # Первоначальный поиск\n        candidate_docs = self.vector_store.search(query, top_k=max_docs * 2)\n        \n        if not self.cross_encoder:\n            return candidate_docs[:max_docs]\n        \n        # Self-evaluation релевантности\n        evaluated_docs = []\n        for doc in candidate_docs:\n            relevance_score = self._evaluate_relevance(query, doc['text'])\n            doc['self_rag_score'] = relevance_score\n            evaluated_docs.append(doc)\n        \n        # Сортировка по self-evaluation score\n        evaluated_docs.sort(key=lambda x: x.get('self_rag_score', 0), reverse=True)\n        \n        # Фильтрация низкорелевантных документов\n        filtered_docs = [doc for doc in evaluated_docs \n                        if doc.get('self_rag_score', 0) > 0.3]\n        \n        return filtered_docs[:max_docs]\n    \n    def _evaluate_relevance(self, query: str, document: str) -> float:\n        \"\"\"\n        Оценка релевантности документа запросу\n        \"\"\"\n        prompt = f\"\"\"Оцени насколько следующий документ релевантен запросу.\n        Верни только число от 0.0 (совсем не релевантен) до 1.0 (полностью релевантен).\n\n        Запрос: {query}\n\n        Документ: {document[:500]}\n\n        Оценка релевантности:\"\"\"\n\n        try:\n            response = self.llm.generate(prompt, max_length=10)\n            score = float(response.strip())\n            return max(0.0, min(1.0, score))  # Клиппинг к [0, 1]\n        except:\n            return 0.5  # Дефолтная оценка при ошибке\n    \n    def generate_with_self_reflection(self, query: str, context: str) -> Dict:\n        \"\"\"\n        Генерация с саморефлексией и проверкой фактов\n        \"\"\"\n        # Генерация первоначального ответа\n        initial_prompt = f\"\"\"Ответь на вопрос на основе предоставленного контекста.\n\n        Контекст: {context}\n\n        Вопрос: {query}\n\n        Ответ:\"\"\"\n        \n        initial_answer = self.llm.generate(initial_prompt)\n        \n        # Self-reflection: проверка качества ответа\n        reflection = self._reflect_on_answer(query, context, initial_answer)\n        \n        # Если ответ плохой, пытаемся улучшить\n        if reflection.get('needs_improvement', True):\n            improved_answer = self._improve_answer(query, context, initial_answer, reflection)\n        else:\n            improved_answer = initial_answer\n        \n        return {\n            'initial_answer': initial_answer,\n            'final_answer': improved_answer,\n            'reflection': reflection,\n            'is_verified': reflection.get('is_supported', False)\n        }\n    \n    def _reflect_on_answer(self, query: str, context: str, answer: str) -> Dict:\n        \"\"\"\n        Саморефлексия над сгенерированным ответом\n        \"\"\"\n        prompt = f\"\"\"Проанализируй ответ на вопрос и оцени:\n        1. Подтверждается ли ответ предоставленным контекстом?\n        2. Является ли ответ полным и точным?\n        3. Есть ли в ответе фактические ошибки?\n\n        Вопрос: {query}\n        Контекст: {context}\n        Ответ: {answer}\n\n        Верни ответ в формате:\n        is_supported: [true/false]\n        is_complete: [true/false] \n        has_errors: [true/false]\n        needs_improvement: [true/false]\n        critique: [краткая критика]\"\"\"\n\n        response = self.llm.generate(prompt)\n        \n        # Парсинг ответа\n        return self._parse_reflection(response)\n    \n    def _parse_reflection(self, text: str) -> Dict:\n        \"\"\"Парсинг self-reflection ответа\"\"\"\n        result = {\n            'is_supported': False,\n            'is_complete': False,\n            'has_errors': True,\n            'needs_improvement': True,\n            'critique': 'Не удалось проанализировать'\n        }\n        \n        lines = text.split('\\n')\n        for line in lines:\n            line = line.lower().strip()\n            if 'is_supported:' in line:\n                result['is_supported'] = 'true' in line\n            elif 'is_complete:' in line:\n                result['is_complete'] = 'true' in line\n            elif 'has_errors:' in line:\n                result['has_errors'] = 'true' in line\n            elif 'needs_improvement:' in line:\n                result['needs_improvement'] = 'true' in line\n            elif 'critique:' in line:\n                result['critique'] = line.split('critique:')[1].strip()\n        \n        return result\n    \n    def _improve_answer(self, query: str, context: str, initial_answer: str, reflection: Dict) -> str:\n        \"\"\"\n        Улучшение ответа на основе саморефлексии\n        \"\"\"\n        prompt = f\"\"\"Улучши следующий ответ на вопрос, учитывая критику.\n        Используй только информацию из предоставленного контекста.\n\n        Исходный вопрос: {query}\n        Контекст: {context}\n        Исходный ответ: {initial_answer}\n        Критика: {reflection.get('critique', '')}\n\n        Улучшенный ответ:\"\"\"\n\n        return self.llm.generate(prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ReActAgent:\n    def __init__(self, llm, rag_system, max_steps: int = 10):\n        self.llm = llm\n        self.rag_system = rag_system\n        self.max_steps = max_steps\n        self.actions = ['search', 'calculate', 'reason', 'final_answer']\n    \n    def solve_with_react(self, question: str) -> Dict:\n        \"\"\"\n        ReAct подход: Reasoning + Acting\n        \"\"\"\n        print(f\"🤖 ReAct решение для: {question}\")\n        \n        trajectory = []\n        context = \"\"\n        step = 0\n        \n        while step < self.max_steps:\n            step += 1\n            print(f\"\\n🔹 Step {step}:\")\n            \n            # Генерация мыслей и действий\n            react_response = self._generate_thought_and_action(question, context, trajectory)\n            \n            trajectory.append(react_response)\n            action = react_response.get('action', 'reason')\n            \n            print(f\"   Мысль: {react_response['thought'][:100]}...\")\n            print(f\"   Действие: {action}\")\n            \n            # Выполнение действия\n            if action == 'search':\n                search_query = react_response.get('query', question)\n                search_results = self.rag_system.search_with_reranking(search_query, vector_top_k=5, final_top_k=2)\n                new_context = \"\\n\".join([doc['text'] for doc in search_results])\n                context += f\"\\nНайденная информация: {new_context}\"\n                react_response['search_results'] = search_results\n                \n            elif action == 'final_answer':\n                # Завершаем процесс\n                react_response['is_final'] = True\n                break\n            \n            # Если слишком долго думаем, принудительно завершаем\n            if step >= self.max_steps:\n                react_response['action'] = 'final_answer'\n                react_response['thought'] = \"Достигнут лимит шагов, формулирую итоговый ответ\"\n        \n        # Генерация финального ответа\n        final_answer = self._synthesize_final_answer(question, trajectory)\n        \n        return {\n            'final_answer': final_answer,\n            'trajectory': trajectory,\n            'steps_taken': step\n        }\n    \n    def _generate_thought_and_action(self, question: str, context: str, trajectory: List) -> Dict:\n        \"\"\"\n        Генерация мысли и следующего действия\n        \"\"\"\n        trajectory_text = \"\\n\".join([\n            f\"Step {i+1}: {step['thought']} -> {step.get('action', 'reason')}\"\n            for i, step in enumerate(trajectory)\n        ])\n        \n        prompt = f\"\"\"Ты - reasoning agent. Думай шаг за шагом и выбирай действия.\n\nДоступные действия:\n- search: поиск информации в базе знаний\n- reason: логические размышления  \n- calculate: вычисления (если нужны)\n- final_answer: дать окончательный ответ\n\nИстория:\n{trajectory_text}\n\nТекущий контекст: {context}\n\nВопрос: {question}\n\nСначала подумай (thought), затем выбери действие (action).\nЕсли нужно искать информацию, укажи поисковый запрос (query).\n\nФормат ответа:\nThought: [твои рассуждения]\nAction: [search|reason|calculate|final_answer]\nQuery: [если action=search, укажи что искать]\"\"\"\n\n        response = self.llm.generate(prompt)\n        \n        # Парсинг ответа\n        return self._parse_react_response(response)\n    \n    def _parse_react_response(self, text: str) -> Dict:\n        \"\"\"Парсинг ReAct ответа\"\"\"\n        result = {\n            'thought': '',\n            'action': 'reason',\n            'query': ''\n        }\n        \n        lines = text.split('\\n')\n        current_key = None\n        \n        for line in lines:\n            line = line.strip()\n            if line.startswith('Thought:'):\n                current_key = 'thought'\n                result['thought'] = line.replace('Thought:', '').strip()\n            elif line.startswith('Action:'):\n                current_key = 'action'\n                action = line.replace('Action:', '').strip().lower()\n                if action in self.actions:\n                    result['action'] = action\n            elif line.startswith('Query:'):\n                current_key = 'query'\n                result['query'] = line.replace('Query:', '').strip()\n            elif current_key and line:\n                result[current_key] += ' ' + line\n        \n        return result\n    \n    def _synthesize_final_answer(self, question: str, trajectory: List) -> str:\n        \"\"\"Синтез финального ответа из trajectory\"\"\"\n        reasoning_chain = \"\\n\".join([\n            f\"Шаг {i+1}: {step['thought']}\"\n            for i, step in enumerate(trajectory)\n        ])\n        \n        prompt = f\"\"\"На основе цепочки рассуждений, дай окончательный ответ на вопрос.\n\nВопрос: {question}\n\nЦепочка рассуждений:\n{reasoning_chain}\n\nОкончательный ответ:\"\"\"\n\n        return self.llm.generate(prompt)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AdvancedRAGSystem:\n    def __init__(self, vector_store, llm, cross_encoder=None):\n        self.vector_store = vector_store\n        self.llm = llm\n        self.cross_encoder = cross_encoder\n        \n        # Инициализация компонентов\n        self.multi_hop = MultiHopReasoning(llm)\n        self.self_rag = SelfRAG(llm, vector_store, cross_encoder)\n        self.react_agent = ReActAgent(llm, self)\n    \n    def search_with_reranking(self, query: str, vector_top_k: int = 20, final_top_k: int = 5) -> List[Dict]:\n        \"\"\"Поиск с re-ranking\"\"\"\n        # Векторный поиск\n        vector_results = self.vector_store.search(query, top_k=vector_top_k)\n        \n        if self.cross_encoder and len(vector_results) > final_top_k:\n            # Re-ranking с cross-encoder\n            reranked = self.cross_encoder.rerank_documents(query, vector_results, top_k=final_top_k)\n            return reranked\n        else:\n            return vector_results[:final_top_k]\n    \n    def query(self, \n             question: str, \n             method: str = \"auto\",\n             use_self_reflection: bool = True) -> Dict:\n        \"\"\"\n        Умный запрос с выбором метода reasoning\n        \n        Args:\n            method: \"multi_hop\", \"react\", \"self_rag\", \"auto\"\n        \"\"\"\n        print(f\"🎯 Вопрос: {question}\")\n        print(f\"📝 Метод: {method}\")\n        \n        # Автоматический выбор метода\n        if method == \"auto\":\n            method = self._select_method(question)\n            print(f\"🤖 Автоматически выбран метод: {method}\")\n        \n        if method == \"multi_hop\":\n            result = self.multi_hop.solve_multi_hop(question, self)\n            result['method'] = 'multi_hop'\n            \n        elif method == \"react\":\n            result = self.react_agent.solve_with_react(question)\n            result['method'] = 'react'\n            \n        elif method == \"self_rag\":\n            # Self-RAG подход\n            docs = self.self_rag.retrieve_with_self_evaluation(question)\n            context = \"\\n\".join([doc['text'] for doc in docs])\n            \n            if use_self_reflection:\n                generation_result = self.self_rag.generate_with_self_reflection(question, context)\n                result = {\n                    'final_answer': generation_result['final_answer'],\n                    'documents': docs,\n                    'reflection': generation_result['reflection'],\n                    'method': 'self_rag'\n                }\n            else:\n                answer = self.llm.generate(question, context)\n                result = {\n                    'final_answer': answer,\n                    'documents': docs,\n                    'method': 'self_rag'\n                }\n        else:\n            # Стандартный RAG\n            docs = self.search_with_reranking(question)\n            context = \"\\n\".join([doc['text'] for doc in docs])\n            answer = self.llm.generate(question, context)\n            result = {\n                'final_answer': answer,\n                'documents': docs,\n                'method': 'standard_rag'\n            }\n        \n        return result\n    \n    def _select_method(self, question: str) -> str:\n        \"\"\"\n        Автоматический выбор метода reasoning на основе вопроса\n        \"\"\"\n        prompt = f\"\"\"Проанализируй вопрос и определи какой метод reasoning лучше подойдет:\n        - multi_hop: для сложных вопросов требующих нескольких шагов\n        - react: для вопросов требующих поиска и логических рассуждений  \n        - self_rag: для точных ответов с проверкой фактов\n        - standard_rag: для простых фактологических вопросов\n\n        Вопрос: {question}\n\n        Верни только название метода: multi_hop, react, self_rag или standard_rag\"\"\"\n\n        response = self.llm.generate(prompt, max_length=20).strip().lower()\n        \n        if any(method in response for method in ['multi_hop', 'multi hop', 'hop']):\n            return \"multi_hop\"\n        elif any(method in response for method in ['react', 'reason']):\n            return \"react\"\n        elif any(method in response for method in ['self_rag', 'self', 'rag']):\n            return \"self_rag\"\n        else:\n            return \"standard_rag\"\n\n# Дополнительные утилиты\nclass ReasoningUtils:\n    @staticmethod\n    def visualize_reasoning(result: Dict):\n        \"\"\"Визуализация процесса reasoning\"\"\"\n        method = result.get('method', 'unknown')\n        print(f\"\\n{'='*60}\")\n        print(f\"🧠 REASONING PROCESS: {method.upper()}\")\n        print(f\"{'='*60}\")\n        \n        if method == 'multi_hop':\n            for i, step in enumerate(result.get('intermediate_answers', [])):\n                print(f\"🔹 Hop {i+1}: {step['sub_question']}\")\n                print(f\"   Ответ: {step['answer'][:100]}...\")\n                \n        elif method == 'react':\n            for i, step in enumerate(result.get('trajectory', [])):\n                print(f\"🔹 Step {i+1}: {step['thought'][:80]}...\")\n                print(f\"   Действие: {step.get('action', 'N/A')}\")\n                \n        elif method == 'self_rag':\n            reflection = result.get('reflection', {})\n            print(f\"✓ Подтверждено: {reflection.get('is_supported', False)}\")\n            print(f\"✓ Полнота: {reflection.get('is_complete', False)}\")\n            print(f\"📝 Критика: {reflection.get('critique', 'N/A')}\")\n        \n        print(f\"\\n🎯 Финальный ответ: {result['final_answer']}\")\n        print(f\"{'='*60}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def demo_advanced_rag():\n    \"\"\"Демонстрация работы расширенной RAG системы\"\"\"\n    \n    # Инициализация компонентов\n    vector_store = YourVectorStore()  # Ваша векторная БД\n    llm = YourLLM()                  # Ваша LLM модель\n    cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n    \n    # Создание системы\n    advanced_rag = AdvancedRAGSystem(vector_store, llm, cross_encoder)\n    \n    # Тестовые вопросы\n    test_questions = [\n        \"Как изменилась политика компании Apple в отношении прав разработчиков после 2020 года и какие это имело последствия для доходов приложений?\",\n        \"Рассчитай среднюю скорость движения автомобиля, который проехал 150 км за 2 часа, а затем 100 км за 1.5 часа.\",\n        \"Какие основные принципы квантовой механики отличают ее от классической физики?\",\n    ]\n    \n    for question in test_questions:\n        print(f\"\\n{'#'*80}\")\n        print(f\"ВОПРОС: {question}\")\n        print(f\"{'#'*80}\")\n        \n        # Автоматический выбор метода\n        result = advanced_rag.query(question, method=\"auto\")\n        \n        # Визуализация процесса\n        ReasoningUtils.visualize_reasoning(result)\n        \n        print(f\"\\n📚 Использовано документов: {len(result.get('documents', []))}\")\n        print(f\"⚡ Метод: {result.get('method', 'N/A')}\")\n\nif __name__ == \"__main__\":\n    demo_advanced_rag()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Суммаризация контекста","metadata":{}},{"cell_type":"code","source":"class ContextSummarizer:\n    def __init__(self, llm):\n        self.llm = llm\n    \n    def summarize_with_sources(self, documents: List[Dict], query: str = None, \n                             max_summary_length: int = 1000) -> Dict:\n        \"\"\"\n        Суммаризация контекста с указанием источников для каждого факта\n        \"\"\"\n        if not documents:\n            return {\n                'summary': 'Нет доступной информации для суммаризации',\n                'source_mapping': {},\n                'key_points': []\n            }\n        \n        print(f\"📝 Суммаризация {len(documents)} документов...\")\n        \n        # Подготавливаем документы с ID\n        prepared_docs = self._prepare_documents_with_ids(documents)\n        \n        # Генерируем суммаризацию с источниками\n        summary_result = self._generate_attributed_summary(prepared_docs, query, max_summary_length)\n        \n        # Извлекаем ключевые точки с источниками\n        key_points = self._extract_key_points_with_sources(prepared_docs, query)\n        \n        return {\n            'summary': summary_result['summary'],\n            'source_mapping': summary_result['source_mapping'],\n            'key_points': key_points,\n            'total_documents': len(documents),\n            'documents_used': list(summary_result['source_mapping'].keys())\n        }\n    \n    def _prepare_documents_with_ids(self, documents: List[Dict]) -> List[Dict]:\n        \"\"\"Подготовка документов с уникальными идентификаторами\"\"\"\n        prepared_docs = []\n        \n        for i, doc in enumerate(documents):\n            doc_id = doc.get('id', f\"doc_{i}\")\n            prepared_docs.append({\n                'id': doc_id,\n                'text': doc['text'],\n                'metadata': doc.get('metadata', {}),\n                'score': doc.get('score', 0.0),\n                'source': doc.get('metadata', {}).get('source', f'Документ {i+1}')\n            })\n        \n        return prepared_docs\n    \n    def _generate_attributed_summary(self, documents: List[Dict], query: str = None, \n                                   max_length: int = 1000) -> Dict:\n        \"\"\"Генерация суммаризации с привязкой фактов к источникам\"\"\"\n        \n        # Сортируем документы по релевантности\n        sorted_docs = sorted(documents, key=lambda x: x.get('score', 0), reverse=True)\n        \n        # Создаем промпт для суммаризации с источниками\n        prompt = self._build_summarization_prompt(sorted_docs, query, max_length)\n        \n        # Генерируем суммаризацию\n        summary = self.llm.generate(prompt, max_length=max_length + 500)\n        \n        # Парсим источники из суммаризации\n        source_mapping = self._parse_sources_from_summary(summary, sorted_docs)\n        \n        # Очищаем суммаризацию от маркеров источников для финального вывода\n        clean_summary = self._clean_summary_text(summary)\n        \n        return {\n            'summary': clean_summary,\n            'source_mapping': source_mapping,\n            'raw_summary': summary\n        }\n    \n    def _build_summarization_prompt(self, documents: List[Dict], query: str = None, \n                                  max_length: int = 1000) -> str:\n        \"\"\"Создание промпта для суммаризации с источниками\"\"\"\n        \n        docs_text = \"\"\n        for i, doc in enumerate(documents[:10]):  # Ограничиваем количество документов\n            docs_text += f\"\\n\\n[Документ {doc['id']}]: {doc['text'][:800]}\"\n            if len(doc['text']) > 800:\n                docs_text += \"...\"\n        \n        query_context = f\" по запросу: '{query}'\" if query else \"\"\n        \n        prompt = f\"\"\"Суммаризируй предоставленные документы{query_context}. \n        Для КАЖДОГО факта в суммаризации указывай источник в формате [Документ X].\n        Будь точен и используй только информацию из предоставленных документов.\n\n        Инструкции:\n        1. Указывай источник для каждого утверждения: [Документ ID]\n        2. Если информация повторяется в нескольких документах, укажи все источники: [Документ A, Документ B]\n        3. Не добавляй информацию, которой нет в документах\n        4. Максимальная длина: {max_length} символов\n\n        Документы:{docs_text}\n\n        Суммаризация с источниками:\"\"\"\n        \n        return prompt\n    \n    def _parse_sources_from_summary(self, summary: str, documents: List[Dict]) -> Dict[str, List[str]]:\n        \"\"\"Парсинг источников из суммаризации\"\"\"\n        source_mapping = {}\n        \n        # Ищем паттерны [Документ X], [Документ X, Документ Y], etc.\n        source_patterns = [\n            r'\\[Документ\\s+([^]]+)\\]',\n            r'\\[Источник:\\s*([^]]+)\\]',\n            r'\\[Doc\\s*([^]]+)\\]',\n        ]\n        \n        for pattern in source_patterns:\n            matches = re.findall(pattern, summary)\n            for match in matches:\n                # Обрабатываем множественные источники\n                source_ids = [src.strip() for src in match.split(',')]\n                \n                for source_id in source_ids:\n                    if source_id not in source_mapping:\n                        source_mapping[source_id] = []\n                    \n                    # Находим соответствующий документ\n                    doc = next((d for d in documents if d['id'] == source_id), None)\n                    if doc:\n                        source_info = {\n                            'text_preview': doc['text'][:200] + \"...\",\n                            'score': doc.get('score', 0.0),\n                            'source': doc.get('source', 'Неизвестный источник')\n                        }\n                        source_mapping[source_id].append(source_info)\n        \n        return source_mapping\n    \n    def _clean_summary_text(self, summary: str) -> str:\n        \"\"\"Очистка текста суммаризации от маркеров источников для финального вывода\"\"\"\n        # Убираем маркеры источников, но сохраняем читаемость\n        cleaned = re.sub(r'\\[Документ\\s+[^]]+\\]', '', summary)\n        cleaned = re.sub(r'\\[Источник:\\s*[^]]+\\]', '', cleaned)\n        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n        return cleaned","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Извлечение источников","metadata":{}},{"cell_type":"code","source":"class ContextSummarizer:\n    def __init__(self, llm):\n        self.llm = llm\n    \n    def _extract_key_points_with_sources(self, documents: List[Dict], query: str = None) -> List[Dict]:\n        \"\"\"Извлечение ключевых точек с привязкой к источникам\"\"\"\n        \n        prompt = self._build_key_points_prompt(documents, query)\n        \n        try:\n            response = self.llm.generate(prompt, max_length=800)\n            key_points = self._parse_key_points_response(response, documents)\n            return key_points\n        except Exception as e:\n            print(f\"Ошибка при извлечении ключевых точек: {e}\")\n            return self._fallback_key_points(documents)\n    \n    def _build_key_points_prompt(self, documents: List[Dict], query: str = None) -> str:\n        \"\"\"Создание промпта для извлечения ключевых точек\"\"\"\n        \n        docs_text = \"\"\n        for i, doc in enumerate(documents[:8]):  # Ограничиваем для экономии токенов\n            docs_text += f\"\\n\\n[Документ {doc['id']}]: {doc['text'][:500]}\"\n            if len(doc['text']) > 500:\n                docs_text += \"...\"\n        \n        query_context = f\" по запросу: '{query}'\" if query else \"\"\n        \n        prompt = f\"\"\"Извлеки 3-5 ключевых точек из предоставленных документов{query_context}. \n        Для каждой точки укажи источники в формате [Документ X].\n\n        Формат:\n        • Точка 1 [Документ A, Документ B]\n        • Точка 2 [Документ C]\n        • ...\n\n        Документы:{docs_text}\n\n        Ключевые точки:\"\"\"\n        \n        return prompt\n    \n    def _parse_key_points_response(self, response: str, documents: List[Dict]) -> List[Dict]:\n        \"\"\"Парсинг ключевых точек из ответа LLM\"\"\"\n        key_points = []\n        \n        lines = response.split('\\n')\n        for line in lines:\n            line = line.strip()\n            if line.startswith(('•', '-', '*', '▸')) or re.match(r'^\\d+\\.', line):\n                # Извлекаем текст точки и источники\n                point_text = re.sub(r'^[•\\-\\*\\d\\.\\s▸]+', '', line).strip()\n                \n                # Ищем источники в формате [Документ X]\n                sources = re.findall(r'\\[Документ\\s+([^]]+)\\]', point_text)\n                \n                # Очищаем текст от маркеров источников\n                clean_text = re.sub(r'\\[Документ\\s+[^]]+\\]', '', point_text).strip()\n                \n                if clean_text and sources:\n                    # Получаем информацию об источниках\n                    source_info = []\n                    for source_id in sources:\n                        doc = next((d for d in documents if d['id'] == source_id), None)\n                        if doc:\n                            source_info.append({\n                                'doc_id': source_id,\n                                'text_preview': doc['text'][:150] + \"...\",\n                                'score': doc.get('score', 0.0),\n                                'source': doc.get('source', 'Неизвестный источник')\n                            })\n                    \n                    key_points.append({\n                        'point': clean_text,\n                        'sources': source_info,\n                        'source_ids': sources\n                    })\n        \n        return key_points if key_points else self._fallback_key_points(documents)\n    \n    def _fallback_key_points(self, documents: List[Dict]) -> List[Dict]:\n        \"\"\"Fallback метод извлечения ключевых точек\"\"\"\n        key_points = []\n        \n        for i, doc in enumerate(documents[:5]):\n            # Простая эвристика: берем первое предложение как ключевую точку\n            first_sentence = doc['text'].split('.')[0] + '.'\n            key_points.append({\n                'point': first_sentence[:200],\n                'sources': [{\n                    'doc_id': doc['id'],\n                    'text_preview': doc['text'][:150] + \"...\",\n                    'score': doc.get('score', 0.0),\n                    'source': doc.get('source', f'Документ {i+1}')\n                }],\n                'source_ids': [doc['id']]\n            })\n        \n        return key_points","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Визуализация по источникам","metadata":{}},{"cell_type":"code","source":"class SummaryVisualizer:\n    \"\"\"Класс для визуализации суммаризации с источниками\"\"\"\n    \n    @staticmethod\n    def display_summary_with_sources(summary_result: Dict, query: str = None):\n        \"\"\"Отображение суммаризации с источниками\"\"\"\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"📊 СУММАРИЗАЦИЯ КОНТЕКСТА С ИСТОЧНИКАМИ\")\n        print(\"=\"*80)\n        \n        if query:\n            print(f\"📝 Запрос: {query}\")\n        \n        print(f\"\\n📄 Обработано документов: {summary_result['total_documents']}\")\n        print(f\"🔗 Использовано источников: {len(summary_result['documents_used'])}\")\n        \n        print(f\"\\n📋 СВОДКА:\")\n        print(\"-\" * 40)\n        print(summary_result['summary'])\n        \n        print(f\"\\n🎯 КЛЮЧЕВЫЕ ТОЧКИ:\")\n        print(\"-\" * 40)\n        for i, point in enumerate(summary_result['key_points'], 1):\n            print(f\"{i}. {point['point']}\")\n            for source in point['sources']:\n                print(f\"   📍 Источник: {source['source']} (сходство: {source['score']:.3f})\")\n        \n        print(f\"\\n🔍 ДЕТАЛИ ИСТОЧНИКОВ:\")\n        print(\"-\" * 40)\n        for doc_id, sources in summary_result['source_mapping'].items():\n            for source in sources:\n                print(f\"📄 {source['source']} (ID: {doc_id}):\")\n                print(f\"   {source['text_preview']}\")\n                print(f\"   ⭐ Релевантность: {source['score']:.3f}\")\n                print()\n    \n    @staticmethod\n    def generate_source_report(summary_result: Dict, format: str = \"text\") -> str:\n        \"\"\"Генерация отчета об источниках\"\"\"\n        \n        if format == \"text\":\n            report = \"ОТЧЕТ ОБ ИСТОЧНИКАХ\\n\"\n            report += \"=\" * 50 + \"\\n\\n\"\n            \n            for doc_id, sources in summary_result['source_mapping'].items():\n                for source in sources:\n                    report += f\"ИСТОЧНИК: {source['source']}\\n\"\n                    report += f\"ID: {doc_id}\\n\"\n                    report += f\"Релевантность: {source['score']:.3f}\\n\"\n                    report += f\"Фрагмент: {source['text_preview']}\\n\"\n                    report += \"-\" * 30 + \"\\n\"\n            \n            return report\n        \n        elif format == \"markdown\":\n            report = \"# Отчет об источниках\\n\\n\"\n            \n            for doc_id, sources in summary_result['source_mapping'].items():\n                for source in sources:\n                    report += f\"## {source['source']}\\n\"\n                    report += f\"- **ID**: {doc_id}\\n\"\n                    report += f\"- **Релевантность**: {source['score']:.3f}\\n\"\n                    report += f\"- **Фрагмент**: {source['text_preview']}\\n\\n\"\n            \n            return report\n        \n        return \"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Интеграция с Rag системой","metadata":{}},{"cell_type":"code","source":"class EnhancedRAGWithSummarization:\n    \"\"\"RAG система с расширенной суммаризацией\"\"\"\n    \n    def __init__(self, vector_store, llm, cross_encoder=None):\n        self.vector_store = vector_store\n        self.llm = llm\n        self.cross_encoder = cross_encoder\n        self.summarizer = ContextSummarizer(llm)\n        self.visualizer = SummaryVisualizer()\n    \n    def query_with_attributed_summary(self, question: str, \n                                    include_sources: bool = True,\n                                    generate_summary: bool = True) -> Dict:\n        \"\"\"\n        Запрос с суммаризацией и указанием источников\n        \"\"\"\n        print(f\"🔍 Поиск информации для: {question}\")\n        \n        # Поиск релевантных документов\n        search_results = self.vector_store.search(question, top_k=10)\n        \n        if not search_results:\n            return {\n                'answer': 'Не найдено релевантной информации',\n                'summary': 'Нет данных для суммаризации',\n                'documents': []\n            }\n        \n        # Re-ranking если доступен cross-encoder\n        if self.cross_encoder:\n            search_results = self.cross_encoder.rerank_documents(question, search_results, top_k=7)\n        \n        # Генерация ответа\n        context = \"\\n\\n\".join([doc['text'] for doc in search_results])\n        answer = self.llm.generate(question, context)\n        \n        result = {\n            'question': question,\n            'answer': answer,\n            'documents': search_results,\n            'total_documents_found': len(search_results)\n        }\n        \n        # Суммаризация если требуется\n        if generate_summary:\n            summary_result = self.summarizer.summarize_with_sources(search_results, question)\n            result['summary'] = summary_result\n            \n            # Визуализация\n            if include_sources:\n                self.visualizer.display_summary_with_sources(summary_result, question)\n        \n        return result\n    \n    def generate_detailed_report(self, question: str, format: str = \"text\") -> Dict:\n        \"\"\"\n        Генерация детального отчета с источниками\n        \"\"\"\n        result = self.query_with_attributed_summary(question, include_sources=False, generate_summary=True)\n        \n        if 'summary' in result:\n            source_report = self.visualizer.generate_source_report(result['summary'], format)\n            result['source_report'] = source_report\n        \n        return result","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Пример использования","metadata":{}},{"cell_type":"code","source":"def demo_context_summarization():\n    \"\"\"Демонстрация суммаризации контекста с источниками\"\"\"\n    \n    # Мок данные для демонстрации\n    mock_documents = [\n        {\n            'id': 'doc_1',\n            'text': 'Машинное обучение - это область искусственного интеллекта, которая позволяет компьютерам обучаться на данных без явного программирования. Основные подходы включают обучение с учителем, без учителя и с подкреплением.',\n            'metadata': {'source': 'Википедия', 'year': 2023},\n            'score': 0.95\n        },\n        {\n            'id': 'doc_2', \n            'text': 'Глубокое обучение использует нейронные сети с множеством слоев для извлечения признаков из данных. Этот метод особенно эффективен для обработки изображений, текста и речи.',\n            'metadata': {'source': 'Научная статья', 'author': 'Иванов'},\n            'score': 0.88\n        },\n        {\n            'id': 'doc_3',\n            'text': 'В 2020 году исследователи достигли значительных успехов в области трансферного обучения, что позволило использовать предобученные модели для различных задач с минимальной дообучкой.',\n            'metadata': {'source': 'Исследовательский отчет', 'year': 2021},\n            'score': 0.82\n        }\n    ]\n    \n    # Инициализация суммаризатора\n    class MockLLM:\n        def generate(self, prompt, max_length=1000):\n            return \"\"\"Машинное обучение позволяет компьютерам обучаться на данных без программирования [Документ doc_1]. \n            Глубокое обучение использует многослойные нейронные сети для обработки сложных данных [Документ doc_2]. \n            В последние годы трансферное обучение стало важным направлением [Документ doc_3].\"\"\"\n    \n    mock_llm = MockLLM()\n    summarizer = ContextSummarizer(mock_llm)\n    \n    # Суммаризация\n    query = \"Что такое машинное обучение и его современные направления?\"\n    summary_result = summarizer.summarize_with_sources(mock_documents, query)\n    \n    # Визуализация\n    visualizer = SummaryVisualizer()\n    visualizer.display_summary_with_sources(summary_result, query)\n    \n    # Генерация отчета\n    report = visualizer.generate_source_report(summary_result, \"markdown\")\n    print(\"\\n📄 ОТЧЕТ В MARKDOWN:\")\n    print(report)\n\nif __name__ == \"__main__\":\n    demo_context_summarization()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Отдельная LLM проверяет соответствие ответ-контекст","metadata":{}},{"cell_type":"code","source":"class AnswerContextVerifier:\n    def __init__(self, verification_llm, verification_mode: str = \"strict\"):\n        \"\"\"\n        Инициализация верификатора\n        \n        Args:\n            verification_llm: отдельная LLM для проверки\n            verification_mode: режим проверки (strict, balanced, lenient)\n        \"\"\"\n        self.verification_llm = verification_llm\n        self.verification_mode = verification_mode\n        self.verification_history = []\n    \n    def verify_answer_context_alignment(self, \n                                      question: str, \n                                      answer: str, \n                                      context: str,\n                                      documents: List[Dict] = None) -> Dict[str, Any]:\n        \"\"\"\n        Проверка соответствия ответа контексту\n        \n        Args:\n            question: исходный вопрос\n            answer: сгенерированный ответ\n            context: исходный контекст\n            documents: список документов с метаданными\n            \n        Returns:\n            Результаты верификации\n        \"\"\"\n        print(\"🔍 Проверка соответствия ответа контексту...\")\n        \n        # Выбираем метод проверки в зависимости от режима\n        if self.verification_mode == \"strict\":\n            verification_result = self._strict_verification(question, answer, context, documents)\n        elif self.verification_mode == \"balanced\":\n            verification_result = self._balanced_verification(question, answer, context, documents)\n        elif self.verification_mode == \"lenient\":\n            verification_result = self._lenient_verification(question, answer, context, documents)\n        else:\n            verification_result = self._balanced_verification(question, answer, context, documents)\n        \n        # Сохраняем в историю\n        self.verification_history.append({\n            'timestamp': datetime.now().isoformat(),\n            'question': question,\n            'answer_preview': answer[:100] + \"...\" if len(answer) > 100 else answer,\n            'verification_result': verification_result\n        })\n        \n        return verification_result\n    \n    def _strict_verification(self, question: str, answer: str, context: str, \n                           documents: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Строгая проверка - каждый факт должен быть подтвержден\"\"\"\n        \n        prompt = f\"\"\"Ты - строгий верификатор. Проверь, что КАЖДЫЙ факт в ответе \n        подтверждается предоставленным контекстом. Будь максимально строгим.\n\n        ВОПРОС: {question}\n\n        КОНТЕКСТ ДЛЯ ПРОВЕРКИ:\n        {context[:2500]}\n\n        ОТВЕТ ДЛЯ ВЕРИФИКАЦИИ:\n        {answer}\n\n        Проанализируй и верни JSON:\n\n        {{\n            \"alignment_score\": 0.85,\n            \"is_fully_supported\": false,\n            \"supported_facts\": [\n                {{\n                    \"fact\": \"утверждение из ответа\",\n                    \"confidence\": 0.95,\n                    \"evidence\": \"подтверждающий фрагмент из контекста\"\n                }}\n            ],\n            \"unsupported_facts\": [\n                {{\n                    \"fact\": \"утверждение без подтверждения\", \n                    \"reason\": \"почему не подтверждено\",\n                    \"severity\": \"high/medium/low\"\n                }}\n            ],\n            \"hallucinations\": [\n                \"полностью выдуманные факты\"\n            ],\n            \"verdict\": \"approved/needs_revision/rejected\",\n            \"explanation\": \"подробное объяснение решения\"\n        }}\n\n        Критерии строгой проверки:\n        - Факт должен быть явно подтвержден в контексте\n        - Не принимай интерпретации или выводы\n        - Отмечай любые расхождения как неподтвержденные\"\"\"\n\n        try:\n            response = self.verification_llm.generate(prompt, max_length=1500, temperature=0.1)\n            return self._parse_verification_response(response, \"strict\")\n        except Exception as e:\n            print(f\"❌ Ошибка при строгой верификации: {e}\")\n            return self._get_error_result()\n    \n    def _balanced_verification(self, question: str, answer: str, context: str,\n                             documents: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Сбалансированная проверка - допускает логические выводы\"\"\"\n        \n        prompt = f\"\"\"Проверь соответствие ответа контексту. Учитывай, что ответ \n        может содержать логические выводы на основе контекста.\n\n        ВОПРОС: {question}\n\n        КОНТЕКСТ:\n        {context[:3000]}\n\n        ОТВЕТ:\n        {answer}\n\n        Проанализируй и верни JSON:\n\n        {{\n            \"alignment_score\": 0.90,\n            \"is_context_aligned\": true,\n            \"factual_accuracy\": 0.85,\n            \"logical_inferences\": [\n                {{\n                    \"inference\": \"логический вывод\",\n                    \"is_justified\": true,\n                    \"basis\": \"на чем основан вывод\"\n                }}\n            ],\n            \"factual_errors\": [\n                {{\n                    \"error\": \"фактическая ошибка\",\n                    \"correct_information\": \"как должно быть\",\n                    \"severity\": \"high/medium/low\"\n                }}\n            ],\n            \"coverage_analysis\": {{\n                \"answered_adequately\": true,\n                \"missing_aspects\": [\"аспекты вопроса, которые не раскрыты\"],\n                \"overclaimed\": [\"утверждения без достаточных оснований\"]\n            }},\n            \"recommendation\": \"accept/revise_minor/revise_major/reject\"\n        }}\"\"\"\n\n        try:\n            response = self.verification_llm.generate(prompt, max_length=1800, temperature=0.2)\n            return self._parse_verification_response(response, \"balanced\")\n        except Exception as e:\n            print(f\"❌ Ошибка при сбалансированной верификации: {e}\")\n            return self._get_error_result()\n    \n    def _lenient_verification(self, question: str, answer: str, context: str,\n                            documents: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Либеральная проверка - допускает интерпретации\"\"\"\n        \n        prompt = f\"\"\"Оцени, насколько хорошо ответ соответствует духу и содержанию контекста.\n        Допускай разумные интерпретации и выводы.\n\n        ВОПРОС: {question}\n        КОНТЕКСТ: {context[:3500]}\n        ОТВЕТ: {answer}\n\n        Верни JSON:\n\n        {{\n            \"semantic_alignment\": 0.88,\n            \"faithfulness\": 0.82,\n            \"completeness\": 0.75,\n            \"strengths\": [\n                \"сильные стороны ответа\"\n            ],\n            \"weaknesses\": [\n                \"слабые стороны или неточности\"\n            ],\n            \"overall_quality\": \"excellent/good/adequate/poor\",\n            \"trust_level\": \"high/medium/low\"\n        }}\"\"\"\n\n        try:\n            response = self.verification_llm.generate(prompt, max_length=1200, temperature=0.3)\n            return self._parse_verification_response(response, \"lenient\")\n        except Exception as e:\n            print(f\"❌ Ошибка при либеральной верификации: {e}\")\n            return self._get_error_result()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AnswerContextVerifier:\n    def __init__(self, verification_llm, verification_mode: str = \"strict\"):\n        self.verification_llm = verification_llm\n        self.verification_mode = verification_mode\n        self.verification_history = []\n    \n    def _parse_verification_response(self, response: str, verification_type: str) -> Dict[str, Any]:\n        \"\"\"Парсинг ответа верификации\"\"\"\n        try:\n            # Пытаемся извлечь JSON\n            json_match = re.search(r'\\{[^{}]*\\{[^{}]*\\}[^{}]*\\}|\\{[^{}]*\\}', response, re.DOTALL)\n            if json_match:\n                json_str = json_match.group()\n                # Чистим JSON от возможных проблем\n                json_str = self._clean_json_string(json_str)\n                data = json.loads(json_str)\n                return self._enrich_verification_data(data, verification_type)\n            else:\n                return self._parse_text_verification(response, verification_type)\n                \n        except json.JSONDecodeError as e:\n            print(f\"⚠️ Ошибка парсинга JSON: {e}\")\n            return self._parse_text_verification(response, verification_type)\n        except Exception as e:\n            print(f\"❌ Неожиданная ошибка при парсинге: {e}\")\n            return self._get_error_result()\n    \n    def _clean_json_string(self, json_str: str) -> str:\n        \"\"\"Очистка JSON строки от распространенных проблем\"\"\"\n        # Заменяем одинарные кавычки на двойные\n        json_str = json_str.replace(\"'\", '\"')\n        \n        # Исправляем незакрытые кавычки\n        json_str = re.sub(r'(\\w+):\\s*([^\",}\\s]+)(?=\\s*[,}])', r'\\1: \"\\2\"', json_str)\n        \n        # Исправляем булевы значения\n        json_str = re.sub(r':\\s*true', ': true', json_str, flags=re.IGNORECASE)\n        json_str = re.sub(r':\\s*false', ': false', json_str, flags=re.IGNORECASE)\n        \n        # Удаляем лишние запятые\n        json_str = re.sub(r',\\s*}', '}', json_str)\n        json_str = re.sub(r',\\s*]', ']', json_str)\n        \n        return json_str\n    \n    def _parse_text_verification(self, response: str, verification_type: str) -> Dict[str, Any]:\n        \"\"\"Парсинг текстового ответа верификации\"\"\"\n        base_result = {\n            'verification_type': verification_type,\n            'alignment_score': 0.5,\n            'is_reliable': False,\n            'parsing_method': 'text_heuristic',\n            'raw_response': response[:500] + \"...\" if len(response) > 500 else response\n        }\n        \n        response_lower = response.lower()\n        \n        # Эвристический анализ текста\n        score_keywords = {\n            'high': ['отличн', 'прекрасн', 'идеальн', 'полност', 'perfect', 'excellent', 'high'],\n            'good': ['хорош', 'удовлетвор', 'достаточн', 'good', 'adequate', 'sufficient'],\n            'poor': ['плох', 'слаб', 'неудовлетвор', 'poor', 'bad', 'inadequate']\n        }\n        \n        # Определяем score по ключевым словам\n        detected_score = 0.5\n        for level, keywords in score_keywords.items():\n            if any(keyword in response_lower for keyword in keywords):\n                if level == 'high':\n                    detected_score = 0.85\n                elif level == 'good':\n                    detected_score = 0.7\n                elif level == 'poor':\n                    detected_score = 0.3\n                break\n        \n        base_result['alignment_score'] = detected_score\n        base_result['is_reliable'] = detected_score > 0.6\n        \n        # Извлекаем основные проблемы\n        problems = []\n        problem_patterns = [\n            (r'не подтвержд[а-я]+', 'Неподтвержденные утверждения'),\n            (r'ошибк[а-я]+', 'Фактические ошибки'),\n            (r'выдум[а-я]+', 'Выдуманные факты'),\n            (r'отсутств[а-я]+', 'Отсутствующая информация')\n        ]\n        \n        for pattern, description in problem_patterns:\n            if re.search(pattern, response_lower):\n                problems.append(description)\n        \n        if problems:\n            base_result['detected_issues'] = problems\n        \n        return base_result\n    \n    def _enrich_verification_data(self, data: Dict, verification_type: str) -> Dict[str, Any]:\n        \"\"\"Обогащение данных верификации\"\"\"\n        enriched = data.copy()\n        \n        # Добавляем метаданные\n        enriched['verification_type'] = verification_type\n        enriched['timestamp'] = datetime.now().isoformat()\n        enriched['parsing_method'] = 'json_direct'\n        \n        # Нормализуем score если нужно\n        if 'alignment_score' in enriched:\n            enriched['alignment_score'] = float(enriched['alignment_score'])\n        elif 'semantic_alignment' in enriched:\n            enriched['alignment_score'] = float(enriched['semantic_alignment'])\n        \n        # Определяем надежность\n        if 'alignment_score' in enriched:\n            enriched['is_reliable'] = enriched['alignment_score'] > 0.7\n        elif 'is_fully_supported' in enriched:\n            enriched['is_reliable'] = enriched['is_fully_supported']\n        elif 'is_context_aligned' in enriched:\n            enriched['is_reliable'] = enriched['is_context_aligned']\n        \n        return enriched\n    \n    def _get_error_result(self) -> Dict[str, Any]:\n        \"\"\"Результат при ошибке верификации\"\"\"\n        return {\n            'alignment_score': 0.0,\n            'is_reliable': False,\n            'verification_type': self.verification_mode,\n            'error': True,\n            'message': 'Не удалось выполнить проверку',\n            'timestamp': datetime.now().isoformat()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Визуализация","metadata":{}},{"cell_type":"code","source":"class VerificationVisualizer:\n    \"\"\"Визуализация результатов верификации\"\"\"\n    \n    @staticmethod\n    def display_verification_report(verification_result: Dict, \n                                  question: str = None, \n                                  answer: str = None):\n        \"\"\"Отображение отчета о верификации\"\"\"\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"🔍 ОТЧЕТ О ВЕРИФИКАЦИИ: СООТВЕТСТВИЕ ОТВЕТА КОНТЕКСТУ\")\n        print(\"=\"*80)\n        \n        if question:\n            print(f\"📝 Вопрос: {question}\")\n        \n        if answer:\n            answer_preview = answer[:150] + \"...\" if len(answer) > 150 else answer\n            print(f\"💬 Ответ: {answer_preview}\")\n        \n        # Основные метрики\n        alignment_score = verification_result.get('alignment_score', 0)\n        is_reliable = verification_result.get('is_reliable', False)\n        verification_type = verification_result.get('verification_type', 'unknown')\n        \n        # Визуализация скора\n        score_bar = VerificationVisualizer._create_score_bar(alignment_score)\n        status_icon = \"✅\" if is_reliable else \"⚠️\" if alignment_score > 0.4 else \"❌\"\n        \n        print(f\"\\n{status_icon} ОЦЕНКА СООТВЕТСТВИЯ: {alignment_score:.2f}/1.0\")\n        print(f\"📊 {score_bar}\")\n        print(f\"🔧 Режим проверки: {verification_type}\")\n        print(f\"🎯 Надежность: {'ВЫСОКАЯ' if is_reliable else 'СРЕДНЯЯ' if alignment_score > 0.4 else 'НИЗКАЯ'}\")\n        \n        # Детальная информация в зависимости от типа проверки\n        if verification_type == \"strict\":\n            VerificationVisualizer._display_strict_verification_details(verification_result)\n        elif verification_type == \"balanced\":\n            VerificationVisualizer._display_balanced_verification_details(verification_result)\n        elif verification_type == \"lenient\":\n            VerificationVisualizer._display_lenient_verification_details(verification_result)\n        \n        # Рекомендации\n        recommendation = verification_result.get('recommendation') or verification_result.get('verdict')\n        if recommendation:\n            print(f\"\\n💡 РЕКОМЕНДАЦИЯ: {recommendation.upper()}\")\n    \n    @staticmethod\n    def _create_score_bar(score: float, width: int = 20) -> str:\n        \"\"\"Создание визуальной шкалы оценки\"\"\"\n        filled = int(score * width)\n        bar = \"█\" * filled + \"░\" * (width - filled)\n        return f\"[{bar}]\"\n    \n    @staticmethod\n    def _display_strict_verification_details(result: Dict):\n        \"\"\"Детали строгой проверки\"\"\"\n        supported_facts = result.get('supported_facts', [])\n        unsupported_facts = result.get('unsupported_facts', [])\n        hallucinations = result.get('hallucinations', [])\n        \n        if supported_facts:\n            print(f\"\\n✅ ПОДТВЕРЖДЕННЫЕ ФАКТЫ ({len(supported_facts)}):\")\n            for i, fact in enumerate(supported_facts[:3], 1):  # Показываем первые 3\n                confidence = fact.get('confidence', 0)\n                confidence_icon = \"🟢\" if confidence > 0.8 else \"🟡\"\n                print(f\"   {confidence_icon} {fact.get('fact', 'N/A')[:80]}...\")\n        \n        if unsupported_facts:\n            print(f\"\\n❌ НЕПОДТВЕРЖДЕННЫЕ ФАКТЫ ({len(unsupported_facts)}):\")\n            for i, fact in enumerate(unsupported_facts[:3], 1):\n                severity = fact.get('severity', 'medium')\n                severity_icon = \"🔴\" if severity == 'high' else \"🟡\" if severity == 'medium' else \"🔵\"\n                print(f\"   {severity_icon} {fact.get('fact', 'N/A')[:80]}...\")\n                print(f\"      Причина: {fact.get('reason', 'N/A')}\")\n        \n        if hallucinations:\n            print(f\"\\n🚫 ВЫДУМАННЫЕ ФАКТЫ ({len(hallucinations)}):\")\n            for i, hallu in enumerate(hallucinations[:3], 1):\n                print(f\"   ⚠️ {hallu[:80]}...\")\n    \n    @staticmethod\n    def _display_balanced_verification_details(result: Dict):\n        \"\"\"Детали сбалансированной проверки\"\"\"\n        factual_accuracy = result.get('factual_accuracy', 0)\n        logical_inferences = result.get('logical_inferences', [])\n        factual_errors = result.get('factual_errors', [])\n        \n        print(f\"\\n📈 Фактическая точность: {factual_accuracy:.2f}\")\n        \n        if logical_inferences:\n            justified = sum(1 for inf in logical_inferences if inf.get('is_justified', False))\n            print(f\"🧠 Логические выводы: {justified}/{len(logical_inferences)} обоснованы\")\n        \n        if factual_errors:\n            print(f\"\\n📉 Фактические ошибки ({len(factual_errors)}):\")\n            for i, error in enumerate(factual_errors[:2], 1):\n                severity = error.get('severity', 'medium')\n                severity_icon = \"🔴\" if severity == 'high' else \"🟡\"\n                print(f\"   {severity_icon} {error.get('error', 'N/A')[:60]}...\")\n    \n    @staticmethod\n    def _display_lenient_verification_details(result: Dict):\n        \"\"\"Детали либеральной проверки\"\"\"\n        faithfulness = result.get('faithfulness', 0)\n        completeness = result.get('completeness', 0)\n        strengths = result.get('strengths', [])\n        weaknesses = result.get('weaknesses', [])\n        \n        print(f\"\\n🎯 Верность контексту: {faithfulness:.2f}\")\n        print(f\"📋 Полнота ответа: {completeness:.2f}\")\n        \n        if strengths:\n            print(f\"\\n🌟 Сильные стороны:\")\n            for strength in strengths[:2]:\n                print(f\"   ✅ {strength[:80]}...\")\n        \n        if weaknesses:\n            print(f\"\\n📝 Области улучшения:\")\n            for weakness in weaknesses[:2]:\n                print(f\"   📝 {weakness[:80]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Интеграция с Rag","metadata":{}},{"cell_type":"code","source":"class VerifiedRAGSystem:\n    \"\"\"RAG система с верификацией ответов отдельной LLM\"\"\"\n    \n    def __init__(self, \n                 vector_store,\n                 answer_llm,  # Основная LLM для генерации ответов\n                 verification_llm,  # Отдельная LLM для проверки\n                 verification_mode: str = \"balanced\"):\n        \n        self.vector_store = vector_store\n        self.answer_llm = answer_llm\n        self.verifier = AnswerContextVerifier(verification_llm, verification_mode)\n        self.visualizer = VerificationVisualizer()\n    \n    def query_with_verification(self, \n                              question: str, \n                              max_retries: int = 2,\n                              confidence_threshold: float = 0.7) -> Dict[str, Any]:\n        \"\"\"\n        Запрос с верификацией ответа отдельной LLM\n        \n        Args:\n            question: вопрос пользователя\n            max_retries: максимальное количество попыток улучшения\n            confidence_threshold: порог уверенности для принятия ответа\n        \"\"\"\n        print(f\"🎯 Запрос: {question}\")\n        \n        # Поиск релевантных документов\n        documents = self.vector_store.search(question, top_k=10)\n        \n        if not documents:\n            return {\n                'question': question,\n                'answer': 'Не найдено релевантной информации',\n                'verification_result': None,\n                'documents': [],\n                'status': 'no_documents'\n            }\n        \n        # Генерация первоначального ответа\n        context = \"\\n\\n\".join([doc['text'] for doc in documents])\n        answer = self.answer_llm.generate(question, context)\n        \n        # Верификация ответа\n        verification_result = self.verifier.verify_answer_context_alignment(\n            question, answer, context, documents\n        )\n        \n        # Попытки улучшения если нужно\n        improved_answer = answer\n        for attempt in range(max_retries):\n            alignment_score = verification_result.get('alignment_score', 0)\n            \n            if alignment_score >= confidence_threshold:\n                break  # Ответ достаточно хорош\n                \n            print(f\"🔄 Попытка улучшения {attempt + 1}/{max_retries}\")\n            improved_answer = self._improve_answer_based_on_verification(\n                question, improved_answer, context, verification_result\n            )\n            \n            # Повторная верификация\n            verification_result = self.verifier.verify_answer_context_alignment(\n                question, improved_answer, context, documents\n            )\n        \n        # Финальные результаты\n        final_answer = improved_answer if improved_answer != answer else answer\n        \n        result = {\n            'question': question,\n            'answer': final_answer,\n            'original_answer': answer,\n            'verification_result': verification_result,\n            'documents': documents,\n            'context_preview': context[:300] + \"...\" if len(context) > 300 else context,\n            'improvement_attempts': min(max_retries, 1 if improved_answer != answer else 0),\n            'final_score': verification_result.get('alignment_score', 0)\n        }\n        \n        # Визуализация результатов\n        self.visualizer.display_verification_report(verification_result, question, final_answer)\n        \n        return result\n    \n    def _improve_answer_based_on_verification(self, \n                                            question: str, \n                                            current_answer: str,\n                                            context: str,\n                                            verification_result: Dict) -> str:\n        \"\"\"Улучшение ответа на основе результатов верификации\"\"\"\n        \n        issues = self._extract_issues_from_verification(verification_result)\n        \n        prompt = f\"\"\"Улучши следующий ответ, исправив выявленные проблемы. \n        Используй ТОЛЬКО информацию из предоставленного контекста.\n\n        ВОПРОС: {question}\n\n        КОНТЕКСТ:\n        {context[:3500]}\n\n        ТЕКУЩИЙ ОТВЕТ (требует улучшения):\n        {current_answer}\n\n        ВЫЯВЛЕННЫЕ ПРОБЛЕМЫ:\n        {issues}\n\n        УЛУЧШЕННЫЙ ОТВЕТ (будь точным и используй только подтвержденные факты):\"\"\"\n\n        try:\n            improved_answer = self.answer_llm.generate(prompt, max_length=1200)\n            return improved_answer\n        except Exception as e:\n            print(f\"⚠️ Не удалось улучшить ответ: {e}\")\n            return current_answer\n    \n    def _extract_issues_from_verification(self, verification_result: Dict) -> str:\n        \"\"\"Извлечение проблем из результатов верификации\"\"\"\n        issues = []\n        \n        verification_type = verification_result.get('verification_type', '')\n        \n        if verification_type == \"strict\":\n            unsupported = verification_result.get('unsupported_facts', [])\n            hallucinations = verification_result.get('hallucinations', [])\n            \n            for fact in unsupported[:3]:  # Берем первые 3\n                issues.append(f\"Неподтвержденный факт: {fact.get('fact', 'N/A')}\")\n            \n            for hallu in hallucinations[:2]:\n                issues.append(f\"Выдуманный факт: {hallu}\")\n                \n        elif verification_type == \"balanced\":\n            errors = verification_result.get('factual_errors', [])\n            for error in errors[:3]:\n                issues.append(f\"Фактическая ошибка: {error.get('error', 'N/A')}\")\n        \n        elif verification_type == \"lenient\":\n            weaknesses = verification_result.get('weaknesses', [])\n            for weakness in weaknesses[:3]:\n                issues.append(f\"Слабая сторона: {weakness}\")\n        \n        return \"\\n\".join(issues) if issues else \"Конкретные проблемы не выявлены\"\n    \n    def get_verification_statistics(self) -> Dict[str, Any]:\n        \"\"\"Статистика верификации\"\"\"\n        if not self.verifier.verification_history:\n            return {}\n        \n        history = self.verifier.verification_history\n        \n        total_verifications = len(history)\n        reliable_count = sum(1 for item in history \n                           if item['verification_result'].get('is_reliable', False))\n        \n        scores = [item['verification_result'].get('alignment_score', 0) \n                 for item in history if 'alignment_score' in item['verification_result']]\n        \n        avg_score = np.mean(scores) if scores else 0\n        \n        return {\n            'total_verifications': total_verifications,\n            'reliable_answers': reliable_count,\n            'reliability_rate': reliable_count / total_verifications,\n            'average_alignment_score': avg_score,\n            'verification_mode': self.verifier.verification_mode,\n            'recent_verifications': history[-5:]  # Последние 5 проверок\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Пример использваниям ","metadata":{}},{"cell_type":"code","source":"def demo_verification_system():\n    \"\"\"Демонстрация системы верификации\"\"\"\n    \n    # Мок LLM для демонстрации\n    class MockLLM:\n        def __init__(self, name=\"MockLLM\"):\n            self.name = name\n        \n        def generate(self, prompt, max_length=1000, temperature=0.1):\n            if \"проверь\" in prompt.lower() or \"verify\" in prompt.lower():\n                # Ответ верификатора\n                return '''{\n                    \"alignment_score\": 0.78,\n                    \"is_fully_supported\": false,\n                    \"supported_facts\": [\n                        {\n                            \"fact\": \"Машинное обучение позволяет компьютерам обучаться на данных\",\n                            \"confidence\": 0.95,\n                            \"evidence\": \"Машинное обучение - это область искусственного интеллекта, которая позволяет компьютерам обучаться на данных\"\n                        }\n                    ],\n                    \"unsupported_facts\": [\n                        {\n                            \"fact\": \"Машинное обучение было изобретено в 2020 году\", \n                            \"reason\": \"В контексте нет информации о дате изобретения\",\n                            \"severity\": \"medium\"\n                        }\n                    ],\n                    \"verdict\": \"needs_revision\",\n                    \"explanation\": \"Ответ в основном корректен, но содержит одно неподтвержденное утверждение\"\n                }'''\n            else:\n                # Ответ основной LLM\n                return \"Машинное обучение позволяет компьютерам обучаться на данных без явного программирования. Эта технология была изобретена в 2020 году и с тех пор активно развивается.\"\n    \n    # Создание системы\n    answer_llm = MockLLM(\"AnswerLLM\")\n    verification_llm = MockLLM(\"VerificationLLM\")\n    \n    verified_rag = VerifiedRAGSystem(\n        vector_store=None,  # В реальности здесь была бы векторная БД\n        answer_llm=answer_llm,\n        verification_llm=verification_llm,\n        verification_mode=\"strict\"\n    )\n    \n    # Тестовый запрос\n    question = \"Что такое машинное обучение и когда оно было изобретено?\"\n    \n    print(\"🚀 Запуск системы с верификацией ответов...\")\n    result = verified_rag.query_with_verification(question)\n    \n    # Статистика\n    stats = verified_rag.get_verification_statistics()\n    print(f\"\\n📊 Статистика верификации:\")\n    print(f\"   Всего проверок: {stats.get('total_verifications', 0)}\")\n    print(f\"   Надежных ответов: {stats.get('reliable_answers', 0)}\")\n    print(f\"   Средняя оценка: {stats.get('average_alignment_score', 0):.2f}\")\n\nif __name__ == \"__main__\":\n    demo_verification_system()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# дообучение cross-encoder","metadata":{}},{"cell_type":"markdown","source":"# Подготовка данных для обучения","metadata":{}},{"cell_type":"code","source":"import spacy\nimport random\nfrom typing import List, Dict, Tuple\nfrom datasets import Dataset\nimport torch\nfrom transformers import AutoTokenizer\nfrom sentence_transformers import InputExample\n\nclass CrossEncoderDataGenerator:\n    def __init__(self, spacy_model: str = \"ru_core_news_sm\"):\n        try:\n            self.nlp = spacy.load(spacy_model)\n        except OSError:\n            print(f\"⚠️ Модель Spacy {spacy_model} не найдена. Установите: python -m spacy download {spacy_model}\")\n            self.nlp = None\n    \n    def generate_chunk_triplets(self, documents: List[Dict], num_triplets: int = 1000) -> List[Tuple[str, str, str]]:\n        \"\"\"\n        Генерация триплетов: anchor (фраза), positive (чанк), negative (случайный чанк)\n        \"\"\"\n        triplets = []\n        all_chunks = [doc['text'] for doc in documents]\n        \n        print(f\"Генерация {num_triplets} триплетов из {len(documents)} документов...\")\n        \n        for _ in range(num_triplets):\n            # Выбираем случайный документ как positive\n            positive_doc = random.choice(documents)\n            positive_chunk = positive_doc['text']\n            \n            # Извлекаем случайную фразу из positive чанка как anchor\n            anchor_phrase = self._extract_random_phrase(positive_chunk)\n            if not anchor_phrase:\n                continue\n            \n            # Выбираем случайный чанк из другого документа как negative\n            negative_doc = random.choice([d for d in documents if d['id'] != positive_doc['id']])\n            negative_chunk = negative_doc['text']\n            \n            triplets.append((anchor_phrase, positive_chunk, negative_chunk))\n        \n        return triplets\n    \n    def _extract_random_phrase(self, text: str, min_words: int = 3, max_words: int = 8) -> str:\n        \"\"\"Извлечение случайной фразы из текста\"\"\"\n        words = text.split()\n        if len(words) < min_words:\n            return text\n        \n        # Выбираем случайную начальную позицию\n        start_idx = random.randint(0, len(words) - min_words)\n        end_idx = min(start_idx + random.randint(min_words, max_words), len(words))\n        \n        phrase = ' '.join(words[start_idx:end_idx])\n        return phrase.strip()\n    \n    def generate_title_paragraph_pairs(self, structured_documents: List[Dict]) -> List[Tuple[str, str]]:\n        \"\"\"\n        Генерация пар заголовок-параграф для документов со структурой\n        \"\"\"\n        pairs = []\n        \n        for doc in structured_documents:\n            if 'title' in doc and 'paragraphs' in doc:\n                title = doc['title']\n                for paragraph in doc['paragraphs']:\n                    pairs.append((title, paragraph))\n        \n        return pairs\n    \n    def extract_keyword_paragraph_pairs(self, documents: List[Dict]) -> List[Tuple[str, str]]:\n        \"\"\"\n        Извлечение пар ключевое слово-абзац с помощью Spacy\n        \"\"\"\n        if not self.nlp:\n            print(\"❌ Spacy модель не загружена\")\n            return []\n        \n        pairs = []\n        \n        for doc in documents:\n            text = doc['text']\n            \n            # Разбиваем на предложения/абзацы\n            paragraphs = self._split_into_paragraphs(text)\n            \n            for paragraph in paragraphs:\n                if len(paragraph.split()) < 10:  # Слишком короткий абзац\n                    continue\n                \n                # Извлекаем ключевые слова с помощью Spacy\n                keywords = self._extract_keywords(paragraph, top_k=1)\n                if keywords:\n                    pairs.append((keywords[0], paragraph))\n        \n        return pairs\n    \n    def _split_into_paragraphs(self, text: str) -> List[str]:\n        \"\"\"Разбивка текста на абзацы\"\"\"\n        # Простая разбивка по двойным переносам строк\n        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n        return paragraphs if paragraphs else [text]\n    \n    def _extract_keywords(self, text: str, top_k: int = 3) -> List[str]:\n        \"\"\"Извлечение ключевых слов с помощью Spacy\"\"\"\n        doc = self.nlp(text)\n        \n        # Извлекаем существительные и собственные существительные\n        keywords = []\n        for token in doc:\n            if (token.pos_ in ['NOUN', 'PROPN'] and \n                not token.is_stop and \n                len(token.text) > 2):\n                keywords.append(token.lemma_)\n        \n        # Убираем дубликаты и возвращаем top_k\n        unique_keywords = list(dict.fromkeys(keywords))\n        return unique_keywords[:top_k]\n    \n    def generate_llm_question_answer_pairs(self, documents: List[Dict], llm, num_pairs: int = 500) -> List[Tuple[str, str]]:\n        \"\"\"\n        Генерация пар вопрос-ответ с помощью LLM\n        \"\"\"\n        pairs = []\n        \n        print(f\"Генерация {num_pairs} пар вопрос-ответ с помощью LLM...\")\n        \n        for doc in random.sample(documents, min(num_pairs, len(documents))):\n            text = doc['text']\n            \n            prompt = f\"\"\"На основе следующего текста сгенерируй естественный вопрос и точный ответ на него.\n\nТекст: {text[:1000]}\n\nВерни в формате:\nВопрос: [сгенерированный вопрос]\nОтвет: [точный ответ из текста]\"\"\"\n\n            try:\n                response = llm.generate(prompt, max_length=200)\n                question, answer = self._parse_qa_response(response)\n                if question and answer:\n                    pairs.append((question, answer))\n            except Exception as e:\n                print(f\"⚠️ Ошибка генерации QA пары: {e}\")\n        \n        return pairs\n    \n    def _parse_qa_response(self, response: str) -> Tuple[str, str]:\n        \"\"\"Парсинг ответа LLM на вопрос и ответ\"\"\"\n        question = None\n        answer = None\n        \n        lines = response.split('\\n')\n        for line in lines:\n            if line.startswith('Вопрос:') or line.startswith('Question:'):\n                question = line.split(':', 1)[1].strip()\n            elif line.startswith('Ответ:') or line.startswith('Answer:'):\n                answer = line.split(':', 1)[1].strip()\n        \n        return question, answer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# подготовка датасета для обучения ","metadata":{}},{"cell_type":"code","source":"class CrossEncoderDatasetBuilder:\n    def __init__(self, tokenizer_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    \n    def prepare_training_dataset(self, positive_pairs: List[Tuple[str, str]], \n                               negative_pairs: List[Tuple[str, str]] = None,\n                               test_size: float = 0.1) -> Dict:\n        \"\"\"\n        Подготовка датасета для обучения кросс-энкодера\n        \"\"\"\n        from sklearn.model_selection import train_test_split\n        \n        # Создаем примеры для обучения\n        train_examples = []\n        \n        # Positive примеры\n        for query, document in positive_pairs:\n            train_examples.append(InputExample(\n                texts=[query, document], \n                label=1.0  # Positive relevance\n            ))\n        \n        # Negative примеры (если предоставлены)\n        if negative_pairs:\n            for query, document in negative_pairs:\n                train_examples.append(InputExample(\n                    texts=[query, document],\n                    label=0.0  # Negative relevance\n                ))\n        else:\n            # Автоматическая генерация negative примеров\n            train_examples.extend(self._generate_negative_examples(positive_pairs))\n        \n        print(f\"Создано {len(train_examples)} примеров для обучения\")\n        \n        # Разделение на train/test\n        train_examples, test_examples = train_test_split(\n            train_examples, test_size=test_size, random_state=42\n        )\n        \n        return {\n            'train': train_examples,\n            'test': test_examples,\n            'total_positive': len(positive_pairs),\n            'total_negative': len(train_examples) - len(positive_pairs)\n        }\n    \n    def _generate_negative_examples(self, positive_pairs: List[Tuple[str, str]], \n                                  num_negative: int = None) -> List[InputExample]:\n        \"\"\"Генерация negative примеров через негативный сэмплинг\"\"\"\n        if num_negative is None:\n            num_negative = len(positive_pairs)  # Столько же negative, сколько positive\n        \n        negative_examples = []\n        \n        for _ in range(num_negative):\n            # Берем случайный query\n            query, _ = random.choice(positive_pairs)\n            # Берем случайный document из другого query\n            attempts = 0\n            while attempts < 10:\n                _, negative_doc = random.choice(positive_pairs)\n                # Проверяем, что это действительно negative пример\n                if not self._is_similar_pair((query, negative_doc), positive_pairs):\n                    negative_examples.append(InputExample(\n                        texts=[query, negative_doc],\n                        label=0.0\n                    ))\n                    break\n                attempts += 1\n        \n        return negative_examples\n    \n    def _is_similar_pair(self, pair: Tuple[str, str], positive_pairs: List[Tuple[str, str]]) -> bool:\n        \"\"\"Проверка, является ли пара похожей на positive пары\"\"\"\n        query, doc = pair\n        for pos_query, pos_doc in positive_pairs:\n            if query == pos_query and doc == pos_doc:\n                return True\n        return False\n    \n    def create_huggingface_dataset(self, examples: List[InputExample]) -> Dataset:\n        \"\"\"Создание датасета в формате HuggingFace\"\"\"\n        texts1 = []\n        texts2 = []\n        labels = []\n        \n        for example in examples:\n            texts1.append(example.texts[0])\n            texts2.append(example.texts[1])\n            labels.append(example.label)\n        \n        return Dataset.from_dict({\n            'text1': texts1,\n            'text2': texts2,\n            'label': labels\n        })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# обучение кросс энкодера ","metadata":{}},{"cell_type":"code","source":"from sentence_transformers.cross_encoder import CrossEncoder\nfrom sentence_transformers.cross_encoder.evaluation import CECorrelationEvaluator\nimport logging\n\nclass CrossEncoderTrainer:\n    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"):\n        self.model_name = model_name\n        self.trainer = None\n        \n    def train_cross_encoder(self, \n                          train_examples: List[InputExample],\n                          test_examples: List[InputExample] = None,\n                          output_path: str = \"./cross-encoder-model\",\n                          num_epochs: int = 3,\n                          batch_size: int = 16,\n                          warmup_steps: int = 100) -> CrossEncoder:\n        \"\"\"\n        Обучение кросс-энкодера\n        \"\"\"\n        print(\"🚀 Начало обучения кросс-энкодера...\")\n        \n        # Инициализация модели\n        model = CrossEncoder(self.model_name, num_labels=1)\n        \n        # Настройка логгера\n        logging.basicConfig(format='%(asctime)s - %(message)s',\n                           datefmt='%Y-%m-%d %H:%M:%S',\n                           level=logging.INFO)\n        \n        # Подготовка данных для обучения\n        train_dataloader = torch.utils.data.DataLoader(\n            train_examples, \n            shuffle=True, \n            batch_size=batch_size\n        )\n        \n        # Evaluator для валидации (если есть test данные)\n        evaluator = None\n        if test_examples:\n            evaluator = CECorrelationEvaluator.from_input_examples(test_examples, name='test')\n        \n        # Обучение модели\n        model.fit(train_dataloader=train_dataloader,\n                 evaluator=evaluator,\n                 epochs=num_epochs,\n                 warmup_steps=warmup_steps,\n                 output_path=output_path,\n                 show_progress_bar=True)\n        \n        print(f\"✅ Обучение завершено. Модель сохранена в: {output_path}\")\n        return model\n    \n    def evaluate_model(self, model: CrossEncoder, test_examples: List[InputExample]) -> Dict[str, float]:\n        \"\"\"Оценка модели на тестовых данных\"\"\"\n        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n        import numpy as np\n        \n        # Предсказания\n        predictions = model.predict([(example.texts[0], example.texts[1]) \n                                   for example in test_examples])\n        \n        # Истинные метки\n        true_labels = [example.label for example in test_examples]\n        \n        # Бинаризация предсказаний (порог 0.5)\n        binary_predictions = [1 if pred > 0.5 else 0 for pred in predictions]\n        \n        # Метрики\n        accuracy = accuracy_score(true_labels, binary_predictions)\n        precision = precision_score(true_labels, binary_predictions, zero_division=0)\n        recall = recall_score(true_labels, binary_predictions, zero_division=0)\n        f1 = f1_score(true_labels, binary_predictions, zero_division=0)\n        \n        return {\n            'accuracy': accuracy,\n            'precision': precision,\n            'recall': recall,\n            'f1_score': f1,\n            'mean_prediction': float(np.mean(predictions)),\n            'std_prediction': float(np.std(predictions))\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# полный пайплайн","metadata":{}},{"cell_type":"code","source":"class CrossEncoderTrainingPipeline:\n    def __init__(self, documents: List[Dict], llm=None):\n        self.documents = documents\n        self.llm = llm\n        self.data_generator = CrossEncoderDataGenerator()\n        self.dataset_builder = CrossEncoderDatasetBuilder()\n        self.trainer = CrossEncoderTrainer()\n        \n    def run_full_pipeline(self, output_path: str = \"./trained-cross-encoder\") -> CrossEncoder:\n        \"\"\"\n        Полный пайплайн подготовки данных и обучения\n        \"\"\"\n        print(\"🔧 Запуск полного пайплайна обучения кросс-энкодера...\")\n        \n        # 1. Генерация различных типов данных\n        all_positive_pairs = []\n        \n        # Триплеты: фраза - чанк (позитив) - случайный чанк (негатив)\n        print(\"📊 Генерация триплетов...\")\n        triplets = self.data_generator.generate_chunk_triplets(self.documents, num_triplets=2000)\n        for anchor, positive, negative in triplets:\n            all_positive_pairs.append((anchor, positive))\n        \n        # Ключевые слова - абзацы (Spacy)\n        print(\"🔑 Извлечение ключевых слов...\")\n        keyword_pairs = self.data_generator.extract_keyword_paragraph_pairs(self.documents)\n        all_positive_pairs.extend(keyword_pairs)\n        \n        # Вопросы-ответы (LLM)\n        if self.llm:\n            print(\"🤖 Генерация вопросов-ответов...\")\n            qa_pairs = self.data_generator.generate_llm_question_answer_pairs(\n                self.documents, self.llm, num_pairs=1000\n            )\n            all_positive_pairs.extend(qa_pairs)\n        \n        print(f\"📈 Всего сгенерировано positive пар: {len(all_positive_pairs)}\")\n        \n        # 2. Подготовка датасета\n        print(\"📚 Подготовка датасета...\")\n        dataset = self.dataset_builder.prepare_training_dataset(\n            positive_pairs=all_positive_pairs,\n            test_size=0.15\n        )\n        \n        # 3. Обучение модели\n        print(\"🎯 Обучение модели...\")\n        model = self.trainer.train_cross_encoder(\n            train_examples=dataset['train'],\n            test_examples=dataset['test'],\n            output_path=output_path,\n            num_epochs=4,\n            batch_size=16\n        )\n        \n        # 4. Оценка модели\n        if dataset['test']:\n            metrics = self.trainer.evaluate_model(model, dataset['test'])\n            print(f\"📊 Результаты оценки:\")\n            for metric, value in metrics.items():\n                print(f\"   {metric}: {value:.4f}\")\n        \n        return model\n    \n    def analyze_training_data(self, positive_pairs: List[Tuple[str, str]]) -> Dict[str, Any]:\n        \"\"\"Анализ сгенерированных данных для обучения\"\"\"\n        query_lengths = [len(query.split()) for query, _ in positive_pairs]\n        doc_lengths = [len(doc.split()) for _, doc in positive_pairs]\n        \n        return {\n            'total_pairs': len(positive_pairs),\n            'avg_query_length': sum(query_lengths) / len(query_lengths),\n            'avg_doc_length': sum(doc_lengths) / len(doc_lengths),\n            'max_query_length': max(query_lengths),\n            'max_doc_length': max(doc_lengths),\n            'min_query_length': min(query_lengths),\n            'min_doc_length': min(doc_lengths)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# использование обученной модели","metadata":{}},{"cell_type":"code","source":"class TrainedCrossEncoderRAG:\n    def __init__(self, cross_encoder_model_path: str, vector_store):\n        self.cross_encoder = CrossEncoder(cross_encoder_model_path)\n        self.vector_store = vector_store\n    \n    def search_with_reranking(self, query: str, top_k: int = 10, \n                            vector_top_k: int = 50) -> List[Dict]:\n        \"\"\"\n        Поиск с реранкингом с помощью обученного кросс-энкодера\n        \"\"\"\n        # Первоначальный векторный поиск (берем больше кандидатов)\n        candidate_docs = self.vector_store.search(query, top_k=vector_top_k)\n        \n        if not candidate_docs:\n            return []\n        \n        # Подготовка пар для кросс-энкодера\n        pairs = [(query, doc['text']) for doc in candidate_docs]\n        \n        # Получение скоринга от кросс-энкодера\n        scores = self.cross_encoder.predict(pairs)\n        \n        # Объединение скоринга с документами\n        for i, doc in enumerate(candidate_docs):\n            doc['cross_encoder_score'] = float(scores[i])\n        \n        # Сортировка по cross-encoder score\n        ranked_docs = sorted(candidate_docs, \n                           key=lambda x: x['cross_encoder_score'], \n                           reverse=True)\n        \n        return ranked_docs[:top_k]\n    \n    def evaluate_retrieval_quality(self, test_queries: List[str], \n                                 ground_truth: Dict[str, List[str]]) -> Dict[str, float]:\n        \"\"\"\n        Оценка качества retrieval на тестовых запросах\n        \"\"\"\n        from sklearn.metrics import ndcg_score\n        \n        ndcg_scores = []\n        precision_scores = []\n        recall_scores = []\n        \n        for query in test_queries:\n            # Получаем предсказания\n            results = self.search_with_reranking(query, top_k=10)\n            predicted_doc_ids = [doc.get('id', '') for doc in results]\n            \n            # Получаем ground truth\n            true_relevant_ids = ground_truth.get(query, [])\n            \n            if not true_relevant_ids:\n                continue\n            \n            # Вычисляем метрики\n            relevance_scores = [1 if doc_id in true_relevant_ids else 0 \n                              for doc_id in predicted_doc_ids]\n            \n            # NDCG\n            if sum(relevance_scores) > 0:\n                true_ideal = [1] * len(true_relevant_ids) + [0] * (len(predicted_doc_ids) - len(true_relevant_ids))\n                ndcg = ndcg_score([true_ideal], [relevance_scores])\n                ndcg_scores.append(ndcg)\n            \n            # Precision@k\n            precision = sum(relevance_scores[:5]) / 5  # Precision@5\n            precision_scores.append(precision)\n            \n            # Recall@k\n            recall = sum(relevance_scores) / len(true_relevant_ids)\n            recall_scores.append(recall)\n        \n        return {\n            'ndcg_mean': np.mean(ndcg_scores) if ndcg_scores else 0,\n            'precision_mean': np.mean(precision_scores) if precision_scores else 0,\n            'recall_mean': np.mean(recall_scores) if recall_scores else 0,\n            'num_queries_evaluated': len(test_queries)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# пример использования","metadata":{}},{"cell_type":"code","source":"def demo_cross_encoder_training():\n    \"\"\"Демонстрация обучения кросс-энкодера\"\"\"\n    \n    # Пример документов\n    sample_documents = [\n        {\n            'id': 'doc1',\n            'text': 'Машинное обучение - это область искусственного интеллекта. Алгоритмы машинного обучения позволяют компьютерам обучаться на данных. Существуют различные типы машинного обучения: с учителем, без учителя и с подкреплением.',\n            'title': 'Введение в машинное обучение'\n        },\n        {\n            'id': 'doc2', \n            'text': 'Глубокое обучение использует нейронные сети с множеством слоев. Этот подход особенно эффективен для обработки изображений и естественного языка. Сверточные сети используются для компьютерного зрения.',\n            'title': 'Глубокое обучение'\n        },\n        {\n            'id': 'doc3',\n            'text': 'Трансформеры - это архитектура нейронных сетей для обработки последовательностей. Они используют механизм внимания и стали основой для современных языковых моделей like BERT и GPT.',\n            'title': 'Архитектура трансформеров'\n        }\n    ]\n    \n    # Мок LLM для генерации вопросов\n    class MockLLM:\n        def generate(self, prompt, max_length=200):\n            return \"\"\"Вопрос: Что такое машинное обучение?\nОтвет: Машинное обучение - это область искусственного интеллекта.\"\"\"\n    \n    mock_llm = MockLLM()\n    \n    # Запуск пайплайна\n    pipeline = CrossEncoderTrainingPipeline(sample_documents, mock_llm)\n    \n    # Анализ данных\n    positive_pairs = []\n    # Добавляем примеры для анализа\n    data_generator = CrossEncoderDataGenerator()\n    keyword_pairs = data_generator.extract_keyword_paragraph_pairs(sample_documents)\n    positive_pairs.extend(keyword_pairs)\n    \n    analysis = pipeline.analyze_training_data(positive_pairs)\n    print(\"📋 Анализ данных для обучения:\")\n    for key, value in analysis.items():\n        print(f\"   {key}: {value}\")\n    \n    # Обучение модели (в реальном сценарии)\n    # model = pipeline.run_full_pipeline(\"./my-cross-encoder\")\n    \n    print(\"✅ Демонстрация завершена. В реальном сценарии запустите pipeline.run_full_pipeline()\")\n\nif __name__ == \"__main__\":\n    demo_cross_encoder_training()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}