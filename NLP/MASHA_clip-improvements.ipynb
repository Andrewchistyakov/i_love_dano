{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13263042,"sourceType":"datasetVersion","datasetId":8404734}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch torchvision transformers ftfy\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:12:58.889140Z","iopub.execute_input":"2025-10-09T09:12:58.889334Z","iopub.status.idle":"2025-10-09T09:14:45.677026Z","shell.execute_reply.started":"2025-10-09T09:12:58.889317Z","shell.execute_reply":"2025-10-09T09:14:45.676220Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\nCollecting ftfy\n  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.9.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nCollecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.5)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.8.3)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 1.0.0rc2\n    Uninstalling huggingface-hub-1.0.0rc2:\n      Successfully uninstalled huggingface-hub-1.0.0rc2\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ftfy-6.3.1 huggingface-hub-0.35.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-6gy0vgbq\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-6gy0vgbq\n  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.9.18)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.19.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=bb25a36e905c128fa32fafeee7a0e0d091e6680c400c7fbe62e17c6b2ac9fcee\n  Stored in directory: /tmp/pip-ephem-wheel-cache-6xxgt4yr/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n\nimport clip  \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:14:54.553036Z","iopub.execute_input":"2025-10-09T09:14:54.553379Z","iopub.status.idle":"2025-10-09T09:15:04.946359Z","shell.execute_reply.started":"2025-10-09T09:14:54.553349Z","shell.execute_reply":"2025-10-09T09:15:04.945674Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:15:08.354823Z","iopub.execute_input":"2025-10-09T09:15:08.355847Z","iopub.status.idle":"2025-10-09T09:15:08.360297Z","shell.execute_reply.started":"2025-10-09T09:15:08.355818Z","shell.execute_reply":"2025-10-09T09:15:08.359562Z"}},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"model, preprocess = clip.load(\"ViT-L/14\", device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:15:11.777630Z","iopub.execute_input":"2025-10-09T09:15:11.777884Z","iopub.status.idle":"2025-10-09T09:15:31.450873Z","shell.execute_reply.started":"2025-10-09T09:15:11.777866Z","shell.execute_reply":"2025-10-09T09:15:31.450200Z"}},"outputs":[{"name":"stderr","text":"100%|████████████████████████████████████████| 890M/890M [00:06<00:00, 137MiB/s]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"for p in model.parameters():\n    p.requires_grad = False #заморозка модели ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:15:33.489857Z","iopub.execute_input":"2025-10-09T09:15:33.490102Z","iopub.status.idle":"2025-10-09T09:15:33.495363Z","shell.execute_reply.started":"2025-10-09T09:15:33.490085Z","shell.execute_reply":"2025-10-09T09:15:33.494472Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class ProjectionHead(nn.Module): #mlp хэды\n    def __init__(self, in_dim=768, out_dim=256):\n        super().__init__()\n        self.proj = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.GELU(),\n            nn.LayerNorm(out_dim)\n        )\n\n    def forward(self, x):\n        return self.proj(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:16:10.830279Z","iopub.execute_input":"2025-10-09T09:16:10.830873Z","iopub.status.idle":"2025-10-09T09:16:10.835275Z","shell.execute_reply.started":"2025-10-09T09:16:10.830850Z","shell.execute_reply":"2025-10-09T09:16:10.834661Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"proj_image = ProjectionHead(768, 256).to(device)\nproj_text = ProjectionHead(768, 256).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:16:22.009448Z","iopub.execute_input":"2025-10-09T09:16:22.010144Z","iopub.status.idle":"2025-10-09T09:16:22.019082Z","shell.execute_reply.started":"2025-10-09T09:16:22.010120Z","shell.execute_reply":"2025-10-09T09:16:22.018353Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"classifier = nn.Sequential(\n    nn.Linear(256, 3)).to(device) #три класса","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:16:26.560182Z","iopub.execute_input":"2025-10-09T09:16:26.560470Z","iopub.status.idle":"2025-10-09T09:16:26.565107Z","shell.execute_reply.started":"2025-10-09T09:16:26.560450Z","shell.execute_reply":"2025-10-09T09:16:26.564451Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class ImageTextDataset(Dataset):\n    def __init__(self, df, image_dir, transform, train_mode=True):\n        self.df = df\n        self.image_dir = image_dir\n        self.transform = transform\n        self.train_mode = train_mode\n        self.label_map = {'Плохо': 0, 'Удовлетворительно': 1, 'Идеально': 2}\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row['text']\n        filename = row['filename'].replace(\"competition_data:\", \"competition_data_\")\n        img_path = os.path.join(self.image_dir, filename)\n\n    #если нет файла\n        if not os.path.exists(img_path):\n            image = Image.new(\"RGB\", (224, 224), (0, 0, 0))\n        else:\n            image = Image.open(img_path).convert(\"RGB\")\n\n        image = self.transform(image)\n\n        if self.train_mode:\n            label = self.label_map[row['mark']]\n            return text, image, label\n        else:\n            return text, image, filename","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:22:43.781607Z","iopub.execute_input":"2025-10-09T09:22:43.781937Z","iopub.status.idle":"2025-10-09T09:22:43.788495Z","shell.execute_reply.started":"2025-10-09T09:22:43.781917Z","shell.execute_reply":"2025-10-09T09:22:43.787561Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"clip_mean = (0.48145466, 0.4578275, 0.40821073)\nclip_std = (0.26862954, 0.26130258, 0.27577711)\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.05),\n    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n    transforms.ToTensor(),\n    transforms.Normalize(clip_mean, clip_std),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(224, interpolation=Image.BICUBIC),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(clip_mean, clip_std),\n])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:27:27.521447Z","iopub.execute_input":"2025-10-09T09:27:27.522269Z","iopub.status.idle":"2025-10-09T09:27:27.529126Z","shell.execute_reply.started":"2025-10-09T09:27:27.522212Z","shell.execute_reply":"2025-10-09T09:27:27.528398Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/image-text-matching-dataset/train_df.tsv\", sep='\\t')\ntest_df = pd.read_csv(\"/kaggle/input/image-text-matching-dataset/test_df.tsv\", sep=\"\\t\")\n\ntrain_split, val_split = train_test_split(train_df, test_size=0.15, random_state=42, stratify=train_df[\"mark\"])\n\nimage_dir_train = \"/kaggle/input/image-text-matching-dataset/train_final/train\"\nimage_dir_test = \"/kaggle/input/image-text-matching-dataset/test_final/test\"\n\ntrain_dataset = ImageTextDataset(train_split, image_dir_train, train_transform, train_mode=True)\nval_dataset = ImageTextDataset(val_split, image_dir_train, val_transform, train_mode=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=2)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:27:30.147367Z","iopub.execute_input":"2025-10-09T09:27:30.147951Z","iopub.status.idle":"2025-10-09T09:27:30.322044Z","shell.execute_reply.started":"2025-10-09T09:27:30.147924Z","shell.execute_reply":"2025-10-09T09:27:30.321166Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(\n    list(proj_image.parameters()) + list(proj_text.parameters()) + list(classifier.parameters()),\n    lr=2e-4, weight_decay=1e-4\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:23:13.586620Z","iopub.execute_input":"2025-10-09T09:23:13.587177Z","iopub.status.idle":"2025-10-09T09:23:13.592364Z","shell.execute_reply.started":"2025-10-09T09:23:13.587150Z","shell.execute_reply":"2025-10-09T09:23:13.591183Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def train_epoch(model, proj_image, proj_text, classifier, dataloader, optimizer, criterion):\n    model.eval()\n    proj_image.train()\n    proj_text.train()\n    classifier.train()\n\n    total_loss = 0\n    for texts, images, labels in tqdm(dataloader):\n        images = images.to(device)\n        labels = labels.to(device)\n        text_tokens = clip.tokenize(texts, truncate=True).to(device)\n\n        with torch.no_grad():\n            img_features = model.encode_image(images)\n            txt_features = model.encode_text(text_tokens)\n\n        img_proj = proj_image(img_features.float())\n        txt_proj = proj_text(txt_features.float())\n\n\n        # Нормализация\n        img_proj = img_proj / img_proj.norm(dim=-1, keepdim=True)\n        txt_proj = txt_proj / txt_proj.norm(dim=-1, keepdim=True)\n\n        combined = (img_proj + txt_proj) / 2\n        preds = classifier(combined)\n\n        loss = criterion(preds, labels)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n\n\ndef evaluate(model, proj_image, proj_text, classifier, dataloader):\n    model.eval()\n    proj_image.eval()\n    proj_text.eval()\n    classifier.eval()\n\n    correct, total = 0, 0\n    with torch.no_grad():\n        for texts, images, labels in dataloader:\n            images = images.to(device)\n            labels = labels.to(device)\n            text_tokens = clip.tokenize(texts, truncate=True).to(device)\n\n            img_features = model.encode_image(images)\n            txt_features = model.encode_text(text_tokens)\n            \n            img_proj = proj_image(img_features.float())\n            txt_proj = proj_text(txt_features.float())\n\n\n            img_proj = img_proj / img_proj.norm(dim=-1, keepdim=True)\n            txt_proj = txt_proj / txt_proj.norm(dim=-1, keepdim=True)\n\n            combined = (img_proj + txt_proj) / 2\n            preds = classifier(combined)\n\n            pred_labels = preds.argmax(dim=1)\n            correct += (pred_labels == labels).sum().item()\n            total += labels.size(0)\n\n    return correct / total","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:29:38.724755Z","iopub.execute_input":"2025-10-09T09:29:38.725759Z","iopub.status.idle":"2025-10-09T09:29:38.735934Z","shell.execute_reply.started":"2025-10-09T09:29:38.725731Z","shell.execute_reply":"2025-10-09T09:29:38.735282Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"for epoch in range(5):\n    loss = train_epoch(model, proj_image, proj_text, classifier, train_loader, optimizer, criterion)\n    acc = evaluate(model, proj_image, proj_text, classifier, val_loader)\n    print(f\"Epoch {epoch+1}/5 | Loss: {loss:.4f} | Val acc: {acc:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:29:42.955025Z","iopub.execute_input":"2025-10-09T09:29:42.955355Z","iopub.status.idle":"2025-10-09T10:22:30.122950Z","shell.execute_reply.started":"2025-10-09T09:29:42.955331Z","shell.execute_reply":"2025-10-09T10:22:30.120300Z"}},"outputs":[{"name":"stderr","text":" 53%|█████▎    | 738/1380 [05:42<05:09,  2.07it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7deecab1dd00>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n 54%|█████▎    | 741/1380 [05:44<04:55,  2.17it/s]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7deecab1dd00>\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n    self._shutdown_workers()\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n    if w.is_alive():\n       ^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n    assert self._parent_pid == os.getpid(), 'can only test a child process'\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nAssertionError: can only test a child process\n100%|██████████| 1380/1380 [10:32<00:00,  2.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5 | Loss: 0.8808 | Val acc: 0.5998\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1380/1380 [09:21<00:00,  2.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5 | Loss: 0.8310 | Val acc: 0.6119\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1380/1380 [08:40<00:00,  2.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5 | Loss: 0.8113 | Val acc: 0.6096\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1380/1380 [08:25<00:00,  2.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5 | Loss: 0.7961 | Val acc: 0.6075\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1380/1380 [08:16<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5 | Loss: 0.7791 | Val acc: 0.6088\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def predict(model, proj_image, proj_text, classifier, df, image_dir, transform):\n    dataset = ImageTextDataset(df, image_dir, transform, train_mode=False)\n    loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0)  # num_workers=0 для стабильности\n\n    preds_all, filenames, texts_list = [], [], []\n    with torch.no_grad():\n        for texts, images, names in tqdm(loader):\n            images = images.to(device)\n            text_tokens = clip.tokenize(texts, truncate=True).to(device)\n\n            # Получаем эмбеддинги\n            img_features = model.encode_image(images)\n            txt_features = model.encode_text(text_tokens)\n\n            # Проекционные головы (float для совместимости с FP16)\n            img_proj = proj_image(img_features.float())\n            txt_proj = proj_text(txt_features.float())\n\n            # Нормализация\n            img_proj = img_proj / img_proj.norm(dim=-1, keepdim=True)\n            txt_proj = txt_proj / txt_proj.norm(dim=-1, keepdim=True)\n\n            # Комбинируем и классифицируем\n            combined = (img_proj + txt_proj) / 2\n            outputs = classifier(combined)\n            preds = outputs.argmax(dim=1).cpu().numpy()\n\n            preds_all.extend(preds)\n            filenames.extend(names)\n            texts_list.extend(texts)\n\n    # Перевод индексов в метки\n    label_map_inv = {0: \"Плохо\", 1: \"Удовлетворительно\", 2: \"Идеально\"}\n\n    return pd.DataFrame({\n        \"filename\": filenames,\n        \"text\": texts_list,\n        \"mark\": [label_map_inv[p] for p in preds_all]\n    })\n\n\n# Генерация submission в TSV\nsubmission = predict(model, proj_image, proj_text, classifier, test_df, image_dir_test, val_transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T11:53:54.737389Z","iopub.execute_input":"2025-10-09T11:53:54.737940Z","iopub.status.idle":"2025-10-09T12:06:13.150703Z","shell.execute_reply.started":"2025-10-09T11:53:54.737903Z","shell.execute_reply":"2025-10-09T12:06:13.149794Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 750/750 [12:18<00:00,  1.02it/s]","output_type":"stream"},{"name":"stdout","text":"✅ submission.tsv создан!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def replace_second_underscore(s):\n    parts = s.split('_', 2)  # разбиваем на максимум 3 части\n    if len(parts) == 3:\n        return parts[0] + '_' + parts[1] + ':' + parts[2]\n    else:\n        return s  # если меньше 2-х _, оставляем как есть\n\nsubmission['filename'] = submission['filename'].apply(replace_second_underscore)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:18:37.879557Z","iopub.execute_input":"2025-10-09T12:18:37.880216Z","iopub.status.idle":"2025-10-09T12:18:37.892103Z","shell.execute_reply.started":"2025-10-09T12:18:37.880192Z","shell.execute_reply":"2025-10-09T12:18:37.891562Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"submission['filename']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:18:40.685624Z","iopub.execute_input":"2025-10-09T12:18:40.685894Z","iopub.status.idle":"2025-10-09T12:18:40.692083Z","shell.execute_reply.started":"2025-10-09T12:18:40.685876Z","shell.execute_reply":"2025-10-09T12:18:40.691510Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"0         competition_data:46008.png\n1        competition_data:354303.png\n2         competition_data:98673.png\n3        competition_data:208734.png\n4        competition_data:260487.png\n                    ...             \n11995     competition_data:85036.png\n11996    competition_data:333914.png\n11997     competition_data:97622.png\n11998    competition_data:237715.png\n11999    competition_data:325273.png\nName: filename, Length: 12000, dtype: object"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:18:46.002436Z","iopub.execute_input":"2025-10-09T12:18:46.003118Z","iopub.status.idle":"2025-10-09T12:18:46.011722Z","shell.execute_reply.started":"2025-10-09T12:18:46.003093Z","shell.execute_reply":"2025-10-09T12:18:46.011074Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"                          filename  \\\n0       competition_data:46008.png   \n1      competition_data:354303.png   \n2       competition_data:98673.png   \n3      competition_data:208734.png   \n4      competition_data:260487.png   \n...                            ...   \n11995   competition_data:85036.png   \n11996  competition_data:333914.png   \n11997   competition_data:97622.png   \n11998  competition_data:237715.png   \n11999  competition_data:325273.png   \n\n                                                    text               mark  \n0                           диаграмма при аритмии сердца  Удовлетворительно  \n1                            оборона полоцка в 1941 году  Удовлетворительно  \n2                                 короба под инсталляцию  Удовлетворительно  \n3                                  раскраски энчантималс           Идеально  \n4                                          дани милохина           Идеально  \n...                                                  ...                ...  \n11995            проект мой город 2 класс окружающий мир  Удовлетворительно  \n11996                    страны и заповедникиподмосковье           Идеально  \n11997  принцип обеспечивающий единство общего специал...  Удовлетворительно  \n11998            инн огрн 440008 г пенза ул ставского 11  Удовлетворительно  \n11999                      мусорка радиоактивных отходов  Удовлетворительно  \n\n[12000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>text</th>\n      <th>mark</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>competition_data:46008.png</td>\n      <td>диаграмма при аритмии сердца</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>competition_data:354303.png</td>\n      <td>оборона полоцка в 1941 году</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>competition_data:98673.png</td>\n      <td>короба под инсталляцию</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>competition_data:208734.png</td>\n      <td>раскраски энчантималс</td>\n      <td>Идеально</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>competition_data:260487.png</td>\n      <td>дани милохина</td>\n      <td>Идеально</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11995</th>\n      <td>competition_data:85036.png</td>\n      <td>проект мой город 2 класс окружающий мир</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>11996</th>\n      <td>competition_data:333914.png</td>\n      <td>страны и заповедникиподмосковье</td>\n      <td>Идеально</td>\n    </tr>\n    <tr>\n      <th>11997</th>\n      <td>competition_data:97622.png</td>\n      <td>принцип обеспечивающий единство общего специал...</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>11998</th>\n      <td>competition_data:237715.png</td>\n      <td>инн огрн 440008 г пенза ул ставского 11</td>\n      <td>Удовлетворительно</td>\n    </tr>\n    <tr>\n      <th>11999</th>\n      <td>competition_data:325273.png</td>\n      <td>мусорка радиоактивных отходов</td>\n      <td>Удовлетворительно</td>\n    </tr>\n  </tbody>\n</table>\n<p>12000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"submission.to_csv(\"submission2.tsv\", sep='\\t', index=False)\nprint(\"✅ submission.tsv создан!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-09T12:19:06.874915Z","iopub.execute_input":"2025-10-09T12:19:06.875249Z","iopub.status.idle":"2025-10-09T12:19:06.917376Z","shell.execute_reply.started":"2025-10-09T12:19:06.875209Z","shell.execute_reply":"2025-10-09T12:19:06.916740Z"}},"outputs":[{"name":"stdout","text":"✅ submission.tsv создан!\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"import pandas as pd\nfrom tqdm import tqdm\nimport torch\nimport clip\nfrom PIL import Image\n# Тестовый DataFrame с колонками 'filename' и 'text'\n# test_df = pd.read_csv(\"test_data.csv\") или другой источник\n\nfilenames = []\ntexts = []\nmarks = []\n\nmodel.eval()\nwith torch.no_grad():\n    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n        filename = row['filename']\n        text = row['text']\n\n        # Загружаем изображение и применяем препроцессинг\n        image = Image.open(f\"{image_dir}/{filename}\").convert(\"RGB\")\n        image = preprocess(image).unsqueeze(0).to(device)\n\n        # Токенизируем текст\n        text_tokens = clip.tokenize([text], truncate=True).to(device)\n\n        # Получаем эмбеддинги\n        image_features = model.encode_image(image)\n        text_features = model.encode_text(text_tokens)\n\n        # Косинусное сходство\n        similarity = torch.cosine_similarity(image_features, text_features)\n        sim_value = similarity.item()\n\n        # Простейшая логика для mark (можно заменить на твою классификацию)\n        if sim_value > 0.75:\n            mark = \"Идеально\"\n        elif sim_value > 0.5:\n            mark = \"Удовлетворительно\"\n        else:\n            mark = \"Плохо\"\n\n        filenames.append(filename)\n        texts.append(text)\n        marks.append(mark)\n\n# Создаём DataFrame и сохраняем в TSV\nsubmission = pd.DataFrame({\n    \"filename\": filenames,\n    \"text\": texts,\n    \"mark\": marks\n})\n\nsubmission.to_csv(\"submission.tsv\", sep='\\t', index=False)\nprint(\"✅ submission.tsv создан!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/image-text-matching-dataset/test.csv\")\ntest_dir = \"/kaggle/input/image-text-matching-dataset/test_final/test\"\n\ntest_df[\"mark\"] = predict(model, proj_image, proj_text, classifier, test_df, test_dir, val_transform)\ntest_df[[\"filename\", \"mark\"]].to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv создан!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Улучшение решения","metadata":{}},{"cell_type":"code","source":"import os\nfrom typing import List, Tuple, Optional, Dict\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport numpy as np","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#улучшение описания фотографий \n\ndef generate_prompts_for_label(label: str) -> List[str]:\n\"\"\"Генерирует разнообразные текстовые подсказки для одного класса/ярлыка.\nИдея: покрыть стиль, контекст, объектный/фотографический описания.\nВозвращает список подсказок.\n\"\"\"\n    templates = [\n\"a photo of a {}\",\n\"a close-up photo of a {}\",\n\"a high quality photo of a {}\",\n\"a detailed image of a {}\",\n\"an image of a {} in the wild\",\n\"a professional photograph of a {}\",\n\"a {} on a plain background\",\n\"a {} with strong lighting\",\n\"a {} in a natural scene\",\n\"a cropped photo of a {}\",\n    ]\n    prompts = [t.format(label) for t in templates]\n    return prompts\n\n\n\n\ndef ensemble_text_features(model, tokenizer, device: torch.device, labels: List[str]) -> Tuple[torch.Tensor, List[str]]:\n\"\"\"Создаёт усреднённые текстовые эмбеддинги для каждого класса, используя набор подсказок.\nmodel: должен поддерживать encode_text / get_text_features интерфейс.\ntokenizer: функция/класс токенизации (например, clip.tokenize или tokenizer.__call__)\nВозвращает тензор shape=(num_labels, dim) и список итоговых prompt-строк.\n\"\"\"\n    all_features = []\n    all_prompts = []\n    with torch.no_grad():\n        for label in labels:\n            prompts = generate_prompts_for_label(label)\n            all_prompts.extend(prompts)\n            tokenized = tokenizer(prompts, return_tensors='pt', padding=True).to(device)\n            text_feats = model.get_text_features(**tokenized) if hasattr(model, 'get_text_features') else model.encode_text(tokenized['input_ids'])\n# L2-normalize и усреднить\n            text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n            mean_feat = text_feats.mean(dim=0)\n            mean_feat = mean_feat / mean_feat.norm()\n            all_features.append(mean_feat.cpu())\n    all_features = torch.stack(all_features, dim=0)\n    return all_features, labels","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Feature extraction: augmentation + averaging + denoising ----\n\ndef build_image_augmentations(image_size: int, n_augment: int = 5) -> List[transforms.Compose]:\n    \"\"\"Возвращает список трансформов для создания n_augment версий изображения.\n    Не использовать случайные сильные изменения (чтобы эмбеддинги оставались релевантными).\n    \"\"\"\n    base = [\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n    ]\n    augment_list = []\n    for i in range(n_augment):\n        ops = []\n        # добавляем легкие вариации: crop, flip, slight color jitter\n        ops.append(transforms.RandomResizedCrop(image_size, scale=(0.9, 1.0)))\n        if i % 2 == 0:\n            ops.append(transforms.RandomHorizontalFlip(p=0.5))\n        if i % 3 == 0:\n            ops.append(transforms.ColorJitter(brightness=0.08, contrast=0.08, saturation=0.08))\n        ops.extend(base)\n        augment_list.append(transforms.Compose(ops))\n    return augment_list\n\n\ndef image_augmented_embedding(model, preprocess, image: Image.Image, device: torch.device, augment_transforms: List[transforms.Compose]) -> torch.Tensor:\n    \"\"\"Для одного PIL image создаёт усреднённый эмбеддинг по augment_transforms.\n    model должен иметь get_image_features(preprocessed_tensor) или encode_image\n    preprocess: дополнительная нормализация/токенизация если нужна\n    \"\"\"\n    feats = []\n    with torch.no_grad():\n        for t in augment_transforms:\n            x = t(image).unsqueeze(0).to(device)\n            if preprocess is not None:\n                x = preprocess(x)\n            image_feat = model.get_image_features(x) if hasattr(model, 'get_image_features') else model.encode_image(x)\n            image_feat = image_feat / image_feat.norm(dim=-1, keepdim=True)\n            feats.append(image_feat.cpu())\n    feats = torch.cat(feats, dim=0)\n    mean_feat = feats.mean(dim=0)\n    mean_feat = mean_feat / mean_feat.norm()\n    return mean_feat\n\n\ndef apply_pca_denoise(embeddings: np.ndarray, n_components: int = None, energy: float = 0.95) -> np.ndarray:\n    \"\"\"Простая PCA-дегенерация шума: если n_components не указан, выбираем по energy.\n    embeddings: (N, D) numpy\n    \"\"\"\n    from sklearn.decomposition import PCA\n    if n_components is None:\n        pca = PCA(n_components=min(embeddings.shape[0], embeddings.shape[1]))\n        pca.fit(embeddings)\n        cum_energy = np.cumsum(pca.explained_variance_ratio_)\n        n = int(np.searchsorted(cum_energy, energy) + 1)\n        n_components = max(1, n)\n    pca = PCA(n_components=n_components)\n    reduced = pca.fit_transform(embeddings)\n    recon = pca.inverse_transform(reduced)\n    return recon\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Simple datasets and linear probe trainer ----\n\nclass SimpleImageTextDataset(Dataset):\n    \"\"\"Ожидается, что items = list of (PIL.Image or path, label_index)\"\"\"\n    def __init__(self, items: List[Tuple[str, int]], transform=None):\n        self.items = items\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        img_path, label = self.items[idx]\n        if isinstance(img_path, str):\n            img = Image.open(img_path).convert('RGB')\n        else:\n            img = img_path\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n\ndef train_linear_probe(image_features: torch.Tensor, labels: torch.Tensor, num_classes: int, device: torch.device, epochs: int = 10, lr: float = 1e-3) -> nn.Module:\n    \"\"\"Обучение простого линейного классификатора на зафиксированных эмбеддингах.\n    image_features: (N, D) torch\n    labels: (N,) torch\n    Возвращает обученную Linear модель.\n    \"\"\"\n    D = image_features.shape[1]\n    clf = nn.Linear(D, num_classes).to(device)\n    opt = torch.optim.Adam(clf.parameters(), lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n    ds = torch.utils.data.TensorDataset(image_features.to(device), labels.to(device))\n    loader = DataLoader(ds, batch_size=64, shuffle=True)\n    clf.train()\n    for epoch in range(epochs):\n        total_loss = 0.0\n        total = 0\n        correct = 0\n        for x, y in loader:\n            opt.zero_grad()\n            logits = clf(x)\n            loss = loss_fn(logits, y)\n            loss.backward()\n            opt.step()\n            total_loss += loss.item() * x.size(0)\n            preds = logits.argmax(dim=1)\n            correct += (preds == y).sum().item()\n            total += x.size(0)\n        # печать прогресса\n        print(f\"Epoch {epoch+1}/{epochs}: loss={total_loss/total:.4f}, acc={correct/total:.4f}\")\n    return clf\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---- Adapter / lightweight fine-tuning (пример) ----\n\nclass SimpleAdapter(nn.Module):\n    \"\"\"Простой адаптер: маленький bottleneck, добавляется к выходу кодировщика изображения/text.\n    Используется как residual: out = feat + alpha * adapter(feat)\n    \"\"\"\n    def __init__(self, dim: int, bottleneck: int = 256):\n        super().__init__()\n        self.down = nn.Linear(dim, bottleneck)\n        self.act = nn.ReLU()\n        self.up = nn.Linear(bottleneck, dim)\n        self.alpha = nn.Parameter(torch.tensor(1.0))\n\n    def forward(self, x):\n        return x + self.alpha * self.up(self.act(self.down(x)))\n\n\ndef attach_adapter_to_model(model, dim: int, bottleneck: int = 256, attach_to: str = 'image'):\n    \"\"\"Пример как прикрепить адаптер к модели. Интерфейс зависит от реализации CLIP.\n    attach_to: 'image' или 'text'\n    Возвращает модель с добавленным адаптером (встраиваем адаптер в pipelinе вызовов).\n    \"\"\"\n    adapter = SimpleAdapter(dim, bottleneck)\n    model.adapter = adapter\n    # Нужно также модифицировать forward / get_image_features чтобы применять adapter\n    # В примере ниже - ожидается, что пользователь интегрирует adapter в нужное место.\n    return model\n\n# ---- Full fine-tuning example (скелет) ----\n\ndef finetune_clip_full(model, train_loader: DataLoader, val_loader: Optional[DataLoader], device: torch.device,\n                       epochs: int = 5, lr: float = 1e-5, freeze_text: bool = False, freeze_image: bool = False):\n    \"\"\"Скелет для полного fine-tune CLIP модели (PyTorch). Внимание: heavy GPU memory.\n    - model должен отдавать logits или эмбеддинги\n    - рекомендуется использовать mixed precision и gradient accumulation при нехватке памяти\n    \"\"\"\n    # Заморозить части\n    if freeze_text and hasattr(model, 'text_encoder'):\n        for p in model.text_encoder.parameters():\n            p.requires_grad = False\n    if freeze_image and hasattr(model, 'vision_encoder'):\n        for p in model.vision_encoder.parameters():\n            p.requires_grad = False\n\n    params = [p for p in model.parameters() if p.requires_grad]\n    opt = torch.optim.AdamW(params, lr=lr)\n    loss_fn = nn.CrossEntropyLoss()\n\n    scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n\n    model.to(device)\n    model.train()\n    for epoch in range(epochs):\n        for imgs, labels in train_loader:\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n            opt.zero_grad()\n            with torch.cuda.amp.autocast(enabled=(scaler is not None)):\n                # зависит от интерфейса: некоторые модели возвращают logits_per_image\n                outputs = model(imgs)\n                if isinstance(outputs, dict) and 'logits' in outputs:\n                    logits = outputs['logits']\n                else:\n                    # fallback: вычисляем эмбеддинги и косинусная классификация\n                    image_feats = model.get_image_features(imgs)\n                    text_feats = model.get_text_features()  # !!!! здесь нужно подготовить text_feats заранее\n                    logits = image_feats @ text_feats.t()\n                loss = loss_fn(logits, labels)\n            if scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.step(opt)\n                scaler.update()\n            else:\n                loss.backward()\n                opt.step()\n        print(f\"Epoch {epoch+1}/{epochs} done\")\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ---- Подсказки при смене модели CLIP / OpenCLIP / ViT variants ----\n\nMODEL_CHANGE_NOTES = {\n    'ViT-B/32': {\n        'image_size': 224,\n        'patch': 32,\n        'notes': 'Быстрая, меньшая разрешающая способность. Обычно использовать стандартный preprocess.'\n    },\n    'ViT-B/16': {\n        'image_size': 224,\n        'patch': 16,\n        'notes': 'Лучший баланс speed/accuracy. Лучше качество по мелким объектам.'\n    },\n    'ViT-L/14': {\n        'image_size': 336,\n        'patch': 14,\n        'notes': 'Нужны большие картинки (336) и больше VRAM. Тонкая настройка learning rates и большее batch size.'\n    },\n    'OpenCLIP-Large': {\n        'image_size': 336,\n        'notes': 'Необходимо проверить tokenizer/входные нормализации; веса обучены на других датасетах.'\n    }\n}\n\n\ndef adapt_preprocess_for_model(model_name: str) -> Dict:\n    \"\"\"Возвращает рекомендации по preprocess и гиперпараметрам при смене модели.\n    \"\"\"\n    info = MODEL_CHANGE_NOTES.get(model_name, None)\n    if info is None:\n        return {'image_size': 224, 'notes': 'Unknown: use defaults'}\n    return info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ---- Быстрый пример: inference pipeline (без загрузки датасетов) ----\n\ndef example_inference_pipeline(model, tokenizer, image_paths: List[str], labels: List[str], device: torch.device):\n    \"\"\"Demo: как объединить prompt-ensemble, image-augment-averaging и предсказание через косинус.\n    model, tokenizer - объекты совместимые с HuggingFace CLIP / open_clip API (get_image_features, get_text_features)\n    \"\"\"\n    # 1) подготовить текстовые эмбеддинги (ансэмблировать prompts)\n    text_feats, label_list = ensemble_text_features(model, tokenizer, device, labels)\n    text_feats = text_feats.to(device)\n\n    # 2) подготовка augment transforms\n    info = adapt_preprocess_for_model('ViT-B/16')\n    aug_transforms = build_image_augmentations(info['image_size'], n_augment=5)\n\n    preds = []\n    for p in image_paths:\n        img = Image.open(p).convert('RGB')\n        img_repr = image_augmented_embedding(model, None, img, device, aug_transforms)\n        img_repr = img_repr.to(device)\n        sims = (img_repr @ text_feats.t()).squeeze(0)\n        pred = sims.argmax().item()\n        preds.append(label_list[pred])\n    return preds\n\n\n# ---- Конфигурационные рекомендации и чеклист для production ----\n\nCONFIG_CHECKLIST = \"\"\"\n1) Подсказки (prompts):\n   - создайте 8-12 вариаций на класс (photographic, context, scene, close-up)\n   - усредняйте текстовые эмбеддинги\n2) Извлечение признаков:\n   - аугментации: легкие random resized crop, flip, color jitter; усредняйте эмбеддинги\n   - L2-нормализация всегда\n   - опционально PCA/TruncatedSVD для удаления высокочастотного шума\n3) Fine-tuning:\n   - Linear probe: быстрый и надёжный\n   - Adapter/tip-adapter: меньший набор обучаемых параметров, fast to converge\n   - Full finetune: требует осторожности (lr в 1e-6..1e-5), gradient accumulation, amp\n4) При смене модели:\n   - проверьте recommended image_size\n   - проверьте tokenizer & text preprocessing\n   - скорректируйте learning rate и batch size (большие модели -> меньший lr)\n5) Inference:\n   - храните нормализованные эмбеддинги (float32/float16)\n   - используйте ANN (FAISS, HNSW) для retrieval\n\"\"\"\n\nif __name__ == '__main__':\n    print(\"This module provides utilities to improve CLIP workflows. Import functions and use with your CLIP model.\")\n    print(\"See CONFIG_CHECKLIST for practical tips.\")\n    print(CONFIG_CHECKLIST)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}