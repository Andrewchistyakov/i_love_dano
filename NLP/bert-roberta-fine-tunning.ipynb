{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13210350,"sourceType":"datasetVersion","datasetId":8372924}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Ноут с Bert и RoBerta + их fine-tunning ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.utils.class_weight import compute_class_weight\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"xWMca9Cl5bjQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train = pd.read_csv('/content/train.csv')\ntest = pd.read_csv('/content/test.csv')\nvalid_classes = ['relevant_minus', 'relevant', 'no_relevant', 'relevant_plus']\ntrain = train[train['target'].isin(valid_classes)].copy()","metadata":{"id":"IgLwdWfl5fWO"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = train.rename(columns={\n    'leftItemId': 'id1_item',\n    'rightItemId': 'id2_item',\n    'target': 'relevant'\n})\n\ndf_test = test.rename(columns={\n    'leftItemId': 'id1_item',\n    'rightItemId': 'id2_item'\n})","metadata":{"id":"273dfXkx5j23"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_items = pd.read_parquet('/content/items (3).parquet')\n\ndef merging_data(df, df_items):\n    df = df.merge(df_items.rename(columns={'itemId': 'id1_item'})[['id1_item', 'title', 'content']],\n                 on='id1_item', how='left')\n    df = df.merge(df_items.rename(columns={'itemId': 'id2_item'})[['id2_item', 'title', 'content']],\n                 on='id2_item', how='left', suffixes=('_1', '_2'))\n    return df\n\ndf_train = merging_data(df_train, df_items)\ndf_test = merging_data(df_test, df_items)","metadata":{"id":"qqvmLqeM5qVm"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_text(text):\n    if pd.isna(text):\n        return \"\"\n    text = str(text)\n    # Удаляем специальные символы и лишние пробелы\n    text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n    text = ' '.join(text.split())\n    return text[:400]","metadata":{"id":"fdIePsfy5uJQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train['text1'] = df_train['title_1'].apply(preprocess_text) + \" [SEP] \" + df_train['content_1'].apply(preprocess_text)\ndf_train['text2'] = df_train['title_2'].apply(preprocess_text) + \" [SEP] \" + df_train['content_2'].apply(preprocess_text)\n\ndf_test['text1'] = df_test['title_1'].apply(preprocess_text) + \" [SEP] \" + df_test['content_1'].apply(preprocess_text)\ndf_test['text2'] = df_test['title_2'].apply(preprocess_text) + \" [SEP] \" + df_test['content_2'].apply(preprocess_text)\n","metadata":{"id":"C0H2lco05ys0"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_name = 'bert-base-multilingual-cased'  \ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"id":"nI4z3e9m549E"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = df_train['relevant'].map({'no_relevant': 0, 'relevant_minus': 1, 'relevant': 2, 'relevant_plus': 3})\nclass_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels.values)\nclass_weights = torch.tensor(class_weights, dtype=torch.float)","metadata":{"id":"ZzpvKjgL56he"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ImprovedPostPairsDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_length=160, is_train=True):  # Увеличил длину\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_train = is_train\n        self.label_map = {'no_relevant': 0, 'relevant_minus': 1, 'relevant': 2, 'relevant_plus': 3}\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        row = self.data.iloc[index]\n        text1 = str(row['text1'])\n        text2 = str(row['text2'])\n\n        encoding = self.tokenizer(\n            text1,\n            text2,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt',\n        )\n\n        if self.is_train:\n            label = self.label_map[row['relevant']]\n            return {\n                'input_ids': encoding['input_ids'].flatten(),\n                'attention_mask': encoding['attention_mask'].flatten(),\n                'labels': torch.tensor(label, dtype=torch.long)\n            }\n        else:\n            return {\n                'input_ids': encoding['input_ids'].flatten(),\n                'attention_mask': encoding['attention_mask'].flatten()\n            }\n","metadata":{"id":"s9cUgU1S577L"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df, val_df = train_test_split(\n    df_train,\n    test_size=0.15,\n    random_state=42,\n    stratify=df_train['relevant']\n)\n\ntrain_dataset = ImprovedPostPairsDataset(train_df, tokenizer, is_train=True)\nval_dataset = ImprovedPostPairsDataset(val_df, tokenizer, is_train=True)","metadata":{"id":"mortqmxk6Efp"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)  # Уменьшил батч для лучшего обучения\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n","metadata":{"id":"qkLD2r0G6L5T"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=4,\n    id2label={0: 'no_relevant', 1: 'relevant_minus', 2: 'relevant', 3: 'relevant_plus'},\n    label2id={'no_relevant': 0, 'relevant_minus': 1, 'relevant': 2, 'relevant_plus': 3},\n    hidden_dropout_prob=0.2,  # Добавил регуляризацию\n    attention_probs_dropout_prob=0.2\n)","metadata":{"id":"c5pj_Wuz6Ogh"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nmodel.to(device)\nclass_weights = class_weights.to(device)","metadata":{"id":"tol4Duoz6R8q"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01, eps=1e-8)\ntotal_steps = len(train_loader) * 3  # 3 эпохи\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=int(total_steps * 0.1), num_training_steps=total_steps\n)\n\nloss_fn = nn.CrossEntropyLoss(weight=class_weights)\n","metadata":{"id":"8SFe5b1S6Vjc"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nbest_val_f1 = 0\npatience = 2\npatience_counter = 0\n\nfor epoch in range(3):\n    model.train()\n    total_loss = 0\n\n    for batch_idx, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = loss_fn(outputs.logits, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        scheduler.step()\n        optimizer.zero_grad()\n\n        total_loss += loss.item()\n\n        if batch_idx % 20 == 0:\n            print(f'Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}')\n\n    # Валидация\n    model.eval()\n    val_predictions = []\n    val_true_labels = []\n    val_probs = []\n\n    with torch.no_grad():\n        for batch in val_loader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels_batch = batch['labels'].to(device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n            probs = torch.softmax(logits, dim=1)\n            predictions = torch.argmax(logits, dim=1)\n\n            val_predictions.extend(predictions.cpu().numpy())\n            val_true_labels.extend(labels_batch.cpu().numpy())\n            val_probs.extend(probs.cpu().numpy())\n\n    val_f1 = f1_score(val_true_labels, val_predictions, average='weighted')\n    print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}, Val Weighted F1: {val_f1:.6f}')\n\n    # Early stopping\n    if val_f1 > best_val_f1:\n        best_val_f1 = val_f1\n        torch.save(model.state_dict(), 'best_model.pt')\n        patience_counter = 0\n    else:\n        patience_counter += 1\n        if patience_counter >= patience:\n            print(f\"Early stopping at epoch {epoch+1}\")\n            break\n\n# Загрузка лучшей модели\nmodel.load_state_dict(torch.load('best_model.pt'))\nmodel.eval()\n\ntest_dataset = ImprovedPostPairsDataset(df_test, tokenizer, is_train=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n\ntest_predictions = []\ntest_probs = []\nreverse_label_map = {0: 'no_relevant', 1: 'relevant_minus', 2: 'relevant', 3: 'relevant_plus'}\n\nwith torch.no_grad():\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.softmax(logits, dim=1)\n        predictions = torch.argmax(logits, dim=1)\n\n        test_predictions.extend(predictions.cpu().numpy())\n        test_probs.extend(probs.cpu().numpy())\n\ntest_probs = np.array(test_probs)\nfinal_test_predictions = []\n\n# Адаптивные пороги уверенности для разных классов\nconfidence_thresholds = {\n    'no_relevant': 0.55,\n    'relevant_minus': 0.5,\n    'relevant': 0.6,\n    'relevant_plus': 0.65\n}\n\nfor i, probs in enumerate(test_probs):\n    pred_class = test_predictions[i]\n    pred_label = reverse_label_map[pred_class]\n    max_prob = probs[pred_class]\n\n    if max_prob < confidence_thresholds[pred_label]:\n        # Если модель неуверена, выбираем наиболее вероятный класс среди уверенных\n        confident_classes = []\n        for j, prob in enumerate(probs):\n            if prob >= confidence_thresholds[reverse_label_map[j]]:\n                confident_classes.append((j, prob))\n\n        if confident_classes:\n            confident_classes.sort(key=lambda x: x[1], reverse=True)\n            final_label = reverse_label_map[confident_classes[0][0]]\n        else:\n            # Если нет уверенных классов, выбираем самый вероятный\n            final_label = reverse_label_map[np.argmax(probs)]\n    else:\n        final_label = pred_label\n\n    final_test_predictions.append(final_label)\n\n# Создание submission\nsubmission = pd.DataFrame({\n    'Unnamed: 0': range(len(df_test)),\n    'target': final_test_predictions\n})\n\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission file created successfully!\")\nprint(f\"Best validation F1: {best_val_f1:.6f}\")\nprint(\"Sample predictions:\")\nprint(submission.head(10))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qpEgQjhhyGw8","outputId":"25023144-3bf8-4c10-ef3b-f7ca68aaef09"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Epoch 1, Batch 0, Loss: 1.4018\n","Epoch 1, Batch 20, Loss: 1.3842\n","Epoch 1, Batch 40, Loss: 1.3879\n","Epoch 1, Batch 60, Loss: 1.4091\n","Epoch 1, Batch 80, Loss: 1.3840\n","Epoch 1, Batch 100, Loss: 1.3803\n","Epoch 1, Batch 120, Loss: 1.3945\n","Epoch 1, Batch 140, Loss: 1.3858\n","Epoch 1, Batch 160, Loss: 1.3879\n","Epoch 1, Batch 180, Loss: 1.3789\n","Epoch 1, Batch 200, Loss: 1.3867\n","Epoch 1, Batch 220, Loss: 1.3934\n","Epoch 1, Batch 240, Loss: 1.4046\n","Epoch 1, Batch 260, Loss: 1.3905\n","Epoch 1, Batch 280, Loss: 1.3758\n","Epoch 1, Batch 300, Loss: 1.3960\n","Epoch 1, Batch 320, Loss: 1.3818\n","Epoch 1, Batch 340, Loss: 1.3755\n","Epoch 1, Batch 360, Loss: 1.3805\n","Epoch 1, Batch 380, Loss: 1.3940\n","Epoch 1, Batch 400, Loss: 1.3898\n","Epoch 1, Batch 420, Loss: 1.3891\n","Epoch 1, Batch 440, Loss: 1.3913\n","Epoch 1, Batch 460, Loss: 1.3682\n","Epoch 1, Batch 480, Loss: 1.3698\n","Epoch 1, Batch 500, Loss: 1.3812\n","Epoch 1, Batch 520, Loss: 1.3883\n","Epoch 1, Batch 540, Loss: 1.3853\n","Epoch 1, Batch 560, Loss: 1.3548\n","Epoch 1, Batch 580, Loss: 1.3903\n","Epoch 1, Batch 600, Loss: 1.3734\n","Epoch 1, Batch 620, Loss: 1.3605\n","Epoch 1, Batch 640, Loss: 1.3986\n","Epoch 1, Batch 660, Loss: 1.3847\n","Epoch 1, Batch 680, Loss: 1.3812\n","Epoch 1, Batch 700, Loss: 1.3498\n","Epoch 1, Batch 720, Loss: 1.3549\n","Epoch 1, Batch 740, Loss: 1.3619\n","Epoch 1, Batch 760, Loss: 1.3663\n","Epoch 1, Batch 780, Loss: 1.3664\n","Epoch 1, Batch 800, Loss: 1.3691\n","Epoch 1, Batch 820, Loss: 1.3794\n","Epoch 1, Batch 840, Loss: 1.3241\n","Epoch 1, Batch 860, Loss: 1.3568\n","Epoch 1, Batch 880, Loss: 1.3402\n","Epoch 1, Batch 900, Loss: 1.3291\n","Epoch 1, Batch 920, Loss: 1.3823\n","Epoch 1, Batch 940, Loss: 1.3700\n","Epoch 1, Batch 960, Loss: 1.3374\n","Epoch 1, Batch 980, Loss: 1.4193\n","Epoch 1, Batch 1000, Loss: 1.3299\n","Epoch 1, Batch 1020, Loss: 1.3237\n","Epoch 1, Batch 1040, Loss: 1.3604\n","Epoch 1, Batch 1060, Loss: 1.3533\n","Epoch 1, Batch 1080, Loss: 1.3604\n","Epoch 1, Batch 1100, Loss: 1.4283\n","Epoch 1, Batch 1120, Loss: 1.2802\n","Epoch 1, Batch 1140, Loss: 1.4179\n","Epoch 1, Batch 1160, Loss: 1.3410\n","Epoch 1, Batch 1180, Loss: 1.3336\n","Epoch 1, Batch 1200, Loss: 1.3902\n","Epoch 1, Batch 1220, Loss: 1.3582\n","Epoch 1, Batch 1240, Loss: 1.3843\n","Epoch 1, Batch 1260, Loss: 1.3628\n","Epoch 1, Batch 1280, Loss: 1.3115\n","Epoch 1, Batch 1300, Loss: 1.3866\n","Epoch 1, Batch 1320, Loss: 1.3405\n","Epoch 1, Batch 1340, Loss: 1.3146\n","Epoch 1, Batch 1360, Loss: 1.3544\n","Epoch 1, Batch 1380, Loss: 1.2376\n","Epoch 1, Batch 1400, Loss: 1.3135\n","Epoch 1, Batch 1420, Loss: 1.3913\n","Epoch 1, Batch 1440, Loss: 1.2686\n","Epoch 1, Batch 1460, Loss: 1.3321\n","Epoch 1, Batch 1480, Loss: 1.3272\n","Epoch 1, Batch 1500, Loss: 1.4719\n","Epoch 1, Batch 1520, Loss: 1.2664\n","Epoch 1, Batch 1540, Loss: 1.2790\n","Epoch 1, Batch 1560, Loss: 1.2889\n","Epoch 1, Batch 1580, Loss: 1.2952\n","Epoch 1, Batch 1600, Loss: 1.2602\n","Epoch 1, Batch 1620, Loss: 1.4305\n","Epoch 1, Batch 1640, Loss: 1.2866\n","Epoch 1, Batch 1660, Loss: 1.3249\n","Epoch 1, Batch 1680, Loss: 1.3530\n","Epoch 1, Batch 1700, Loss: 1.3264\n","Epoch 1, Batch 1720, Loss: 1.3639\n","Epoch 1, Batch 1740, Loss: 1.2767\n","Epoch 1, Batch 1760, Loss: 1.3385\n","Epoch 1, Batch 1780, Loss: 1.3449\n","Epoch 1, Batch 1800, Loss: 1.3898\n","Epoch 1, Batch 1820, Loss: 1.2360\n","Epoch 1, Batch 1840, Loss: 1.3088\n","Epoch 1, Batch 1860, Loss: 1.4250\n","Epoch 1, Batch 1880, Loss: 1.3958\n","Epoch 1, Batch 1900, Loss: 1.3919\n","Epoch 1, Batch 1920, Loss: 1.2699\n","Epoch 1, Batch 1940, Loss: 1.1608\n","Epoch 1, Batch 1960, Loss: 1.2034\n","Epoch 1, Batch 1980, Loss: 1.3612\n","Epoch 1, Batch 2000, Loss: 1.3989\n","Epoch 1, Batch 2020, Loss: 1.2663\n","Epoch 1, Batch 2040, Loss: 1.3359\n","Epoch 1, Batch 2060, Loss: 1.3772\n","Epoch 1, Batch 2080, Loss: 1.1913\n","Epoch 1, Batch 2100, Loss: 1.2594\n","Epoch 1, Batch 2120, Loss: 1.3834\n","Epoch 1, Batch 2140, Loss: 1.2681\n","Epoch 1, Batch 2160, Loss: 1.3463\n","Epoch 1, Batch 2180, Loss: 1.2916\n","Epoch 1, Batch 2200, Loss: 1.3755\n","Epoch 1, Batch 2220, Loss: 1.2321\n","Epoch 1, Batch 2240, Loss: 1.4809\n","Epoch 1, Batch 2260, Loss: 1.2463\n","Epoch 1, Batch 2280, Loss: 1.2448\n","Epoch 1, Batch 2300, Loss: 1.2616\n","Epoch 1, Batch 2320, Loss: 1.5076\n","Epoch 1, Batch 2340, Loss: 1.2793\n","Epoch 1, Batch 2360, Loss: 1.3706\n","Epoch 1, Batch 2380, Loss: 1.2704\n","Epoch 1, Batch 2400, Loss: 1.2586\n","Epoch 1, Batch 2420, Loss: 1.3522\n","Epoch 1, Batch 2440, Loss: 1.2337\n","Epoch 1, Batch 2460, Loss: 1.2261\n","Epoch 1, Batch 2480, Loss: 1.4148\n","Epoch 1, Batch 2500, Loss: 1.2215\n","Epoch 1, Batch 2520, Loss: 1.3343\n","Epoch 1, Batch 2540, Loss: 1.3271\n","Epoch 1, Batch 2560, Loss: 1.3700\n","Epoch 1, Batch 2580, Loss: 1.2449\n","Epoch 1, Batch 2600, Loss: 1.2203\n","Epoch 1, Batch 2620, Loss: 1.2809\n","Epoch 1, Batch 2640, Loss: 1.3865\n","Epoch 1, Batch 2660, Loss: 1.1724\n","Epoch 1, Batch 2680, Loss: 1.3066\n","Epoch 1, Batch 2700, Loss: 1.3691\n","Epoch 1, Batch 2720, Loss: 1.2474\n","Epoch 1, Batch 2740, Loss: 1.5489\n","Epoch 1, Batch 2760, Loss: 1.3496\n","Epoch 1, Batch 2780, Loss: 1.3906\n","Epoch 1, Batch 2800, Loss: 1.3563\n","Epoch 1, Batch 2820, Loss: 1.3260\n","Epoch 1, Batch 2840, Loss: 1.2333\n","Epoch 1, Batch 2860, Loss: 1.2988\n","Epoch 1, Batch 2880, Loss: 1.2768\n","Epoch 1, Batch 2900, Loss: 1.3802\n","Epoch 1, Batch 2920, Loss: 1.3001\n","Epoch 1, Batch 2940, Loss: 1.2875\n","Epoch 1, Batch 2960, Loss: 1.3174\n","Epoch 1, Batch 2980, Loss: 1.2075\n","Epoch 1, Batch 3000, Loss: 1.3037\n","Epoch 1, Batch 3020, Loss: 1.3338\n","Epoch 1, Batch 3040, Loss: 1.3265\n","Epoch 1, Batch 3060, Loss: 1.3802\n","Epoch 1, Batch 3080, Loss: 1.2306\n","Epoch 1, Batch 3100, Loss: 1.4249\n","Epoch 1, Batch 3120, Loss: 1.2464\n","Epoch 1, Batch 3140, Loss: 1.2781\n","Epoch 1, Batch 3160, Loss: 1.1407\n","Epoch 1, Batch 3180, Loss: 1.3011\n","Epoch 1, Batch 3200, Loss: 1.2662\n","Epoch 1, Batch 3220, Loss: 1.3413\n","Epoch 1, Batch 3240, Loss: 1.2447\n","Epoch 1, Batch 3260, Loss: 1.2461\n","Epoch 1, Batch 3280, Loss: 1.3076\n","Epoch 1, Batch 3300, Loss: 1.2847\n","Epoch 1, Batch 3320, Loss: 1.2572\n","Epoch 1, Batch 3340, Loss: 1.1721\n","Epoch 1, Batch 3360, Loss: 1.3532\n","Epoch 1, Batch 3380, Loss: 1.2674\n","Epoch 1, Batch 3400, Loss: 1.1338\n","Epoch 1, Batch 3420, Loss: 1.1187\n","Epoch 1, Batch 3440, Loss: 1.2170\n","Epoch 1, Batch 3460, Loss: 1.3666\n","Epoch 1, Batch 3480, Loss: 1.2356\n","Epoch 1, Batch 3500, Loss: 1.0772\n","Epoch 1, Batch 3520, Loss: 1.3208\n","Epoch 1, Batch 3540, Loss: 1.3976\n","Epoch 1, Batch 3560, Loss: 1.2741\n","Epoch 1, Batch 3580, Loss: 1.4125\n","Epoch 1, Batch 3600, Loss: 1.4006\n","Epoch 1, Batch 3620, Loss: 1.2628\n","Epoch 1, Batch 3640, Loss: 1.3155\n","Epoch 1, Batch 3660, Loss: 1.2497\n","Epoch 1, Batch 3680, Loss: 1.2030\n","Epoch 1, Batch 3700, Loss: 1.3914\n","Epoch 1, Batch 3720, Loss: 1.3298\n","Epoch 1, Batch 3740, Loss: 1.3806\n","Epoch 1, Batch 3760, Loss: 1.5666\n","Epoch 1, Batch 3780, Loss: 1.1715\n","Epoch 1, Batch 3800, Loss: 1.2238\n","Epoch 1, Batch 3820, Loss: 1.0876\n","Epoch 1, Batch 3840, Loss: 1.3011\n","Epoch 1, Batch 3860, Loss: 1.2512\n","Epoch 1, Batch 3880, Loss: 1.5171\n","Epoch 1, Batch 3900, Loss: 1.2483\n","Epoch 1, Batch 3920, Loss: 1.3189\n","Epoch 1, Batch 3940, Loss: 1.1818\n","Epoch 1, Batch 3960, Loss: 1.1453\n","Epoch 1, Batch 3980, Loss: 1.3195\n","Epoch 1, Batch 4000, Loss: 1.1996\n","Epoch 1, Batch 4020, Loss: 1.2282\n","Epoch 1, Batch 4040, Loss: 1.1965\n","Epoch 1, Batch 4060, Loss: 1.3005\n","Epoch 1, Batch 4080, Loss: 1.3041\n","Epoch 1, Batch 4100, Loss: 1.3245\n","Epoch 1, Batch 4120, Loss: 1.4693\n","Epoch 1, Batch 4140, Loss: 1.2184\n","Epoch 1, Batch 4160, Loss: 1.1792\n","Epoch 1, Batch 4180, Loss: 1.1726\n","Epoch 1, Batch 4200, Loss: 1.2462\n","Epoch 1, Batch 4220, Loss: 1.2815\n","Epoch 1, Batch 4240, Loss: 1.0488\n","Epoch 1, Batch 4260, Loss: 1.2636\n","Epoch 1, Batch 4280, Loss: 1.1597\n","Epoch 1, Batch 4300, Loss: 1.3346\n","Epoch 1, Batch 4320, Loss: 1.1186\n","Epoch 1, Batch 4340, Loss: 1.3707\n","Epoch 1, Batch 4360, Loss: 1.1814\n","Epoch 1, Batch 4380, Loss: 1.1768\n","Epoch 1, Batch 4400, Loss: 1.1952\n","Epoch 1, Batch 4420, Loss: 1.5308\n","Epoch 1, Batch 4440, Loss: 1.2869\n","Epoch 1, Batch 4460, Loss: 1.1201\n","Epoch 1, Batch 4480, Loss: 1.2644\n","Epoch 1, Batch 4500, Loss: 1.3110\n","Epoch 1, Batch 4520, Loss: 1.3152\n","Epoch 1, Batch 4540, Loss: 1.3539\n","Epoch 1, Batch 4560, Loss: 1.3393\n","Epoch 1, Batch 4580, Loss: 1.3139\n","Epoch 1, Batch 4600, Loss: 1.2461\n","Epoch 1, Batch 4620, Loss: 1.2360\n","Epoch 1, Batch 4640, Loss: 1.1606\n","Epoch 1, Batch 4660, Loss: 1.0720\n","Epoch 1, Batch 4680, Loss: 1.1862\n","Epoch 1, Batch 4700, Loss: 1.2037\n","Epoch 1, Batch 4720, Loss: 1.3496\n","Epoch 1, Batch 4740, Loss: 1.4952\n","Epoch 1, Batch 4760, Loss: 1.3552\n","Epoch 1, Batch 4780, Loss: 1.1413\n","Epoch 1, Batch 4800, Loss: 1.3663\n","Epoch 1, Batch 4820, Loss: 1.3070\n","Epoch 1, Batch 4840, Loss: 1.2934\n","Epoch 1, Batch 4860, Loss: 1.4970\n","Epoch 1, Batch 4880, Loss: 1.2233\n","Epoch 1, Batch 4900, Loss: 1.2213\n","Epoch 1, Batch 4920, Loss: 1.3484\n","Epoch 1, Batch 4940, Loss: 1.1176\n","Epoch 1, Batch 4960, Loss: 1.1647\n","Epoch 1, Batch 4980, Loss: 1.2272\n","Epoch 1, Batch 5000, Loss: 1.2123\n","Epoch 1, Batch 5020, Loss: 1.0867\n","Epoch 1, Batch 5040, Loss: 1.2590\n","Epoch 1, Batch 5060, Loss: 1.1316\n","Epoch 1, Batch 5080, Loss: 1.2327\n","Epoch 1, Batch 5100, Loss: 1.1565\n","Epoch 1, Batch 5120, Loss: 1.1171\n","Epoch 1, Batch 5140, Loss: 1.1937\n","Epoch 1, Batch 5160, Loss: 1.2902\n","Epoch 1, Batch 5180, Loss: 1.1586\n","Epoch 1, Batch 5200, Loss: 1.0901\n","Epoch 1, Batch 5220, Loss: 1.3726\n","Epoch 1, Batch 5240, Loss: 1.2245\n","Epoch 1, Batch 5260, Loss: 1.1315\n","Epoch 1, Batch 5280, Loss: 1.6058\n","Epoch 1, Batch 5300, Loss: 1.2807\n","Epoch 1, Batch 5320, Loss: 1.2946\n","Epoch 1, Batch 5340, Loss: 1.1717\n","Epoch 1, Batch 5360, Loss: 1.1846\n","Epoch 1, Batch 5380, Loss: 1.0839\n","Epoch 1, Batch 5400, Loss: 1.3380\n","Epoch 1, Batch 5420, Loss: 1.1736\n","Epoch 1, Batch 5440, Loss: 1.1809\n","Epoch 1, Batch 5460, Loss: 1.4339\n","Epoch 1, Batch 5480, Loss: 1.3455\n","Epoch 1, Batch 5500, Loss: 1.1885\n","Epoch 1, Batch 5520, Loss: 1.1935\n","Epoch 1, Average Loss: 1.3084, Val Weighted F1: 0.378641\n","Epoch 2, Batch 0, Loss: 1.1219\n","Epoch 2, Batch 20, Loss: 1.2504\n","Epoch 2, Batch 40, Loss: 1.2301\n","Epoch 2, Batch 60, Loss: 1.4008\n","Epoch 2, Batch 80, Loss: 1.3213\n","Epoch 2, Batch 100, Loss: 1.4050\n","Epoch 2, Batch 120, Loss: 1.3706\n","Epoch 2, Batch 140, Loss: 1.4129\n","Epoch 2, Batch 160, Loss: 1.2864\n","Epoch 2, Batch 180, Loss: 1.2831\n","Epoch 2, Batch 200, Loss: 1.1707\n","Epoch 2, Batch 220, Loss: 1.1426\n","Epoch 2, Batch 240, Loss: 1.2631\n","Epoch 2, Batch 260, Loss: 1.5030\n","Epoch 2, Batch 280, Loss: 1.1920\n","Epoch 2, Batch 300, Loss: 1.1462\n","Epoch 2, Batch 320, Loss: 1.1279\n","Epoch 2, Batch 340, Loss: 1.2500\n","Epoch 2, Batch 360, Loss: 1.1961\n","Epoch 2, Batch 380, Loss: 1.0801\n","Epoch 2, Batch 400, Loss: 0.9984\n","Epoch 2, Batch 420, Loss: 1.4210\n","Epoch 2, Batch 440, Loss: 1.2533\n","Epoch 2, Batch 460, Loss: 1.2041\n","Epoch 2, Batch 480, Loss: 1.2498\n","Epoch 2, Batch 500, Loss: 1.2059\n","Epoch 2, Batch 520, Loss: 1.0682\n","Epoch 2, Batch 540, Loss: 1.3111\n","Epoch 2, Batch 560, Loss: 1.0985\n","Epoch 2, Batch 580, Loss: 1.0939\n","Epoch 2, Batch 600, Loss: 1.3272\n","Epoch 2, Batch 620, Loss: 1.0624\n","Epoch 2, Batch 640, Loss: 1.1669\n","Epoch 2, Batch 660, Loss: 1.3871\n","Epoch 2, Batch 680, Loss: 1.2985\n","Epoch 2, Batch 700, Loss: 1.1221\n","Epoch 2, Batch 720, Loss: 1.2020\n","Epoch 2, Batch 740, Loss: 1.4854\n","Epoch 2, Batch 760, Loss: 1.1243\n","Epoch 2, Batch 780, Loss: 1.2795\n","Epoch 2, Batch 800, Loss: 1.3535\n","Epoch 2, Batch 820, Loss: 1.2931\n","Epoch 2, Batch 840, Loss: 1.0820\n","Epoch 2, Batch 860, Loss: 1.1084\n","Epoch 2, Batch 880, Loss: 1.1474\n","Epoch 2, Batch 900, Loss: 1.1005\n","Epoch 2, Batch 920, Loss: 1.0658\n","Epoch 2, Batch 940, Loss: 1.3080\n","Epoch 2, Batch 960, Loss: 1.1050\n","Epoch 2, Batch 980, Loss: 1.1575\n","Epoch 2, Batch 1000, Loss: 1.3476\n","Epoch 2, Batch 1020, Loss: 1.2462\n","Epoch 2, Batch 1040, Loss: 1.1229\n","Epoch 2, Batch 1060, Loss: 1.3354\n","Epoch 2, Batch 1080, Loss: 1.1182\n","Epoch 2, Batch 1100, Loss: 1.2019\n","Epoch 2, Batch 1120, Loss: 1.0302\n","Epoch 2, Batch 1140, Loss: 1.2018\n","Epoch 2, Batch 1160, Loss: 1.1459\n","Epoch 2, Batch 1180, Loss: 1.1565\n","Epoch 2, Batch 1200, Loss: 1.3992\n","Epoch 2, Batch 1220, Loss: 1.1478\n","Epoch 2, Batch 1240, Loss: 1.1301\n","Epoch 2, Batch 1260, Loss: 1.2058\n","Epoch 2, Batch 1280, Loss: 1.0536\n","Epoch 2, Batch 1300, Loss: 1.0635\n","Epoch 2, Batch 1320, Loss: 1.2152\n","Epoch 2, Batch 1340, Loss: 1.3705\n","Epoch 2, Batch 1360, Loss: 1.1441\n","Epoch 2, Batch 1380, Loss: 1.3544\n","Epoch 2, Batch 1400, Loss: 1.0899\n","Epoch 2, Batch 1420, Loss: 1.1691\n","Epoch 2, Batch 1440, Loss: 1.1224\n","Epoch 2, Batch 1460, Loss: 1.1328\n","Epoch 2, Batch 1480, Loss: 1.4003\n","Epoch 2, Batch 1500, Loss: 1.1040\n","Epoch 2, Batch 1520, Loss: 1.3112\n","Epoch 2, Batch 1540, Loss: 1.4296\n","Epoch 2, Batch 1560, Loss: 0.9758\n","Epoch 2, Batch 1580, Loss: 1.0312\n","Epoch 2, Batch 1600, Loss: 1.2463\n","Epoch 2, Batch 1620, Loss: 1.0029\n","Epoch 2, Batch 1640, Loss: 1.1046\n","Epoch 2, Batch 1660, Loss: 1.3781\n","Epoch 2, Batch 1680, Loss: 1.1092\n","Epoch 2, Batch 1700, Loss: 1.0548\n","Epoch 2, Batch 1720, Loss: 1.2288\n","Epoch 2, Batch 1740, Loss: 1.1414\n","Epoch 2, Batch 1760, Loss: 1.0940\n","Epoch 2, Batch 1780, Loss: 1.0783\n","Epoch 2, Batch 1800, Loss: 1.2232\n","Epoch 2, Batch 1820, Loss: 1.4114\n","Epoch 2, Batch 1840, Loss: 1.2332\n","Epoch 2, Batch 1860, Loss: 1.1372\n","Epoch 2, Batch 1880, Loss: 1.1482\n","Epoch 2, Batch 1900, Loss: 1.3029\n","Epoch 2, Batch 1920, Loss: 1.1337\n","Epoch 2, Batch 1940, Loss: 1.0927\n","Epoch 2, Batch 1960, Loss: 1.3758\n","Epoch 2, Batch 1980, Loss: 1.2467\n","Epoch 2, Batch 2000, Loss: 0.9610\n","Epoch 2, Batch 2020, Loss: 1.0928\n","Epoch 2, Batch 2040, Loss: 1.3491\n","Epoch 2, Batch 2060, Loss: 1.4023\n","Epoch 2, Batch 2080, Loss: 1.3073\n","Epoch 2, Batch 2100, Loss: 1.2861\n","Epoch 2, Batch 2120, Loss: 1.1571\n","Epoch 2, Batch 2140, Loss: 1.2924\n","Epoch 2, Batch 2160, Loss: 1.1216\n","Epoch 2, Batch 2180, Loss: 1.0777\n","Epoch 2, Batch 2200, Loss: 1.2756\n","Epoch 2, Batch 2220, Loss: 0.9088\n","Epoch 2, Batch 2240, Loss: 1.2220\n","Epoch 2, Batch 2260, Loss: 1.2420\n","Epoch 2, Batch 2280, Loss: 1.1069\n","Epoch 2, Batch 2300, Loss: 1.0660\n","Epoch 2, Batch 2320, Loss: 1.2089\n","Epoch 2, Batch 2340, Loss: 1.1305\n","Epoch 2, Batch 2360, Loss: 1.2308\n","Epoch 2, Batch 2380, Loss: 1.1618\n","Epoch 2, Batch 2400, Loss: 1.2366\n","Epoch 2, Batch 2420, Loss: 1.0982\n","Epoch 2, Batch 2440, Loss: 1.2092\n","Epoch 2, Batch 2460, Loss: 1.1745\n","Epoch 2, Batch 2480, Loss: 1.2380\n","Epoch 2, Batch 2500, Loss: 1.0443\n","Epoch 2, Batch 2520, Loss: 1.1573\n","Epoch 2, Batch 2540, Loss: 1.0877\n","Epoch 2, Batch 2560, Loss: 1.2550\n","Epoch 2, Batch 2580, Loss: 1.0631\n","Epoch 2, Batch 2600, Loss: 1.1618\n","Epoch 2, Batch 2620, Loss: 1.1494\n","Epoch 2, Batch 2640, Loss: 1.2402\n","Epoch 2, Batch 2660, Loss: 1.2190\n","Epoch 2, Batch 2680, Loss: 1.1107\n","Epoch 2, Batch 2700, Loss: 1.1235\n","Epoch 2, Batch 2720, Loss: 1.2841\n","Epoch 2, Batch 2740, Loss: 1.4278\n","Epoch 2, Batch 2760, Loss: 1.3422\n","Epoch 2, Batch 2780, Loss: 1.0559\n","Epoch 2, Batch 2800, Loss: 1.2083\n","Epoch 2, Batch 2820, Loss: 1.1937\n","Epoch 2, Batch 2840, Loss: 1.3208\n","Epoch 2, Batch 2860, Loss: 1.2127\n","Epoch 2, Batch 2880, Loss: 1.0719\n","Epoch 2, Batch 2900, Loss: 1.0831\n","Epoch 2, Batch 2920, Loss: 1.3155\n","Epoch 2, Batch 2940, Loss: 1.1389\n","Epoch 2, Batch 2960, Loss: 1.4980\n","Epoch 2, Batch 2980, Loss: 1.4645\n","Epoch 2, Batch 3000, Loss: 1.1833\n","Epoch 2, Batch 3020, Loss: 1.1149\n","Epoch 2, Batch 3040, Loss: 1.4780\n","Epoch 2, Batch 3060, Loss: 1.1821\n","Epoch 2, Batch 3080, Loss: 1.1025\n","Epoch 2, Batch 3100, Loss: 1.2569\n","Epoch 2, Batch 3120, Loss: 1.4076\n","Epoch 2, Batch 3140, Loss: 1.3160\n","Epoch 2, Batch 3160, Loss: 1.2353\n","Epoch 2, Batch 3180, Loss: 1.1444\n","Epoch 2, Batch 3200, Loss: 1.2486\n","Epoch 2, Batch 3220, Loss: 1.1512\n","Epoch 2, Batch 3240, Loss: 0.9409\n","Epoch 2, Batch 3260, Loss: 1.0423\n","Epoch 2, Batch 3280, Loss: 0.8930\n","Epoch 2, Batch 3300, Loss: 1.1644\n","Epoch 2, Batch 3320, Loss: 1.1889\n","Epoch 2, Batch 3340, Loss: 1.0723\n","Epoch 2, Batch 3360, Loss: 0.9750\n","Epoch 2, Batch 3380, Loss: 1.1333\n","Epoch 2, Batch 3400, Loss: 1.1268\n","Epoch 2, Batch 3420, Loss: 0.9737\n","Epoch 2, Batch 3440, Loss: 0.9962\n","Epoch 2, Batch 3460, Loss: 1.2607\n","Epoch 2, Batch 3480, Loss: 1.0179\n","Epoch 2, Batch 3500, Loss: 1.2646\n","Epoch 2, Batch 3520, Loss: 1.2563\n","Epoch 2, Batch 3540, Loss: 1.0627\n","Epoch 2, Batch 3560, Loss: 1.3561\n","Epoch 2, Batch 3580, Loss: 1.2724\n","Epoch 2, Batch 3600, Loss: 1.0198\n","Epoch 2, Batch 3620, Loss: 1.3669\n","Epoch 2, Batch 3640, Loss: 1.2041\n","Epoch 2, Batch 3660, Loss: 1.2379\n","Epoch 2, Batch 3680, Loss: 1.2864\n","Epoch 2, Batch 3700, Loss: 1.3136\n","Epoch 2, Batch 3720, Loss: 1.1198\n","Epoch 2, Batch 3740, Loss: 1.4147\n","Epoch 2, Batch 3760, Loss: 1.1480\n","Epoch 2, Batch 3780, Loss: 1.1952\n","Epoch 2, Batch 3800, Loss: 1.2076\n","Epoch 2, Batch 3820, Loss: 1.2250\n","Epoch 2, Batch 3840, Loss: 1.2170\n","Epoch 2, Batch 3860, Loss: 1.1937\n","Epoch 2, Batch 3880, Loss: 1.2318\n","Epoch 2, Batch 3900, Loss: 1.3225\n","Epoch 2, Batch 3920, Loss: 1.1625\n","Epoch 2, Batch 3940, Loss: 1.0760\n","Epoch 2, Batch 3960, Loss: 1.3766\n","Epoch 2, Batch 3980, Loss: 0.9865\n","Epoch 2, Batch 4000, Loss: 1.2231\n","Epoch 2, Batch 4020, Loss: 1.1696\n","Epoch 2, Batch 4040, Loss: 1.1100\n","Epoch 2, Batch 4060, Loss: 1.2292\n","Epoch 2, Batch 4080, Loss: 1.4471\n","Epoch 2, Batch 4100, Loss: 1.2785\n","Epoch 2, Batch 4120, Loss: 1.2093\n","Epoch 2, Batch 4140, Loss: 1.4540\n","Epoch 2, Batch 4160, Loss: 1.3526\n","Epoch 2, Batch 4180, Loss: 1.1202\n","Epoch 2, Batch 4200, Loss: 1.0794\n","Epoch 2, Batch 4220, Loss: 1.2213\n","Epoch 2, Batch 4240, Loss: 1.3395\n","Epoch 2, Batch 4260, Loss: 1.3060\n","Epoch 2, Batch 4280, Loss: 1.4477\n","Epoch 2, Batch 4300, Loss: 0.9161\n","Epoch 2, Batch 4320, Loss: 1.2783\n","Epoch 2, Batch 4340, Loss: 1.2370\n","Epoch 2, Batch 4360, Loss: 1.1601\n","Epoch 2, Batch 4380, Loss: 1.2163\n","Epoch 2, Batch 4400, Loss: 1.2584\n","Epoch 2, Batch 4420, Loss: 1.3680\n","Epoch 2, Batch 4440, Loss: 1.0750\n","Epoch 2, Batch 4460, Loss: 1.2836\n","Epoch 2, Batch 4480, Loss: 1.1844\n","Epoch 2, Batch 4500, Loss: 1.1989\n","Epoch 2, Batch 4520, Loss: 0.9804\n","Epoch 2, Batch 4540, Loss: 1.1470\n","Epoch 2, Batch 4560, Loss: 1.1575\n","Epoch 2, Batch 4580, Loss: 1.1440\n","Epoch 2, Batch 4600, Loss: 1.1279\n","Epoch 2, Batch 4620, Loss: 1.2206\n","Epoch 2, Batch 4640, Loss: 1.0999\n","Epoch 2, Batch 4660, Loss: 1.2561\n","Epoch 2, Batch 4680, Loss: 1.1516\n","Epoch 2, Batch 4700, Loss: 1.2855\n","Epoch 2, Batch 4720, Loss: 1.0894\n","Epoch 2, Batch 4740, Loss: 1.2262\n","Epoch 2, Batch 4760, Loss: 1.1299\n","Epoch 2, Batch 4780, Loss: 1.1596\n","Epoch 2, Batch 4800, Loss: 1.4102\n","Epoch 2, Batch 4820, Loss: 1.1692\n","Epoch 2, Batch 4840, Loss: 1.1118\n","Epoch 2, Batch 4860, Loss: 1.2483\n","Epoch 2, Batch 4880, Loss: 1.3077\n","Epoch 2, Batch 4900, Loss: 1.3815\n","Epoch 2, Batch 4920, Loss: 1.3524\n","Epoch 2, Batch 4940, Loss: 1.3715\n","Epoch 2, Batch 4960, Loss: 1.1202\n","Epoch 2, Batch 4980, Loss: 1.1563\n","Epoch 2, Batch 5000, Loss: 1.4716\n","Epoch 2, Batch 5020, Loss: 1.2747\n","Epoch 2, Batch 5040, Loss: 1.1858\n","Epoch 2, Batch 5060, Loss: 1.3389\n","Epoch 2, Batch 5080, Loss: 0.9970\n","Epoch 2, Batch 5100, Loss: 1.0938\n","Epoch 2, Batch 5120, Loss: 1.4142\n","Epoch 2, Batch 5140, Loss: 1.2387\n","Epoch 2, Batch 5160, Loss: 1.2356\n","Epoch 2, Batch 5180, Loss: 1.3224\n","Epoch 2, Batch 5200, Loss: 1.1745\n","Epoch 2, Batch 5220, Loss: 1.0604\n","Epoch 2, Batch 5240, Loss: 1.1108\n","Epoch 2, Batch 5260, Loss: 1.0470\n","Epoch 2, Batch 5280, Loss: 1.2139\n","Epoch 2, Batch 5300, Loss: 1.4055\n","Epoch 2, Batch 5320, Loss: 1.2960\n","Epoch 2, Batch 5340, Loss: 1.1158\n","Epoch 2, Batch 5360, Loss: 1.3182\n","Epoch 2, Batch 5380, Loss: 1.1764\n","Epoch 2, Batch 5400, Loss: 1.3479\n","Epoch 2, Batch 5420, Loss: 1.1824\n","Epoch 2, Batch 5440, Loss: 1.2105\n","Epoch 2, Batch 5460, Loss: 1.0280\n","Epoch 2, Batch 5480, Loss: 1.3273\n","Epoch 2, Batch 5500, Loss: 1.1883\n","Epoch 2, Batch 5520, Loss: 0.9493\n","Epoch 2, Average Loss: 1.2067, Val Weighted F1: 0.409713\n","Epoch 3, Batch 0, Loss: 1.2105\n","Epoch 3, Batch 20, Loss: 1.3010\n","Epoch 3, Batch 40, Loss: 1.1564\n","Epoch 3, Batch 60, Loss: 1.0637\n","Epoch 3, Batch 80, Loss: 1.2542\n","Epoch 3, Batch 100, Loss: 1.1808\n","Epoch 3, Batch 120, Loss: 1.1372\n","Epoch 3, Batch 140, Loss: 0.9626\n","Epoch 3, Batch 160, Loss: 1.1334\n","Epoch 3, Batch 180, Loss: 1.2353\n","Epoch 3, Batch 200, Loss: 1.0947\n","Epoch 3, Batch 220, Loss: 1.1345\n","Epoch 3, Batch 240, Loss: 1.1582\n","Epoch 3, Batch 260, Loss: 1.2218\n","Epoch 3, Batch 280, Loss: 1.3724\n","Epoch 3, Batch 300, Loss: 1.2961\n","Epoch 3, Batch 320, Loss: 1.1352\n","Epoch 3, Batch 340, Loss: 1.0135\n","Epoch 3, Batch 360, Loss: 1.1679\n","Epoch 3, Batch 380, Loss: 1.0298\n","Epoch 3, Batch 400, Loss: 1.2739\n","Epoch 3, Batch 420, Loss: 1.2848\n","Epoch 3, Batch 440, Loss: 1.2318\n","Epoch 3, Batch 460, Loss: 1.1023\n","Epoch 3, Batch 480, Loss: 1.0702\n","Epoch 3, Batch 500, Loss: 1.3480\n","Epoch 3, Batch 520, Loss: 1.2057\n","Epoch 3, Batch 540, Loss: 0.9184\n","Epoch 3, Batch 560, Loss: 1.3730\n","Epoch 3, Batch 580, Loss: 1.0921\n","Epoch 3, Batch 600, Loss: 1.1575\n","Epoch 3, Batch 620, Loss: 1.1506\n","Epoch 3, Batch 640, Loss: 0.9775\n","Epoch 3, Batch 660, Loss: 1.1954\n","Epoch 3, Batch 680, Loss: 1.3546\n","Epoch 3, Batch 700, Loss: 1.1073\n","Epoch 3, Batch 720, Loss: 1.1622\n","Epoch 3, Batch 740, Loss: 1.3526\n","Epoch 3, Batch 760, Loss: 1.3523\n","Epoch 3, Batch 780, Loss: 1.0674\n","Epoch 3, Batch 800, Loss: 1.0545\n","Epoch 3, Batch 820, Loss: 1.1867\n","Epoch 3, Batch 840, Loss: 1.3095\n","Epoch 3, Batch 860, Loss: 1.3154\n","Epoch 3, Batch 880, Loss: 1.0355\n","Epoch 3, Batch 900, Loss: 1.0951\n","Epoch 3, Batch 920, Loss: 1.2806\n","Epoch 3, Batch 940, Loss: 1.1126\n","Epoch 3, Batch 960, Loss: 1.2563\n","Epoch 3, Batch 980, Loss: 1.2540\n","Epoch 3, Batch 1000, Loss: 1.1625\n","Epoch 3, Batch 1020, Loss: 1.2164\n","Epoch 3, Batch 1040, Loss: 1.1023\n","Epoch 3, Batch 1060, Loss: 1.2176\n","Epoch 3, Batch 1080, Loss: 1.3788\n","Epoch 3, Batch 1100, Loss: 1.3673\n","Epoch 3, Batch 1120, Loss: 1.1326\n","Epoch 3, Batch 1140, Loss: 0.9511\n","Epoch 3, Batch 1160, Loss: 1.1450\n","Epoch 3, Batch 1180, Loss: 0.9182\n","Epoch 3, Batch 1200, Loss: 1.1975\n","Epoch 3, Batch 1220, Loss: 1.3056\n","Epoch 3, Batch 1240, Loss: 1.2358\n","Epoch 3, Batch 1260, Loss: 1.4640\n","Epoch 3, Batch 1280, Loss: 1.3551\n","Epoch 3, Batch 1300, Loss: 1.3766\n","Epoch 3, Batch 1320, Loss: 1.0509\n","Epoch 3, Batch 1340, Loss: 1.4617\n","Epoch 3, Batch 1360, Loss: 1.0764\n","Epoch 3, Batch 1380, Loss: 1.0796\n","Epoch 3, Batch 1400, Loss: 1.2109\n","Epoch 3, Batch 1420, Loss: 1.1457\n","Epoch 3, Batch 1440, Loss: 1.0207\n","Epoch 3, Batch 1460, Loss: 0.9676\n","Epoch 3, Batch 1480, Loss: 1.3753\n","Epoch 3, Batch 1500, Loss: 1.2060\n","Epoch 3, Batch 1520, Loss: 1.1549\n","Epoch 3, Batch 1540, Loss: 1.3640\n","Epoch 3, Batch 1560, Loss: 1.0545\n","Epoch 3, Batch 1580, Loss: 0.9792\n","Epoch 3, Batch 1600, Loss: 1.3908\n","Epoch 3, Batch 1620, Loss: 1.1406\n","Epoch 3, Batch 1640, Loss: 1.2312\n","Epoch 3, Batch 1660, Loss: 1.1323\n","Epoch 3, Batch 1680, Loss: 1.1264\n","Epoch 3, Batch 1700, Loss: 1.1763\n","Epoch 3, Batch 1720, Loss: 1.1405\n","Epoch 3, Batch 1740, Loss: 1.2188\n","Epoch 3, Batch 1760, Loss: 1.1939\n","Epoch 3, Batch 1780, Loss: 1.0930\n","Epoch 3, Batch 1800, Loss: 1.2101\n","Epoch 3, Batch 1820, Loss: 1.2035\n","Epoch 3, Batch 1840, Loss: 1.0806\n","Epoch 3, Batch 1860, Loss: 1.1908\n","Epoch 3, Batch 1880, Loss: 1.1517\n","Epoch 3, Batch 1900, Loss: 1.1718\n","Epoch 3, Batch 1920, Loss: 1.0804\n","Epoch 3, Batch 1940, Loss: 1.1625\n","Epoch 3, Batch 1960, Loss: 1.4244\n","Epoch 3, Batch 1980, Loss: 1.3460\n","Epoch 3, Batch 2000, Loss: 0.9838\n","Epoch 3, Batch 2020, Loss: 1.1318\n","Epoch 3, Batch 2040, Loss: 1.3207\n","Epoch 3, Batch 2060, Loss: 1.1877\n","Epoch 3, Batch 2080, Loss: 1.1519\n","Epoch 3, Batch 2100, Loss: 1.0707\n","Epoch 3, Batch 2120, Loss: 1.1960\n","Epoch 3, Batch 2140, Loss: 1.0133\n","Epoch 3, Batch 2160, Loss: 1.1759\n","Epoch 3, Batch 2180, Loss: 1.2631\n","Epoch 3, Batch 2200, Loss: 1.0230\n","Epoch 3, Batch 2220, Loss: 1.0588\n","Epoch 3, Batch 2240, Loss: 1.1213\n","Epoch 3, Batch 2260, Loss: 1.2419\n","Epoch 3, Batch 2280, Loss: 1.4467\n","Epoch 3, Batch 2300, Loss: 1.3497\n","Epoch 3, Batch 2320, Loss: 1.1359\n","Epoch 3, Batch 2340, Loss: 1.1105\n","Epoch 3, Batch 2360, Loss: 1.0983\n","Epoch 3, Batch 2380, Loss: 1.2802\n","Epoch 3, Batch 2400, Loss: 1.3118\n","Epoch 3, Batch 2420, Loss: 0.9693\n","Epoch 3, Batch 2440, Loss: 1.1351\n","Epoch 3, Batch 2460, Loss: 1.1420\n","Epoch 3, Batch 2480, Loss: 1.4115\n","Epoch 3, Batch 2500, Loss: 1.0536\n","Epoch 3, Batch 2520, Loss: 1.0848\n","Epoch 3, Batch 2540, Loss: 1.2831\n","Epoch 3, Batch 2560, Loss: 0.9477\n","Epoch 3, Batch 2580, Loss: 1.0527\n","Epoch 3, Batch 2600, Loss: 1.1325\n","Epoch 3, Batch 2620, Loss: 1.0967\n","Epoch 3, Batch 2640, Loss: 1.0884\n","Epoch 3, Batch 2660, Loss: 1.0449\n","Epoch 3, Batch 2680, Loss: 0.9895\n","Epoch 3, Batch 2700, Loss: 1.1712\n","Epoch 3, Batch 2720, Loss: 1.3103\n","Epoch 3, Batch 2740, Loss: 1.1515\n","Epoch 3, Batch 2760, Loss: 1.1383\n","Epoch 3, Batch 2780, Loss: 1.2058\n","Epoch 3, Batch 2800, Loss: 1.5120\n","Epoch 3, Batch 2820, Loss: 1.2297\n","Epoch 3, Batch 2840, Loss: 1.3232\n","Epoch 3, Batch 2860, Loss: 0.9902\n","Epoch 3, Batch 2880, Loss: 1.1985\n","Epoch 3, Batch 2900, Loss: 1.0611\n","Epoch 3, Batch 2920, Loss: 1.2912\n","Epoch 3, Batch 2940, Loss: 1.2290\n","Epoch 3, Batch 2960, Loss: 1.2230\n","Epoch 3, Batch 2980, Loss: 1.2784\n","Epoch 3, Batch 3000, Loss: 0.9385\n","Epoch 3, Batch 3020, Loss: 1.2004\n","Epoch 3, Batch 3040, Loss: 1.2337\n","Epoch 3, Batch 3060, Loss: 1.0823\n","Epoch 3, Batch 3080, Loss: 1.2413\n","Epoch 3, Batch 3100, Loss: 1.0293\n","Epoch 3, Batch 3120, Loss: 1.4088\n","Epoch 3, Batch 3140, Loss: 0.9398\n","Epoch 3, Batch 3160, Loss: 1.1475\n","Epoch 3, Batch 3180, Loss: 1.2523\n","Epoch 3, Batch 3200, Loss: 1.0191\n","Epoch 3, Batch 3220, Loss: 1.3587\n","Epoch 3, Batch 3240, Loss: 1.2739\n","Epoch 3, Batch 3260, Loss: 1.1483\n","Epoch 3, Batch 3280, Loss: 1.0324\n","Epoch 3, Batch 3300, Loss: 1.2863\n","Epoch 3, Batch 3320, Loss: 1.3212\n","Epoch 3, Batch 3340, Loss: 1.3248\n","Epoch 3, Batch 3360, Loss: 1.1751\n","Epoch 3, Batch 3380, Loss: 1.2406\n","Epoch 3, Batch 3400, Loss: 1.2579\n","Epoch 3, Batch 3420, Loss: 1.1600\n","Epoch 3, Batch 3440, Loss: 1.1854\n","Epoch 3, Batch 3460, Loss: 0.9943\n","Epoch 3, Batch 3480, Loss: 0.9951\n","Epoch 3, Batch 3500, Loss: 1.0966\n","Epoch 3, Batch 3520, Loss: 1.0555\n","Epoch 3, Batch 3540, Loss: 1.2169\n","Epoch 3, Batch 3560, Loss: 1.3023\n","Epoch 3, Batch 3580, Loss: 1.1367\n","Epoch 3, Batch 3600, Loss: 1.1974\n","Epoch 3, Batch 3620, Loss: 1.2045\n","Epoch 3, Batch 3640, Loss: 1.2179\n","Epoch 3, Batch 3660, Loss: 1.1484\n","Epoch 3, Batch 3680, Loss: 1.0490\n","Epoch 3, Batch 3700, Loss: 1.1572\n","Epoch 3, Batch 3720, Loss: 1.0750\n","Epoch 3, Batch 3740, Loss: 0.9353\n","Epoch 3, Batch 3760, Loss: 1.0390\n","Epoch 3, Batch 3780, Loss: 1.0141\n","Epoch 3, Batch 3800, Loss: 1.0586\n","Epoch 3, Batch 3820, Loss: 1.3602\n","Epoch 3, Batch 3840, Loss: 1.2793\n","Epoch 3, Batch 3860, Loss: 0.9703\n","Epoch 3, Batch 3880, Loss: 1.1249\n","Epoch 3, Batch 3900, Loss: 1.2384\n","Epoch 3, Batch 3920, Loss: 1.3573\n","Epoch 3, Batch 3940, Loss: 0.9757\n","Epoch 3, Batch 3960, Loss: 1.1832\n","Epoch 3, Batch 3980, Loss: 1.3357\n","Epoch 3, Batch 4000, Loss: 1.0038\n","Epoch 3, Batch 4020, Loss: 1.0611\n","Epoch 3, Batch 4040, Loss: 0.9620\n","Epoch 3, Batch 4060, Loss: 1.1543\n","Epoch 3, Batch 4080, Loss: 1.0547\n","Epoch 3, Batch 4100, Loss: 1.1467\n","Epoch 3, Batch 4120, Loss: 0.9842\n","Epoch 3, Batch 4140, Loss: 1.2915\n","Epoch 3, Batch 4160, Loss: 1.3566\n","Epoch 3, Batch 4180, Loss: 1.2395\n","Epoch 3, Batch 4200, Loss: 1.0649\n","Epoch 3, Batch 4220, Loss: 1.0633\n","Epoch 3, Batch 4240, Loss: 1.1606\n","Epoch 3, Batch 4260, Loss: 1.3212\n","Epoch 3, Batch 4280, Loss: 1.1544\n","Epoch 3, Batch 4300, Loss: 1.0315\n","Epoch 3, Batch 4320, Loss: 1.1706\n","Epoch 3, Batch 4340, Loss: 1.0484\n","Epoch 3, Batch 4360, Loss: 1.0439\n","Epoch 3, Batch 4380, Loss: 1.0740\n","Epoch 3, Batch 4400, Loss: 1.1137\n","Epoch 3, Batch 4420, Loss: 1.1192\n","Epoch 3, Batch 4440, Loss: 0.9877\n","Epoch 3, Batch 4460, Loss: 1.2187\n","Epoch 3, Batch 4480, Loss: 1.2942\n","Epoch 3, Batch 4500, Loss: 1.2016\n","Epoch 3, Batch 4520, Loss: 1.2056\n","Epoch 3, Batch 4540, Loss: 1.1947\n","Epoch 3, Batch 4560, Loss: 1.1990\n","Epoch 3, Batch 4580, Loss: 1.2640\n","Epoch 3, Batch 4600, Loss: 1.2608\n","Epoch 3, Batch 4620, Loss: 1.0272\n","Epoch 3, Batch 4640, Loss: 1.0896\n","Epoch 3, Batch 4660, Loss: 1.0702\n","Epoch 3, Batch 4680, Loss: 1.0777\n","Epoch 3, Batch 4700, Loss: 1.1677\n","Epoch 3, Batch 4720, Loss: 1.1927\n","Epoch 3, Batch 4740, Loss: 1.1331\n","Epoch 3, Batch 4760, Loss: 1.1973\n","Epoch 3, Batch 4780, Loss: 1.1617\n","Epoch 3, Batch 4800, Loss: 1.1003\n","Epoch 3, Batch 4820, Loss: 1.4109\n","Epoch 3, Batch 4840, Loss: 1.0576\n","Epoch 3, Batch 4860, Loss: 1.1205\n","Epoch 3, Batch 4880, Loss: 0.9016\n","Epoch 3, Batch 4900, Loss: 1.4671\n","Epoch 3, Batch 4920, Loss: 1.2392\n","Epoch 3, Batch 4940, Loss: 1.2915\n","Epoch 3, Batch 4960, Loss: 1.3172\n","Epoch 3, Batch 4980, Loss: 1.1832\n","Epoch 3, Batch 5000, Loss: 1.0335\n","Epoch 3, Batch 5020, Loss: 1.0906\n","Epoch 3, Batch 5040, Loss: 1.1883\n","Epoch 3, Batch 5060, Loss: 1.2875\n","Epoch 3, Batch 5080, Loss: 1.1344\n","Epoch 3, Batch 5100, Loss: 0.9970\n","Epoch 3, Batch 5120, Loss: 1.1276\n","Epoch 3, Batch 5140, Loss: 1.3209\n","Epoch 3, Batch 5160, Loss: 1.0857\n","Epoch 3, Batch 5180, Loss: 1.3809\n","Epoch 3, Batch 5200, Loss: 0.9806\n","Epoch 3, Batch 5220, Loss: 1.2426\n","Epoch 3, Batch 5240, Loss: 1.2818\n","Epoch 3, Batch 5260, Loss: 1.0703\n","Epoch 3, Batch 5280, Loss: 1.3487\n","Epoch 3, Batch 5300, Loss: 1.2334\n","Epoch 3, Batch 5320, Loss: 1.3341\n","Epoch 3, Batch 5340, Loss: 1.1164\n","Epoch 3, Batch 5360, Loss: 1.1797\n","Epoch 3, Batch 5380, Loss: 1.2504\n","Epoch 3, Batch 5400, Loss: 0.9727\n","Epoch 3, Batch 5420, Loss: 1.1601\n","Epoch 3, Batch 5440, Loss: 0.9442\n","Epoch 3, Batch 5460, Loss: 1.1292\n","Epoch 3, Batch 5480, Loss: 1.1666\n","Epoch 3, Batch 5500, Loss: 1.1566\n","Epoch 3, Batch 5520, Loss: 1.1124\n","Epoch 3, Average Loss: 1.1567, Val Weighted F1: 0.427565\n","Submission file created successfully!\n","Best validation F1: 0.427565\n","Sample predictions:\n","   Unnamed: 0          target\n","0           0  relevant_minus\n","1           1  relevant_minus\n","2           2   relevant_plus\n","3           3     no_relevant\n","4           4     no_relevant\n","5           5     no_relevant\n","6           6  relevant_minus\n","7           7     no_relevant\n","8           8     no_relevant\n","9           9        relevant\n"]}],"execution_count":6},{"cell_type":"markdown","source":"# Bert и Berta","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom catboost import CatBoostClassifier\nimport numpy as np\n\n\n# ------------------------------\n# 1. Загружаем обычный BERT\n# ------------------------------\nMODEL_NAME = \"bert-base-uncased\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nbert_model = AutoModel.from_pretrained(MODEL_NAME)\n\n\n# ------------------------------\n# 2. Функция получения эмбеддингов BERT\n# ------------------------------\ndef bert_embed(texts, batch_size=16):\n    bert_model.eval()\n    embeddings = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n\n        tokens = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=256,\n            return_tensors='pt'\n        )\n\n        with torch.no_grad():\n            outputs = bert_model(**tokens)\n\n        # Используем CLS-токен\n        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n        embeddings.append(cls_embeddings)\n\n    return np.vstack(embeddings)\n\n\n# ------------------------------\n# 3. Пример данных\n# ------------------------------\ntexts = [\n    \"This movie is amazing!\",\n    \"I hated every second of it.\",\n    \"The service was excellent.\",\n    \"I will never come back.\"\n]\n\nlabels = [1, 0, 1, 0]\n\n\n# ------------------------------\n# 4. Генерация BERT-эмбеддингов\n# ------------------------------\nX = bert_embed(texts)\nprint(\"Embedding shape:\", X.shape)\n\n\n# ------------------------------\n# 5. Обучение CatBoostClassifier\n# ------------------------------\nmodel = CatBoostClassifier(\n    iterations=300,\n    depth=6,\n    learning_rate=0.05,\n    loss_function=\"Logloss\",\n    verbose=50\n)\n\nmodel.fit(X, labels)\n\n\n# ------------------------------\n# 6. Предсказание\n# ------------------------------\ntest_texts = [\"Fantastic product!\", \"Terrible experience.\"]\nX_test = bert_embed(test_texts)\n\npreds = model.predict_proba(X_test)\nprint(preds)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\nfrom catboost import CatBoostClassifier\nimport numpy as np\n\n\n# ============================================================\n# 1. Загружаем RoBERTa (berta)\n# ============================================================\nMODEL_NAME = \"roberta-base\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nroberta_model = AutoModel.from_pretrained(MODEL_NAME)\n\n\n# ============================================================\n# 2. Функция получения эмбеддингов RoBERTa\n# ============================================================\ndef berta_embed(texts, batch_size=16):\n    roberta_model.eval()\n    embeddings = []\n\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n\n        tokens = tokenizer(\n            batch,\n            padding=True,\n            truncation=True,\n            max_length=256,\n            return_tensors='pt'\n        )\n\n        with torch.no_grad():\n            outputs = roberta_model(**tokens)\n\n        # У RoBERTa роль CLS играет токен <s>\n        cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n        embeddings.append(cls_embeddings)\n\n    return np.vstack(embeddings)\n\n\n# ============================================================\n# 3. Пример тренировочных данных\n# ============================================================\ntexts = [\n    \"This movie is amazing!\",\n    \"I hated every minute of it.\",\n    \"The service was excellent.\",\n    \"I will never return here.\"\n]\n\nlabels = [1, 0, 1, 0]   # 1 — позитив, 0 — негатив\n\n\n# ============================================================\n# 4. Генерация эмбеддингов\n# ============================================================\nX = berta_embed(texts)\nprint(\"Размер эмбеддингов:\", X.shape)   # (N, 768)\n\n\n# ============================================================\n# 5. Обучение CatBoostClassifier\n# ============================================================\nmodel = CatBoostClassifier(\n    iterations=300,\n    learning_rate=0.05,\n    depth=6,\n    loss_function=\"Logloss\",\n    verbose=50\n)\n\nmodel.fit(X, labels)\n\n\n# ============================================================\n# 6. Применение модели\n# ============================================================\ntest_texts = [\n    \"Fantastic product!\",\n    \"Terrible experience.\"\n]\n\nX_test = berta_embed(test_texts)\npreds = model.predict_proba(X_test)\n\nprint(\"Предсказания:\")\nprint(preds)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tunning и объединение с TF-IDF признаками","metadata":{}},{"cell_type":"code","source":"\"\"\"\nПолный скрипт: Fine-tune BERT и RoBERTa, извлечение эмбеддингов,\nобъединение с TF-IDF признаками (feature stacking) и обучение CatBoost.\n\nЗависимости:\n    pip install transformers datasets torch scikit-learn catboost tqdm\n\nИспользование:\n    - Подготовь CSV с колонками: 'text' и 'label' или передай списки texts/labels\n    - Настрой параметры в разделе CONFIG\n\nСкрипт выполняет для каждой модели (BERT и RoBERTa):\n    1) Fine-tune модели на train/val\n    2) Извлекает эмбеддинги (mean-pooling по токенам)\n    3) Строит TF-IDF и уменьшает размерность (TruncatedSVD)\n    4) Конкатенирует эмбеддинги + TF-IDF\n    5) Обучает CatBoost на получённых признаках\n\nФайл сохраняет: fine-tuned модели и CatBoost-классификаторы.\n\"\"\"\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom typing import List, Tuple\n\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n)\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom tqdm import tqdm\n\n\n# ========== CONFIG ==========\nRANDOM_SEED = 42\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMODELS = {\n    'bert': 'bert-base-uncased',\n    'berta': 'roberta-base'  # \"BERTA\" == RoBERTa в этом скрипте\n}\nMAX_LENGTH = 256\nBATCH_SIZE = 16\nEPOCHS = 3\nTFIDF_MAX_FEATURES = 20000  # для векторизатора\nSVD_COMPONENTS = 200  # снизим размер TF-IDF до этих компонент\nCATBOOST_ITERS = 500\nOUTPUT_DIR = './outputs_bert_roberta'\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n# ============================\n\n\ndef set_seed(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n\nset_seed(RANDOM_SEED)\n\n\n# ------------------ Data helpers ------------------\n\ndef load_csv(path: str, text_col: str = 'text', label_col: str = 'label') -> Tuple[List[str], List[int]]:\n    df = pd.read_csv(path)\n    texts = df[text_col].astype(str).tolist()\n    labels = df[label_col].astype(int).tolist()\n    return texts, labels\n\n\n# ------------------ Fine-tune (Trainer) ------------------\nclass HFDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\n    def __getitem__(self, idx):\n        item = {k: v[idx] for k, v in self.encodings.items()}\n        if self.labels is not None:\n            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n\n\ndef fine_tune_model(model_name: str,\n                    train_texts: List[str],\n                    train_labels: List[int],\n                    val_texts: List[str],\n                    val_labels: List[int],\n                    output_dir: str,\n                    epochs: int = EPOCHS,\n                    batch_size: int = BATCH_SIZE) -> str:\n    \"\"\"\n    Fine-tune AutoModelForSequenceClassification и сохраняет модель в output_dir.\n    Возвращает путь к сохранённой модели.\n    \"\"\"\n    print(f\"Fine-tuning {model_name} on device {DEVICE}...\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(set(train_labels)))\n\n    # Токенизация\n    train_enc = tokenizer(train_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n    val_enc = tokenizer(val_texts, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n\n    train_dataset = HFDataset(train_enc, train_labels)\n    val_dataset = HFDataset(val_enc, val_labels)\n\n    training_args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        evaluation_strategy='epoch',\n        save_strategy='epoch',\n        logging_strategy='epoch',\n        learning_rate=2e-5,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model='accuracy',\n        push_to_hub=False,\n        fp16=torch.cuda.is_available(),\n        seed=RANDOM_SEED,\n    )\n\n    # Простая метрика accuracy\n    def compute_metrics(p):\n        preds = np.argmax(p.predictions, axis=1)\n        labels = p.label_ids\n        acc = (preds == labels).mean()\n        return {'accuracy': acc}\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n\n    save_path = os.path.join(output_dir, model_name.replace('/', '_'))\n    trainer.save_model(save_path)\n    print(f\"Saved fine-tuned model to {save_path}\")\n    return save_path\n\n\n# ------------------ Embedding extraction ------------------\n\ndef mean_pooling(last_hidden_state: torch.Tensor, attention_mask: torch.Tensor) -> np.ndarray:\n    \"\"\"Mean pooling over tokens, учитывая attention mask. Возвращает numpy массив.\n    last_hidden_state: (batch, seq_len, hidden)\n    attention_mask: (batch, seq_len)\n    \"\"\"\n    input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n    summed = torch.sum(last_hidden_state * input_mask_expanded, 1)\n    summed_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    mean_pooled = summed / summed_mask\n    return mean_pooled.cpu().numpy()\n\n\ndef extract_embeddings(model_dir: str, model_name: str, texts: List[str], batch_size: int = BATCH_SIZE) -> np.ndarray:\n    \"\"\"\n    Загружает fine-tuned базовую модель (AutoModel) и извлекает эмбеддинги mean-pooling.\n    model_dir: путь к сохранённой fine-tuned модели (Trainer.save_model path)\n    model_name: имя токенизатора/архитектуры (для tokenizer)\n    \"\"\"\n    print(f\"Extracting embeddings from {model_dir}... device={DEVICE}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    # Загружаем модель базовую часть из сохранённого каталога (если сохранена full модель, AutoModel.from_pretrained возьмёт base)\n    model = AutoModel.from_pretrained(model_dir)\n    model.to(DEVICE)\n    model.eval()\n\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        enc = tokenizer(batch, truncation=True, padding=True, max_length=MAX_LENGTH, return_tensors='pt')\n        input_ids = enc['input_ids'].to(DEVICE)\n        attention_mask = enc['attention_mask'].to(DEVICE)\n\n        with torch.no_grad():\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n            last_hidden = outputs.last_hidden_state  # (batch, seq_len, hidden)\n            emb = mean_pooling(last_hidden, attention_mask)\n            embeddings.append(emb)\n\n    embeddings = np.vstack(embeddings)\n    print(f\"Embeddings shape: {embeddings.shape}\")\n    return embeddings\n\n\n# ------------------ TF-IDF + SVD ------------------\n\ndef build_tfidf_svd(train_texts: List[str], all_texts: List[str], max_features: int = TFIDF_MAX_FEATURES,\n                    n_components: int = SVD_COMPONENTS):\n    \"\"\"\n    Fit TF-IDF на train_texts, и TruncatedSVD на TF-IDF матрицу всех текстов.\n    Возвращает vectorizer, svd и преобразованные матрицы для all_texts.\n    \"\"\"\n    print(\"Fitting TF-IDF and SVD...\")\n    tfidf = TfidfVectorizer(max_features=max_features, ngram_range=(1,2))\n    tfidf_train = tfidf.fit_transform(train_texts)\n\n    # Применяем TF-IDF ко всем текстам\n    tfidf_all = tfidf.transform(all_texts)\n\n    svd = TruncatedSVD(n_components=n_components, random_state=RANDOM_SEED)\n    svd_all = svd.fit_transform(tfidf_all)\n\n    print(f\"TF-IDF -> SVD transformed shape: {svd_all.shape}\")\n    return tfidf, svd, svd_all\n\n\n# ------------------ Обучение CatBoost на фичах ------------------\n\ndef train_catboost(X_train: np.ndarray, y_train: List[int], X_val: np.ndarray, y_val: List[int], save_path: str):\n    model = CatBoostClassifier(\n        iterations=CATBOOST_ITERS,\n        learning_rate=0.05,\n        depth=6,\n        loss_function='Logloss',\n        eval_metric='Accuracy',\n        verbose=50,\n        random_seed=RANDOM_SEED,\n    )\n\n    model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n    model.save_model(save_path)\n    print(f\"Saved CatBoost model to {save_path}\")\n    return model\n\n\n# ------------------ Полный пайплайн для одной модели ------------------\n\ndef run_pipeline_for_model(model_key: str,\n                           model_name: str,\n                           texts: List[str],\n                           labels: List[int],\n                           output_dir: str = OUTPUT_DIR):\n    \"\"\"\n    Выполняет: split -> fine-tune -> extract embeddings -> TF-IDF+SVD -> concat -> CatBoost\n    Сохраняет fine-tuned модель и CatBoost модель.\n    \"\"\"\n    print('='*40)\n    print(f\"RUN pipeline for {model_key} ({model_name})\")\n\n    # split\n    X_train_texts, X_temp_texts, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=RANDOM_SEED, stratify=labels)\n    X_val_texts, X_test_texts, y_val, y_test = train_test_split(X_temp_texts, y_temp, test_size=0.5, random_state=RANDOM_SEED, stratify=y_temp)\n\n    # 1) Fine-tune\n    model_save_dir = fine_tune_model(model_name, X_train_texts, y_train, X_val_texts, y_val, output_dir=os.path.join(output_dir, 'fine_tuned'), epochs=EPOCHS, batch_size=BATCH_SIZE)\n\n    # 2) Extract embeddings for train/val/test using fine-tuned base model\n    emb_train = extract_embeddings(model_save_dir, model_name, X_train_texts)\n    emb_val = extract_embeddings(model_save_dir, model_name, X_val_texts)\n    emb_test = extract_embeddings(model_save_dir, model_name, X_test_texts)\n\n    # 3) TF-IDF on train -> SVD on all (train+val+test)\n    all_texts = X_train_texts + X_val_texts + X_test_texts\n    _, svd, svd_all = build_tfidf_svd(X_train_texts, all_texts, max_features=TFIDF_MAX_FEATURES, n_components=SVD_COMPONENTS)\n\n    # Разбиваем svd_all обратно на train/val/test\n    n_train = len(X_train_texts)\n    n_val = len(X_val_texts)\n    svd_train = svd_all[:n_train]\n    svd_val = svd_all[n_train:n_train + n_val]\n    svd_test = svd_all[n_train + n_val:]\n\n    # 4) Конкатенируем эмбеддинги + svd признаки\n    X_train_stack = np.hstack([emb_train, svd_train])\n    X_val_stack = np.hstack([emb_val, svd_val])\n    X_test_stack = np.hstack([emb_test, svd_test])\n\n    print(f\"Stacked feature shapes: train={X_train_stack.shape}, val={X_val_stack.shape}, test={X_test_stack.shape}\")\n\n    # 5) Обучаем CatBoost\n    cb_save_path = os.path.join(output_dir, f'catboost_{model_key}.cbm')\n    cb_model = train_catboost(X_train_stack, y_train, X_val_stack, y_val, cb_save_path)\n\n    # Оцениваем на test\n    test_preds = cb_model.predict(X_test_stack)\n    test_proba = cb_model.predict_proba(X_test_stack)\n    test_acc = (test_preds == np.array(y_test)).mean()\n    print(f\"Test accuracy for {model_key}: {test_acc:.4f}\")\n\n    # Сохраняем дополнительно артефакты\n    np.save(os.path.join(output_dir, f'emb_train_{model_key}.npy'), emb_train)\n    np.save(os.path.join(output_dir, f'emb_val_{model_key}.npy'), emb_val)\n    np.save(os.path.join(output_dir, f'emb_test_{model_key}.npy'), emb_test)\n\n    return {\n        'model_key': model_key,\n        'fine_tuned_dir': model_save_dir,\n        'catboost_path': cb_save_path,\n        'test_accuracy': test_acc,\n    }\n\n\n# ------------------ Пример вызова ------------------\nif __name__ == '__main__':\n    # Пример: загрузка данных из CSV. Замените путь на ваш датасет.\n    DATA_CSV = 'data.csv'  # <-- замените\n\n    if not os.path.exists(DATA_CSV):\n        print(f\"Файл {DATA_CSV} не найден. Создайте CSV с колонками 'text' и 'label' или измените путь.\")\n        # Для демонстрации создаём toy-датасет\n        texts = [\n            \"This movie is amazing.\",\n            \"I hated the film, it was terrible.\",\n            \"Fantastic acting and story.\",\n            \"Worst experience ever.\",\n            \"I would recommend it to my friends.\",\n            \"Not worth the time.\",\n            \"Absolutely loved it!\",\n            \"I will never watch it again.\"\n        ]\n        labels = [1, 0, 1, 0, 1, 0, 1, 0]\n    else:\n        texts, labels = load_csv(DATA_CSV, text_col='text', label_col='label')\n\n    results = {}\n    for key, name in MODELS.items():\n        res = run_pipeline_for_model(key, name, texts, labels, output_dir=OUTPUT_DIR)\n        results[key] = res\n\n    print('\\nALL RESULTS:')\n    print(results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}