{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6447697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f65e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train = train.sample(frac=0.4, random_state=42)\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3174431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet('items.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c303e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>leftItemId</th>\n",
       "      <th>rightItemId</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20987</th>\n",
       "      <td>4646476163753910898</td>\n",
       "      <td>5123122680727853126</td>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7077</th>\n",
       "      <td>179602477240964301</td>\n",
       "      <td>13519372438923722859</td>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23064</th>\n",
       "      <td>5413851602206336754</td>\n",
       "      <td>5343826051963097072</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36501</th>\n",
       "      <td>12591732453231246389</td>\n",
       "      <td>12597193763191104739</td>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126719</th>\n",
       "      <td>13542769079320751337</td>\n",
       "      <td>886550916145567852</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205708</th>\n",
       "      <td>12601449588782387507</td>\n",
       "      <td>10167081963908444521</td>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12918</th>\n",
       "      <td>3755409024178116673</td>\n",
       "      <td>6996526768143679659</td>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177852</th>\n",
       "      <td>12983183246897348976</td>\n",
       "      <td>16972574557361610135</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7066</th>\n",
       "      <td>16839436918394481486</td>\n",
       "      <td>12534343399457166925</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189638</th>\n",
       "      <td>6167327406216826512</td>\n",
       "      <td>4408688326235457244</td>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>84557 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  leftItemId           rightItemId          target\n",
       "20987    4646476163753910898   5123122680727853126  relevant_minus\n",
       "7077      179602477240964301  13519372438923722859  relevant_minus\n",
       "23064    5413851602206336754   5343826051963097072        relevant\n",
       "36501   12591732453231246389  12597193763191104739  relevant_minus\n",
       "126719  13542769079320751337    886550916145567852        relevant\n",
       "...                      ...                   ...             ...\n",
       "205708  12601449588782387507  10167081963908444521     no_relevant\n",
       "12918    3755409024178116673   6996526768143679659     no_relevant\n",
       "177852  12983183246897348976  16972574557361610135        relevant\n",
       "7066    16839436918394481486  12534343399457166925        relevant\n",
       "189638   6167327406216826512   4408688326235457244  relevant_minus\n",
       "\n",
       "[84557 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd00401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge для left\n",
    "train = train.merge(\n",
    "    data.add_suffix(\"_left\").rename(columns={\"itemId_left\": \"itemId\"}),\n",
    "    left_on=\"leftItemId\",\n",
    "    right_on=\"itemId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# merge для right\n",
    "train = train.merge(\n",
    "    data.add_suffix(\"_right\").rename(columns={\"itemId_right\": \"itemId\"}),\n",
    "    left_on=\"rightItemId\",\n",
    "    right_on=\"itemId\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ff10853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=['leftItemId', 'rightItemId', 'itemId_x', 'itemId_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bcb789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(\n",
    "    columns=['authorId_left', 'authorId_right'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9857c096",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['target'].isin(['relevant_plus', 'relevant', 'relevant_minus', 'no_relevant'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15b59024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train.drop(columns=['target']),\n",
    "    train['target'],\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc9cf84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "relevant_minus    24023\n",
       "relevant          23052\n",
       "no_relevant       13143\n",
       "relevant_plus      6497\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4598fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=-1)\n",
    "    return {\n",
    "        \"weighted_f1\": f1_score(labels, preds, average=\"weighted\")\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# используйте эти параметры обучения\n",
    "training_args = TrainingArguments(\n",
    "    # Основные параметры\n",
    "    output_dir='./intfloat',  # Директория для сохранения\n",
    "    \n",
    "    # Параметры обучения\n",
    "    num_train_epochs=1,                     # Количество эпох\n",
    "    per_device_train_batch_size=1,         # Размер батча для обучения\n",
    "    per_device_eval_batch_size=16,          # Размер батча для валидации\n",
    "    learning_rate=1e-5,                     # Learning rate\n",
    "    warmup_ratio = 0.1,                     # 10% от общего числа шагов для вармапа или warmup_steps = int(0.1 * total_training_steps)\n",
    "    lr_scheduler_type = 'cosine',           # Можете посмотреть на них в \n",
    "                                            # https://www.kaggle.com/code/snnclsr/learning-rate-schedulers \n",
    "                                            # соответсвующий ему будет get_cosine_schedule_with_warmup\n",
    "    gradient_accumulation_steps=32,\n",
    "    # Сохранение и логирование\n",
    "    logging_dir='./logs',                   # Директория для логов\n",
    "    logging_steps=20,                      # Частота логирования\n",
    "    save_steps=200,                         # Частота сохранения\n",
    "    save_total_limit=2,                     # Максимум чекпоинтов\n",
    "    save_strategy='steps',                  # Стратегия сохранения\n",
    "    \n",
    "    # Валидация\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=200,            # Стратегия валидации\n",
    "    load_best_model_at_end=True,            # Загружать лучшую модель\n",
    "    metric_for_best_model='weighted_f1',             # Метрика для выбора лучшей\n",
    "    greater_is_better=True,                 # Больше значение = лучше\n",
    "    # воспроизводимость\n",
    "    seed=42,                                # Seed для воспроизводимости\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bde0c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-large-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4faa835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9bfa7997bce425ea3269c90a10618bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee07bf621f04695988970b25bec7134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, ClassLabel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"right_title\": X_train['title_right'].tolist(), \"left_title\": X_train['title_left'].tolist(), \"label\": y_train.tolist()})\n",
    "val_dataset  = Dataset.from_dict({\"right_title\": X_val['title_right'].tolist(), \"left_title\": X_val['title_left'].tolist(), \"label\": y_val.tolist()})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "def preprocess(examples):\n",
    "    return tokenizer(\n",
    "        text=examples[\"right_title\"],\n",
    "        text_pair=examples[\"left_title\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=200\n",
    "    )\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0973907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f51fd7fb8ce413db3cd969d4871bc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/66715 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3054d1deb5a24f8bbf02147c88cd169d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16679 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ClassLabel(names=[\"relevant_plus\", \"relevant\", \"relevant_minus\", \"no_relevant\"])\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x: {\"label\": labels.str2int(x[\"label\"])})\n",
    "val_dataset   = val_dataset.map(lambda x: {\"label\": labels.str2int(x[\"label\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3861793",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.remove_columns([\"right_title\", \"left_title\"])\n",
    "val_dataset   = val_dataset.remove_columns([\"right_title\", \"left_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e3a198a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 1,\n",
       " 'input_ids': [0,\n",
       "  4805,\n",
       "  90520,\n",
       "  134354,\n",
       "  4476,\n",
       "  419,\n",
       "  77,\n",
       "  118763,\n",
       "  4,\n",
       "  252,\n",
       "  1203,\n",
       "  23590,\n",
       "  87152,\n",
       "  114299,\n",
       "  35,\n",
       "  16721,\n",
       "  40519,\n",
       "  32,\n",
       "  6,\n",
       "  89077,\n",
       "  155031,\n",
       "  44333,\n",
       "  10090,\n",
       "  10573,\n",
       "  743,\n",
       "  46,\n",
       "  56756,\n",
       "  227,\n",
       "  35,\n",
       "  127353,\n",
       "  227,\n",
       "  5,\n",
       "  2,\n",
       "  2,\n",
       "  2443,\n",
       "  20351,\n",
       "  194682,\n",
       "  49,\n",
       "  77005,\n",
       "  19448,\n",
       "  12,\n",
       "  121124,\n",
       "  1794,\n",
       "  103,\n",
       "  35397,\n",
       "  4,\n",
       "  38511,\n",
       "  9526,\n",
       "  245,\n",
       "  19789,\n",
       "  32889,\n",
       "  35,\n",
       "  121592,\n",
       "  29,\n",
       "  117037,\n",
       "  53173,\n",
       "  105,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2b97d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at intfloat/multilingual-e5-large-instruct and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import os\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=4,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fe2f4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91ee2a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d216d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2084' max='2084' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2084/2084 1:34:46, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.228300</td>\n",
       "      <td>1.177123</td>\n",
       "      <td>0.358746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.085300</td>\n",
       "      <td>1.080440</td>\n",
       "      <td>0.506529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.063200</td>\n",
       "      <td>1.040879</td>\n",
       "      <td>0.529878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.128800</td>\n",
       "      <td>1.036119</td>\n",
       "      <td>0.534159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.021700</td>\n",
       "      <td>1.050485</td>\n",
       "      <td>0.529744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.042900</td>\n",
       "      <td>1.048480</td>\n",
       "      <td>0.532914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>1.027639</td>\n",
       "      <td>0.540098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.039500</td>\n",
       "      <td>1.012860</td>\n",
       "      <td>0.545442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.035800</td>\n",
       "      <td>1.018664</td>\n",
       "      <td>0.544619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.005800</td>\n",
       "      <td>1.015321</td>\n",
       "      <td>0.545605</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2084, training_loss=1.0685231171963083, metrics={'train_runtime': 5688.5981, 'train_samples_per_second': 11.728, 'train_steps_per_second': 0.366, 'total_flos': 2.42769760671744e+16, 'train_loss': 1.0685231171963083, 'epoch': 0.9995952934122762})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6858725",
   "metadata": {},
   "source": [
    "Эксперименты \n",
    "\n",
    "sergeyzh/BERTA - 0.41\n",
    "\n",
    "BAAI/bge-m3 - 0.48\n",
    "\n",
    "intfloat/multilingual-e5-large-instruct - 0.51\n",
    "\n",
    "\n",
    "sentence-transformers/paraphrase-multilingual-mpnet-base-v2 - 0.39\n",
    "\n",
    "\n",
    "sergeyzh/BERTA - 0.2 датасета 1e-5 4 bs 4 ga - 0.51\n",
    "\n",
    "\n",
    "sergeyzh/BERTA - 0.4 датасета 1e-5 32 bs - 0.517\n",
    "\n",
    "sergeyzh/BERTA - 0.8 датасета 1e-5 32 bs - 0.53\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd3bbe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4af887be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge для left\n",
    "test = test.merge(\n",
    "    data.add_suffix(\"_left\").rename(columns={\"itemId_left\": \"itemId\"}),\n",
    "    left_on=\"leftItemId\",\n",
    "    right_on=\"itemId\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# merge для right\n",
    "test = test.merge(\n",
    "    data.add_suffix(\"_right\").rename(columns={\"itemId_right\": \"itemId\"}),\n",
    "    left_on=\"rightItemId\",\n",
    "    right_on=\"itemId\",\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0584b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(columns=['leftItemId', 'rightItemId', 'itemId_x', 'itemId_y'], inplace=True)\n",
    "test.drop(\n",
    "    columns=['authorId_left', 'authorId_right', 'content_right', 'content_left'],\n",
    "    inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ad737bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "test = Dataset.from_dict({\"right_title\": test['title_right'].tolist(), \"left_title\": test['title_left'].tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff2a84ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a1677a83b44cb08e065a973a053933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51636 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_test(examples):\n",
    "    return tokenizer(\n",
    "        text=examples[\"right_title\"],\n",
    "        text_pair=examples[\"left_title\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=200\n",
    "    )\n",
    "test_dataset = test.map(preprocess_test, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a959487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset.remove_columns([\"right_title\", \"left_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1588258c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint_dir = \"intfloat/checkpoint-2084\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd8f405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b8d86c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.model.eval()  # ставим модель в eval-режим (отключает dropout и т.п.)\n",
    "predictions = trainer.predict(test_dataset)\n",
    "logits = predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e950778",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01021bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant_plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51631</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51632</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51633</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51634</th>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51635</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51636 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "0            relevant\n",
       "1            relevant\n",
       "2       relevant_plus\n",
       "3         no_relevant\n",
       "4      relevant_minus\n",
       "...               ...\n",
       "51631  relevant_minus\n",
       "51632        relevant\n",
       "51633        relevant\n",
       "51634     no_relevant\n",
       "51635        relevant\n",
       "\n",
       "[51636 rows x 1 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(preds, columns=['target'])\n",
    "predictions['target'] = predictions['target'].map(lambda x: labels.int2str(x))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67258f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions_intfloat.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f2e43",
   "metadata": {},
   "source": [
    "**АНСАМБЛИРУЕМ ДВЕ МОДЕЛИ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4325e24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "berta_logits = np.load(\"logits_BERTA.npy\")\n",
    "intfloat_logits = np.load(\"logits_intfloat.npy\")\n",
    "logits = berta_logits * 0.4 + intfloat_logits * 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ebc83102",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbe2059f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant_plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51631</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51632</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51633</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51634</th>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51635</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51636 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "0            relevant\n",
       "1      relevant_minus\n",
       "2       relevant_plus\n",
       "3         no_relevant\n",
       "4      relevant_minus\n",
       "...               ...\n",
       "51631  relevant_minus\n",
       "51632        relevant\n",
       "51633        relevant\n",
       "51634     no_relevant\n",
       "51635        relevant\n",
       "\n",
       "[51636 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(preds, columns=['target'])\n",
    "predictions['target'] = predictions['target'].map(lambda x: labels.int2str(x))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e357f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('predictions_ansamble_04.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9f6fd9",
   "metadata": {},
   "source": [
    "**КАТБУСТ НА ЭМБЕДДИНГАХ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9eaea902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(55083, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import default_data_collator\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # выключаем dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89aa131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=default_data_collator)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=default_data_collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, collate_fn=default_data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fee8c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = []\n",
    "train_embeddings = []\n",
    "val_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa6352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        # если датасет HuggingFace, у него обычно есть input_ids, attention_mask, ...\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]}\n",
    "\n",
    "        # Получаем все скрытые состояния\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "        # Берём последний слой ([-1]) -> (batch, seq_len, 768)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "        # Пуллим в один вектор (например CLS-токен)\n",
    "        cls_emb = last_hidden[:, 0, :]   # (batch, 768)\n",
    "        test_embeddings.append(cls_emb.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "724317d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4170/4170 [08:39<00:00,  8.03it/s]\n",
      "100%|██████████| 1043/1043 [02:08<00:00,  8.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(train_loader):\n",
    "        # если датасет HuggingFace, у него обычно есть input_ids, attention_mask, ...\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]}\n",
    "\n",
    "        # Получаем все скрытые состояния\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "        # Берём последний слой ([-1]) -> (batch, seq_len, 768)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "        # Пуллим в один вектор (например CLS-токен)\n",
    "        cls_emb = last_hidden[:, 0, :]   # (batch, 768)\n",
    "        train_embeddings.append(cls_emb.cpu().numpy())\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader):\n",
    "        # если датасет HuggingFace, у него обычно есть input_ids, attention_mask, ...\n",
    "        inputs = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\",\"attention_mask\",\"token_type_ids\"]}\n",
    "\n",
    "        # Получаем все скрытые состояния\n",
    "        outputs = model.bert(**inputs, output_hidden_states=True, return_dict=True)\n",
    "\n",
    "        # Берём последний слой ([-1]) -> (batch, seq_len, 768)\n",
    "        last_hidden = outputs.hidden_states[-1]\n",
    "\n",
    "        # Пуллим в один вектор (например CLS-токен)\n",
    "        cls_emb = last_hidden[:, 0, :]   # (batch, 768)\n",
    "        val_embeddings.append(cls_emb.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4f5a07f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = np.concatenate(train_embeddings, axis=0)\n",
    "val_embeddings = np.concatenate(val_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b3b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: (51636, 768)\n"
     ]
    }
   ],
   "source": [
    "# Объединяем и сохраняем\n",
    "test_embeddings = np.concatenate(test_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8e480d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = pd.DataFrame(train_embeddings)\n",
    "X_val_processed = pd.DataFrame(val_embeddings)\n",
    "X_test_processed = pd.DataFrame(test_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d882831a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c6ae438b7644c3a052bca657bbf7b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 1387.898438 Total: 12281.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.4922321\ttest: 0.4831985\tbest: 0.4831985 (0)\ttotal: 38ms\tremaining: 3m 9s\n",
      "100:\tlearn: 0.5414123\ttest: 0.5251432\tbest: 0.5251432 (100)\ttotal: 2.41s\tremaining: 1m 56s\n",
      "200:\tlearn: 0.5472344\ttest: 0.5268458\tbest: 0.5277029 (181)\ttotal: 4.68s\tremaining: 1m 51s\n",
      "300:\tlearn: 0.5522035\ttest: 0.5278320\tbest: 0.5282948 (250)\ttotal: 6.89s\tremaining: 1m 47s\n",
      "400:\tlearn: 0.5558517\ttest: 0.5273309\tbest: 0.5282948 (250)\ttotal: 9.04s\tremaining: 1m 43s\n",
      "bestTest = 0.5282948337\n",
      "bestIteration = 250\n",
      "Shrink model to first 251 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x264554cb5d0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "train_pool = Pool(X_train_processed, label=y_train)\n",
    "val_pool = Pool(X_val_processed, label=y_val)\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    iterations=5000,\n",
    "    depth=5,\n",
    "    learning_rate=0.05,\n",
    "    early_stopping_rounds=200,\n",
    "    loss_function=\"MultiClass\",\n",
    "    eval_metric=\"TotalF1:average=Weighted\",\n",
    "    verbose=100,\n",
    "    random_seed=42,\n",
    "    task_type=\"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    ")\n",
    "\n",
    "catboost_model.fit(train_pool, eval_set=val_pool, use_best_model=True, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "715cbb63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0485918 , 0.5035331 , 0.38963356, 0.05824154],\n",
       "       [0.04530167, 0.43056493, 0.48583563, 0.03829777],\n",
       "       [0.00377576, 0.16880043, 0.02049642, 0.80692739],\n",
       "       ...,\n",
       "       [0.04653209, 0.43652009, 0.47987317, 0.03707466],\n",
       "       [0.27982797, 0.18537426, 0.5140263 , 0.02077147],\n",
       "       [0.02541443, 0.58608656, 0.24048502, 0.148014  ]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = catboost_model.predict_proba(X_test_processed)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d2fa406",
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTA_logits = np.load('logits_BERTA.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcf6428c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.54381147,  0.80700903,  0.76759381, -0.44102645],\n",
       "       [-0.71404672,  0.77461168,  0.92037181, -0.67592782],\n",
       "       [ 1.31953462,  0.56312398, -0.56972424, -1.00657048],\n",
       "       ...,\n",
       "       [-0.51949075,  0.86156499,  0.82769739, -0.64948548],\n",
       "       [-0.97698074,  0.24418196,  0.88159854,  0.443408  ],\n",
       "       [ 0.11198893,  0.93156876,  0.37651033, -0.94059185]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_logits = (BERTA_logits + logits) / 2\n",
    "mean_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c996354e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_predictions = np.argmax(mean_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1e70290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant_plus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no_relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51631</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51632</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51633</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51634</th>\n",
       "      <td>relevant_minus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51635</th>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51636 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               target\n",
       "0            relevant\n",
       "1      relevant_minus\n",
       "2       relevant_plus\n",
       "3         no_relevant\n",
       "4      relevant_minus\n",
       "...               ...\n",
       "51631  relevant_minus\n",
       "51632        relevant\n",
       "51633        relevant\n",
       "51634  relevant_minus\n",
       "51635        relevant\n",
       "\n",
       "[51636 rows x 1 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pd.DataFrame(mean_predictions, columns=['target'])\n",
    "predictions['target'] = predictions['target'].map(lambda x: labels.int2str(x))\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0755f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('mean_predictions_BERTA_CatBoost.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
