{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2433d0f1-8024-4b7f-bbef-af55252c74de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys,subprocess\n",
    "pkgs=[\"autogluon.tabular==1.4.0\",\"catboost==1.2.8\",\"tqdm\"]\n",
    "subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",\"-q\"]+pkgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef10c8b5-1721-46fb-9df3-8ef0d8c52171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a679f445-f1a1-439d-98e7-c8d9d4c6fa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "from catboost import CatBoostRegressor,Pool\n",
    "from catboost.utils import get_gpu_device_count\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(\".\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "275b3318-8eb5-4a9e-88e8-849c3e610a77",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 74 fields in line 4, saw 75\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train1 = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m test2 = pd.read_csv(path/\u001b[33m\"\u001b[39m\u001b[33mtest.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/venv/main/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: Expected 74 fields in line 4, saw 75\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(path/\"train.csv\")\n",
    "test = pd.read_csv(path/\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2082ea7a-5081-49d0-81e4-26449b5801fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'name', 'coordinates', 'category', 'address', 'target',\n",
       "       'traffic_300m', 'homes_300m', 'works_300m', 'female_300m',\n",
       "       ...\n",
       "       'doramas_1000m', 'computer_components_1000m', 'humor_1000m',\n",
       "       'car_market_1000m', 'no_higher_education_1000m',\n",
       "       'goods_for_moms_and_babies_1000m', 'age_25-34_1000m', 'male_1000m',\n",
       "       'phone_repair_1000m', 'mean_income_1000m'],\n",
       "      dtype='object', length=286)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0500cdf-a01f-4afa-b81c-57096632d45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>category</th>\n",
       "      <th>address</th>\n",
       "      <th>traffic_300m</th>\n",
       "      <th>homes_300m</th>\n",
       "      <th>works_300m</th>\n",
       "      <th>female_300m</th>\n",
       "      <th>train_ticket_order_300m</th>\n",
       "      <th>...</th>\n",
       "      <th>doramas_1000m</th>\n",
       "      <th>computer_components_1000m</th>\n",
       "      <th>humor_1000m</th>\n",
       "      <th>car_market_1000m</th>\n",
       "      <th>no_higher_education_1000m</th>\n",
       "      <th>goods_for_moms_and_babies_1000m</th>\n",
       "      <th>age_25-34_1000m</th>\n",
       "      <th>male_1000m</th>\n",
       "      <th>phone_repair_1000m</th>\n",
       "      <th>mean_income_1000m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>Счастье</td>\n",
       "      <td>[37.533334, 55.790246]</td>\n",
       "      <td>candy_shop</td>\n",
       "      <td>Ходынский бул., 4, Москва</td>\n",
       "      <td>62672</td>\n",
       "      <td>4709.110524</td>\n",
       "      <td>4298.125296</td>\n",
       "      <td>38987.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1706.0</td>\n",
       "      <td>5764.0</td>\n",
       "      <td>32190.0</td>\n",
       "      <td>12370.0</td>\n",
       "      <td>265916.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>226914.0</td>\n",
       "      <td>399582.0</td>\n",
       "      <td>2255.0</td>\n",
       "      <td>122883.795473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>O'STIN</td>\n",
       "      <td>[37.886829, 55.751627]</td>\n",
       "      <td>baby_clothes</td>\n",
       "      <td>Носовихинское ш., 45, Реутов</td>\n",
       "      <td>110226</td>\n",
       "      <td>12987.989255</td>\n",
       "      <td>15235.256665</td>\n",
       "      <td>96081.0</td>\n",
       "      <td>1346.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1266.0</td>\n",
       "      <td>3930.0</td>\n",
       "      <td>17037.0</td>\n",
       "      <td>9044.0</td>\n",
       "      <td>233487.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>214465.0</td>\n",
       "      <td>320022.0</td>\n",
       "      <td>1801.0</td>\n",
       "      <td>113878.735454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>Дровосек</td>\n",
       "      <td>[37.474419, 55.863549]</td>\n",
       "      <td>barbershop</td>\n",
       "      <td>Беломорская ул., 18А, корп. 2, Москва</td>\n",
       "      <td>81080</td>\n",
       "      <td>9575.248571</td>\n",
       "      <td>9463.322898</td>\n",
       "      <td>57147.0</td>\n",
       "      <td>1506.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1915.0</td>\n",
       "      <td>6549.0</td>\n",
       "      <td>32133.0</td>\n",
       "      <td>13745.0</td>\n",
       "      <td>298014.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>253803.0</td>\n",
       "      <td>431408.0</td>\n",
       "      <td>2426.0</td>\n",
       "      <td>119179.257929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>Милти</td>\n",
       "      <td>[37.532347, 55.607307]</td>\n",
       "      <td>cooking</td>\n",
       "      <td>Новоясеневский просп., 7, Москва</td>\n",
       "      <td>131046</td>\n",
       "      <td>21152.067433</td>\n",
       "      <td>22155.559230</td>\n",
       "      <td>113526.0</td>\n",
       "      <td>1777.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2345.0</td>\n",
       "      <td>5683.0</td>\n",
       "      <td>26201.0</td>\n",
       "      <td>12758.0</td>\n",
       "      <td>318360.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>245132.0</td>\n",
       "      <td>427442.0</td>\n",
       "      <td>2053.0</td>\n",
       "      <td>110069.641621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>Белая лебедь</td>\n",
       "      <td>[37.559468, 55.863468]</td>\n",
       "      <td>laundry</td>\n",
       "      <td>Бескудниковский бул., 13, Москва</td>\n",
       "      <td>87605</td>\n",
       "      <td>18499.759458</td>\n",
       "      <td>18868.131714</td>\n",
       "      <td>68314.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2868.0</td>\n",
       "      <td>8647.0</td>\n",
       "      <td>36731.0</td>\n",
       "      <td>17519.0</td>\n",
       "      <td>465743.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>409782.0</td>\n",
       "      <td>657632.0</td>\n",
       "      <td>3121.0</td>\n",
       "      <td>114652.304715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>30097</td>\n",
       "      <td>Uhostels</td>\n",
       "      <td>[37.382774, 55.711967]</td>\n",
       "      <td>hostel</td>\n",
       "      <td>Можайское ш., 165, рабочий посёлок Новоивановское</td>\n",
       "      <td>36378</td>\n",
       "      <td>2968.320703</td>\n",
       "      <td>1891.917847</td>\n",
       "      <td>16430.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>...</td>\n",
       "      <td>780.0</td>\n",
       "      <td>2220.0</td>\n",
       "      <td>11360.0</td>\n",
       "      <td>7518.0</td>\n",
       "      <td>152220.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>141823.0</td>\n",
       "      <td>251806.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>115239.815808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>21993</td>\n",
       "      <td>Детский мир</td>\n",
       "      <td>[37.493409, 55.424235]</td>\n",
       "      <td>toys</td>\n",
       "      <td>Ленинградская ул., 21, Подольск</td>\n",
       "      <td>94187</td>\n",
       "      <td>19244.142246</td>\n",
       "      <td>17823.348711</td>\n",
       "      <td>87563.0</td>\n",
       "      <td>1116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>3917.0</td>\n",
       "      <td>16118.0</td>\n",
       "      <td>10627.0</td>\n",
       "      <td>305390.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>237858.0</td>\n",
       "      <td>391213.0</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>104614.619289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>43919</td>\n",
       "      <td>Ice Curly</td>\n",
       "      <td>[37.670418, 55.780504]</td>\n",
       "      <td>hair_salon</td>\n",
       "      <td>Гаврикова ул., 2/38, Москва</td>\n",
       "      <td>178064</td>\n",
       "      <td>19000.549439</td>\n",
       "      <td>20373.539460</td>\n",
       "      <td>127901.0</td>\n",
       "      <td>3529.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9144.0</td>\n",
       "      <td>21962.0</td>\n",
       "      <td>111717.0</td>\n",
       "      <td>43129.0</td>\n",
       "      <td>1166066.0</td>\n",
       "      <td>544.0</td>\n",
       "      <td>920617.0</td>\n",
       "      <td>1509700.0</td>\n",
       "      <td>8968.0</td>\n",
       "      <td>115478.531317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>46598</td>\n",
       "      <td>Мясницкий ряд</td>\n",
       "      <td>[37.464845, 55.785135]</td>\n",
       "      <td>butcher_shop</td>\n",
       "      <td>ул. Маршала Тухачевского, 49, Москва</td>\n",
       "      <td>49927</td>\n",
       "      <td>11251.957179</td>\n",
       "      <td>9505.878594</td>\n",
       "      <td>36235.0</td>\n",
       "      <td>655.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1868.0</td>\n",
       "      <td>4650.0</td>\n",
       "      <td>23989.0</td>\n",
       "      <td>10899.0</td>\n",
       "      <td>254020.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>209882.0</td>\n",
       "      <td>373401.0</td>\n",
       "      <td>1817.0</td>\n",
       "      <td>117662.523365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9275</th>\n",
       "      <td>29393</td>\n",
       "      <td>Школа плавания № 1</td>\n",
       "      <td>[37.683324, 55.7013]</td>\n",
       "      <td>swimming_pool</td>\n",
       "      <td>ул. Трофимова, 30, корп. 3, Москва</td>\n",
       "      <td>57863</td>\n",
       "      <td>14590.798175</td>\n",
       "      <td>13734.850884</td>\n",
       "      <td>39695.0</td>\n",
       "      <td>646.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2251.0</td>\n",
       "      <td>6694.0</td>\n",
       "      <td>30397.0</td>\n",
       "      <td>16073.0</td>\n",
       "      <td>358765.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>312019.0</td>\n",
       "      <td>520273.0</td>\n",
       "      <td>2791.0</td>\n",
       "      <td>115169.513047</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9276 rows × 285 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                name             coordinates       category  \\\n",
       "0     21472             Счастье  [37.533334, 55.790246]     candy_shop   \n",
       "1      9837              O'STIN  [37.886829, 55.751627]   baby_clothes   \n",
       "2     41791            Дровосек  [37.474419, 55.863549]     barbershop   \n",
       "3     18441               Милти  [37.532347, 55.607307]        cooking   \n",
       "4     49348        Белая лебедь  [37.559468, 55.863468]        laundry   \n",
       "...     ...                 ...                     ...            ...   \n",
       "9271  30097            Uhostels  [37.382774, 55.711967]         hostel   \n",
       "9272  21993         Детский мир  [37.493409, 55.424235]           toys   \n",
       "9273  43919           Ice Curly  [37.670418, 55.780504]     hair_salon   \n",
       "9274  46598       Мясницкий ряд  [37.464845, 55.785135]   butcher_shop   \n",
       "9275  29393  Школа плавания № 1    [37.683324, 55.7013]  swimming_pool   \n",
       "\n",
       "                                                address  traffic_300m  \\\n",
       "0                             Ходынский бул., 4, Москва         62672   \n",
       "1                          Носовихинское ш., 45, Реутов        110226   \n",
       "2                 Беломорская ул., 18А, корп. 2, Москва         81080   \n",
       "3                      Новоясеневский просп., 7, Москва        131046   \n",
       "4                      Бескудниковский бул., 13, Москва         87605   \n",
       "...                                                 ...           ...   \n",
       "9271  Можайское ш., 165, рабочий посёлок Новоивановское         36378   \n",
       "9272                    Ленинградская ул., 21, Подольск         94187   \n",
       "9273                        Гаврикова ул., 2/38, Москва        178064   \n",
       "9274               ул. Маршала Тухачевского, 49, Москва         49927   \n",
       "9275                 ул. Трофимова, 30, корп. 3, Москва         57863   \n",
       "\n",
       "        homes_300m    works_300m  female_300m  train_ticket_order_300m  ...  \\\n",
       "0      4709.110524   4298.125296      38987.0                    961.0  ...   \n",
       "1     12987.989255  15235.256665      96081.0                   1346.0  ...   \n",
       "2      9575.248571   9463.322898      57147.0                   1506.0  ...   \n",
       "3     21152.067433  22155.559230     113526.0                   1777.0  ...   \n",
       "4     18499.759458  18868.131714      68314.0                   1201.0  ...   \n",
       "...            ...           ...          ...                      ...  ...   \n",
       "9271   2968.320703   1891.917847      16430.0                    293.0  ...   \n",
       "9272  19244.142246  17823.348711      87563.0                   1116.0  ...   \n",
       "9273  19000.549439  20373.539460     127901.0                   3529.0  ...   \n",
       "9274  11251.957179   9505.878594      36235.0                    655.0  ...   \n",
       "9275  14590.798175  13734.850884      39695.0                    646.0  ...   \n",
       "\n",
       "      doramas_1000m  computer_components_1000m  humor_1000m  car_market_1000m  \\\n",
       "0            1706.0                     5764.0      32190.0           12370.0   \n",
       "1            1266.0                     3930.0      17037.0            9044.0   \n",
       "2            1915.0                     6549.0      32133.0           13745.0   \n",
       "3            2345.0                     5683.0      26201.0           12758.0   \n",
       "4            2868.0                     8647.0      36731.0           17519.0   \n",
       "...             ...                        ...          ...               ...   \n",
       "9271          780.0                     2220.0      11360.0            7518.0   \n",
       "9272         1616.0                     3917.0      16118.0           10627.0   \n",
       "9273         9144.0                    21962.0     111717.0           43129.0   \n",
       "9274         1868.0                     4650.0      23989.0           10899.0   \n",
       "9275         2251.0                     6694.0      30397.0           16073.0   \n",
       "\n",
       "      no_higher_education_1000m  goods_for_moms_and_babies_1000m  \\\n",
       "0                      265916.0                            221.0   \n",
       "1                      233487.0                            102.0   \n",
       "2                      298014.0                            305.0   \n",
       "3                      318360.0                            233.0   \n",
       "4                      465743.0                            379.0   \n",
       "...                         ...                              ...   \n",
       "9271                   152220.0                             67.0   \n",
       "9272                   305390.0                            176.0   \n",
       "9273                  1166066.0                            544.0   \n",
       "9274                   254020.0                            407.0   \n",
       "9275                   358765.0                            228.0   \n",
       "\n",
       "      age_25-34_1000m  male_1000m  phone_repair_1000m  mean_income_1000m  \n",
       "0            226914.0    399582.0              2255.0      122883.795473  \n",
       "1            214465.0    320022.0              1801.0      113878.735454  \n",
       "2            253803.0    431408.0              2426.0      119179.257929  \n",
       "3            245132.0    427442.0              2053.0      110069.641621  \n",
       "4            409782.0    657632.0              3121.0      114652.304715  \n",
       "...               ...         ...                 ...                ...  \n",
       "9271         141823.0    251806.0              1014.0      115239.815808  \n",
       "9272         237858.0    391213.0              1753.0      104614.619289  \n",
       "9273         920617.0   1509700.0              8968.0      115478.531317  \n",
       "9274         209882.0    373401.0              1817.0      117662.523365  \n",
       "9275         312019.0    520273.0              2791.0      115169.513047  \n",
       "\n",
       "[9276 rows x 285 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2fb01c39-b8e8-43dc-a67c-ec81092f8c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"target\"  # или как называется таргет в новом датасете\n",
    "feature_cols = [c for c in train.columns if c != target_col and c in test.columns]\n",
    "\n",
    "X = train[feature_cols]\n",
    "y = train[target_col].values\n",
    "X_test = test[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd1890e6-83d0-4c67-b7d6-9b14b9b89c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "До автофичей: (41105, 285)\n",
      "Число исходных фич: 285\n"
     ]
    }
   ],
   "source": [
    "print(\"До автофичей:\", X.shape)          # (41105, N_исходных_фич)\n",
    "print(\"Число исходных фич:\", X.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb3df2cf-72ae-4af4-a57e-11dc67300376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    440410.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 101.85 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['address']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 792\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])          :   3 | ['id', 'traffic_300m', 'traffic_1000m']\n",
      "\t\t('object', [])       :   3 | ['name', 'coordinates', 'category']\n",
      "\t\t('object', ['text']) :   1 | ['address']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   3 | ['name', 'coordinates', 'category']\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['address']\n",
      "\t\t('float', [])                       : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])                         :   3 | ['id', 'traffic_300m', 'traffic_1000m']\n",
      "\t\t('int', ['binned', 'text_special']) :  12 | ['address.char_count', 'address.word_count', 'address.capital_ratio', 'address.digit_ratio', 'address.special_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 619 | ['__nlp__.10', '__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', ...]\n",
      "\t6.8s = Fit runtime\n",
      "\t285 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 137.40 MB (0.0% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41105, 916) (9276, 916)\n",
      "Index(['id', 'traffic_300m', 'homes_300m', 'works_300m', 'female_300m',\n",
      "       'train_ticket_order_300m', 'mortgage_300m', 'recipes_300m',\n",
      "       'online_shops_300m', 'manga_300m', 'children_goods_300m',\n",
      "       'language_courses_300m', 'commercial_real_estate_purchase_300m',\n",
      "       'grocery_stores_300m', 'preschool_300m', 'cultural_leisure_events_300m',\n",
      "       'banking_services_300m', 'no_children_300m', 'mid_class_cars_300m',\n",
      "       'home_appliances_300m', 'beauty_and_health_devices_300m',\n",
      "       'computer_games_300m', 'cosplay_300m', 'unemployed_300m',\n",
      "       'pizza_delivery_300m', 'science_and_technology_300m', 'married_300m',\n",
      "       'mobile_phones_300m', 'events_300m', 'deposits_300m'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from autogluon.features.generators import AutoMLPipelineFeatureGenerator\n",
    "\n",
    "fg = AutoMLPipelineFeatureGenerator(\n",
    "    enable_numeric_features=True,\n",
    "    enable_categorical_features=True,\n",
    "    enable_datetime_features=True,\n",
    "    enable_text_special_features=True,\n",
    "    enable_text_ngram_features=True,\n",
    "    enable_raw_text_features=False,\n",
    ")\n",
    "\n",
    "X_tr_feat = fg.fit_transform(X=X, y=y)\n",
    "X_te_feat = fg.transform(X_test)\n",
    "\n",
    "print(X_tr_feat.shape, X_te_feat.shape)\n",
    "print(X_tr_feat.columns[:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2728437a-c174-4234-8dd4-7186b85d6d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После автофичей: (41105, 916)\n",
      "Число фич после генерации: 916\n"
     ]
    }
   ],
   "source": [
    "print(\"После автофичей:\", X_tr_feat.shape)   # (41105, 916)\n",
    "print(\"Число фич после генерации:\", X_tr_feat.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "677f5db2-cb31-4898-8b4c-e3b7a9e931f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: /venv/main/bin/python\n",
      "catboost version: 1.2.8\n",
      "GPU devices seen by catboost: 2\n"
     ]
    }
   ],
   "source": [
    "from catboost.utils import get_gpu_device_count\n",
    "import catboost, sys\n",
    "print(\"python:\", sys.executable)\n",
    "print(\"catboost version:\", catboost.__version__)\n",
    "print(\"GPU devices seen by catboost:\", get_gpu_device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fef3d256-65b6-47d6-8367-d28474cb00b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Категориальных колонок: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cat_cols = [\n",
    "    c for c in X_tr_feat.columns\n",
    "    if str(X_tr_feat[c].dtype) in (\"category\", \"object\")\n",
    "]\n",
    "\n",
    "for c in cat_cols:\n",
    "    X_tr_feat[c] = X_tr_feat[c].astype(str).replace({\"nan\": \"missing\"}).fillna(\"missing\")\n",
    "    if c in X_te_feat.columns:\n",
    "        X_te_feat[c] = X_te_feat[c].astype(str).replace({\"nan\": \"missing\"}).fillna(\"missing\")\n",
    "\n",
    "print(\"Категориальных колонок:\", len(cat_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1570d3ee-e78a-4434-a730-d30b13409ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 cat_features\n"
     ]
    }
   ],
   "source": [
    "cat_features = [\n",
    "    c for c in X_tr_feat.columns\n",
    "    if str(X_tr_feat[c].dtype) in (\"object\",)  # теперь всё строковое\n",
    "]\n",
    "print(len(cat_features), \"cat_features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb16849-4758-4eed-89d9-8160a659592d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f1e03b40-02e8-46e9-bfcb-df6361bb16a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GPU sanity fit (300 iters on 1000 rows) ===\n",
      "0:\tlearn: 1.1599236\ttotal: 13.5ms\tremaining: 4.03s\n",
      "50:\tlearn: 1.0597656\ttotal: 510ms\tremaining: 2.49s\n",
      "100:\tlearn: 0.9966601\ttotal: 1s\tremaining: 1.97s\n",
      "150:\tlearn: 0.9436507\ttotal: 1.47s\tremaining: 1.45s\n",
      "200:\tlearn: 0.8919346\ttotal: 1.95s\tremaining: 958ms\n",
      "250:\tlearn: 0.8496613\ttotal: 2.4s\tremaining: 468ms\n",
      "299:\tlearn: 0.8015994\ttotal: 2.83s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x75e4479090d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "idx = np.arange(len(X_tr_feat))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(idx)\n",
    "small_idx = idx[:1000]\n",
    "\n",
    "X_small = X_tr_feat.iloc[small_idx]\n",
    "y_small = y[small_idx]\n",
    "\n",
    "train_pool = Pool(X_small, y_small, cat_features=cat_features)\n",
    "\n",
    "model_gpu_test = CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    iterations=300,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    task_type=\"GPU\",\n",
    "    devices=\"0\",\n",
    "    verbose=50,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "print(\"=== GPU sanity fit (300 iters on 1000 rows) ===\")\n",
    "model_gpu_test.fit(train_pool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a28a7b65-8e5d-40c7-9495-a1faee436bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_features count: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "for c in X_tr_feat.columns:\n",
    "    if X_tr_feat[c].dtype == \"object\":\n",
    "        X_tr_feat[c] = X_tr_feat[c].fillna(\"missing\").astype(str)\n",
    "        if c in X_te_feat.columns:\n",
    "            X_te_feat[c] = X_te_feat[c].fillna(\"missing\").astype(str)\n",
    "    else:\n",
    "        X_tr_feat[c] = pd.to_numeric(X_tr_feat[c], errors=\"coerce\")\n",
    "        if c in X_te_feat.columns:\n",
    "            X_te_feat[c] = pd.to_numeric(X_te_feat[c], errors=\"coerce\")\n",
    "\n",
    "cat_features = [\n",
    "    i for i, c in enumerate(X_tr_feat.columns)\n",
    "    if X_tr_feat[c].dtype == \"object\"\n",
    "]\n",
    "\n",
    "print(\"cat_features count:\", len(cat_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633ae418-4203-44dc-a557-99585dd29058",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''надо поменять вообще было функцию на MAE для этой соревы но я проебался малек'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "049be6d5-42a9-46c2-82c6-d296546b7072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52fc4b5ca91f49bf9d8bce7252ef0853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "0:\tlearn: 1.1961353\ttest: 1.1567529\tbest: 1.1567529 (0)\ttotal: 60.7ms\tremaining: 3m 1s\n",
      "200:\tlearn: 1.0681650\ttest: 1.0396617\tbest: 1.0396617 (200)\ttotal: 11.6s\tremaining: 2m 41s\n",
      "400:\tlearn: 1.0506453\ttest: 1.0278411\tbest: 1.0278411 (400)\ttotal: 23.3s\tremaining: 2m 30s\n",
      "600:\tlearn: 1.0429004\ttest: 1.0241462\tbest: 1.0241462 (600)\ttotal: 34.6s\tremaining: 2m 18s\n",
      "800:\tlearn: 1.0370540\ttest: 1.0220961\tbest: 1.0220961 (800)\ttotal: 45.8s\tremaining: 2m 5s\n",
      "1000:\tlearn: 1.0313323\ttest: 1.0203050\tbest: 1.0203050 (1000)\ttotal: 57s\tremaining: 1m 53s\n",
      "1200:\tlearn: 1.0259834\ttest: 1.0190701\tbest: 1.0190701 (1200)\ttotal: 1m 8s\tremaining: 1m 42s\n",
      "1400:\tlearn: 1.0213685\ttest: 1.0183588\tbest: 1.0183525 (1396)\ttotal: 1m 18s\tremaining: 1m 30s\n",
      "1600:\tlearn: 1.0170179\ttest: 1.0179110\tbest: 1.0179110 (1600)\ttotal: 1m 29s\tremaining: 1m 18s\n",
      "1800:\tlearn: 1.0130720\ttest: 1.0175272\tbest: 1.0175096 (1793)\ttotal: 1m 41s\tremaining: 1m 7s\n",
      "2000:\tlearn: 1.0092340\ttest: 1.0171160\tbest: 1.0171160 (2000)\ttotal: 1m 52s\tremaining: 56.1s\n",
      "2200:\tlearn: 1.0058918\ttest: 1.0169751\tbest: 1.0169631 (2191)\ttotal: 2m 3s\tremaining: 44.9s\n",
      "2400:\tlearn: 1.0024967\ttest: 1.0167759\tbest: 1.0167683 (2351)\ttotal: 2m 14s\tremaining: 33.7s\n",
      "2600:\tlearn: 0.9991940\ttest: 1.0166725\tbest: 1.0166725 (2600)\ttotal: 2m 26s\tremaining: 22.4s\n",
      "2800:\tlearn: 0.9961792\ttest: 1.0165415\tbest: 1.0165335 (2796)\ttotal: 2m 37s\tremaining: 11.2s\n",
      "2999:\tlearn: 0.9935489\ttest: 1.0166446\tbest: 1.0165333 (2807)\ttotal: 2m 48s\tremaining: 0us\n",
      "bestTest = 1.016533251\n",
      "bestIteration = 2807\n",
      "Shrink model to first 2808 iterations.\n",
      "Fold 1 RMSE: 1.01653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f673d8f2b8584bb7a08743febf5242ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict fold 1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2/5 ===\n",
      "0:\tlearn: 1.1864868\ttest: 1.1966088\tbest: 1.1966088 (0)\ttotal: 57.9ms\tremaining: 2m 53s\n",
      "200:\tlearn: 1.0625701\ttest: 1.0634628\tbest: 1.0634628 (200)\ttotal: 11.7s\tremaining: 2m 43s\n",
      "400:\tlearn: 1.0465040\ttest: 1.0490837\tbest: 1.0490837 (400)\ttotal: 23.2s\tremaining: 2m 30s\n",
      "600:\tlearn: 1.0383157\ttest: 1.0441042\tbest: 1.0441042 (600)\ttotal: 34.9s\tremaining: 2m 19s\n",
      "800:\tlearn: 1.0325374\ttest: 1.0416734\tbest: 1.0416719 (798)\ttotal: 46.3s\tremaining: 2m 7s\n",
      "1000:\tlearn: 1.0274891\ttest: 1.0395436\tbest: 1.0395391 (999)\ttotal: 57.7s\tremaining: 1m 55s\n",
      "1200:\tlearn: 1.0229930\ttest: 1.0382931\tbest: 1.0382931 (1200)\ttotal: 1m 8s\tremaining: 1m 43s\n",
      "1400:\tlearn: 1.0187774\ttest: 1.0374908\tbest: 1.0374908 (1400)\ttotal: 1m 20s\tremaining: 1m 31s\n",
      "1600:\tlearn: 1.0140191\ttest: 1.0362455\tbest: 1.0362455 (1600)\ttotal: 1m 31s\tremaining: 1m 19s\n",
      "1800:\tlearn: 1.0103969\ttest: 1.0355906\tbest: 1.0355842 (1794)\ttotal: 1m 42s\tremaining: 1m 8s\n",
      "2000:\tlearn: 1.0069181\ttest: 1.0350323\tbest: 1.0350323 (2000)\ttotal: 1m 53s\tremaining: 56.8s\n",
      "2200:\tlearn: 1.0030901\ttest: 1.0341512\tbest: 1.0341467 (2199)\ttotal: 2m 4s\tremaining: 45.3s\n",
      "2400:\tlearn: 0.9998816\ttest: 1.0339508\tbest: 1.0339502 (2393)\ttotal: 2m 16s\tremaining: 33.9s\n",
      "2600:\tlearn: 0.9968082\ttest: 1.0336739\tbest: 1.0336509 (2584)\ttotal: 2m 27s\tremaining: 22.6s\n",
      "2800:\tlearn: 0.9940473\ttest: 1.0334723\tbest: 1.0334636 (2778)\ttotal: 2m 38s\tremaining: 11.2s\n",
      "2999:\tlearn: 0.9910672\ttest: 1.0331532\tbest: 1.0331485 (2989)\ttotal: 2m 49s\tremaining: 0us\n",
      "bestTest = 1.03314847\n",
      "bestIteration = 2989\n",
      "Shrink model to first 2990 iterations.\n",
      "Fold 2 RMSE: 1.03315\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ef5fbab62148c5a3d53d76fce40d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict fold 2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3/5 ===\n",
      "0:\tlearn: 1.1922094\ttest: 1.1747486\tbest: 1.1747486 (0)\ttotal: 60.5ms\tremaining: 3m 1s\n",
      "200:\tlearn: 1.0621388\ttest: 1.0504706\tbest: 1.0504706 (200)\ttotal: 11.7s\tremaining: 2m 42s\n",
      "400:\tlearn: 1.0453366\ttest: 1.0373411\tbest: 1.0373411 (400)\ttotal: 23.8s\tremaining: 2m 34s\n",
      "600:\tlearn: 1.0379879\ttest: 1.0329583\tbest: 1.0329583 (600)\ttotal: 36s\tremaining: 2m 23s\n",
      "800:\tlearn: 1.0318690\ttest: 1.0298413\tbest: 1.0298413 (800)\ttotal: 48.2s\tremaining: 2m 12s\n",
      "1000:\tlearn: 1.0266775\ttest: 1.0280075\tbest: 1.0280075 (1000)\ttotal: 1m\tremaining: 2m\n",
      "1200:\tlearn: 1.0221090\ttest: 1.0269049\tbest: 1.0269049 (1200)\ttotal: 1m 12s\tremaining: 1m 47s\n",
      "1400:\tlearn: 1.0174507\ttest: 1.0259123\tbest: 1.0259123 (1400)\ttotal: 1m 23s\tremaining: 1m 35s\n",
      "1600:\tlearn: 1.0127962\ttest: 1.0250593\tbest: 1.0250593 (1600)\ttotal: 1m 34s\tremaining: 1m 22s\n",
      "1800:\tlearn: 1.0086757\ttest: 1.0246060\tbest: 1.0245937 (1798)\ttotal: 1m 45s\tremaining: 1m 10s\n",
      "2000:\tlearn: 1.0045683\ttest: 1.0238379\tbest: 1.0238379 (2000)\ttotal: 1m 56s\tremaining: 58.4s\n",
      "2200:\tlearn: 1.0013810\ttest: 1.0235190\tbest: 1.0234942 (2180)\ttotal: 2m 8s\tremaining: 46.8s\n",
      "2400:\tlearn: 0.9982808\ttest: 1.0232519\tbest: 1.0232519 (2400)\ttotal: 2m 20s\tremaining: 35.1s\n",
      "2600:\tlearn: 0.9952890\ttest: 1.0231240\tbest: 1.0230986 (2570)\ttotal: 2m 32s\tremaining: 23.4s\n",
      "2800:\tlearn: 0.9923058\ttest: 1.0229073\tbest: 1.0228889 (2794)\ttotal: 2m 43s\tremaining: 11.6s\n",
      "2999:\tlearn: 0.9893273\ttest: 1.0226872\tbest: 1.0226597 (2991)\ttotal: 2m 55s\tremaining: 0us\n",
      "bestTest = 1.022659702\n",
      "bestIteration = 2991\n",
      "Shrink model to first 2992 iterations.\n",
      "Fold 3 RMSE: 1.02266\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf72ca2514474036823387d767f2cb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict fold 3:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4/5 ===\n",
      "0:\tlearn: 1.1777454\ttest: 1.2313605\tbest: 1.2313605 (0)\ttotal: 56.7ms\tremaining: 2m 50s\n",
      "200:\tlearn: 1.0542061\ttest: 1.0938335\tbest: 1.0938335 (200)\ttotal: 11.5s\tremaining: 2m 40s\n",
      "400:\tlearn: 1.0390503\ttest: 1.0779432\tbest: 1.0779432 (400)\ttotal: 22.7s\tremaining: 2m 27s\n",
      "600:\tlearn: 1.0319989\ttest: 1.0729167\tbest: 1.0729167 (600)\ttotal: 34s\tremaining: 2m 15s\n",
      "800:\tlearn: 1.0261835\ttest: 1.0694670\tbest: 1.0694670 (800)\ttotal: 45.3s\tremaining: 2m 4s\n",
      "1000:\tlearn: 1.0212496\ttest: 1.0670310\tbest: 1.0670310 (1000)\ttotal: 56.4s\tremaining: 1m 52s\n",
      "1200:\tlearn: 1.0166460\ttest: 1.0656923\tbest: 1.0656923 (1200)\ttotal: 1m 7s\tremaining: 1m 41s\n",
      "1400:\tlearn: 1.0121602\ttest: 1.0645263\tbest: 1.0645254 (1398)\ttotal: 1m 18s\tremaining: 1m 29s\n",
      "1600:\tlearn: 1.0076293\ttest: 1.0630999\tbest: 1.0630971 (1597)\ttotal: 1m 29s\tremaining: 1m 18s\n",
      "1800:\tlearn: 1.0034396\ttest: 1.0620857\tbest: 1.0620848 (1798)\ttotal: 1m 41s\tremaining: 1m 7s\n",
      "2000:\tlearn: 0.9997029\ttest: 1.0611826\tbest: 1.0611789 (1999)\ttotal: 1m 51s\tremaining: 55.8s\n",
      "2200:\tlearn: 0.9962439\ttest: 1.0603501\tbest: 1.0603501 (2200)\ttotal: 2m 2s\tremaining: 44.6s\n",
      "2400:\tlearn: 0.9928842\ttest: 1.0597328\tbest: 1.0597264 (2399)\ttotal: 2m 13s\tremaining: 33.4s\n",
      "2600:\tlearn: 0.9894135\ttest: 1.0588030\tbest: 1.0588030 (2600)\ttotal: 2m 24s\tremaining: 22.2s\n",
      "2800:\tlearn: 0.9863976\ttest: 1.0581645\tbest: 1.0581605 (2796)\ttotal: 2m 35s\tremaining: 11.1s\n",
      "2999:\tlearn: 0.9833809\ttest: 1.0577231\tbest: 1.0577151 (2994)\ttotal: 2m 46s\tremaining: 0us\n",
      "bestTest = 1.057715147\n",
      "bestIteration = 2994\n",
      "Shrink model to first 2995 iterations.\n",
      "Fold 4 RMSE: 1.05772\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5098cebbd3db4253bd6ab9c016d23d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict fold 4:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 5/5 ===\n",
      "0:\tlearn: 1.1899427\ttest: 1.1819733\tbest: 1.1819733 (0)\ttotal: 56.3ms\tremaining: 2m 48s\n",
      "200:\tlearn: 1.0640221\ttest: 1.0522572\tbest: 1.0522572 (200)\ttotal: 11.6s\tremaining: 2m 41s\n",
      "400:\tlearn: 1.0476615\ttest: 1.0378508\tbest: 1.0378508 (400)\ttotal: 22.8s\tremaining: 2m 27s\n",
      "600:\tlearn: 1.0399211\ttest: 1.0326006\tbest: 1.0326006 (600)\ttotal: 33.9s\tremaining: 2m 15s\n",
      "800:\tlearn: 1.0345471\ttest: 1.0298604\tbest: 1.0298604 (800)\ttotal: 44.8s\tremaining: 2m 3s\n",
      "1000:\tlearn: 1.0297241\ttest: 1.0278080\tbest: 1.0278080 (1000)\ttotal: 55.8s\tremaining: 1m 51s\n",
      "1200:\tlearn: 1.0249414\ttest: 1.0262644\tbest: 1.0262644 (1200)\ttotal: 1m 6s\tremaining: 1m 39s\n",
      "1400:\tlearn: 1.0202616\ttest: 1.0251082\tbest: 1.0250985 (1397)\ttotal: 1m 17s\tremaining: 1m 28s\n",
      "1600:\tlearn: 1.0158677\ttest: 1.0240951\tbest: 1.0240878 (1598)\ttotal: 1m 28s\tremaining: 1m 17s\n",
      "1800:\tlearn: 1.0116655\ttest: 1.0231540\tbest: 1.0231540 (1800)\ttotal: 1m 39s\tremaining: 1m 6s\n",
      "2000:\tlearn: 1.0074044\ttest: 1.0222016\tbest: 1.0221957 (1999)\ttotal: 1m 52s\tremaining: 56s\n",
      "2200:\tlearn: 1.0038223\ttest: 1.0216245\tbest: 1.0216245 (2200)\ttotal: 2m 4s\tremaining: 45.2s\n",
      "2400:\tlearn: 1.0001990\ttest: 1.0208818\tbest: 1.0208616 (2378)\ttotal: 2m 15s\tremaining: 33.9s\n",
      "2600:\tlearn: 0.9971338\ttest: 1.0204198\tbest: 1.0204198 (2600)\ttotal: 2m 27s\tremaining: 22.6s\n",
      "2800:\tlearn: 0.9940753\ttest: 1.0198903\tbest: 1.0198895 (2798)\ttotal: 2m 39s\tremaining: 11.4s\n",
      "2999:\tlearn: 0.9912725\ttest: 1.0196232\tbest: 1.0196001 (2990)\ttotal: 2m 50s\tremaining: 0us\n",
      "bestTest = 1.019600088\n",
      "bestIteration = 2990\n",
      "Shrink model to first 2991 iterations.\n",
      "Fold 5 RMSE: 1.01960\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d44300b0cc4f6e881c4e00450127de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predict fold 5:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "fold_rmse: [1.0165332683284727, 1.0331484794498649, 1.0226597596212912, 1.0577151482642992, 1.019600057844265]\n",
      "mean_rmse: 1.0299313427016386\n",
      "oof_rmse: 1.0300402189216311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def rmse_func(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred) ** 0.5\n",
    "\n",
    "N_SPLITS = 5\n",
    "N_ITER = 3000\n",
    "DEPTH = 8\n",
    "OD_WAIT = 300\n",
    "TASK_TYPE = \"GPU\"\n",
    "\n",
    "X = X_tr_feat\n",
    "y_arr = y\n",
    "\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "oof = np.zeros(len(X))\n",
    "test_pred = np.zeros(len(X_te_feat))\n",
    "scores = []\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(tqdm(kf.split(X), total=N_SPLITS, desc=\"Folds\"), 1):\n",
    "    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
    "    y_tr, y_val = y_arr[tr_idx], y_arr[val_idx]\n",
    "\n",
    "    train_pool = Pool(X_tr, y_tr, cat_features=cat_features)\n",
    "    val_pool = Pool(X_val, y_val, cat_features=cat_features)\n",
    "\n",
    "    params = dict(\n",
    "        loss_function=\"RMSE\",\n",
    "        eval_metric=\"RMSE\",\n",
    "        iterations=N_ITER,\n",
    "        learning_rate=0.01,\n",
    "        depth=DEPTH,\n",
    "        l2_leaf_reg=3.0,\n",
    "        random_seed=42 + fold,\n",
    "        verbose=200,\n",
    "        od_type=\"Iter\",\n",
    "        od_wait=OD_WAIT,\n",
    "        task_type=TASK_TYPE,\n",
    "    )\n",
    "\n",
    "    if TASK_TYPE == \"GPU\":\n",
    "        params[\"devices\"] = \"0\"\n",
    "\n",
    "    print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n",
    "    model = CatBoostRegressor(**params)\n",
    "    model.fit(train_pool, eval_set=val_pool, use_best_model=True)\n",
    "\n",
    "    val_pred = model.predict(X_val)\n",
    "    oof[val_idx] = val_pred\n",
    "    rmse = rmse_func(y_val, val_pred)\n",
    "    scores.append(rmse)\n",
    "    print(f\"Fold {fold} RMSE: {rmse:.5f}\")\n",
    "\n",
    "    bs = 1024\n",
    "    fold_test = np.zeros(len(X_te_feat))\n",
    "    for s in tqdm(range(0, len(X_te_feat), bs), leave=False, desc=f\"Predict fold {fold}\"):\n",
    "        e = s + bs\n",
    "        fold_test[s:e] = model.predict(X_te_feat.iloc[s:e])\n",
    "    test_pred += fold_test / N_SPLITS\n",
    "\n",
    "oof_rmse = rmse_func(y_arr, oof)\n",
    "print(\"\\nfold_rmse:\", scores)\n",
    "print(\"mean_rmse:\", np.mean(scores))\n",
    "print(\"oof_rmse:\", oof_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f985a72-c57c-48b3-818b-ffdbbddc0e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.420300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.249490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>3.861935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.117375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>2.765881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target\n",
       "0  21472  3.420300\n",
       "1   9837  3.249490\n",
       "2  41791  3.861935\n",
       "3  18441  3.117375\n",
       "4  49348  2.765881"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# предполагаем, что есть:\n",
    "# - test (оригинальный тестовый df)\n",
    "# - test_pred (предсказания из k-fold)\n",
    "# - target_col (имя таргета, как в train)\n",
    "\n",
    "id_col = \"id\"  # или \"ID\", или другое\n",
    "for c in [\"id\", \"ID\", \"Id\"]:\n",
    "    if c in test.columns:\n",
    "        id_col = c\n",
    "        break\n",
    "\n",
    "if id_col is not None:\n",
    "    sub = pd.DataFrame({\n",
    "        id_col: test[id_col].values,\n",
    "        target_col: test_pred\n",
    "    })\n",
    "else:\n",
    "    sub = pd.DataFrame({\n",
    "        \"id\": np.arange(len(test_pred)),\n",
    "        target_col: test_pred\n",
    "    })\n",
    "\n",
    "sub.to_csv(\"submission_catboost_kfold_autofeat_gpu.csv\", index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c4a56ea-dc30-4dbb-8658-b4e6ddb46c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41105, 917) (9276, 916)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>traffic_300m</th>\n",
       "      <th>homes_300m</th>\n",
       "      <th>works_300m</th>\n",
       "      <th>female_300m</th>\n",
       "      <th>train_ticket_order_300m</th>\n",
       "      <th>mortgage_300m</th>\n",
       "      <th>recipes_300m</th>\n",
       "      <th>online_shops_300m</th>\n",
       "      <th>manga_300m</th>\n",
       "      <th>...</th>\n",
       "      <th>__nlp__.юбилейный</th>\n",
       "      <th>__nlp__.юго</th>\n",
       "      <th>__nlp__.южная</th>\n",
       "      <th>__nlp__.южная ул</th>\n",
       "      <th>__nlp__.южнобутовская</th>\n",
       "      <th>__nlp__.южнопортовая</th>\n",
       "      <th>__nlp__.южный</th>\n",
       "      <th>__nlp__.ярославское</th>\n",
       "      <th>__nlp__._total_</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1365</td>\n",
       "      <td>75429</td>\n",
       "      <td>16113.582471</td>\n",
       "      <td>15756.246444</td>\n",
       "      <td>51316.0</td>\n",
       "      <td>734.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4820.0</td>\n",
       "      <td>10943.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8230</td>\n",
       "      <td>246535</td>\n",
       "      <td>8578.458740</td>\n",
       "      <td>31315.672794</td>\n",
       "      <td>192547.0</td>\n",
       "      <td>4701.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19354.0</td>\n",
       "      <td>53112.0</td>\n",
       "      <td>561.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29071</td>\n",
       "      <td>83490</td>\n",
       "      <td>12650.492399</td>\n",
       "      <td>12490.096776</td>\n",
       "      <td>56045.0</td>\n",
       "      <td>1204.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5093.0</td>\n",
       "      <td>12265.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22591</td>\n",
       "      <td>43421</td>\n",
       "      <td>6413.279217</td>\n",
       "      <td>4963.058046</td>\n",
       "      <td>28705.0</td>\n",
       "      <td>540.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>7495.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27621</td>\n",
       "      <td>155094</td>\n",
       "      <td>18638.410128</td>\n",
       "      <td>19868.190570</td>\n",
       "      <td>123492.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>11066.0</td>\n",
       "      <td>28705.0</td>\n",
       "      <td>237.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 917 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  traffic_300m    homes_300m    works_300m  female_300m  \\\n",
       "0   1365         75429  16113.582471  15756.246444      51316.0   \n",
       "1   8230        246535   8578.458740  31315.672794     192547.0   \n",
       "2  29071         83490  12650.492399  12490.096776      56045.0   \n",
       "3  22591         43421   6413.279217   4963.058046      28705.0   \n",
       "4  27621        155094  18638.410128  19868.190570     123492.0   \n",
       "\n",
       "   train_ticket_order_300m  mortgage_300m  recipes_300m  online_shops_300m  \\\n",
       "0                    734.0           35.0        4820.0            10943.0   \n",
       "1                   4701.0          305.0       19354.0            53112.0   \n",
       "2                   1204.0           53.0        5093.0            12265.0   \n",
       "3                    540.0           42.0        2936.0             7495.0   \n",
       "4                   2411.0           95.0       11066.0            28705.0   \n",
       "\n",
       "   manga_300m  ...  __nlp__.юбилейный  __nlp__.юго  __nlp__.южная  \\\n",
       "0       120.0  ...                  0            0              0   \n",
       "1       561.0  ...                  0            0              0   \n",
       "2       108.0  ...                  0            0              0   \n",
       "3        70.0  ...                  0            0              0   \n",
       "4       237.0  ...                  0            0              0   \n",
       "\n",
       "   __nlp__.южная ул  __nlp__.южнобутовская  __nlp__.южнопортовая  \\\n",
       "0                 0                      0                     0   \n",
       "1                 0                      0                     0   \n",
       "2                 0                      0                     0   \n",
       "3                 0                      0                     0   \n",
       "4                 0                      0                     0   \n",
       "\n",
       "   __nlp__.южный  __nlp__.ярославское  __nlp__._total_  target  \n",
       "0              0                    0                8     4.1  \n",
       "1              0                    0                4     3.6  \n",
       "2              0                    0                5     3.5  \n",
       "3              0                    0                8     4.0  \n",
       "4              0                    0                3     4.2  \n",
       "\n",
       "[5 rows x 917 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ag = X_tr_feat.copy()\n",
    "train_ag[target_col] = y\n",
    "\n",
    "test_ag = X_te_feat.copy()\n",
    "\n",
    "print(train_ag.shape, test_ag.shape)\n",
    "train_ag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a8fd937-6cab-4572-ab54-de24f39307e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251116_173254\"\n",
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n",
      "CPU Count:          62\n",
      "Memory Avail:       429.29 GB / 503.46 GB (85.3%)\n",
      "Disk Space Avail:   128.52 GB / 130.00 GB (98.9%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Using hyperparameters preset: hyperparameters='default'\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_173254\"\n",
      "Train Data Rows:    41105\n",
      "Train Data Columns: 916\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    439528.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 145.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 593 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])    : 634 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('object', []) :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])     : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])       :  41 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('int', ['bool']) : 593 | ['__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', '__nlp__.10 стр москва', ...]\n",
      "\t5.1s = Fit runtime\n",
      "\t916 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 114.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.29s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.060819851599562096, Train Rows: 38605, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': [{}],\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
      "\t'CAT': [{}],\n",
      "\t'XGB': [{}],\n",
      "\t'FASTAI': [{}],\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "}\n",
      "Fitting 9 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMXT ... Training model for up to 1794.71s of the 1794.70s of remaining time.\n",
      "\tWarning: Exception caused LightGBMXT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBM ... Training model for up to 1794.24s of the 1794.24s of remaining time.\n",
      "\tWarning: Exception caused LightGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: RandomForestMSE ... Training model for up to 1793.81s of the 1793.81s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=0.0/429.0 GB\n",
      "\t-1.0078\t = Validation score   (-root_mean_squared_error)\n",
      "\t303.95s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 1489.20s of the 1489.20s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.7/427.7 GB\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.9819\t = Validation score   (-root_mean_squared_error)\n",
      "\t11.37s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 1477.75s of the 1477.75s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=0.0/428.0 GB\n",
      "\t-1.008\t = Validation score   (-root_mean_squared_error)\n",
      "\t285.31s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 1191.84s of the 1191.84s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.0/427.9 GB\n",
      "\tWarning: Exception caused NeuralNetFastAI to fail during training (ImportError)... Skipping this model.\n",
      "\t\tImport fastai failed. A quick tip is to install via `pip install autogluon.tabular[fastai]==1.4.0`. \n",
      "Fitting model: XGBoost ... Training model for up to 1191.21s of the 1191.20s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.1/427.9 GB\n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1190.03s of the 1190.03s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=0.5/427.8 GB\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: LightGBMLarge ... Training model for up to 1189.49s of the 1189.49s of remaining time.\n",
      "\tWarning: Exception caused LightGBMLarge to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1188.98s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 0.625, 'RandomForestMSE': 0.312, 'ExtraTreesMSE': 0.062}\n",
      "\t-0.969\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 611.22s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 10508.1 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_173254\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(\n",
    "    label=target_col,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\"\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    time_limit=1800,  # можно 600/900/3600 по ситуации\n",
    "    presets=\"medium_quality_faster_train\",\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58d0f23b-2eed-47c0-a80f-d855dd4c67d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working\n"
     ]
    }
   ],
   "source": [
    "print('working')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238261e6-55f7-4e69-88f2-64ef6b8cd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' если нужна модель тяжелее хотим presets=\"best_quality\" '''\n",
    "\n",
    "'''лидерборд'''\n",
    "'''\n",
    "leaderboard = predictor.leaderboard(train_ag, silent=True)\n",
    "leaderboard\n",
    "'''\n",
    "\n",
    "'''предсказания и сабмит'''\n",
    "\n",
    "'''\n",
    "id_col = \"id\"  # или \"ID\", или другое имя, если нужно\n",
    "\n",
    "ag_pred = predictor.predict(test_ag)\n",
    "\n",
    "sub_ag = test[[id_col]].copy()\n",
    "sub_ag[target_col] = ag_pred\n",
    "\n",
    "sub_ag.to_csv(\"submission_autogluon_autofeat.csv\", index=False)\n",
    "sub_ag.head()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1495a40d-3323-4e27-914e-8bf99072d8c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictor_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m leaderboard_cat = \u001b[43mpredictor_cat\u001b[49m.leaderboard(train_ag, silent=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      2\u001b[39m leaderboard_cat\n\u001b[32m      4\u001b[39m ag_train_pred = predictor_cat.predict(train_ag)\n",
      "\u001b[31mNameError\u001b[39m: name 'predictor_cat' is not defined"
     ]
    }
   ],
   "source": [
    "leaderboard_cat = predictor_cat.leaderboard(train_ag, silent=True)\n",
    "leaderboard_cat\n",
    "\n",
    "ag_train_pred = predictor_cat.predict(train_ag)\n",
    "ag_test_pred = predictor_cat.predict(test_ag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507448a-0062-418a-9d98-d342e771afe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63870e94-daa9-4140-9806-124af99356d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6ca4cb-da39-49c6-a6d7-600628cfed2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a35670-7304-44ba-b61d-f1082579fb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b5fe4-ef7a-4287-869e-1cd06b159848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eecb38-7454-43ae-a2eb-d7dcbf7fbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"глюон тока с катбустом\"\n",
    "\n",
    "predictor_cat = TabularPredictor(\n",
    "    label=target_col,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    time_limit=1800,  # можно уменьшить/увеличить\n",
    "    hyperparameters={\n",
    "        'CAT': {}   # включаем только CatBoost-модели\n",
    "    },\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9540c4-7653-4e12-9dd3-207cba0a7919",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c369a2-3457-433d-9c43-13d8dd131c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b17a14-2625-480c-a659-141115fe2543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cabcd-b524-4bd2-bbd9-a0af36f98336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b7795c5-b3f6-4eec-b870-fa1e9b3c96e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.245652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>4.003885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.178790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>2.697172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target\n",
       "0  21472  3.371200\n",
       "1   9837  3.245652\n",
       "2  41791  4.003885\n",
       "3  18441  3.178790\n",
       "4  49348  2.697172"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_col = \"id\"  # или \"ID\", или другое имя, если нужно\n",
    "\n",
    "ag_pred = predictor.predict(test_ag)\n",
    "\n",
    "sub_ag = test[[id_col]].copy()\n",
    "sub_ag[target_col] = ag_pred\n",
    "\n",
    "sub_ag.to_csv(\"submission_autogluon_autofeat_id_target.csv\", index=False)\n",
    "sub_ag.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c8b4a3-9abd-47d7-9abc-2a4453c5cc16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fab672-7faf-4546-a55e-6aedd0cc681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_pred = predictor.predict(test_ag)\n",
    "\n",
    "sub_ag = pd.DataFrame({target_col: ag_pred})\n",
    "sub_ag.to_csv(\"submission_autogluon_autofeat_only_id.csv\", index=False)\n",
    "sub_ag.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "533b0553-b752-4fdb-aa29-d3ead03e3038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41105, 917), (9276, 916))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "train_ag = X_tr_feat.copy()\n",
    "train_ag[target_col] = y\n",
    "test_ag = X_te_feat.copy()\n",
    "\n",
    "train_ag.shape, test_ag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "36dea177-2150-4f8e-aa6e-36f6025b0de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251116_190043\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n",
      "CPU Count:          62\n",
      "Memory Avail:       427.80 GB / 503.46 GB (85.0%)\n",
      "Disk Space Avail:   127.20 GB / 130.00 GB (97.8%)\n",
      "===================================================\n",
      "No presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets. Defaulting to `'medium'`...\n",
      "\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n",
      "\tpresets='extreme' : New in v1.4: Massively better than 'best' on datasets <30000 samples by using new models meta-learned on https://tabarena.ai: TabPFNv2, TabICL, Mitra, and TabM. Absolute best accuracy. Requires a GPU. Recommended 64 GB CPU memory and 32+ GB GPU memory.\n",
      "\tpresets='best'    : Maximize accuracy. Recommended for most users. Use in competitions and benchmarks.\n",
      "\tpresets='high'    : Strong accuracy with fast inference speed.\n",
      "\tpresets='good'    : Good accuracy with very fast inference speed.\n",
      "\tpresets='medium'  : Fast training time, ideal for initial prototyping.\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_190043\"\n",
      "Train Data Rows:    41105\n",
      "Train Data Columns: 916\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    438059.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 145.56 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 593 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])  : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])    : 634 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('object', []) :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])     : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])       :  41 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('int', ['bool']) : 593 | ['__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', '__nlp__.10 стр москва', ...]\n",
      "\t5.3s = Fit runtime\n",
      "\t916 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 114.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.47s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.060819851599562096, Train Rows: 38605, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{}],\n",
      "}\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoost ... Training model for up to 1794.53s of the 1794.53s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.7/427.6 GB\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.9819\t = Validation score   (-root_mean_squared_error)\n",
      "\t10.77s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1783.68s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t-0.9819\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 16.52s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 55500.8 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_190043\")\n"
     ]
    }
   ],
   "source": [
    "predictor_cat = TabularPredictor(\n",
    "    label=target_col,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    time_limit=1800,  # можно уменьшить/увеличить\n",
    "    hyperparameters={\n",
    "        'CAT': {}   # включаем только CatBoost-модели\n",
    "    },\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "39c45a5e-dc81-4beb-8a5c-0d3b4aa594ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-1.003955</td>\n",
       "      <td>-0.981936</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.091085</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>10.768425</td>\n",
       "      <td>0.091085</td>\n",
       "      <td>0.044779</td>\n",
       "      <td>10.768425</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-1.003955</td>\n",
       "      <td>-0.981936</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.093464</td>\n",
       "      <td>0.045044</td>\n",
       "      <td>10.770169</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  score_test  score_val              eval_metric  \\\n",
       "0             CatBoost   -1.003955  -0.981936  root_mean_squared_error   \n",
       "1  WeightedEnsemble_L2   -1.003955  -0.981936  root_mean_squared_error   \n",
       "\n",
       "   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n",
       "0        0.091085       0.044779  10.768425                 0.091085   \n",
       "1        0.093464       0.045044  10.770169                 0.002378   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.044779          10.768425            1       True   \n",
       "1                0.000265           0.001745            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaderboard_cat = predictor_cat.leaderboard(train_ag, silent=True)\n",
    "leaderboard_cat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c1bc2e50-56e8-45b2-9096-caeeba8bf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "ag_train_pred = predictor_cat.predict(train_ag)\n",
    "ag_test_pred = predictor_cat.predict(test_ag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eea09f4-7d0d-415f-b2c9-d6807b699d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''blend'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1dd08de-e792-4274-9bec-368a46561e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.1803559\ttotal: 16.7ms\tremaining: 33.3s\n",
      "100:\tlearn: 0.9898654\ttotal: 784ms\tremaining: 14.7s\n",
      "200:\tlearn: 0.9848862\ttotal: 1.65s\tremaining: 14.8s\n",
      "300:\tlearn: 0.9827911\ttotal: 2.56s\tremaining: 14.4s\n",
      "400:\tlearn: 0.9804082\ttotal: 3.36s\tremaining: 13.4s\n",
      "500:\tlearn: 0.9781054\ttotal: 4.17s\tremaining: 12.5s\n",
      "600:\tlearn: 0.9761623\ttotal: 4.79s\tremaining: 11.2s\n",
      "700:\tlearn: 0.9744053\ttotal: 5.1s\tremaining: 9.45s\n",
      "800:\tlearn: 0.9728192\ttotal: 5.49s\tremaining: 8.22s\n",
      "900:\tlearn: 0.9713416\ttotal: 5.91s\tremaining: 7.21s\n",
      "1000:\tlearn: 0.9699321\ttotal: 6.33s\tremaining: 6.32s\n",
      "1100:\tlearn: 0.9686531\ttotal: 6.74s\tremaining: 5.5s\n",
      "1200:\tlearn: 0.9674257\ttotal: 7.16s\tremaining: 4.76s\n",
      "1300:\tlearn: 0.9662867\ttotal: 7.59s\tremaining: 4.08s\n",
      "1400:\tlearn: 0.9651149\ttotal: 8.02s\tremaining: 3.43s\n",
      "1500:\tlearn: 0.9640560\ttotal: 8.43s\tremaining: 2.8s\n",
      "1600:\tlearn: 0.9631643\ttotal: 8.83s\tremaining: 2.2s\n",
      "1700:\tlearn: 0.9622188\ttotal: 9.25s\tremaining: 1.63s\n",
      "1800:\tlearn: 0.9613573\ttotal: 9.67s\tremaining: 1.07s\n",
      "1900:\tlearn: 0.9605074\ttotal: 10.1s\tremaining: 525ms\n",
      "1999:\tlearn: 0.9596072\ttotal: 10.5s\tremaining: 0us\n",
      "blend RMSE: 0.959607192599873\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "import numpy as np\n",
    "\n",
    "Z_train = np.vstack([\n",
    "    oof,                    # OOF из твоего KFold CatBoost\n",
    "    ag_train_pred.values    # предсказания AutoGluon на train\n",
    "]).T\n",
    "\n",
    "Z_test = np.vstack([\n",
    "    test_pred,              # предсказания твоего KFold CatBoost на test\n",
    "    ag_test_pred.values     # предсказания AutoGluon на test\n",
    "]).T\n",
    "\n",
    "meta_model = CatBoostRegressor(\n",
    "    loss_function=\"RMSE\",\n",
    "    eval_metric=\"RMSE\",\n",
    "    iterations=2000,\n",
    "    learning_rate=0.03,\n",
    "    depth=6,\n",
    "    l2_leaf_reg=3.0,\n",
    "    random_seed=777,\n",
    "    verbose=100,\n",
    "    task_type=\"CPU\",   # можно сменить на \"GPU\", devices=\"0\"\n",
    ")\n",
    "\n",
    "meta_model.fit(Z_train, y)\n",
    "\n",
    "blend_oof = meta_model.predict(Z_train)\n",
    "blend_rmse = rmse_func(y, blend_oof)\n",
    "print(\"blend RMSE:\", blend_rmse)\n",
    "\n",
    "blend_test = meta_model.predict(Z_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "03d34f23-39f5-4e58-b694-3c26eef16545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# если в test есть id-колонка\n",
    "id_col = \"id\"  # или своё имя\n",
    "\n",
    "sub_blend = test[[id_col]].copy()\n",
    "sub_blend[target_col] = blend_test\n",
    "sub_blend.to_csv(\"submission_blend_catboost_meta.csv\", index=False)\n",
    "sub_blend.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11b6fb3-0313-4939-b335-3108cc9f3d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''далее улучшение всего вышеперечисленного (потяжелее модели) + маленько иного'''\n",
    "'''почти с нуля все'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eaf606f4-60ef-481e-a6ca-556e99937c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41105, 285), (9276, 285))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = Path(\".\")\n",
    "\n",
    "TRAIN_PATH = DATA_DIR / \"train.tsv\"   # или train.csv\n",
    "TEST_PATH  = DATA_DIR / \"test.tsv\"    # или test.csv\n",
    "SEP = \"\\t\"                             # \"\\t\" для tsv, \",\" для csv\n",
    "\n",
    "TARGET_COL = \"target\"            # имя таргета\n",
    "ID_COL = \"id\"                         # имя id-колонки в test (или None, если её нет)\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH, sep=SEP)\n",
    "test  = pd.read_csv(TEST_PATH,  sep=SEP)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c != TARGET_COL and c in test.columns]\n",
    "\n",
    "X_raw = train[feature_cols].copy()\n",
    "y = train[TARGET_COL].values\n",
    "X_test_raw = test[feature_cols].copy()\n",
    "\n",
    "X_raw.shape, X_test_raw.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c16939ac-090a-4762-99ab-1fbe9974bd07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    437899.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 101.85 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting TextSpecialFeatureGenerator...\n",
      "\t\t\tFitting BinnedFeatureGenerator...\n",
      "\t\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\t\tFitting TextNgramFeatureGenerator...\n",
      "\t\t\tFitting CountVectorizer for text features: ['address']\n",
      "\t\t\tCountVectorizer fit with vocabulary size = 792\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])        : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])          :   3 | ['id', 'traffic_300m', 'traffic_1000m']\n",
      "\t\t('object', [])       :   3 | ['name', 'coordinates', 'category']\n",
      "\t\t('object', ['text']) :   1 | ['address']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])                    :   3 | ['name', 'coordinates', 'category']\n",
      "\t\t('category', ['text_as_category'])  :   1 | ['address']\n",
      "\t\t('float', [])                       : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])                         :   3 | ['id', 'traffic_300m', 'traffic_1000m']\n",
      "\t\t('int', ['binned', 'text_special']) :  12 | ['address.char_count', 'address.word_count', 'address.capital_ratio', 'address.digit_ratio', 'address.special_ratio', ...]\n",
      "\t\t('int', ['text_ngram'])             : 619 | ['__nlp__.10', '__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', ...]\n",
      "\t7.0s = Fit runtime\n",
      "\t285 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 137.40 MB (0.0% of available memory)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((41105, 916), (9276, 916))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fg = AutoMLPipelineFeatureGenerator(\n",
    "    enable_numeric_features=True,\n",
    "    enable_categorical_features=True,\n",
    "    enable_datetime_features=True,\n",
    "    enable_text_special_features=True,\n",
    "    enable_text_ngram_features=True,\n",
    "    enable_raw_text_features=False,\n",
    ")\n",
    "\n",
    "X_tr_feat = fg.fit_transform(X=X_raw, y=y)\n",
    "X_te_feat = fg.transform(X_test_raw)\n",
    "\n",
    "X_tr_feat.shape, X_te_feat.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ea82a69-46d6-4927-a360-8a5b3fb434f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41105, 917), (9276, 916))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ag = X_tr_feat.copy()\n",
    "train_ag[TARGET_COL] = y\n",
    "\n",
    "test_ag = X_te_feat.copy()\n",
    "\n",
    "train_ag.shape, test_ag.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cff032f-3bb0-4033-9c40-95d5a2385e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251116_215837\"\n",
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n",
      "CPU Count:          62\n",
      "Memory Avail:       427.95 GB / 503.46 GB (85.0%)\n",
      "Disk Space Avail:   127.07 GB / 130.00 GB (97.7%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_215837\"\n",
      "Train Data Rows:    41105\n",
      "Train Data Columns: 916\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    438182.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 137.40 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 593 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])    : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])      : 634 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])     : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])       :  41 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('int', ['bool']) : 593 | ['__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', '__nlp__.10 стр москва', ...]\n",
      "\t5.5s = Fit runtime\n",
      "\t916 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 114.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.060819851599562096, Train Rows: 38605, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': False, 'ag_args': {'name_suffix': 'GBM'}}, {'extra_trees': True, 'ag_args': {'name_suffix': 'GBM_XT'}}],\n",
      "\t'CAT': [{'iterations': 4000, 'depth': 8, 'learning_rate': 0.01}],\n",
      "\t'XGB': [{}],\n",
      "\t'NN_TORCH': [{}],\n",
      "}\n",
      "Fitting 5 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM ... Training model for up to 1794.36s of the 1794.36s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_XT ... Training model for up to 1793.95s of the 1793.95s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_XT to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost ... Training model for up to 1793.54s of the 1793.54s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=2.5/427.5 GB\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.9789\t = Validation score   (-root_mean_squared_error)\n",
      "\t63.01s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 1730.40s of the 1730.40s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.1/427.5 GB\n",
      "\tWarning: Exception caused XGBoost to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 1729.24s of the 1729.24s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=0.5/427.3 GB\n",
      "\tWarning: Exception caused NeuralNetTorch to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1728.76s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t-0.9789\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 71.45s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 48657.1 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_215837\")\n"
     ]
    }
   ],
   "source": [
    "hyperparams_base = {\n",
    "    \"GBM\": [\n",
    "        {\"extra_trees\": False, \"ag_args\": {\"name_suffix\": \"GBM\"}},\n",
    "        {\"extra_trees\": True,  \"ag_args\": {\"name_suffix\": \"GBM_XT\"}},\n",
    "    ],\n",
    "    \"CAT\": {\n",
    "        \"iterations\": 4000,\n",
    "        \"depth\": 8,\n",
    "        \"learning_rate\": 0.01,\n",
    "    },\n",
    "    \"XGB\": {},\n",
    "    \"NN_TORCH\": {},\n",
    "}\n",
    "\n",
    "predictor = TabularPredictor(\n",
    "    label=TARGET_COL,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"rmse\",\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    time_limit=1800,\n",
    "    presets=\"medium_quality_faster_train\",\n",
    "    hyperparameters=hyperparams_base,\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a93ef89-36b3-4d09-89ee-d45d97d8acf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 3 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_extra ... Training model for up to 1200.00s of the 1200.00s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_extra to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_XT_extra ... Training model for up to 1199.56s of the 1199.56s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_XT_extra to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost_2 ... Training model for up to 1199.11s of the 1199.11s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=2.5/427.6 GB\n",
      "\tTraining CatBoost_2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.9797\t = Validation score   (-root_mean_squared_error)\n",
      "\t30.82s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_2_L2 ... Training model for up to 360.00s of the 1168.20s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_2': 1.0}\n",
      "\t-0.9797\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_215837\")\n"
     ]
    }
   ],
   "source": [
    "hyperparams_extra = {\n",
    "    \"CAT\": {\n",
    "        \"iterations\": 4000,\n",
    "        \"depth\": 8,\n",
    "        \"learning_rate\": 0.02,\n",
    "    },\n",
    "    \"GBM\": [\n",
    "        {\"extra_trees\": False, \"ag_args\": {\"name_suffix\": \"GBM_extra\"}},\n",
    "        {\"extra_trees\": True,  \"ag_args\": {\"name_suffix\": \"GBM_XT_extra\"}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "predictor = predictor.fit_extra(\n",
    "    hyperparameters=hyperparams_extra,\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    "    time_limit=1200,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f1f1677-a8d2-4509-9aef-108f8abdb832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-0.999949</td>\n",
       "      <td>-0.978940</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.148261</td>\n",
       "      <td>0.051061</td>\n",
       "      <td>63.012222</td>\n",
       "      <td>0.148261</td>\n",
       "      <td>0.051061</td>\n",
       "      <td>63.012222</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.999949</td>\n",
       "      <td>-0.978940</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.151422</td>\n",
       "      <td>0.051380</td>\n",
       "      <td>63.014216</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoost_2</td>\n",
       "      <td>-1.003683</td>\n",
       "      <td>-0.979668</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.102016</td>\n",
       "      <td>0.051478</td>\n",
       "      <td>30.824694</td>\n",
       "      <td>0.102016</td>\n",
       "      <td>0.051478</td>\n",
       "      <td>30.824694</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_2_L2</td>\n",
       "      <td>-1.003683</td>\n",
       "      <td>-0.979668</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.104718</td>\n",
       "      <td>0.051771</td>\n",
       "      <td>30.826800</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  score_test  score_val              eval_metric  \\\n",
       "0               CatBoost   -0.999949  -0.978940  root_mean_squared_error   \n",
       "1    WeightedEnsemble_L2   -0.999949  -0.978940  root_mean_squared_error   \n",
       "2             CatBoost_2   -1.003683  -0.979668  root_mean_squared_error   \n",
       "3  WeightedEnsemble_2_L2   -1.003683  -0.979668  root_mean_squared_error   \n",
       "\n",
       "   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n",
       "0        0.148261       0.051061  63.012222                 0.148261   \n",
       "1        0.151422       0.051380  63.014216                 0.003161   \n",
       "2        0.102016       0.051478  30.824694                 0.102016   \n",
       "3        0.104718       0.051771  30.826800                 0.002702   \n",
       "\n",
       "   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       "0                0.051061          63.012222            1       True   \n",
       "1                0.000319           0.001994            2       True   \n",
       "2                0.051478          30.824694            1       True   \n",
       "3                0.000293           0.002107            2       True   \n",
       "\n",
       "   fit_order  \n",
       "0          1  \n",
       "1          2  \n",
       "2          3  \n",
       "3          4  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb = predictor.leaderboard(train_ag, silent=True)\n",
    "lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "221e816c-4198-4ecf-b1b7-9fe596ff0a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 916 features using 5000 rows with 5 shuffle sets...\n",
      "\t1571.19s\t= Expected runtime (314.24s per shuffle set)\n",
      "\t133.35s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0.190031</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>1.067495e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.196789</td>\n",
       "      <td>0.183273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0.095356</td>\n",
       "      <td>0.005808</td>\n",
       "      <td>1.643011e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.107314</td>\n",
       "      <td>0.083398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.011836</td>\n",
       "      <td>0.002606</td>\n",
       "      <td>2.648099e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.017202</td>\n",
       "      <td>0.006469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>0.008139</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>1.486646e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.011320</td>\n",
       "      <td>0.004957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>0.005589</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>5.540305e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008659</td>\n",
       "      <td>0.002519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.москва</th>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>4.270179e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_income_1000m</th>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1.655252e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.троицк</th>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>5.234521e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>-0.000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.зеленоград</th>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>3.444998e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.000069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_income_300m</th>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>4.294521e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnancy_websites_300m</th>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>1.991233e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.000353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children_goods_for_walks_and_travel_300m</th>\n",
       "      <td>0.000597</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>3.240168e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.боровское</th>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>8.354882e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.word_count</th>\n",
       "      <td>0.000501</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>2.359398e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime_1000m</th>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>2.717358e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.000063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>baby_food_1000m</th>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>7.664168e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.symbol_count..</th>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>4.478139e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.арбат</th>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>4.707519e-02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>-0.000328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.балашиха</th>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>6.852329e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>-0.000027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bars_1000m</th>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>8.304146e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>-0.000042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          importance    stddev       p_value  \\\n",
       "category                                    0.190031  0.003282  1.067495e-08   \n",
       "name                                        0.095356  0.005808  1.643011e-06   \n",
       "id                                          0.011836  0.002606  2.648099e-04   \n",
       "coordinates                                 0.008139  0.001545  1.486646e-04   \n",
       "address                                     0.005589  0.001491  5.540305e-04   \n",
       "__nlp__.москва                              0.000860  0.000214  4.270179e-04   \n",
       "mean_income_1000m                           0.000773  0.000084  1.655252e-05   \n",
       "__nlp__.троицк                              0.000737  0.000363  5.234521e-03   \n",
       "__nlp__.зеленоград                          0.000690  0.000302  3.444998e-03   \n",
       "mean_income_300m                            0.000645  0.000161  4.294521e-04   \n",
       "pregnancy_websites_300m                     0.000611  0.000125  1.991233e-04   \n",
       "children_goods_for_walks_and_travel_300m    0.000597  0.000139  3.240168e-04   \n",
       "__nlp__.боровское                           0.000567  0.000169  8.354882e-04   \n",
       "address.word_count                          0.000501  0.000197  2.359398e-03   \n",
       "anime_1000m                                 0.000395  0.000162  2.717358e-03   \n",
       "baby_food_1000m                             0.000354  0.000195  7.664168e-03   \n",
       "address.symbol_count..                      0.000351  0.000165  4.478139e-03   \n",
       "__nlp__.арбат                               0.000297  0.000303  4.707519e-02   \n",
       "__nlp__.балашиха                            0.000279  0.000149  6.852329e-03   \n",
       "bars_1000m                                  0.000261  0.000147  8.304146e-03   \n",
       "\n",
       "                                          n  p99_high   p99_low  \n",
       "category                                  5  0.196789  0.183273  \n",
       "name                                      5  0.107314  0.083398  \n",
       "id                                        5  0.017202  0.006469  \n",
       "coordinates                               5  0.011320  0.004957  \n",
       "address                                   5  0.008659  0.002519  \n",
       "__nlp__.москва                            5  0.001301  0.000418  \n",
       "mean_income_1000m                         5  0.000946  0.000600  \n",
       "__nlp__.троицк                            5  0.001484 -0.000010  \n",
       "__nlp__.зеленоград                        5  0.001311  0.000069  \n",
       "mean_income_300m                          5  0.000977  0.000314  \n",
       "pregnancy_websites_300m                   5  0.000868  0.000353  \n",
       "children_goods_for_walks_and_travel_300m  5  0.000882  0.000312  \n",
       "__nlp__.боровское                         5  0.000914  0.000220  \n",
       "address.word_count                        5  0.000907  0.000096  \n",
       "anime_1000m                               5  0.000728  0.000063  \n",
       "baby_food_1000m                           5  0.000755 -0.000047  \n",
       "address.symbol_count..                    5  0.000692  0.000011  \n",
       "__nlp__.арбат                             5  0.000922 -0.000328  \n",
       "__nlp__.балашиха                          5  0.000585 -0.000027  \n",
       "bars_1000m                                5  0.000564 -0.000042  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi = predictor.feature_importance(train_ag)\n",
    "fi.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfacfb9c-b9fd-48c4-bbfb-1eabf772afe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features = fi[fi[\"importance\"] > 0].index.tolist()\n",
    "len(important_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "04ead45c-3b57-4a1a-963b-d57d9ff83bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((41105, 727), (9276, 726))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ag_imp = train_ag[important_features + [TARGET_COL]].copy()\n",
    "test_ag_imp  = test_ag[important_features].copy()\n",
    "\n",
    "train_ag_imp.shape, test_ag_imp.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "67760612-3089-40ee-b970-953f0aff6507",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251116_220420\"\n",
      "Preset alias specified: 'medium_quality_faster_train' maps to 'medium_quality'.\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n",
      "CPU Count:          62\n",
      "Memory Avail:       423.48 GB / 503.46 GB (84.1%)\n",
      "Disk Space Avail:   126.92 GB / 130.00 GB (97.6%)\n",
      "===================================================\n",
      "Presets specified: ['medium_quality_faster_train']\n",
      "Beginning AutoGluon training ... Time limit = 1200s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_220420\"\n",
      "Train Data Rows:    41105\n",
      "Train Data Columns: 726\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    433659.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 121.33 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 411 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :   4 | ['category', 'name', 'coordinates', 'address']\n",
      "\t\t('float', [])    : 273 | ['mean_income_1000m', 'mean_income_300m', 'pregnancy_websites_300m', 'children_goods_for_walks_and_travel_300m', 'anime_1000m', ...]\n",
      "\t\t('int', [])      : 449 | ['id', '__nlp__.москва', '__nlp__.троицк', '__nlp__.зеленоград', '__nlp__.боровское', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['category', 'name', 'coordinates', 'address']\n",
      "\t\t('float', [])     : 273 | ['mean_income_1000m', 'mean_income_300m', 'pregnancy_websites_300m', 'children_goods_for_walks_and_travel_300m', 'anime_1000m', ...]\n",
      "\t\t('int', [])       :  38 | ['id', '__nlp__.москва', 'address.word_count', 'address.symbol_count..', '__nlp__.мытищи', ...]\n",
      "\t\t('int', ['bool']) : 411 | ['__nlp__.троицк', '__nlp__.зеленоград', '__nlp__.боровское', '__nlp__.арбат', '__nlp__.балашиха', ...]\n",
      "\t4.3s = Fit runtime\n",
      "\t726 features in original data used to generate 726 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 105.22 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.43s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.060819851599562096, Train Rows: 38605, Val Rows: 2500\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'CAT': [{'iterations': 4000, 'depth': 7, 'learning_rate': 0.01}],\n",
      "\t'GBM': [{'extra_trees': False, 'ag_args': {'name_suffix': 'GBM_slim'}}],\n",
      "}\n",
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_slim ... Training model for up to 1195.57s of the 1195.57s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_slim to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoost ... Training model for up to 1195.21s of the 1195.21s of remaining time.\n",
      "\tFitting with cpus=62, gpus=1, mem=1.5/423.3 GB\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.9622\t = Validation score   (-mean_squared_error)\n",
      "\t50.96s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1144.12s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost': 1.0}\n",
      "\t-0.9622\t = Validation score   (-mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.06s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 52511.8 rows/s (2500 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_220420\")\n"
     ]
    }
   ],
   "source": [
    "hyperparams_slim = {\n",
    "    \"CAT\": {\n",
    "        \"iterations\": 4000,\n",
    "        \"depth\": 7,\n",
    "        \"learning_rate\": 0.01,\n",
    "    },\n",
    "    \"GBM\": [\n",
    "        {\"extra_trees\": False, \"ag_args\": {\"name_suffix\": \"GBM_slim\"}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "predictor_slim = TabularPredictor(\n",
    "    label=TARGET_COL,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"mse\",\n",
    ").fit(\n",
    "    train_data=train_ag_imp,\n",
    "    time_limit=1200,\n",
    "    presets=\"medium_quality_faster_train\",\n",
    "    hyperparameters=hyperparams_slim,\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "97d4aea5-244b-4258-b02b-b7c3cce53caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_base = predictor.predict(test_ag)\n",
    "test_pred_slim = predictor_slim.predict(test_ag_imp)\n",
    "\n",
    "test_pred_blend = 0.5 * test_pred_base.values + 0.5 * test_pred_slim.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5142bd8-96c3-4a8d-b0e7-e5b9f7d952c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.402225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.115150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>3.925786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.096871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>2.726351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target\n",
       "0  21472  3.402225\n",
       "1   9837  3.115150\n",
       "2  41791  3.925786\n",
       "3  18441  3.096871\n",
       "4  49348  2.726351"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if ID_COL is not None and ID_COL in test.columns:\n",
    "    sub_base = test[[ID_COL]].copy()\n",
    "    sub_slim = test[[ID_COL]].copy()\n",
    "    sub_blend = test[[ID_COL]].copy()\n",
    "else:\n",
    "    sub_base = pd.DataFrame()\n",
    "    sub_slim = pd.DataFrame()\n",
    "    sub_blend = pd.DataFrame()\n",
    "\n",
    "sub_base[TARGET_COL] = test_pred_base\n",
    "sub_slim[TARGET_COL] = test_pred_slim\n",
    "sub_blend[TARGET_COL] = test_pred_blend\n",
    "\n",
    "sub_base.to_csv(\"submission_ag_base.csv\", index=False)\n",
    "sub_slim.to_csv(\"submission_ag_slim.csv\", index=False)\n",
    "sub_blend.to_csv(\"submission_ag_blend.csv\", index=False)\n",
    "\n",
    "sub_blend.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abd345-cf01-430e-9640-710f5f40781a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''сейчас попробую взять еще толще модели, а то скор сильно не меняется'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3b1cdcba-a25a-48db-9ba8-30d0d9ad965d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_hps = {\n",
    "    \"GBM\": [\n",
    "        {\n",
    "            \"extra_trees\": False,\n",
    "            \"ag_args\": {\"name_suffix\": \"GBM_main\"},\n",
    "            \"num_leaves\": 64,\n",
    "            \"feature_fraction\": 0.9,\n",
    "            \"bagging_fraction\": 0.9,\n",
    "            \"bagging_freq\": 1,\n",
    "        },\n",
    "        {\n",
    "            \"extra_trees\": True,\n",
    "            \"ag_args\": {\"name_suffix\": \"GBM_xt\"},\n",
    "            \"num_leaves\": 128,\n",
    "            \"feature_fraction\": 0.8,\n",
    "            \"bagging_fraction\": 0.8,\n",
    "            \"bagging_freq\": 1,\n",
    "        },\n",
    "    ],\n",
    "    \"CAT\": [\n",
    "        {\n",
    "            \"iterations\": 6000,\n",
    "            \"depth\": 8,\n",
    "            \"learning_rate\": 0.03,\n",
    "            \"l2_leaf_reg\": 3.0,\n",
    "            \"border_count\": 254,\n",
    "            \"random_strength\": 0.5,\n",
    "            \"bagging_temperature\": 0.8,\n",
    "            \"ag_args\": {\"name_suffix\": \"CAT_main\"},\n",
    "        },\n",
    "        {\n",
    "            \"iterations\": 8000,\n",
    "            \"depth\": 10,\n",
    "            \"learning_rate\": 0.02,\n",
    "            \"l2_leaf_reg\": 4.0,\n",
    "            \"border_count\": 254,\n",
    "            \"random_strength\": 0.3,\n",
    "            \"bagging_temperature\": 0.5,\n",
    "            \"ag_args\": {\"name_suffix\": \"CAT_deep\"},\n",
    "        },\n",
    "    ],\n",
    "    \"XGB\": {\n",
    "        \"max_depth\": 8,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.9,\n",
    "        \"n_estimators\": 4000,\n",
    "        \"learning_rate\": 0.03,\n",
    "    },\n",
    "    \"NN_TORCH\": {\n",
    "        \"ag_args\": {\"name_suffix\": \"NN_heavy\"},\n",
    "        \"layers\": \"128-64-64\",\n",
    "        \"dropout_prob\": 0.1,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "845a2649-65d2-4b34-b999-7ff4fd6ca40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20251116_221138\"\n",
      "Verbosity: 2 (Standard Logging)\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.4.0\n",
      "Python Version:     3.12.11\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #52~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Dec  9 15:00:52 UTC 2\n",
      "CPU Count:          62\n",
      "Memory Avail:       427.52 GB / 503.46 GB (84.9%)\n",
      "Disk Space Avail:   126.78 GB / 130.00 GB (97.5%)\n",
      "===================================================\n",
      "Presets specified: ['best_quality']\n",
      "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
      "Stack configuration (auto_stack=True): num_stack_levels=2, num_bag_folds=5, num_bag_sets=2\n",
      "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
      "\tRunning DyStack for up to 1800s of the 7200s of remaining time (25%).\n",
      "/venv/main/lib/python3.12/site-packages/autogluon/tabular/predictor/predictor.py:1444: UserWarning: Failed to use ray for memory safe fits. Falling back to normal fit. Error: ImportError('ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`')\n",
      "  stacked_overfitting = self._sub_fit_memory_save_wrapper(\n",
      "\t\tContext path: \"/workspace/121/AutogluonModels/ag-20251116_221138/ds_sub_fit/sub_fit_ho\"\n",
      "Running DyStack sub-fit ...\n",
      "Beginning AutoGluon training ... Time limit = 1800s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_221138/ds_sub_fit/sub_fit_ho\"\n",
      "Train Data Rows:    36537\n",
      "Train Data Columns: 916\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    437501.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 122.13 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 594 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tUnused Original Features (Count: 26): ['__nlp__.11 стр москва', '__nlp__.12 корп москва', '__nlp__.13 корп москва', '__nlp__.1с1 москва', '__nlp__.20 корп москва', '__nlp__.22 стр москва', '__nlp__.23а', '__nlp__.30 стр', '__nlp__.32 корп москва', '__nlp__.бул корп москва', '__nlp__.городской округ подольск', '__nlp__.калужское шоссе', '__nlp__.киевское шоссе', '__nlp__.комсомольский просп', '__nlp__.кутузовский просп', '__nlp__.ореховый бул', '__nlp__.пр 12', '__nlp__.просп вернадского', '__nlp__.просп мира', '__nlp__.семёновская ул', '__nlp__.стр 12 москва', '__nlp__.территория', '__nlp__.ул академика', '__nlp__.ул александры', '__nlp__.центральная ул', '__nlp__.южная ул']\n",
      "\t\tThese features were not used to generate any of the output features. Add a feature generator compatible with these features to utilize them.\n",
      "\t\tFeatures can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n",
      "\t\tThese features do not need to be present at inference time.\n",
      "\t\t('int', []) : 26 | ['__nlp__.11 стр москва', '__nlp__.12 корп москва', '__nlp__.13 корп москва', '__nlp__.1с1 москва', '__nlp__.20 корп москва', ...]\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])    : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])      : 608 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])     : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])       :  40 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('int', ['bool']) : 568 | ['__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', '__nlp__.10 стр москва', ...]\n",
      "\t4.7s = Fit runtime\n",
      "\t890 features in original data used to generate 890 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 100.53 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 4.89s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': False, 'ag_args': {'name_suffix': 'GBM_main'}, 'num_leaves': 64, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}, {'extra_trees': True, 'ag_args': {'name_suffix': 'GBM_xt'}, 'num_leaves': 128, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}],\n",
      "\t'CAT': [{'iterations': 6000, 'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'border_count': 254, 'random_strength': 0.5, 'bagging_temperature': 0.8, 'ag_args': {'name_suffix': 'CAT_main'}}, {'iterations': 8000, 'depth': 10, 'learning_rate': 0.02, 'l2_leaf_reg': 4.0, 'border_count': 254, 'random_strength': 0.3, 'bagging_temperature': 0.5, 'ag_args': {'name_suffix': 'CAT_deep'}}],\n",
      "\t'XGB': [{'max_depth': 8, 'subsample': 0.9, 'colsample_bytree': 0.9, 'n_estimators': 4000, 'learning_rate': 0.03}],\n",
      "\t'NN_TORCH': [{'ag_args': {'name_suffix': 'NN_heavy'}, 'layers': '128-64-64', 'dropout_prob': 0.1}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Fitting 6 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L1 ... Training model for up to 797.62s of the 1795.10s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L1 ... Training model for up to 797.23s of the 1794.70s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L1 ... Training model for up to 796.82s of the 1794.30s of remaining time.\n",
      "Will use sequential fold fitting strategy because import of ray failed. Reason: ray is required to train folds in parallel for TabularPredictor or HPO for MultiModalPredictor. A quick tip is to install via `pip install \"ray>=2.10.0,<2.45.0\"`\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0314\t = Validation score   (-root_mean_squared_error)\n",
      "\t426.5s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L1 ... Training model for up to 368.77s of the 1366.25s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0358\t = Validation score   (-root_mean_squared_error)\n",
      "\t324.49s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 42.96s of the 1040.43s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L1 ... Training model for up to 41.39s of the 1038.87s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 1037.85s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_main_BAG_L1': 1.0}\n",
      "\t-1.0314\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 6 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L2 ... Training model for up to 691.71s of the 1037.81s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L2 ... Training model for up to 691.26s of the 1037.36s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L2 ... Training model for up to 690.79s of the 1036.89s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0257\t = Validation score   (-root_mean_squared_error)\n",
      "\t183.68s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L2 ... Training model for up to 505.77s of the 851.87s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0247\t = Validation score   (-root_mean_squared_error)\n",
      "\t290.18s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 214.26s of the 560.36s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L2 ... Training model for up to 212.63s of the 558.73s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 557.65s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 1.0}\n",
      "\t-1.0247\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 6 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L3 ... Training model for up to 557.62s of the 557.59s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L3 ... Training model for up to 557.10s of the 557.06s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L3 ... Training model for up to 556.66s of the 556.62s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0257\t = Validation score   (-root_mean_squared_error)\n",
      "\t129.4s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L3 ... Training model for up to 425.86s of the 425.83s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0258\t = Validation score   (-root_mean_squared_error)\n",
      "\t211.42s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 213.15s of the 213.11s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L3 ... Training model for up to 211.62s of the 211.58s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 210.57s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 0.941, 'CatBoostCAT_deep_BAG_L3': 0.059}\n",
      "\t-1.0247\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1589.57s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 2901.9 rows/s (7308 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_221138/ds_sub_fit/sub_fit_ho\")\n",
      "Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
      "Leaderboard on holdout data (DyStack):\n",
      "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L4      -1.020582  -1.024687  root_mean_squared_error        1.347988       2.518853  1436.298435                 0.001827                0.000630           0.018928            4       True          9\n",
      "1  CatBoostCAT_deep_BAG_L2      -1.020604  -1.024688  root_mean_squared_error        0.847170       1.545372  1041.173695                 0.264493                0.447902         290.183823            2       True          5\n",
      "2      WeightedEnsemble_L3      -1.020604  -1.024688  root_mean_squared_error        0.848729       1.545838  1041.182635                 0.001559                0.000465           0.008940            3       True          6\n",
      "3  CatBoostCAT_deep_BAG_L3      -1.020867  -1.025821  root_mean_squared_error        1.346161       2.518223  1436.279507                 0.239977                0.419555         211.420836            3       True          8\n",
      "4  CatBoostCAT_main_BAG_L2      -1.021309  -1.025741  root_mean_squared_error        0.841690       1.650766   934.674849                 0.259014                0.553296         183.684977            2       True          4\n",
      "5  CatBoostCAT_main_BAG_L3      -1.021463  -1.025715  root_mean_squared_error        1.338814       2.723049  1354.260983                 0.232631                0.624382         129.402311            3       True          7\n",
      "6  CatBoostCAT_main_BAG_L1      -1.026909  -1.031446  root_mean_squared_error        0.321155       0.636704   426.495583                 0.321155                0.636704         426.495583            1       True          1\n",
      "7      WeightedEnsemble_L2      -1.026909  -1.031446  root_mean_squared_error        0.323346       0.637138   426.504266                 0.002191                0.000434           0.008683            2       True          3\n",
      "8  CatBoostCAT_deep_BAG_L1      -1.029161  -1.035799  root_mean_squared_error        0.261522       0.460766   324.494289                 0.261522                0.460766         324.494289            1       True          2\n",
      "\t2\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
      "\t1592s\t = DyStack   runtime |\t5608s\t = Remaining runtime\n",
      "Starting main fit with num_stack_levels=2.\n",
      "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=2)`\n",
      "Beginning AutoGluon training ... Time limit = 5608s\n",
      "AutoGluon will save models to \"/workspace/121/AutogluonModels/ag-20251116_221138\"\n",
      "Train Data Rows:    41105\n",
      "Train Data Columns: 916\n",
      "Label Column:       target\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    437969.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 137.40 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 593 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])    : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])      : 634 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])  :   4 | ['name', 'coordinates', 'category', 'address']\n",
      "\t\t('float', [])     : 278 | ['homes_300m', 'works_300m', 'female_300m', 'train_ticket_order_300m', 'mortgage_300m', ...]\n",
      "\t\t('int', [])       :  41 | ['id', 'traffic_300m', 'traffic_1000m', 'address.char_count', 'address.word_count', ...]\n",
      "\t\t('int', ['bool']) : 593 | ['__nlp__.10 корп', '__nlp__.10 корп москва', '__nlp__.10 москва', '__nlp__.10 стр', '__nlp__.10 стр москва', ...]\n",
      "\t5.3s = Fit runtime\n",
      "\t916 features in original data used to generate 916 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 114.16 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 5.46s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'GBM': [{'extra_trees': False, 'ag_args': {'name_suffix': 'GBM_main'}, 'num_leaves': 64, 'feature_fraction': 0.9, 'bagging_fraction': 0.9, 'bagging_freq': 1}, {'extra_trees': True, 'ag_args': {'name_suffix': 'GBM_xt'}, 'num_leaves': 128, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1}],\n",
      "\t'CAT': [{'iterations': 6000, 'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 3.0, 'border_count': 254, 'random_strength': 0.5, 'bagging_temperature': 0.8, 'ag_args': {'name_suffix': 'CAT_main'}}, {'iterations': 8000, 'depth': 10, 'learning_rate': 0.02, 'l2_leaf_reg': 4.0, 'border_count': 254, 'random_strength': 0.3, 'bagging_temperature': 0.5, 'ag_args': {'name_suffix': 'CAT_deep'}}],\n",
      "\t'XGB': [{'max_depth': 8, 'subsample': 0.9, 'colsample_bytree': 0.9, 'n_estimators': 4000, 'learning_rate': 0.03}],\n",
      "\t'NN_TORCH': [{'ag_args': {'name_suffix': 'NN_heavy'}, 'layers': '128-64-64', 'dropout_prob': 0.1}],\n",
      "}\n",
      "AutoGluon will fit 3 stack levels (L1 to L3) ...\n",
      "Fitting 6 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L1 ... Training model for up to 2489.58s of the 5602.95s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L1 ... Training model for up to 2489.11s of the 5602.49s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L1 ... Training model for up to 2488.64s of the 5602.01s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0259\t = Validation score   (-root_mean_squared_error)\n",
      "\t538.83s\t = Training   runtime\n",
      "\t0.55s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L1 ... Training model for up to 1948.19s of the 5061.56s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0222\t = Validation score   (-root_mean_squared_error)\n",
      "\t1653.11s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 292.80s of the 3406.18s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L1 ... Training model for up to 291.20s of the 3404.57s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 3403.52s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L1': 1.0}\n",
      "\t-1.0222\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 6 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L2 ... Training model for up to 2268.43s of the 3403.48s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L2 ... Training model for up to 2267.99s of the 3403.04s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L2 ... Training model for up to 2267.56s of the 3402.61s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0195\t = Validation score   (-root_mean_squared_error)\n",
      "\t175.95s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L2 ... Training model for up to 2090.07s of the 3225.12s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0193\t = Validation score   (-root_mean_squared_error)\n",
      "\t518.58s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1569.84s of the 2704.89s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L2 ... Training model for up to 1568.24s of the 2703.29s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the 2702.25s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 0.9, 'CatBoostCAT_main_BAG_L2': 0.1}\n",
      "\t-1.0193\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 6 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_main_BAG_L3 ... Training model for up to 2702.21s of the 2702.19s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_main_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: LightGBMGBM_xt_BAG_L3 ... Training model for up to 2701.79s of the 2701.77s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_xt_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_main_BAG_L3 ... Training model for up to 2701.38s of the 2701.37s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0204\t = Validation score   (-root_mean_squared_error)\n",
      "\t154.95s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Fitting model: CatBoostCAT_deep_BAG_L3 ... Training model for up to 2544.97s of the 2544.95s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0205\t = Validation score   (-root_mean_squared_error)\n",
      "\t461.0s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L3 ... Training model for up to 2082.38s of the 2082.36s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused XGBoost_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import xgboost` failed. A quick tip is to install via `pip install autogluon.tabular[xgboost]==1.4.0`.\n",
      "Fitting model: NeuralNetTorchNN_heavy_BAG_L3 ... Training model for up to 2080.79s of the 2080.77s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tWarning: Exception caused NeuralNetTorchNN_heavy_BAG_L3 to fail during training (ImportError)... Skipping this model.\n",
      "\t\tUnable to import dependency torch\n",
      "A quick tip is to install via `pip install torch`.\n",
      "The minimum torch version is currently 2.2.\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 360.00s of the 2079.70s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 0.909, 'CatBoostCAT_deep_BAG_L1': 0.091}\n",
      "\t-1.0193\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3528.86s ... Best model: WeightedEnsemble_L4 | Estimated inference throughput: 3871.3 rows/s (8221 batch size)\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_221138\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "heavy_time_limit = 7200  # 2 часа, под задачу можно меньше/больше\n",
    "\n",
    "predictor_heavy = TabularPredictor(\n",
    "    label=TARGET_COL,\n",
    "    problem_type=\"regression\",\n",
    "    eval_metric=\"mse\",\n",
    ").fit(\n",
    "    train_data=train_ag,\n",
    "    time_limit=heavy_time_limit,\n",
    "    presets=\"best_quality\",          # heavy режим\n",
    "    num_bag_folds=5,                 # бэггинг по фолдам\n",
    "    num_bag_sets=2,                  # повторение бэггинга\n",
    "    num_stack_levels=2,              # двухуровневый stacking\n",
    "    hyperparameters=heavy_hps,\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3b5d707e-e031-4347-8aa1-69336fe77ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting 2 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: LightGBMGBM_extra_BAG_L1 ... Training model for up to 1800.00s of the 1800.00s of remaining time.\n",
      "\tWarning: Exception caused LightGBMGBM_extra_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
      "\t\t`import lightgbm` failed. A quick tip is to install via `pip install autogluon.tabular[lightgbm]==1.4.0`.\n",
      "Fitting model: CatBoostCAT_extra_BAG_L1 ... Training model for up to 1799.58s of the 1799.58s of remaining time.\n",
      "\tFitting 10 child models (S1F1 - S2F5) | Fitting with SequentialLocalFoldFittingStrategy (sequential: cpus=62, gpus=1)\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S1F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F2 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F3 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F4 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\tTraining S2F5 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-1.0326\t = Validation score   (-root_mean_squared_error)\n",
      "\t867.98s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_2_L2 ... Training model for up to 360.00s of the 929.56s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoostCAT_extra_BAG_L1': 1.0}\n",
      "\t-1.0326\t = Validation score   (-root_mean_squared_error)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/workspace/121/AutogluonModels/ag-20251116_221138\")\n"
     ]
    }
   ],
   "source": [
    "extra_hps_heavy = {\n",
    "    \"CAT\": {\n",
    "        \"iterations\": 8000,\n",
    "        \"depth\": 8,\n",
    "        \"learning_rate\": 0.008,\n",
    "        \"l2_leaf_reg\": 3.0,\n",
    "        \"ag_args\": {\"name_suffix\": \"CAT_extra\"},\n",
    "    },\n",
    "    \"GBM\": [\n",
    "        {\n",
    "            \"extra_trees\": False,\n",
    "            \"ag_args\": {\"name_suffix\": \"GBM_extra\"},\n",
    "            \"num_leaves\": 128,\n",
    "            \"feature_fraction\": 0.85,\n",
    "            \"bagging_fraction\": 0.85,\n",
    "            \"bagging_freq\": 1,\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "predictor_heavy = predictor_heavy.fit_extra(\n",
    "    hyperparameters=extra_hps_heavy,\n",
    "    ag_args_fit={\"num_gpus\": 1},\n",
    "    time_limit=1800,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "190eb0b3-e770-4803-a3a6-24bb3eb39cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n",
      "\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n",
      "\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n",
      "\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_main_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t47.04s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_deep_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t155.07s\t = Training   runtime\n",
      "Fitting 1 L1 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_extra_BAG_L1_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t83.0s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L1': 1.0}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_main_BAG_L2_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t4.93s\t = Training   runtime\n",
      "Fitting 1 L2 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_deep_BAG_L2_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t9.55s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_2_L2_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoostCAT_extra_BAG_L1': 1.0}\n",
      "\t0.0s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L3_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 0.9, 'CatBoostCAT_main_BAG_L2': 0.1}\n",
      "\t0.01s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_main_BAG_L3_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t4.49s\t = Training   runtime\n",
      "Fitting 1 L3 models, fit_strategy=\"sequential\" ...\n",
      "Fitting model: CatBoostCAT_deep_BAG_L3_FULL ...\n",
      "\tTraining S1F1 with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t8.46s\t = Training   runtime\n",
      "Fitting model: WeightedEnsemble_L4_FULL | Skipping fit via cloning parent ...\n",
      "\tEnsemble Weights: {'CatBoostCAT_deep_BAG_L2': 0.909, 'CatBoostCAT_deep_BAG_L1': 0.091}\n",
      "\t0.02s\t = Training   runtime\n",
      "Updated best model to \"WeightedEnsemble_L4_FULL\" (Previously \"WeightedEnsemble_L4\"). AutoGluon will default to using \"WeightedEnsemble_L4_FULL\" for predict() and predict_proba().\n",
      "Refit complete, total runtime = 315.99s ... Best model: \"WeightedEnsemble_L4_FULL\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CatBoostCAT_main_BAG_L1': 'CatBoostCAT_main_BAG_L1_FULL',\n",
       " 'CatBoostCAT_deep_BAG_L1': 'CatBoostCAT_deep_BAG_L1_FULL',\n",
       " 'CatBoostCAT_extra_BAG_L1': 'CatBoostCAT_extra_BAG_L1_FULL',\n",
       " 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL',\n",
       " 'CatBoostCAT_main_BAG_L2': 'CatBoostCAT_main_BAG_L2_FULL',\n",
       " 'CatBoostCAT_deep_BAG_L2': 'CatBoostCAT_deep_BAG_L2_FULL',\n",
       " 'WeightedEnsemble_2_L2': 'WeightedEnsemble_2_L2_FULL',\n",
       " 'WeightedEnsemble_L3': 'WeightedEnsemble_L3_FULL',\n",
       " 'CatBoostCAT_main_BAG_L3': 'CatBoostCAT_main_BAG_L3_FULL',\n",
       " 'CatBoostCAT_deep_BAG_L3': 'CatBoostCAT_deep_BAG_L3_FULL',\n",
       " 'WeightedEnsemble_L4': 'WeightedEnsemble_L4_FULL'}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refit_models = predictor_heavy.refit_full()  # вернёт список имён моделей\n",
    "refit_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aeef6e40-a33f-4137-aedc-d3bda5b91e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_test</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_test</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_test_marginal</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L3_FULL</td>\n",
       "      <td>-0.916279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.599813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>225.043656</td>\n",
       "      <td>0.099124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.463934</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CatBoostCAT_main_BAG_L3_FULL</td>\n",
       "      <td>-0.923221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.584995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221.072683</td>\n",
       "      <td>0.084306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.492962</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L2_FULL</td>\n",
       "      <td>-0.923662</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.411435</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.649469</td>\n",
       "      <td>0.100992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.546101</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WeightedEnsemble_L3_FULL</td>\n",
       "      <td>-0.924691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.501887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>216.589046</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WeightedEnsemble_L4_FULL</td>\n",
       "      <td>-0.925978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.413476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.667761</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018292</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CatBoostCAT_main_BAG_L2_FULL</td>\n",
       "      <td>-0.935022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.399697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>207.033620</td>\n",
       "      <td>0.089254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.930252</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L3</td>\n",
       "      <td>-0.943104</td>\n",
       "      <td>-1.020547</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.916770</td>\n",
       "      <td>3.570393</td>\n",
       "      <td>3347.472876</td>\n",
       "      <td>0.467275</td>\n",
       "      <td>0.723779</td>\n",
       "      <td>460.999281</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CatBoostCAT_main_BAG_L3</td>\n",
       "      <td>-0.944375</td>\n",
       "      <td>-1.020356</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.881808</td>\n",
       "      <td>3.521692</td>\n",
       "      <td>3041.426079</td>\n",
       "      <td>0.432313</td>\n",
       "      <td>0.675078</td>\n",
       "      <td>154.952484</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L2</td>\n",
       "      <td>-0.946427</td>\n",
       "      <td>-1.019281</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.014383</td>\n",
       "      <td>2.123481</td>\n",
       "      <td>2710.522289</td>\n",
       "      <td>0.443499</td>\n",
       "      <td>0.746373</td>\n",
       "      <td>518.578130</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>-0.946615</td>\n",
       "      <td>-1.019280</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.451519</td>\n",
       "      <td>2.847040</td>\n",
       "      <td>2886.482920</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>WeightedEnsemble_L4</td>\n",
       "      <td>-0.947829</td>\n",
       "      <td>-1.019257</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.016348</td>\n",
       "      <td>2.123899</td>\n",
       "      <td>2710.540581</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.018292</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CatBoostCAT_main_BAG_L2</td>\n",
       "      <td>-0.948376</td>\n",
       "      <td>-1.019471</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>2.005996</td>\n",
       "      <td>2.100241</td>\n",
       "      <td>2367.895465</td>\n",
       "      <td>0.435112</td>\n",
       "      <td>0.723133</td>\n",
       "      <td>175.951306</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L1_FULL</td>\n",
       "      <td>-0.954988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.110367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.067148</td>\n",
       "      <td>0.110367</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.067148</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WeightedEnsemble_L2_FULL</td>\n",
       "      <td>-0.954988</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.112044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>155.076736</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>CatBoostCAT_main_BAG_L1_FULL</td>\n",
       "      <td>-0.964836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.200077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.036220</td>\n",
       "      <td>0.200077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47.036220</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>CatBoostCAT_deep_BAG_L1</td>\n",
       "      <td>-0.965217</td>\n",
       "      <td>-1.022203</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.798954</td>\n",
       "      <td>0.827065</td>\n",
       "      <td>1653.112687</td>\n",
       "      <td>0.798954</td>\n",
       "      <td>0.827065</td>\n",
       "      <td>1653.112687</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>-0.965217</td>\n",
       "      <td>-1.022203</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.800689</td>\n",
       "      <td>0.827510</td>\n",
       "      <td>1653.122275</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>CatBoostCAT_main_BAG_L1</td>\n",
       "      <td>-0.973938</td>\n",
       "      <td>-1.025913</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.550043</td>\n",
       "      <td>538.831471</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.550043</td>\n",
       "      <td>538.831471</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>CatBoostCAT_extra_BAG_L1_FULL</td>\n",
       "      <td>-0.989324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.130311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.004028</td>\n",
       "      <td>0.130311</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.004028</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WeightedEnsemble_2_L2_FULL</td>\n",
       "      <td>-0.989324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.006668</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>CatBoostCAT_extra_BAG_L1</td>\n",
       "      <td>-1.000057</td>\n",
       "      <td>-1.032590</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.115262</td>\n",
       "      <td>0.632822</td>\n",
       "      <td>867.975810</td>\n",
       "      <td>1.115262</td>\n",
       "      <td>0.632822</td>\n",
       "      <td>867.975810</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>WeightedEnsemble_2_L2</td>\n",
       "      <td>-1.000057</td>\n",
       "      <td>-1.032590</td>\n",
       "      <td>root_mean_squared_error</td>\n",
       "      <td>1.117133</td>\n",
       "      <td>0.633230</td>\n",
       "      <td>867.978451</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            model  score_test  score_val  \\\n",
       "0    CatBoostCAT_deep_BAG_L3_FULL   -0.916279        NaN   \n",
       "1    CatBoostCAT_main_BAG_L3_FULL   -0.923221        NaN   \n",
       "2    CatBoostCAT_deep_BAG_L2_FULL   -0.923662        NaN   \n",
       "3        WeightedEnsemble_L3_FULL   -0.924691        NaN   \n",
       "4        WeightedEnsemble_L4_FULL   -0.925978        NaN   \n",
       "5    CatBoostCAT_main_BAG_L2_FULL   -0.935022        NaN   \n",
       "6         CatBoostCAT_deep_BAG_L3   -0.943104  -1.020547   \n",
       "7         CatBoostCAT_main_BAG_L3   -0.944375  -1.020356   \n",
       "8         CatBoostCAT_deep_BAG_L2   -0.946427  -1.019281   \n",
       "9             WeightedEnsemble_L3   -0.946615  -1.019280   \n",
       "10            WeightedEnsemble_L4   -0.947829  -1.019257   \n",
       "11        CatBoostCAT_main_BAG_L2   -0.948376  -1.019471   \n",
       "12   CatBoostCAT_deep_BAG_L1_FULL   -0.954988        NaN   \n",
       "13       WeightedEnsemble_L2_FULL   -0.954988        NaN   \n",
       "14   CatBoostCAT_main_BAG_L1_FULL   -0.964836        NaN   \n",
       "15        CatBoostCAT_deep_BAG_L1   -0.965217  -1.022203   \n",
       "16            WeightedEnsemble_L2   -0.965217  -1.022203   \n",
       "17        CatBoostCAT_main_BAG_L1   -0.973938  -1.025913   \n",
       "18  CatBoostCAT_extra_BAG_L1_FULL   -0.989324        NaN   \n",
       "19     WeightedEnsemble_2_L2_FULL   -0.989324        NaN   \n",
       "20       CatBoostCAT_extra_BAG_L1   -1.000057  -1.032590   \n",
       "21          WeightedEnsemble_2_L2   -1.000057  -1.032590   \n",
       "\n",
       "                eval_metric  pred_time_test  pred_time_val     fit_time  \\\n",
       "0   root_mean_squared_error        0.599813            NaN   225.043656   \n",
       "1   root_mean_squared_error        0.584995            NaN   221.072683   \n",
       "2   root_mean_squared_error        0.411435            NaN   211.649469   \n",
       "3   root_mean_squared_error        0.501887            NaN   216.589046   \n",
       "4   root_mean_squared_error        0.413476            NaN   211.667761   \n",
       "5   root_mean_squared_error        0.399697            NaN   207.033620   \n",
       "6   root_mean_squared_error        2.916770       3.570393  3347.472876   \n",
       "7   root_mean_squared_error        2.881808       3.521692  3041.426079   \n",
       "8   root_mean_squared_error        2.014383       2.123481  2710.522289   \n",
       "9   root_mean_squared_error        2.451519       2.847040  2886.482920   \n",
       "10  root_mean_squared_error        2.016348       2.123899  2710.540581   \n",
       "11  root_mean_squared_error        2.005996       2.100241  2367.895465   \n",
       "12  root_mean_squared_error        0.110367            NaN   155.067148   \n",
       "13  root_mean_squared_error        0.112044            NaN   155.076736   \n",
       "14  root_mean_squared_error        0.200077            NaN    47.036220   \n",
       "15  root_mean_squared_error        0.798954       0.827065  1653.112687   \n",
       "16  root_mean_squared_error        0.800689       0.827510  1653.122275   \n",
       "17  root_mean_squared_error        0.771930       0.550043   538.831471   \n",
       "18  root_mean_squared_error        0.130311            NaN    83.004028   \n",
       "19  root_mean_squared_error        0.131992            NaN    83.006668   \n",
       "20  root_mean_squared_error        1.115262       0.632822   867.975810   \n",
       "21  root_mean_squared_error        1.117133       0.633230   867.978451   \n",
       "\n",
       "    pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  \\\n",
       "0                  0.099124                     NaN           8.463934   \n",
       "1                  0.084306                     NaN           4.492962   \n",
       "2                  0.100992                     NaN           9.546101   \n",
       "3                  0.001198                     NaN           0.009325   \n",
       "4                  0.002040                     NaN           0.018292   \n",
       "5                  0.089254                     NaN           4.930252   \n",
       "6                  0.467275                0.723779         460.999281   \n",
       "7                  0.432313                0.675078         154.952484   \n",
       "8                  0.443499                0.746373         518.578130   \n",
       "9                  0.002024                0.000426           0.009325   \n",
       "10                 0.001966                0.000418           0.018292   \n",
       "11                 0.435112                0.723133         175.951306   \n",
       "12                 0.110367                     NaN         155.067148   \n",
       "13                 0.001678                     NaN           0.009588   \n",
       "14                 0.200077                     NaN          47.036220   \n",
       "15                 0.798954                0.827065        1653.112687   \n",
       "16                 0.001735                0.000446           0.009588   \n",
       "17                 0.771930                0.550043         538.831471   \n",
       "18                 0.130311                     NaN          83.004028   \n",
       "19                 0.001681                     NaN           0.002641   \n",
       "20                 1.115262                0.632822         867.975810   \n",
       "21                 0.001871                0.000409           0.002641   \n",
       "\n",
       "    stack_level  can_infer  fit_order  \n",
       "0             3       True         21  \n",
       "1             3       True         20  \n",
       "2             2       True         17  \n",
       "3             3       True         19  \n",
       "4             4       True         22  \n",
       "5             2       True         16  \n",
       "6             3       True          8  \n",
       "7             3       True          7  \n",
       "8             2       True          5  \n",
       "9             3       True          6  \n",
       "10            4       True          9  \n",
       "11            2       True          4  \n",
       "12            1       True         13  \n",
       "13            2       True         15  \n",
       "14            1       True         12  \n",
       "15            1       True          2  \n",
       "16            2       True          3  \n",
       "17            1       True          1  \n",
       "18            1       True         14  \n",
       "19            2       True         18  \n",
       "20            1       True         10  \n",
       "21            2       True         11  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb_heavy = predictor_heavy.leaderboard(train_ag, silent=True)\n",
    "lb_heavy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "627f0944-bc16-4f5e-a97e-fe176e7c869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing feature importance via permutation shuffling for 916 features using 5000 rows with 5 shuffle sets...\n",
      "\t1222.19s\t= Expected runtime (244.44s per shuffle set)\n",
      "\t160.98s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "      <th>stddev</th>\n",
       "      <th>p_value</th>\n",
       "      <th>n</th>\n",
       "      <th>p99_high</th>\n",
       "      <th>p99_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>0.256087</td>\n",
       "      <td>0.006250</td>\n",
       "      <td>4.254124e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.268956</td>\n",
       "      <td>0.243218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>0.143222</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>2.756245e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.163670</td>\n",
       "      <td>0.122774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0.026039</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>6.249724e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.034201</td>\n",
       "      <td>0.017877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coordinates</th>\n",
       "      <td>0.009665</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>9.073447e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.012996</td>\n",
       "      <td>0.006335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>0.007237</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>8.226482e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.009669</td>\n",
       "      <td>0.004805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_income_1000m</th>\n",
       "      <td>0.004080</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>1.859026e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>0.003139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_income_300m</th>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>7.712692e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.005409</td>\n",
       "      <td>0.002721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__.москва</th>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>4.327484e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.001455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.symbol_count..</th>\n",
       "      <td>0.002978</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>5.027841e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.001383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homes_300m</th>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>1.432752e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.001775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.digit_ratio</th>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>5.269784e-08</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.002709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>children_goods_for_walks_and_travel_300m</th>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>2.400257e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.001528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.symbol_ratio.</th>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.000130</td>\n",
       "      <td>1.058161e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.002113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>garden_supplies_300m</th>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>2.900566e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.001226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pregnancy_websites_300m</th>\n",
       "      <td>0.002091</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>1.558369e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002919</td>\n",
       "      <td>0.001264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>childrens_sports_300m</th>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>4.294483e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.000941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>manicure_300m</th>\n",
       "      <td>0.001917</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>4.090548e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002890</td>\n",
       "      <td>0.000944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age_&gt;55_300m</th>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>7.102597e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>0.001078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>childrens_websites_1000m</th>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>6.376947e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.001240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.word_count</th>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>1.464618e-03</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>0.000432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>__nlp__._total_</th>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>1.256272e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>homes_1000m</th>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>6.389504e-06</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.001139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address.special_ratio</th>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>6.616075e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.000943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>childrens_sports_1000m</th>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>3.895437e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mortgage_300m</th>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>5.363154e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002034</td>\n",
       "      <td>0.000600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laser_hair_removal_300m</th>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>9.345924e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.000486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>works_300m</th>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>5.504926e-05</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>computer_games_300m</th>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>1.100289e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.000790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anime_1000m</th>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>1.187991e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>0.000780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>childrens_transport_300m</th>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>4.111978e-04</td>\n",
       "      <td>5</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          importance    stddev       p_value  \\\n",
       "category                                    0.256087  0.006250  4.254124e-08   \n",
       "name                                        0.143222  0.009931  2.756245e-06   \n",
       "id                                          0.026039  0.003964  6.249724e-05   \n",
       "coordinates                                 0.009665  0.001618  9.073447e-05   \n",
       "address                                     0.007237  0.001181  8.226482e-05   \n",
       "mean_income_1000m                           0.004080  0.000457  1.859026e-05   \n",
       "mean_income_300m                            0.004065  0.000653  7.712692e-05   \n",
       "__nlp__.москва                              0.003000  0.000750  4.327484e-04   \n",
       "address.symbol_count..                      0.002978  0.000774  5.027841e-04   \n",
       "homes_300m                                  0.002896  0.000545  1.432752e-04   \n",
       "address.digit_ratio                         0.002861  0.000074  5.269784e-08   \n",
       "children_goods_for_walks_and_travel_300m    0.002739  0.000588  2.400257e-04   \n",
       "address.symbol_ratio.                       0.002380  0.000130  1.058161e-06   \n",
       "garden_supplies_300m                        0.002287  0.000516  2.900566e-04   \n",
       "pregnancy_websites_300m                     0.002091  0.000402  1.558369e-04   \n",
       "childrens_sports_300m                       0.001936  0.000483  4.294483e-04   \n",
       "manicure_300m                               0.001917  0.000472  4.090548e-04   \n",
       "age_>55_300m                                0.001594  0.000251  7.102597e-05   \n",
       "childrens_websites_1000m                    0.001506  0.000129  6.376947e-06   \n",
       "address.word_count                          0.001493  0.000516  1.464618e-03   \n",
       "__nlp__._total_                             0.001414  0.000144  1.256272e-05   \n",
       "homes_1000m                                 0.001383  0.000118  6.389504e-06   \n",
       "address.special_ratio                       0.001382  0.000213  6.616075e-05   \n",
       "childrens_sports_1000m                      0.001370  0.000185  3.895437e-05   \n",
       "mortgage_300m                               0.001317  0.000348  5.363154e-04   \n",
       "laser_hair_removal_300m                     0.001315  0.000403  9.345924e-04   \n",
       "works_300m                                  0.001301  0.000192  5.504926e-05   \n",
       "computer_games_300m                         0.001238  0.000218  1.100289e-04   \n",
       "anime_1000m                                 0.001236  0.000222  1.187991e-04   \n",
       "childrens_transport_300m                    0.001222  0.000302  4.111978e-04   \n",
       "\n",
       "                                          n  p99_high   p99_low  \n",
       "category                                  5  0.268956  0.243218  \n",
       "name                                      5  0.163670  0.122774  \n",
       "id                                        5  0.034201  0.017877  \n",
       "coordinates                               5  0.012996  0.006335  \n",
       "address                                   5  0.009669  0.004805  \n",
       "mean_income_1000m                         5  0.005022  0.003139  \n",
       "mean_income_300m                          5  0.005409  0.002721  \n",
       "__nlp__.москва                            5  0.004545  0.001455  \n",
       "address.symbol_count..                    5  0.004572  0.001383  \n",
       "homes_300m                                5  0.004017  0.001775  \n",
       "address.digit_ratio                       5  0.003012  0.002709  \n",
       "children_goods_for_walks_and_travel_300m  5  0.003950  0.001528  \n",
       "address.symbol_ratio.                     5  0.002648  0.002113  \n",
       "garden_supplies_300m                      5  0.003349  0.001226  \n",
       "pregnancy_websites_300m                   5  0.002919  0.001264  \n",
       "childrens_sports_300m                     5  0.002931  0.000941  \n",
       "manicure_300m                             5  0.002890  0.000944  \n",
       "age_>55_300m                              5  0.002110  0.001078  \n",
       "childrens_websites_1000m                  5  0.001771  0.001240  \n",
       "address.word_count                        5  0.002555  0.000432  \n",
       "__nlp__._total_                           5  0.001709  0.001118  \n",
       "homes_1000m                               5  0.001627  0.001139  \n",
       "address.special_ratio                     5  0.001821  0.000943  \n",
       "childrens_sports_1000m                    5  0.001751  0.000989  \n",
       "mortgage_300m                             5  0.002034  0.000600  \n",
       "laser_hair_removal_300m                   5  0.002144  0.000486  \n",
       "works_300m                                5  0.001695  0.000906  \n",
       "computer_games_300m                       5  0.001686  0.000790  \n",
       "anime_1000m                               5  0.001693  0.000780  \n",
       "childrens_transport_300m                  5  0.001843  0.000601  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_heavy = predictor_heavy.feature_importance(train_ag)\n",
    "fi_heavy.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f32734a-c150-48bb-b2b7-ceb0a02bf92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3.433025\n",
       "1    3.425081\n",
       "2    4.002472\n",
       "3    3.274601\n",
       "4    2.883332\n",
       "Name: target, dtype: float32"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred_heavy = predictor_heavy.predict(test_ag)\n",
    "test_pred_heavy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b22877-b11f-4c2e-b458-407fbae3a6b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d319e347-1b29-48e0-8963-4169e79695fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21472</td>\n",
       "      <td>3.433025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9837</td>\n",
       "      <td>3.425081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41791</td>\n",
       "      <td>4.002472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18441</td>\n",
       "      <td>3.274601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49348</td>\n",
       "      <td>2.883332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target\n",
       "0  21472  3.433025\n",
       "1   9837  3.425081\n",
       "2  41791  4.002472\n",
       "3  18441  3.274601\n",
       "4  49348  2.883332"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Предсказания medium (если predictor ещё есть)\n",
    "try:\n",
    "    test_pred_medium = predictor.predict(test_ag)\n",
    "except NameError:\n",
    "    test_pred_medium = None\n",
    "\n",
    "if ID_COL is not None and ID_COL in test.columns:\n",
    "    sub_heavy = test[[ID_COL]].copy()\n",
    "    sub_blend = test[[ID_COL]].copy()\n",
    "else:\n",
    "    sub_heavy = pd.DataFrame()\n",
    "    sub_blend = pd.DataFrame()\n",
    "\n",
    "sub_heavy[TARGET_COL] = test_pred_heavy\n",
    "\n",
    "if test_pred_medium is not None:\n",
    "    test_pred_blend = 0.5 * test_pred_heavy.values + 0.5 * test_pred_medium.values\n",
    "    sub_blend[TARGET_COL] = test_pred_blend\n",
    "else:\n",
    "    sub_blend[TARGET_COL] = test_pred_heavy.values\n",
    "\n",
    "sub_heavy.to_csv(\"submission_ag_heavy.csv\", index=False)\n",
    "sub_blend.to_csv(\"submission_ag_heavy_blend.csv\", index=False)\n",
    "\n",
    "sub_heavy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fc05a3-f9cd-4890-a598-c716421bcf80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (main venv)",
   "language": "python",
   "name": "main"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
